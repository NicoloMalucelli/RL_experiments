{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Keep average distance\n",
    "\n",
    "the agents goal is to position close to each others at a distance previously defined\n",
    "\n",
    "challenges:\n",
    "- deal with continuous space environment\n",
    "- limited vision of an agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Set\n",
    "from ray.rllib.env.multi_agent_env import MultiAgentEnv\n",
    "import random as rnd\n",
    "from gymnasium.spaces import Discrete, Box, Dict, Tuple\n",
    "from gymnasium.spaces.utils import flatten, flatten_space\n",
    "import numpy as np\n",
    "from IPython.display import clear_output\n",
    "import math\n",
    "from ipycanvas import Canvas, hold_canvas\n",
    "\n",
    "\n",
    "class EnvironmentConfiguration: \n",
    "    def __init__(self, n_agents, target_distance, speed, max_steps=None):\n",
    "        self.n_agents = n_agents\n",
    "        self.target_distance = target_distance\n",
    "        self.max_steps = max_steps\n",
    "        self.speed = speed\n",
    "\n",
    "class KeepTheDistance(MultiAgentEnv):\n",
    "\n",
    "    canvas = None\n",
    "\n",
    "    def __init__(self, config: EnvironmentConfiguration):\n",
    "        assert config.n_agents == 2 # just base case implemented \n",
    "             \n",
    "        self.n_agents = config.n_agents\n",
    "        self.target_distance = config.target_distance\n",
    "        self.max_steps = config.max_steps\n",
    "        self.speed = config.speed\n",
    "        \n",
    "        self.agents_ids = ['agent-' + str(i) for i in range(self.n_agents)]\n",
    "        self.agent_colors = {agent: self.rgb_to_hex(rnd.randint(0, 255), rnd.randint(0, 255), rnd.randint(0, 255)) for agent in self.agents_ids}\n",
    "        self.observation_space = self.observation_space('agent-0')\n",
    "        self.action_space = self.action_space(\"\")\n",
    "\n",
    "    def unflatten_observation_space(self, agent):\n",
    "        distance_vector = Box(low=-np.inf, high=np.inf, shape=(2,1), dtype=np.float32)\n",
    "        obs_space = Dict({\"nbr-1\": distance_vector})\n",
    "        return obs_space\n",
    "\n",
    "    def observation_space(self, agent):\n",
    "        return flatten_space(self.unflatten_observation_space(agent))\n",
    "\n",
    "    def action_space(self, agent):\n",
    "        direction = Box(low=-1.0, high=1.0, shape=(2,1), dtype=np.float32)\n",
    "        speed = Box(0.0, 1.0, dtype=np.float32)\n",
    "        return flatten_space(Tuple([direction, speed]))\n",
    "    \n",
    "    def __get_random_point(self, max_x, max_y, min_x=0, min_y=0):\n",
    "        return (rnd.randint(min_x, max_x-1), rnd.randint(min_y, max_y-1))\n",
    "    \n",
    "    def __get_observation(self, agent):\n",
    "        nbr = self.__get_other_agents(agent)\n",
    "        obs = {\"nbr-1\": self.__compute_distance_vector(agent, nbr[0])}\n",
    "        return flatten(self.unflatten_observation_space(agent), obs)\n",
    "\n",
    "    def rgb_to_hex(self, r, g, b):\n",
    "        return f'#{r:02x}{g:02x}{b:02x}'\n",
    "\n",
    "    def __compute_distance_vector(self, agent1, agent2):\n",
    "        agent1_pos = self.agents_pos[agent1]\n",
    "        agent2_pos = self.agents_pos[agent2]\n",
    "        return (agent1_pos[0]-agent2_pos[0], agent1_pos[1]-agent2_pos[1])\n",
    "\n",
    "    def __compute_distance(self, distance_vector):\n",
    "        return math.sqrt(math.pow(distance_vector[0], 2) + math.pow(distance_vector[1], 2))\n",
    "\n",
    "    def __compute_norm(self, vector):\n",
    "        return math.sqrt(math.pow(vector[0], 2) + math.pow(vector[1], 2))\n",
    "    \n",
    "    def __compute_unit_vector(self, vector):\n",
    "        norm = self.__compute_norm(vector)\n",
    "        return [vector[0]/norm, vector[1]/norm]\n",
    "\n",
    "    def __get_local_reward(self, agent):\n",
    "         obs = [self.__compute_distance_vector(agent, self.__get_other_agents(agent)[0])]\n",
    "         return -np.array([abs(self.__compute_distance(distance_vector) - self.target_distance) for distance_vector in obs]).sum()\n",
    "    \n",
    "    def __get_global_reward(self):\n",
    "        return 0\n",
    "    \n",
    "    def __get_other_agents(self, agent):\n",
    "        return [other for other in self.agents_ids if other != agent]\n",
    "\n",
    "    def __update_agent_position(self, agent, action):\n",
    "        unit_movement = self.__compute_unit_vector([action[0], action[1]])\n",
    "        self.agents_pos[agent] = (self.agents_pos[agent][0] + unit_movement[0]*action[2]*self.config.speed, \n",
    "                                 self.agents_pos[agent][1] + unit_movement[1]*action[2]*self.config.speed)\n",
    "\n",
    "    def reset(self, seed=None, options=None):\n",
    "        self.steps = 0\n",
    "        self.agents_pos = {agent: self.__get_random_point(max_x=100, max_y=100) for agent in self.agents_ids}\n",
    "        return {agent: self.__get_observation(agent) for agent in self.agents_ids}, {}\n",
    "     \n",
    "    def step(self, actions):\n",
    "        self.steps += 1\n",
    "        observations, rewards, terminated, truncated, infos = {}, {}, {}, {}, {}\n",
    "\n",
    "        for agent, action in actions.items():\n",
    "            self.__update_agent_position(agent, action)\n",
    "\n",
    "        for agent in actions.keys():\n",
    "            observations[agent] = self.__get_observation(agent)\n",
    "            rewards[agent] = self.__get_local_reward(agent) + self.__get_global_reward()\n",
    "            terminated[agent] = False\n",
    "            truncated[agent] = False\n",
    "            infos[agent] = {}\n",
    "\n",
    "        truncated['__all__'] = False\n",
    "        if self.max_steps != None and self.steps == self.max_steps:\n",
    "            terminated['__all__'] = True\n",
    "        else:\n",
    "            terminated['__all__'] = False\n",
    "\n",
    "        return observations, rewards, terminated, truncated, infos\n",
    "     \n",
    "    def rgb_to_hex(self, r, g, b):\n",
    "        return f'#{r:02x}{g:02x}{b:02x}'\n",
    "\n",
    "    def render(self):\n",
    "        width, height = 100, 100\n",
    "        if self.canvas is None:\n",
    "            self.canvas = Canvas()\n",
    "            display(self.canvas)\n",
    "        \n",
    "        with hold_canvas():\n",
    "            agent_size = 3\n",
    "            top_left = (0.0,0.0)\n",
    "            bottom_right = (100.0, 100.0)\n",
    "            self.canvas.clear()\n",
    "\n",
    "            for agent in self.agents_ids:\n",
    "                raw_pos = self.agents_pos[agent]\n",
    "                color = self.agent_colors[agent]\n",
    "                \n",
    "                agent_pos_in_frame = [((raw_pos[0]-top_left[0])/(bottom_right[0]-top_left[0]))*width,\n",
    "                            ((raw_pos[1]-top_left[1])/(bottom_right[1]-top_left[1]))*height,]\n",
    "\n",
    "                self.canvas.fill_style = color\n",
    "                self.canvas.fill_circle(\n",
    "                    agent_pos_in_frame[0],\n",
    "                    agent_pos_in_frame[1],\n",
    "                    agent_size\n",
    "                )\n",
    "\n",
    "    def get_agent_ids(self):\n",
    "       return self.agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ad7a90a4b0f1477ba4532b46dde950ef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "env_config = EnvironmentConfiguration(n_agents=2, target_distance=5, max_steps=100, speed=2)\n",
    "env = KeepTheDistance(env_config)\n",
    "obs, _ = env.reset()\n",
    "env.render()\n",
    "\n",
    "for i in range(10):\n",
    "\n",
    "    actions = {agent: np.array([1.0, 1.0, 1.0], np.float32) for agent in obs.keys()}\n",
    "    #actions = {agent: env.action_space.sample() for agent in obs.keys()}\n",
    "    obs, _, _, _, _ = env.step(actions)\n",
    "    env.render()\n",
    "    time.sleep(0.5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tkinter import Tk, Canvas, PhotoImage, mainloop\n",
    "root = Tk()\n",
    "root.title(\"Canvas Pixel Painting\")\n",
    "width, height = 500, 500\n",
    "\n",
    "# Create a canvas widget\n",
    "canvas = Canvas(root, width=width, height=height)\n",
    "canvas.pack()\n",
    "\n",
    "x, y = (200, 100)\n",
    "w = 10\n",
    "canvas.create_oval(x-w/2, y-w/2, x+w/2, y+w/2, outline=\"black\", fill=\"red\")\n",
    "x, y = (300, 100)\n",
    "canvas.delete(\"all\")\n",
    "canvas.create_rectangle( (x, y)*2 )\n",
    "x, y = (400, 100)\n",
    "canvas.create_rectangle( (x, y)*2 )\n",
    "#canvas.put(\"red\", (100, 200))\n",
    "root.mainloop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tianEnv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
