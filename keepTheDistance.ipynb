{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Keep average distance\n",
    "\n",
    "the agents goal is to position close to each others at a distance previously defined\n",
    "\n",
    "challenges:\n",
    "- deal with continuous space environment\n",
    "- limited vision of an agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ipycanvas import Canvas, hold_canvas\n",
    "\n",
    "class CanvasWithBorders(Canvas):\n",
    "    def clear(self):\n",
    "        super().clear()\n",
    "        border_color = 'black'  # You can customize the border color here\n",
    "        border_width = 1  # You can customize the border width here\n",
    "        \n",
    "        self.fill_style = border_color\n",
    "        # Draw top border\n",
    "        self.fill_rect(0, 0, self.width, border_width)\n",
    "        # Draw bottom border\n",
    "        self.fill_rect(0, self.height - border_width, self.width, border_width)\n",
    "        # Draw left border\n",
    "        self.fill_rect(0, 0, border_width, self.height)\n",
    "        # Draw right border\n",
    "        self.fill_rect(self.width - border_width, 0, border_width, self.height)\n",
    "\n",
    "import os\n",
    "from ray.rllib.algorithms.algorithm import Algorithm\n",
    "\n",
    "def save_algo(algo, name):\n",
    "    base_dir = os.path.join(os.getcwd(), \"algos\")\n",
    "    subfolder_path = os.path.join(base_dir, name)\n",
    "    os.makedirs(subfolder_path, exist_ok=True)\n",
    "    path_to_checkpoint  = algo.save(subfolder_path)\n",
    "    print(f\"An Algorithm checkpoint has been created inside directory: '{path_to_checkpoint}'.\")\n",
    "\n",
    "def load_algo(name):\n",
    "    base_dir = os.path.join(os.getcwd(), \"algos\")\n",
    "    subfolder_path = os.path.join(base_dir, name)\n",
    "    if not os.path.exists(subfolder_path):\n",
    "        raise FileNotFoundError(f\"The specified subfolder '{subfolder_path}' does not exist.\")\n",
    "    \n",
    "    return Algorithm.from_checkpoint(subfolder_path)\n",
    "\n",
    "#save_algo(algo, \"KeepTheDistance_dst=0_agent=2_100x100train\")\n",
    "#algo2 = load_algo(\"KeepTheDistance_dst=0_agent=2_100x100train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import random as rnd\n",
    "\n",
    "def simulate_episode(env, policy, steps, sleep_between_frames=0.3, print_info=True):\n",
    "    obs, _ = env.reset()\n",
    "    env.render()\n",
    "    \n",
    "\n",
    "    for i in range(steps):\n",
    "        if print_info:\n",
    "            print(f\"obs: \", obs)\n",
    "        actions = policy.compute_actions(obs)\n",
    "        #actions = {agent: np.array([rnd.random()*2-1, rnd.random()*2-1, 1.0], np.float32) for agent in obs.keys()}\n",
    "        #actions = {agent: env.action_space.sample() for agent in obs.keys()}\n",
    "        obs, reward, _, _, _ = env.step(actions)\n",
    "        env.render()\n",
    "        if print_info:\n",
    "            print(f\"action: \", actions)\n",
    "            print(f\"reward: \", reward, \"\\n\")\n",
    "        time.sleep(sleep_between_frames)\n",
    "\n",
    "def ppo_result_format(result):\n",
    "    return (f\"iteration [{result['training_iteration']}] => \" +\n",
    "          f\"episode_reward_mean: {result['sampler_results']['episode_reward_mean']}, \" +\n",
    "          f\"episode_len_mean: {result['sampler_results']['episode_len_mean']}, \" +\n",
    "          f\"agent_steps_trained: {result['info']['num_agent_steps_trained']}, \" +\n",
    "          f\"env_steps_trained: {result['info']['num_env_steps_trained']}, \" + \n",
    "          f\"entropy: {result['info']['learner']['default_policy']['learner_stats']['entropy']}, \" +\n",
    "          f\"learning_rate: {result['info']['learner']['default_policy']['learner_stats']['cur_lr']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### environment definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Set\n",
    "from ray.rllib.env.multi_agent_env import MultiAgentEnv\n",
    "import random as rnd\n",
    "from gymnasium.spaces import Discrete, Box, Dict, Tuple\n",
    "from gymnasium.spaces.utils import flatten, flatten_space\n",
    "import numpy as np\n",
    "from IPython.display import clear_output\n",
    "import math\n",
    "from ipycanvas import Canvas, hold_canvas\n",
    "\n",
    "class EnvironmentConfiguration: \n",
    "    def __init__(self, n_agents, target_distance, speed, spawn_area=100, max_steps=None):\n",
    "        self.n_agents = n_agents\n",
    "        self.target_distance = target_distance\n",
    "        self.max_steps = max_steps\n",
    "        self.speed = speed\n",
    "        self.spawn_area = spawn_area\n",
    "\n",
    "class KeepTheDistance(MultiAgentEnv):\n",
    "\n",
    "    canvas = None\n",
    "    CANVAS_WIDTH, CANVAS_HEIGHT = 300.0, 300.0\n",
    "\n",
    "    def __init__(self, config: EnvironmentConfiguration):\n",
    "        assert config.n_agents == 2 # just base case implemented \n",
    "             \n",
    "        self.n_agents = config.n_agents\n",
    "        self.target_distance = config.target_distance\n",
    "        self.max_steps = config.max_steps\n",
    "        self.speed = config.speed\n",
    "        self.spawn_area = config.spawn_area\n",
    "        \n",
    "        self.agents_ids = ['agent-' + str(i) for i in range(self.n_agents)]\n",
    "        self.agent_colors = {agent: self.rgb_to_hex(rnd.randint(0, 255), rnd.randint(0, 255), rnd.randint(0, 255)) for agent in self.agents_ids}\n",
    "        self.observation_space = self.observation_space('agent-0')\n",
    "        self.action_space = self.action_space(\"\")\n",
    "\n",
    "    def unflatten_observation_space(self, agent):\n",
    "        #distance_vector = Box(low=-np.inf, high=np.inf, shape=(2,1), dtype=np.float32)\n",
    "        #obs_space = Dict({\"nbr-1\": distance_vector})\n",
    "        direction = Box(low=-1, high=1, shape=(2,1), dtype=np.float32)\n",
    "        distance = Box(low=-np.inf, high=np.inf, shape=(1,1), dtype=np.float32)\n",
    "        return Dict({\"nbr-1\": Dict({'direction': direction, 'distance': distance})})\n",
    "\n",
    "    def observation_space(self, agent):\n",
    "        return flatten_space(self.unflatten_observation_space(agent))\n",
    "\n",
    "    def action_space(self, agent):\n",
    "        direction = Box(low=-1.0, high=1.0, shape=(2,1), dtype=np.float32)\n",
    "        speed = Box(0.0, 1.0, dtype=np.float32)\n",
    "        return flatten_space(Tuple([direction, speed]))\n",
    "    \n",
    "    def __get_random_point(self, max_x, max_y, min_x=0, min_y=0):\n",
    "        return (rnd.randint(min_x, max_x-1), rnd.randint(min_y, max_y-1))\n",
    "    \n",
    "    def __get_observation(self, agent):\n",
    "        nbr = self.__get_other_agents(agent)\n",
    "        distance_vector = self.__compute_distance_vector(agent, nbr[0])\n",
    "        obs = {\n",
    "            \"nbr-1\": {\n",
    "                \"direction\": self.__compute_unit_vector(distance_vector),\n",
    "                \"distance\": self.__compute_distance(distance_vector)\n",
    "            }\n",
    "        }\n",
    "        return flatten(self.unflatten_observation_space(agent), obs)\n",
    "\n",
    "    def rgb_to_hex(self, r, g, b):\n",
    "        return f'#{r:02x}{g:02x}{b:02x}'\n",
    "\n",
    "    def __compute_distance_vector(self, agent1, agent2):\n",
    "        agent1_pos = self.agents_pos[agent1]\n",
    "        agent2_pos = self.agents_pos[agent2]\n",
    "        return (agent1_pos[0]-agent2_pos[0], agent1_pos[1]-agent2_pos[1])\n",
    "\n",
    "    def __compute_distance(self, distance_vector):\n",
    "        return math.sqrt(math.pow(distance_vector[0], 2) + math.pow(distance_vector[1], 2))\n",
    "\n",
    "    def __compute_norm(self, vector):\n",
    "        return math.sqrt(math.pow(vector[0], 2) + math.pow(vector[1], 2))\n",
    "    \n",
    "    def __compute_unit_vector(self, vector):\n",
    "        norm = self.__compute_norm(vector)\n",
    "        if norm == 0:\n",
    "            return [0,0]\n",
    "        return [vector[0]/norm, vector[1]/norm]\n",
    "\n",
    "    def __compute_distance_from_closest_neighbours(self, agent):\n",
    "        obs = [self.__compute_distance_vector(agent, self.__get_other_agents(agent)[0])]\n",
    "        distance = np.array([abs(self.__compute_distance(distance_vector) - self.target_distance) for distance_vector in obs]).sum()\n",
    "        return distance\n",
    "\n",
    "    def __get_local_reward(self, agent, action):\n",
    "        closest_nbrs = self.__get_n_closest_neighbours(agent, 1)\n",
    "\n",
    "        newDistance = sum([abs(distance - self.target_distance) for distance in closest_nbrs.values()])\n",
    "        reward_1 = self.last_step_distances[agent] - newDistance\n",
    "        self.last_step_distances[agent] = newDistance\n",
    "\n",
    "        reward_2 = sum([1 if abs(distance - self.target_distance) < 0.5 else 0 for distance in closest_nbrs.values()])\n",
    "\n",
    "        #reward_3 = - action[2]\n",
    "        #return -newDistance + reward_2 \n",
    "        return reward_1 + reward_2 #+ reward_3\n",
    "\n",
    "    def __get_global_reward(self):\n",
    "        return 0\n",
    "    \n",
    "    def __get_other_agents(self, agent):\n",
    "        return [other for other in self.agents_ids if other != agent]\n",
    "\n",
    "    def __get_n_closest_neighbours(self, agent, n=1):\n",
    "        dst = {other: self.__compute_distance(self.__compute_distance_vector(agent, other)) for other in self.__get_other_agents(agent)}\n",
    "        return {neighbour[0]: neighbour[1] for neighbour in sorted(list(dst.items()), key=lambda d: d[0])[:n]}\n",
    "\n",
    "    def __update_agent_position(self, agent, action):\n",
    "        unit_movement = self.__compute_unit_vector([action[0], action[1]])\n",
    "        self.agents_pos[agent] = (self.agents_pos[agent][0] + unit_movement[0]*action[2]*self.speed, \n",
    "                                 self.agents_pos[agent][1] + unit_movement[1]*action[2]*self.speed)\n",
    "\n",
    "    def reset(self, seed=None, options=None):\n",
    "        self.steps = 0\n",
    "        self.agents_pos = {agent: self.__get_random_point(max_x=self.spawn_area, max_y=self.spawn_area) for agent in self.agents_ids}\n",
    "        self.last_step_distances = {agent: self.__compute_distance_from_closest_neighbours(agent) for agent in self.agents_ids}\n",
    "        return {agent: self.__get_observation(agent) for agent in self.agents_ids}, {}\n",
    "     \n",
    "    def step(self, actions):\n",
    "        self.steps += 1\n",
    "        observations, rewards, terminated, truncated, infos = {}, {}, {}, {}, {}\n",
    "\n",
    "        for agent, action in actions.items():\n",
    "            self.__update_agent_position(agent, action)\n",
    "\n",
    "        for agent, action in actions.items():\n",
    "            observations[agent] = self.__get_observation(agent)\n",
    "            rewards[agent] = self.__get_local_reward(agent, action) + self.__get_global_reward()\n",
    "            terminated[agent] = False\n",
    "            truncated[agent] = False\n",
    "            infos[agent] = {}\n",
    "\n",
    "        truncated['__all__'] = False\n",
    "        if self.max_steps != None and self.steps == self.max_steps:\n",
    "            terminated['__all__'] = True\n",
    "        else:\n",
    "            terminated['__all__'] = False\n",
    "\n",
    "        return observations, rewards, terminated, truncated, infos\n",
    "     \n",
    "    def rgb_to_hex(self, r, g, b):\n",
    "        return f'#{r:02x}{g:02x}{b:02x}'\n",
    "\n",
    "    def render(self):\n",
    "        pass\n",
    "\n",
    "    def get_agent_ids(self):\n",
    "       return self.agents\n",
    "\n",
    "\n",
    "class RenderableKeepTheDistance(KeepTheDistance):\n",
    "    def render(self):\n",
    "        if self.canvas is None:\n",
    "            self.canvas = CanvasWithBorders(width=self.CANVAS_WIDTH, height=self.CANVAS_HEIGHT)\n",
    "            display(self.canvas)\n",
    "        \n",
    "        with hold_canvas():\n",
    "            agent_size = max(self.CANVAS_WIDTH/float(self.spawn_area),1)\n",
    "            top_left = (0.0,0.0)\n",
    "            bottom_right = (self.spawn_area, self.spawn_area)\n",
    "            self.canvas.clear()\n",
    "\n",
    "            for agent in self.agents_ids:\n",
    "                raw_pos = self.agents_pos[agent]\n",
    "                color = self.agent_colors[agent]\n",
    "                \n",
    "                agent_pos_in_frame = [((raw_pos[0]-top_left[0])/(bottom_right[0]-top_left[0]))*self.CANVAS_WIDTH,\n",
    "                            ((raw_pos[1]-top_left[1])/(bottom_right[1]-top_left[1]))*self.CANVAS_HEIGHT,]\n",
    "\n",
    "                self.canvas.fill_style = color\n",
    "                self.canvas.fill_circle(\n",
    "                    agent_pos_in_frame[0],\n",
    "                    agent_pos_in_frame[1],\n",
    "                    agent_size/2.0\n",
    "                )\n",
    "                \n",
    "                self.canvas.stroke_style = \"black\"\n",
    "                self.canvas.stroke_circle(\n",
    "                    agent_pos_in_frame[0],\n",
    "                    agent_pos_in_frame[1],\n",
    "                    agent_size/2.0\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "63f284c353194d229220437124ee9763",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "CanvasWithBorders(height=300, width=300)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'agent-0': array([1., 0., 2.], dtype=float32), 'agent-1': array([-1.,  0.,  2.], dtype=float32)}\n",
      "{'agent-0': array([-1., -1.,  1.], dtype=float32), 'agent-1': array([0., 0., 1.], dtype=float32)}\n",
      "{'agent-0': 0.5263742417920994, 'agent-1': 0.5263742417920994} \n",
      "\n",
      "{'agent-0': array([ 0.8773552, -0.4798415,  1.4736258], dtype=float32), 'agent-1': array([-0.8773552,  0.4798415,  1.4736258], dtype=float32)}\n",
      "{'agent-0': array([-1., -1.,  1.], dtype=float32), 'agent-1': array([0., 0., 1.], dtype=float32)}\n",
      "{'agent-0': -0.057107971252458256, 'agent-1': -0.057107971252458256} \n",
      "\n",
      "{'agent-0': array([ 0.38268343, -0.9238795 ,  1.5307337 ], dtype=float32), 'agent-1': array([-0.38268343,  0.9238795 ,  1.5307337 ], dtype=float32)}\n",
      "{'agent-0': array([-1., -1.,  1.], dtype=float32), 'agent-1': array([0., 0., 1.], dtype=float32)}\n",
      "{'agent-0': -0.5940529953102542, 'agent-1': -0.5940529953102542} \n",
      "\n",
      "{'agent-0': array([-0.05709766, -0.9983686 ,  2.1247866 ], dtype=float32), 'agent-1': array([0.05709766, 0.9983686 , 2.1247866 ], dtype=float32)}\n",
      "{'agent-0': array([-1., -1.,  1.], dtype=float32), 'agent-1': array([0., 0., 1.], dtype=float32)}\n",
      "{'agent-0': -0.8224647916451877, 'agent-1': -0.8224647916451877} \n",
      "\n",
      "{'agent-0': array([-0.28108463, -0.959683  ,  2.9472516 ], dtype=float32), 'agent-1': array([0.28108463, 0.959683  , 2.9472516 ], dtype=float32)}\n",
      "{'agent-0': array([-1., -1.,  1.], dtype=float32), 'agent-1': array([0., 0., 1.], dtype=float32)}\n",
      "{'agent-0': -0.9073385235584954, 'agent-1': -0.9073385235584954} \n",
      "\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "env_config = EnvironmentConfiguration(n_agents=2, target_distance=0, max_steps=100, speed=1, spawn_area=5)\n",
    "env = RenderableKeepTheDistance(env_config)\n",
    "obs, _ = env.reset()\n",
    "env.render()\n",
    "\n",
    "for i in range(5):\n",
    "    print(obs)\n",
    "    #actions = {agent: np.array([rnd.random()*2-1, rnd.random()*2-1, 1.0], np.float32) for agent in obs.keys()}\n",
    "    actions = {'agent-0': np.array([-1.0, -1.0, 1.0], np.float32),\n",
    "               'agent-1': np.array([0.0, 0.0, 1], np.float32)}\n",
    "    #actions = {agent: env.action_space.sample() for agent in obs.keys()}\n",
    "    obs, reward, _, _, _ = env.step(actions)\n",
    "    print(actions)\n",
    "    print(reward, \"\\n\")\n",
    "    env.render()\n",
    "    time.sleep(0.3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## policy training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ray\n",
    "ray.shutdown()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KeepTheDistance?dst=0&agent=2&spawn_area=100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ray.tune.registry import register_env\n",
    "\n",
    "env_config = EnvironmentConfiguration(n_agents=2, target_distance=0, max_steps=300, speed=1, spawn_area=100)\n",
    "register_env(\"KeepTheDistance?dst=0&agent=2&spawn_area=100\", lambda _: KeepTheDistance(env_config))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "algo = load_algo(\"KeepTheDistance?dst=0&agent=2&spawn_area=100\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration [1] => episode_reward_mean: -4.95664815891495, episode_len_mean: 300.0, agent_steps_trained: 8192, env_steps_trained: 4096, entropy: 4.245799648761749, learning_rate: 0.0010000000000000005\n",
      "iteration [2] => episode_reward_mean: 25.736145597755076, episode_len_mean: 300.0, agent_steps_trained: 16384, env_steps_trained: 8192, entropy: 4.2675507590174675, learning_rate: 0.0010000000000000005\n",
      "iteration [3] => episode_reward_mean: 46.87691646133618, episode_len_mean: 300.0, agent_steps_trained: 24576, env_steps_trained: 12288, entropy: 4.28767983019352, learning_rate: 0.0010000000000000005\n",
      "iteration [4] => episode_reward_mean: 63.17620430531192, episode_len_mean: 300.0, agent_steps_trained: 32768, env_steps_trained: 16384, entropy: 4.272764860590299, learning_rate: 0.0010000000000000005\n",
      "iteration [5] => episode_reward_mean: 83.51466914032467, episode_len_mean: 300.0, agent_steps_trained: 40960, env_steps_trained: 20480, entropy: 4.080347933868567, learning_rate: 0.0010000000000000005\n",
      "iteration [6] => episode_reward_mean: 97.2725900401565, episode_len_mean: 300.0, agent_steps_trained: 49152, env_steps_trained: 24576, entropy: 4.035366955151161, learning_rate: 0.0010000000000000005\n",
      "iteration [7] => episode_reward_mean: 111.82767845339714, episode_len_mean: 300.0, agent_steps_trained: 57344, env_steps_trained: 28672, entropy: 3.9716993694504104, learning_rate: 0.0010000000000000005\n",
      "iteration [8] => episode_reward_mean: 136.62884647100984, episode_len_mean: 300.0, agent_steps_trained: 65536, env_steps_trained: 32768, entropy: 3.955365744729837, learning_rate: 0.0010000000000000005\n",
      "iteration [9] => episode_reward_mean: 164.88432376674643, episode_len_mean: 300.0, agent_steps_trained: 73728, env_steps_trained: 36864, entropy: 3.8659255050122736, learning_rate: 0.0010000000000000005\n",
      "iteration [10] => episode_reward_mean: 195.05470119939545, episode_len_mean: 300.0, agent_steps_trained: 81920, env_steps_trained: 40960, entropy: 3.8061600769559543, learning_rate: 0.0010000000000000005\n",
      "iteration [11] => episode_reward_mean: 226.75336202358946, episode_len_mean: 300.0, agent_steps_trained: 90112, env_steps_trained: 45056, entropy: 3.7295295434693494, learning_rate: 0.0010000000000000005\n",
      "iteration [12] => episode_reward_mean: 254.77739146322128, episode_len_mean: 300.0, agent_steps_trained: 98304, env_steps_trained: 49152, entropy: 3.7193174014488855, learning_rate: 0.0010000000000000005\n",
      "iteration [13] => episode_reward_mean: 289.128688300256, episode_len_mean: 300.0, agent_steps_trained: 106496, env_steps_trained: 53248, entropy: 3.6365751929581167, learning_rate: 0.0010000000000000005\n",
      "iteration [14] => episode_reward_mean: 325.6940806998776, episode_len_mean: 300.0, agent_steps_trained: 114688, env_steps_trained: 57344, entropy: 3.6089514570931596, learning_rate: 0.0010000000000000005\n",
      "iteration [15] => episode_reward_mean: 358.4698142168033, episode_len_mean: 300.0, agent_steps_trained: 122880, env_steps_trained: 61440, entropy: 3.6058398122588793, learning_rate: 0.0010000000000000005\n",
      "iteration [16] => episode_reward_mean: 397.62178977112677, episode_len_mean: 300.0, agent_steps_trained: 131072, env_steps_trained: 65536, entropy: 3.5509075559675694, learning_rate: 0.0010000000000000005\n",
      "iteration [17] => episode_reward_mean: 430.0716298706977, episode_len_mean: 300.0, agent_steps_trained: 139264, env_steps_trained: 69632, entropy: 3.5135758027434347, learning_rate: 0.0010000000000000005\n",
      "iteration [18] => episode_reward_mean: 459.53089803529224, episode_len_mean: 300.0, agent_steps_trained: 147456, env_steps_trained: 73728, entropy: 3.491550320883592, learning_rate: 0.0010000000000000005\n",
      "iteration [19] => episode_reward_mean: 488.62102982390604, episode_len_mean: 300.0, agent_steps_trained: 155648, env_steps_trained: 77824, entropy: 3.4550479906300704, learning_rate: 0.0010000000000000005\n",
      "iteration [20] => episode_reward_mean: 510.65966583025437, episode_len_mean: 300.0, agent_steps_trained: 163840, env_steps_trained: 81920, entropy: 3.4639352411031723, learning_rate: 0.0010000000000000005\n",
      "iteration [21] => episode_reward_mean: 529.9404729947166, episode_len_mean: 300.0, agent_steps_trained: 172032, env_steps_trained: 86016, entropy: 3.4784888106087846, learning_rate: 0.0010000000000000005\n",
      "iteration [22] => episode_reward_mean: 546.4029696187386, episode_len_mean: 300.0, agent_steps_trained: 180224, env_steps_trained: 90112, entropy: 3.4598235140244165, learning_rate: 0.0010000000000000005\n",
      "iteration [23] => episode_reward_mean: 559.27084482982, episode_len_mean: 300.0, agent_steps_trained: 188416, env_steps_trained: 94208, entropy: 3.4430141928295295, learning_rate: 0.0010000000000000005\n",
      "iteration [24] => episode_reward_mean: 568.6788158436156, episode_len_mean: 300.0, agent_steps_trained: 196608, env_steps_trained: 98304, entropy: 3.4166217731932798, learning_rate: 0.0010000000000000005\n",
      "iteration [25] => episode_reward_mean: 576.044729303482, episode_len_mean: 300.0, agent_steps_trained: 204800, env_steps_trained: 102400, entropy: 3.3975195427735647, learning_rate: 0.0010000000000000005\n",
      "iteration [26] => episode_reward_mean: 584.9244146978964, episode_len_mean: 300.0, agent_steps_trained: 212992, env_steps_trained: 106496, entropy: 3.3621628026167554, learning_rate: 0.0010000000000000005\n",
      "iteration [27] => episode_reward_mean: 590.5687960574658, episode_len_mean: 300.0, agent_steps_trained: 221184, env_steps_trained: 110592, entropy: 3.3647831340630847, learning_rate: 0.0010000000000000005\n",
      "iteration [28] => episode_reward_mean: 596.0001495913068, episode_len_mean: 300.0, agent_steps_trained: 229376, env_steps_trained: 114688, entropy: 3.328010957191388, learning_rate: 0.0010000000000000005\n",
      "iteration [29] => episode_reward_mean: 599.8579169317861, episode_len_mean: 300.0, agent_steps_trained: 237568, env_steps_trained: 118784, entropy: 3.349336307992538, learning_rate: 0.0010000000000000005\n",
      "iteration [30] => episode_reward_mean: 603.4171706591706, episode_len_mean: 300.0, agent_steps_trained: 245760, env_steps_trained: 122880, entropy: 3.2973313165207703, learning_rate: 0.0010000000000000005\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "abcec2f0359a44ce9ab36e6a70d2cd1a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "CanvasWithBorders(height=300, width=300)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "obs:  {'agent-0': array([-0.4472136,  0.8944272, 35.77709  ], dtype=float32), 'agent-1': array([ 0.4472136, -0.8944272, 35.77709  ], dtype=float32)}\n",
      "action:  {'agent-0': array([ 0.93228984, -0.17839646,  0.80335176], dtype=float32), 'agent-1': array([-1.        ,  0.72599816,  1.        ], dtype=float32)}\n",
      "reward:  {'agent-0': 1.35772393719202, 'agent-1': 1.35772393719202} \n",
      "\n",
      "obs:  {'agent-0': array([-0.41841963,  0.90825385, 34.419365  ], dtype=float32), 'agent-1': array([ 0.41841963, -0.90825385, 34.419365  ], dtype=float32)}\n",
      "action:  {'agent-0': array([ 0.47593868, -1.        ,  0.8037338 ], dtype=float32), 'agent-1': array([0.15418804, 1.        , 1.        ], dtype=float32)}\n",
      "reward:  {'agent-0': 1.63307693167814, 'agent-1': 1.63307693167814} \n",
      "\n",
      "obs:  {'agent-0': array([-0.43337393,  0.9012142 , 32.786285  ], dtype=float32), 'agent-1': array([ 0.43337393, -0.9012142 , 32.786285  ], dtype=float32)}\n",
      "action:  {'agent-0': array([ 0.24133325, -1.        ,  1.        ], dtype=float32), 'agent-1': array([-0.15044987,  1.        ,  1.        ], dtype=float32)}\n",
      "reward:  {'agent-0': 1.9292706061529614, 'agent-1': 1.9292706061529614} \n",
      "\n",
      "obs:  {'agent-0': array([-0.44804552,  0.8940107 , 30.857016  ], dtype=float32), 'agent-1': array([ 0.44804552, -0.8940107 , 30.857016  ], dtype=float32)}\n",
      "action:  {'agent-0': array([ 1., -1.,  1.], dtype=float32), 'agent-1': array([-0.8417731 ,  1.        ,  0.87403256], dtype=float32)}\n",
      "reward:  {'agent-0': 1.7943279554823377, 'agent-1': 1.7943279554823377} \n",
      "\n",
      "obs:  {'agent-0': array([-0.4320101 ,  0.90186876, 29.062689  ], dtype=float32), 'agent-1': array([ 0.4320101 , -0.90186876, 29.062689  ], dtype=float32)}\n",
      "action:  {'agent-0': array([ 0.4099177 , -1.        ,  0.78837353], dtype=float32), 'agent-1': array([-0.6240711,  1.       ,  1.       ], dtype=float32)}\n",
      "reward:  {'agent-0': 1.7808056048701033, 'agent-1': 1.7808056048701033} \n",
      "\n",
      "obs:  {'agent-0': array([-0.42984286,  0.90290374, 27.281883  ], dtype=float32), 'agent-1': array([ 0.42984286, -0.90290374, 27.281883  ], dtype=float32)}\n",
      "action:  {'agent-0': array([ 0.1998682, -1.       ,  1.       ], dtype=float32), 'agent-1': array([-0.04411101,  1.        ,  1.        ], dtype=float32)}\n",
      "reward:  {'agent-0': 1.8826885044724726, 'agent-1': 1.8826885044724726} \n",
      "\n",
      "obs:  {'agent-0': array([-0.45225304,  0.8918897 , 25.399195  ], dtype=float32), 'agent-1': array([ 0.45225304, -0.8918897 , 25.399195  ], dtype=float32)}\n",
      "action:  {'agent-0': array([ 1., -1.,  1.], dtype=float32), 'agent-1': array([-0.51298016,  1.        ,  0.9756477 ], dtype=float32)}\n",
      "reward:  {'agent-0': 1.9239699401411663, 'agent-1': 1.9239699401411663} \n",
      "\n",
      "obs:  {'agent-0': array([-0.4402276,  0.8978862, 23.475224 ], dtype=float32), 'agent-1': array([ 0.4402276, -0.8978862, 23.475224 ], dtype=float32)}\n",
      "action:  {'agent-0': array([0.58056355, 0.45263517, 0.26482058], dtype=float32), 'agent-1': array([-0.71045536,  1.        ,  1.        ], dtype=float32)}\n",
      "reward:  {'agent-0': 0.9287508892932763, 'agent-1': 0.9287508892932763} \n",
      "\n",
      "obs:  {'agent-0': array([-0.42341104,  0.9059377 , 22.546473  ], dtype=float32), 'agent-1': array([ 0.42341104, -0.9059377 , 22.546473  ], dtype=float32)}\n",
      "action:  {'agent-0': array([ 0.9055842 , -0.96766686,  0.49681768], dtype=float32), 'agent-1': array([-0.91058475,  1.        ,  0.9557551 ], dtype=float32)}\n",
      "reward:  {'agent-0': 1.3805011743677476, 'agent-1': 1.3805011743677476} \n",
      "\n",
      "obs:  {'agent-0': array([-0.40458637,  0.9144998 , 21.165972  ], dtype=float32), 'agent-1': array([ 0.40458637, -0.9144998 , 21.165972  ], dtype=float32)}\n",
      "action:  {'agent-0': array([ 0.6303904, -1.       ,  0.7119421], dtype=float32), 'agent-1': array([-0.08203042,  0.354841  ,  0.9280351 ], dtype=float32)}\n",
      "reward:  {'agent-0': 1.6156939521001128, 'agent-1': 1.6156939521001128} \n",
      "\n",
      "obs:  {'agent-0': array([-0.4079112,  0.9130216, 19.550278 ], dtype=float32), 'agent-1': array([ 0.4079112, -0.9130216, 19.550278 ], dtype=float32)}\n",
      "action:  {'agent-0': array([ 1., -1.,  1.], dtype=float32), 'agent-1': array([-0.36872065,  1.        ,  0.9706974 ], dtype=float32)}\n",
      "reward:  {'agent-0': 1.9001458684887567, 'agent-1': 1.9001458684887567} \n",
      "\n",
      "obs:  {'agent-0': array([-0.39273673,  0.919651  , 17.650133  ], dtype=float32), 'agent-1': array([ 0.39273673, -0.919651  , 17.650133  ], dtype=float32)}\n",
      "action:  {'agent-0': array([ 0.80490613, -1.        ,  0.8519956 ], dtype=float32), 'agent-1': array([-0.32113492,  1.        ,  0.2170971 ], dtype=float32)}\n",
      "reward:  {'agent-0': 1.0350133360674647, 'agent-1': 1.0350133360674647} \n",
      "\n",
      "obs:  {'agent-0': array([-0.3810539,  0.9245528, 16.615118 ], dtype=float32), 'agent-1': array([ 0.3810539, -0.9245528, 16.615118 ], dtype=float32)}\n",
      "action:  {'agent-0': array([ 0.40411544, -0.45990586,  1.        ], dtype=float32), 'agent-1': array([-0.59355986,  0.75297976,  1.        ], dtype=float32)}\n",
      "reward:  {'agent-0': 1.8959157323069942, 'agent-1': 1.8959157323069942} \n",
      "\n",
      "obs:  {'agent-0': array([-0.3432329 ,  0.93925035, 14.719203  ], dtype=float32), 'agent-1': array([ 0.3432329 , -0.93925035, 14.719203  ], dtype=float32)}\n",
      "action:  {'agent-0': array([-0.42342377, -1.        ,  0.8688137 ], dtype=float32), 'agent-1': array([-0.11630499,  1.        ,  1.        ], dtype=float32)}\n",
      "reward:  {'agent-0': 1.581843984905607, 'agent-1': 1.581843984905607} \n",
      "\n",
      "obs:  {'agent-0': array([-0.40155318,  0.9158357 , 13.13736   ], dtype=float32), 'agent-1': array([ 0.40155318, -0.9158357 , 13.13736   ], dtype=float32)}\n",
      "action:  {'agent-0': array([ 1., -1.,  1.], dtype=float32), 'agent-1': array([-0.70778656,  1.        ,  1.        ], dtype=float32)}\n",
      "reward:  {'agent-0': 1.896848954333766, 'agent-1': 1.896848954333766} \n",
      "\n",
      "obs:  {'agent-0': array([-0.35501248,  0.93486154, 11.24051   ], dtype=float32), 'agent-1': array([ 0.35501248, -0.93486154, 11.24051   ], dtype=float32)}\n",
      "action:  {'agent-0': array([-0.2564398, -1.       ,  1.       ], dtype=float32), 'agent-1': array([-0.32356697,  1.        ,  1.        ], dtype=float32)}\n",
      "reward:  {'agent-0': 1.7953520892789747, 'agent-1': 1.7953520892789747} \n",
      "\n",
      "obs:  {'agent-0': array([-0.4161996 ,  0.90927327,  9.445158  ], dtype=float32), 'agent-1': array([ 0.4161996 , -0.90927327,  9.445158  ], dtype=float32)}\n",
      "action:  {'agent-0': array([ 0.15766299, -1.        ,  1.        ], dtype=float32), 'agent-1': array([-0.3565457 ,  1.        ,  0.69993985], dtype=float32)}\n",
      "reward:  {'agent-0': 1.6533055175741893, 'agent-1': 1.6533055175741893} \n",
      "\n",
      "obs:  {'agent-0': array([-0.45435482,  0.8908208 ,  7.7918525 ], dtype=float32), 'agent-1': array([ 0.45435482, -0.8908208 ,  7.7918525 ], dtype=float32)}\n",
      "action:  {'agent-0': array([ 0.5184549, -1.       ,  1.       ], dtype=float32), 'agent-1': array([-0.55000377,  1.        ,  1.        ], dtype=float32)}\n",
      "reward:  {'agent-0': 1.999367584637631, 'agent-1': 1.999367584637631} \n",
      "\n",
      "obs:  {'agent-0': array([-0.44852456,  0.8937705 ,  5.792485  ], dtype=float32), 'agent-1': array([ 0.44852456, -0.8937705 ,  5.792485  ], dtype=float32)}\n",
      "action:  {'agent-0': array([ 0.24656296, -1.        ,  0.96329516], dtype=float32), 'agent-1': array([-0.8465425 ,  1.        ,  0.68376493], dtype=float32)}\n",
      "reward:  {'agent-0': 1.6036247212782122, 'agent-1': 1.6036247212782122} \n",
      "\n",
      "obs:  {'agent-0': array([-0.45971322,  0.8880674 ,  4.1888604 ], dtype=float32), 'agent-1': array([ 0.45971322, -0.8880674 ,  4.1888604 ], dtype=float32)}\n",
      "action:  {'agent-0': array([ 0.36702335, -0.75211805,  1.        ], dtype=float32), 'agent-1': array([-1.        ,  0.48883414,  1.        ], dtype=float32)}\n",
      "reward:  {'agent-0': 1.7350751842971932, 'agent-1': 1.7350751842971932} \n",
      "\n",
      "obs:  {'agent-0': array([-0.23992129,  0.97079235,  2.4537852 ], dtype=float32), 'agent-1': array([ 0.23992129, -0.97079235,  2.4537852 ], dtype=float32)}\n",
      "action:  {'agent-0': array([ 1.       , -1.       ,  0.9247196], dtype=float32), 'agent-1': array([-0.47703385,  1.        ,  1.        ], dtype=float32)}\n",
      "reward:  {'agent-0': 1.4907313554752741, 'agent-1': 1.4907313554752741} \n",
      "\n",
      "obs:  {'agent-0': array([0.5147315, 0.8573514, 0.9630538], dtype=float32), 'agent-1': array([-0.5147315, -0.8573514,  0.9630538], dtype=float32)}\n",
      "action:  {'agent-0': array([-1.      , -0.958294,  1.      ], dtype=float32), 'agent-1': array([1.        , 0.79119456, 0.23362559], dtype=float32)}\n",
      "reward:  {'agent-0': 1.5533979069332475, 'agent-1': 1.5533979069332475} \n",
      "\n",
      "obs:  {'agent-0': array([-0.99962795, -0.02727634,  0.4096559 ], dtype=float32), 'agent-1': array([0.99962795, 0.02727634, 0.4096559 ], dtype=float32)}\n",
      "action:  {'agent-0': array([1.       , 0.7540169, 0.1373634], dtype=float32), 'agent-1': array([-1.        ,  1.        ,  0.10824865], dtype=float32)}\n",
      "reward:  {'agent-0': 1.1863182953900577, 'agent-1': 1.1863182953900577} \n",
      "\n",
      "obs:  {'agent-0': array([-0.99974763, -0.02246614,  0.22333762], dtype=float32), 'agent-1': array([0.99974763, 0.02246614, 0.22333762], dtype=float32)}\n",
      "action:  {'agent-0': array([0.5470309, 0.9152627, 0.       ], dtype=float32), 'agent-1': array([ 0.22920477, -1.        ,  0.17412215], dtype=float32)}\n",
      "reward:  {'agent-0': 0.9137140896318069, 'agent-1': 0.9137140896318069} \n",
      "\n",
      "obs:  {'agent-0': array([-0.84677714,  0.5319478 ,  0.3096235 ], dtype=float32), 'agent-1': array([ 0.84677714, -0.5319478 ,  0.3096235 ], dtype=float32)}\n",
      "action:  {'agent-0': array([ 0.40859485, -0.20052803,  0.31935647], dtype=float32), 'agent-1': array([-0.84670174, -0.02143335,  0.45737344], dtype=float32)}\n",
      "reward:  {'agent-0': 0.826575562920339, 'agent-1': 0.826575562920339} \n",
      "\n",
      "obs:  {'agent-0': array([0.99728405, 0.07365101, 0.48304796], dtype=float32), 'agent-1': array([-0.99728405, -0.07365101,  0.48304796], dtype=float32)}\n",
      "action:  {'agent-0': array([ 0.09389639, -1.        ,  0.3488434 ], dtype=float32), 'agent-1': array([0.65122104, 0.03060663, 0.13314426], dtype=float32)}\n",
      "reward:  {'agent-0': 0.9865146533211222, 'agent-1': 0.9865146533211222} \n",
      "\n",
      "obs:  {'agent-0': array([ 0.7680256, -0.6404192,  0.4965333], dtype=float32), 'agent-1': array([-0.7680256,  0.6404192,  0.4965333], dtype=float32)}\n",
      "action:  {'agent-0': array([-0.00822723,  1.        ,  0.30663303], dtype=float32), 'agent-1': array([ 1.        , -0.5083235 ,  0.35619044], dtype=float32)}\n",
      "reward:  {'agent-0': 1.3344545786672932, 'agent-1': 1.3344545786672932} \n",
      "\n",
      "obs:  {'agent-0': array([0.37824443, 0.92570573, 0.16207872], dtype=float32), 'agent-1': array([-0.37824443, -0.92570573,  0.16207872], dtype=float32)}\n",
      "action:  {'agent-0': array([ 1., -1.,  0.], dtype=float32), 'agent-1': array([0.8896339 , 1.        , 0.10598114], dtype=float32)}\n",
      "reward:  {'agent-0': 1.0906367396954288, 'agent-1': 1.0906367396954288} \n",
      "\n",
      "obs:  {'agent-0': array([-0.12790248,  0.9917868 ,  0.07144199], dtype=float32), 'agent-1': array([ 0.12790248, -0.9917868 ,  0.07144199], dtype=float32)}\n",
      "action:  {'agent-0': array([-0.15478194, -1.        ,  0.        ], dtype=float32), 'agent-1': array([-0.61905867, -0.7208432 ,  0.        ], dtype=float32)}\n",
      "reward:  {'agent-0': 1.0, 'agent-1': 1.0} \n",
      "\n",
      "obs:  {'agent-0': array([-0.12790248,  0.9917868 ,  0.07144199], dtype=float32), 'agent-1': array([ 0.12790248, -0.9917868 ,  0.07144199], dtype=float32)}\n",
      "action:  {'agent-0': array([ 1.        , -0.16922808,  0.        ], dtype=float32), 'agent-1': array([0.08091855, 0.04517484, 0.        ], dtype=float32)}\n",
      "reward:  {'agent-0': 1.0, 'agent-1': 1.0} \n",
      "\n",
      "obs:  {'agent-0': array([-0.12790248,  0.9917868 ,  0.07144199], dtype=float32), 'agent-1': array([ 0.12790248, -0.9917868 ,  0.07144199], dtype=float32)}\n",
      "action:  {'agent-0': array([-1.        , -0.44836354,  0.        ], dtype=float32), 'agent-1': array([0.7723088 , 0.31505895, 0.        ], dtype=float32)}\n",
      "reward:  {'agent-0': 1.0, 'agent-1': 1.0} \n",
      "\n",
      "obs:  {'agent-0': array([-0.12790248,  0.9917868 ,  0.07144199], dtype=float32), 'agent-1': array([ 0.12790248, -0.9917868 ,  0.07144199], dtype=float32)}\n",
      "action:  {'agent-0': array([-0.00817412, -1.        ,  0.48356867], dtype=float32), 'agent-1': array([-0.5574116 ,  0.6079099 ,  0.28671002], dtype=float32)}\n",
      "reward:  {'agent-0': -0.5782070392673244, 'agent-1': -0.5782070392673244} \n",
      "\n",
      "obs:  {'agent-0': array([ 0.27811506, -0.96054775,  0.649649  ], dtype=float32), 'agent-1': array([-0.27811506,  0.96054775,  0.649649  ], dtype=float32)}\n",
      "action:  {'agent-0': array([0.22241807, 1.        , 0.3326051 ], dtype=float32), 'agent-1': array([ 1.        , -1.        ,  0.18771994], dtype=float32)}\n",
      "reward:  {'agent-0': 1.44423426071512, 'agent-1': 1.44423426071512} \n",
      "\n",
      "obs:  {'agent-0': array([ 0.5849234 , -0.8110885 ,  0.20541477], dtype=float32), 'agent-1': array([-0.5849234 ,  0.8110885 ,  0.20541477], dtype=float32)}\n",
      "action:  {'agent-0': array([-1.        , -0.06030774,  0.        ], dtype=float32), 'agent-1': array([ 1., -1.,  0.], dtype=float32)}\n",
      "reward:  {'agent-0': 1.0, 'agent-1': 1.0} \n",
      "\n",
      "obs:  {'agent-0': array([ 0.5849234 , -0.8110885 ,  0.20541477], dtype=float32), 'agent-1': array([-0.5849234 ,  0.8110885 ,  0.20541477], dtype=float32)}\n",
      "action:  {'agent-0': array([ 0.7301152 , -0.14625478,  0.        ], dtype=float32), 'agent-1': array([ 0.7052932 , -0.93488014,  0.24875894], dtype=float32)}\n",
      "reward:  {'agent-0': 1.1617979992636864, 'agent-1': 1.1617979992636864} \n",
      "\n",
      "obs:  {'agent-0': array([-0.68012244,  0.7330985 ,  0.04361677], dtype=float32), 'agent-1': array([ 0.68012244, -0.7330985 ,  0.04361677], dtype=float32)}\n",
      "action:  {'agent-0': array([1.       , 0.9839598, 0.8176191], dtype=float32), 'agent-1': array([0.15816367, 0.42118979, 0.        ], dtype=float32)}\n",
      "reward:  {'agent-0': -0.7764435036964044, 'agent-1': -0.7764435036964044} \n",
      "\n",
      "obs:  {'agent-0': array([0.67450464, 0.7382706 , 0.82006025], dtype=float32), 'agent-1': array([-0.67450464, -0.7382706 ,  0.82006025], dtype=float32)}\n",
      "action:  {'agent-0': array([-0.7986876 ,  0.5347936 ,  0.06736454], dtype=float32), 'agent-1': array([0.01599765, 0.56049466, 1.        ], dtype=float32)}\n",
      "reward:  {'agent-0': 0.23113055698059237, 'agent-1': 0.23113055698059237} \n",
      "\n",
      "obs:  {'agent-0': array([ 0.79573   , -0.60565156,  0.5889297 ], dtype=float32), 'agent-1': array([-0.79573   ,  0.60565156,  0.5889297 ], dtype=float32)}\n",
      "action:  {'agent-0': array([-1.        ,  0.04184282,  0.        ], dtype=float32), 'agent-1': array([ 1.       , -0.7001514,  0.       ], dtype=float32)}\n",
      "reward:  {'agent-0': 0.0, 'agent-1': 0.0} \n",
      "\n",
      "obs:  {'agent-0': array([ 0.79573   , -0.60565156,  0.5889297 ], dtype=float32), 'agent-1': array([-0.79573   ,  0.60565156,  0.5889297 ], dtype=float32)}\n",
      "action:  {'agent-0': array([-1.        ,  0.07244563,  0.        ], dtype=float32), 'agent-1': array([ 1.        , -1.        ,  0.02269915], dtype=float32)}\n",
      "reward:  {'agent-0': 0.022484967844318637, 'agent-1': 0.022484967844318637} \n",
      "\n",
      "obs:  {'agent-0': array([ 0.79898053, -0.6013569 ,  0.56644475], dtype=float32), 'agent-1': array([-0.79898053,  0.6013569 ,  0.56644475], dtype=float32)}\n",
      "action:  {'agent-0': array([-1.        ,  1.        ,  0.15107897], dtype=float32), 'agent-1': array([ 1.        , -0.5256555 ,  0.33330703], dtype=float32)}\n",
      "reward:  {'agent-0': 1.472797987668293, 'agent-1': 1.472797987668293} \n",
      "\n",
      "obs:  {'agent-0': array([ 0.54160595, -0.8406325 ,  0.09364676], dtype=float32), 'agent-1': array([-0.54160595,  0.8406325 ,  0.09364676], dtype=float32)}\n",
      "action:  {'agent-0': array([-0.97458315,  0.4275186 ,  0.        ], dtype=float32), 'agent-1': array([-0.8102484 ,  0.17263508,  0.        ], dtype=float32)}\n",
      "reward:  {'agent-0': 1.0, 'agent-1': 1.0} \n",
      "\n",
      "obs:  {'agent-0': array([ 0.54160595, -0.8406325 ,  0.09364676], dtype=float32), 'agent-1': array([-0.54160595,  0.8406325 ,  0.09364676], dtype=float32)}\n",
      "action:  {'agent-0': array([0.5435258 , 0.84047127, 0.5836774 ], dtype=float32), 'agent-1': array([-0.9905745 , -0.79861546,  0.        ], dtype=float32)}\n",
      "reward:  {'agent-0': -0.4581085388656593, 'agent-1': -0.4581085388656593} \n",
      "\n",
      "obs:  {'agent-0': array([0.6663757 , 0.74561614, 0.5517553 ], dtype=float32), 'agent-1': array([-0.6663757 , -0.74561614,  0.5517553 ], dtype=float32)}\n",
      "action:  {'agent-0': array([-1.       , -1.       ,  0.2054568], dtype=float32), 'agent-1': array([ 1.        , -0.01508552,  0.03017402], dtype=float32)}\n",
      "reward:  {'agent-0': 1.2231036740897099, 'agent-1': 1.2231036740897099} \n",
      "\n",
      "obs:  {'agent-0': array([0.5848924 , 0.8111109 , 0.32865164], dtype=float32), 'agent-1': array([-0.5848924 , -0.8111109 ,  0.32865164], dtype=float32)}\n",
      "action:  {'agent-0': array([-1.       , -0.5558251,  0.427484 ], dtype=float32), 'agent-1': array([1.        , 1.        , 0.20390978], dtype=float32)}\n",
      "reward:  {'agent-0': 0.9920595701612989, 'agent-1': 0.9920595701612989} \n",
      "\n",
      "obs:  {'agent-0': array([-0.9673599 , -0.25340652,  0.33659205], dtype=float32), 'agent-1': array([0.9673599 , 0.25340652, 0.33659205], dtype=float32)}\n",
      "action:  {'agent-0': array([ 1.       , -0.9205156,  0.       ], dtype=float32), 'agent-1': array([ 0.5983063 , -0.32849813,  0.18264729], dtype=float32)}\n",
      "reward:  {'agent-0': 0.8508765340585381, 'agent-1': 0.8508765340585381} \n",
      "\n",
      "obs:  {'agent-0': array([-0.9999856 ,  0.00537215,  0.4857155 ], dtype=float32), 'agent-1': array([ 0.9999856 , -0.00537215,  0.4857155 ], dtype=float32)}\n",
      "action:  {'agent-0': array([1.        , 0.08839989, 0.3886686 ], dtype=float32), 'agent-1': array([-0.638591  , -1.        ,  0.07449734], dtype=float32)}\n",
      "reward:  {'agent-0': 1.3702109241546583, 'agent-1': 1.3702109241546583} \n",
      "\n",
      "obs:  {'agent-0': array([-0.5060787 ,  0.8624873 ,  0.11550459], dtype=float32), 'agent-1': array([ 0.5060787 , -0.8624873 ,  0.11550459], dtype=float32)}\n",
      "action:  {'agent-0': array([ 0.50268126, -0.08223605,  0.        ], dtype=float32), 'agent-1': array([-0.9466722 ,  0.4555931 ,  0.18127996], dtype=float32)}\n",
      "reward:  {'agent-0': 1.0085279492255528, 'agent-1': 1.0085279492255528} \n",
      "\n",
      "obs:  {'agent-0': array([0.9805264 , 0.19638744, 0.10697664], dtype=float32), 'agent-1': array([-0.9805264 , -0.19638744,  0.10697664], dtype=float32)}\n",
      "action:  {'agent-0': array([-0.39089406, -1.        ,  0.        ], dtype=float32), 'agent-1': array([1.       , 0.6985618, 0.       ], dtype=float32)}\n",
      "reward:  {'agent-0': 1.0, 'agent-1': 1.0} \n",
      "\n",
      "obs:  {'agent-0': array([0.9805264 , 0.19638744, 0.10697664], dtype=float32), 'agent-1': array([-0.9805264 , -0.19638744,  0.10697664], dtype=float32)}\n",
      "action:  {'agent-0': array([-1.       ,  0.3490293,  0.       ], dtype=float32), 'agent-1': array([0.344759  , 0.44704628, 0.        ], dtype=float32)}\n",
      "reward:  {'agent-0': 1.0, 'agent-1': 1.0} \n",
      "\n",
      "obs:  {'agent-0': array([0.9805264 , 0.19638744, 0.10697664], dtype=float32), 'agent-1': array([-0.9805264 , -0.19638744,  0.10697664], dtype=float32)}\n",
      "action:  {'agent-0': array([ 0.7875941, -1.       ,  0.       ], dtype=float32), 'agent-1': array([ 0.85621333, -0.10429955,  0.        ], dtype=float32)}\n",
      "reward:  {'agent-0': 1.0, 'agent-1': 1.0} \n",
      "\n"
     ]
    }
   ],
   "source": [
    "from ray.rllib.algorithms.ppo import PPOConfig\n",
    "from ray.tune.logger import pretty_print\n",
    "\n",
    "from gymnasium.wrappers.time_limit import TimeLimit\n",
    "\n",
    "trainin_steps = 30\n",
    "\n",
    "algo = (\n",
    "    PPOConfig()\n",
    "    .training(gamma = 0.95, \n",
    "              lr = 0.001,\n",
    "              train_batch_size = 4096, \n",
    "              sgd_minibatch_size = 256, \n",
    "              num_sgd_iter = 30,\n",
    "              #entropy_coeff=0.005,\n",
    "              )\n",
    "    .env_runners(num_env_runners=1)\n",
    "    .resources(num_gpus=0)\n",
    "    .environment(env=\"KeepTheDistance?dst=0&agent=2&spawn_area=100\")\n",
    "    .build()\n",
    ")\n",
    "clear_output()\n",
    "\n",
    "out = \"\"\n",
    "for i in range(trainin_steps):\n",
    "    result = algo.train()\n",
    "    clear_output()\n",
    "    out += ppo_result_format(result) + \"\\n\"\n",
    "    print(out)\n",
    "    simulate_episode(RenderableKeepTheDistance(env_config), algo, 50, sleep_between_frames=0.08, print_info=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3bbf86ca5ff74da487ebf89baa653890",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "CanvasWithBorders(height=300, width=300)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "env_config_2 = EnvironmentConfiguration(n_agents=2, target_distance=0, max_steps=500, speed=1, spawn_area=500)\n",
    "simulate_episode(RenderableKeepTheDistance(env_config_2), algo, 200, sleep_between_frames=0.03, print_info=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "An Algorithm checkpoint has been created inside directory: 'TrainingResult(checkpoint=Checkpoint(filesystem=local, path=/mnt/c/Users/nicol/Desktop/Università/tesi/experiments/RL_experiments/algos/KeepTheDistance?dst=0&agent=2&spawn_area=100), metrics={'custom_metrics': {}, 'episode_media': {}, 'info': {'learner': {'default_policy': {'learner_stats': {'allreduce_latency': 0.0, 'grad_gnorm': 5.732406087343891, 'cur_kl_coeff': 0.6750000000000002, 'cur_lr': 0.0010000000000000005, 'total_loss': 2.0669429610628867, 'policy_loss': -0.009641030859590198, 'vf_loss': 2.0618812701043985, 'vf_explained_var': 0.41663303778817257, 'kl': 0.021781805181126438, 'entropy': 3.2973313165207703, 'entropy_coeff': 0.0}, 'model': {}, 'custom_metrics': {}, 'num_agent_steps_trained': 256.0, 'num_grad_updates_lifetime': 28320.5, 'diff_num_grad_updates_vs_sampler_policy': 479.5}}, 'num_env_steps_sampled': 122880, 'num_env_steps_trained': 122880, 'num_agent_steps_sampled': 245760, 'num_agent_steps_trained': 245760}, 'sampler_results': {'episode_reward_max': 656.3426232576753, 'episode_reward_min': 554.0001613174567, 'episode_reward_mean': 603.4171706591706, 'episode_len_mean': 300.0, 'episode_media': {}, 'episodes_this_iter': 14, 'episodes_timesteps_total': 30000, 'policy_reward_min': {}, 'policy_reward_max': {}, 'policy_reward_mean': {}, 'custom_metrics': {}, 'hist_stats': {'episode_reward': [557.9595628590914, 598.6723485242048, 576.7905364410914, 623.9128949566787, 593.9106186498275, 569.062897996119, 590.2013885065847, 589.5060260096283, 597.8507385469798, 603.3792850540186, 569.6136375997933, 644.0588800895705, 593.328203558257, 570.5709290327773, 571.9257168448096, 588.9998994945554, 598.7589479373542, 596.5288003010861, 570.1350830746087, 568.0480647727866, 620.0024518546635, 576.4197241751825, 601.7157758808075, 584.2411923236373, 604.9850591253866, 554.0001613174567, 611.4221957176887, 584.6438986226336, 576.1361015052117, 589.8023402153758, 626.6456881142497, 572.008304005685, 591.9557121248316, 612.8474983675018, 609.5166032129912, 603.8943007676796, 636.1130024943114, 611.9740266708382, 602.5425770692818, 628.1434802365283, 610.6536786191289, 609.0682140526151, 578.0150787062627, 625.5863911032764, 588.5307544495234, 622.7886128417579, 621.1413480448987, 632.7839459952713, 610.1124621743139, 592.1165754452259, 611.1495018386091, 583.7511119350513, 617.5665974881094, 616.9876188074414, 568.6591477732395, 619.8716461880304, 594.7810736546629, 611.3439781196436, 593.3561105262047, 591.1017537228189, 625.1183979513283, 632.7680447687559, 614.8452452281645, 627.5385971308539, 596.5194669359113, 592.5112024770766, 573.9262997589445, 630.0462283258863, 587.5262928094561, 587.3002309029821, 649.5510743605195, 647.6793445327738, 587.8095600275365, 601.3975749074691, 584.5722420902252, 595.9262462625873, 606.7030501305279, 625.70331948732, 623.5837370558404, 607.4836750975663, 589.7268695213564, 617.6180812142297, 603.5783583390864, 599.7037170633893, 603.8087675304085, 649.9313244777413, 588.2463279765618, 638.4685068391265, 611.2580967862943, 637.421943401637, 621.1299963854806, 591.8926883816252, 583.5864293679192, 606.1657572522845, 656.3426232576753, 625.6637389284845, 598.4456045648764, 630.5142560957363, 610.6707109474588, 607.4432818060959], 'episode_lengths': [300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300]}, 'sampler_perf': {'mean_raw_obs_processing_ms': 0.34054384452797193, 'mean_inference_ms': 0.8364510221606176, 'mean_action_processing_ms': 0.21899213523915542, 'mean_env_wait_ms': 0.3332731499742183, 'mean_env_render_ms': 0.0}, 'num_faulty_episodes': 0, 'connector_metrics': {'ObsPreprocessorConnector_ms': 0.0053632259368896484, 'StateBufferConnector_ms': 0.0042836666107177734, 'ViewRequirementAgentConnector_ms': 0.12398958206176758}, 'num_episodes': 14, 'episode_return_max': 656.3426232576753, 'episode_return_min': 554.0001613174567, 'episode_return_mean': 603.4171706591706}, 'env_runner_results': {'episode_reward_max': 656.3426232576753, 'episode_reward_min': 554.0001613174567, 'episode_reward_mean': 603.4171706591706, 'episode_len_mean': 300.0, 'episode_media': {}, 'episodes_this_iter': 14, 'episodes_timesteps_total': 30000, 'policy_reward_min': {}, 'policy_reward_max': {}, 'policy_reward_mean': {}, 'custom_metrics': {}, 'hist_stats': {'episode_reward': [557.9595628590914, 598.6723485242048, 576.7905364410914, 623.9128949566787, 593.9106186498275, 569.062897996119, 590.2013885065847, 589.5060260096283, 597.8507385469798, 603.3792850540186, 569.6136375997933, 644.0588800895705, 593.328203558257, 570.5709290327773, 571.9257168448096, 588.9998994945554, 598.7589479373542, 596.5288003010861, 570.1350830746087, 568.0480647727866, 620.0024518546635, 576.4197241751825, 601.7157758808075, 584.2411923236373, 604.9850591253866, 554.0001613174567, 611.4221957176887, 584.6438986226336, 576.1361015052117, 589.8023402153758, 626.6456881142497, 572.008304005685, 591.9557121248316, 612.8474983675018, 609.5166032129912, 603.8943007676796, 636.1130024943114, 611.9740266708382, 602.5425770692818, 628.1434802365283, 610.6536786191289, 609.0682140526151, 578.0150787062627, 625.5863911032764, 588.5307544495234, 622.7886128417579, 621.1413480448987, 632.7839459952713, 610.1124621743139, 592.1165754452259, 611.1495018386091, 583.7511119350513, 617.5665974881094, 616.9876188074414, 568.6591477732395, 619.8716461880304, 594.7810736546629, 611.3439781196436, 593.3561105262047, 591.1017537228189, 625.1183979513283, 632.7680447687559, 614.8452452281645, 627.5385971308539, 596.5194669359113, 592.5112024770766, 573.9262997589445, 630.0462283258863, 587.5262928094561, 587.3002309029821, 649.5510743605195, 647.6793445327738, 587.8095600275365, 601.3975749074691, 584.5722420902252, 595.9262462625873, 606.7030501305279, 625.70331948732, 623.5837370558404, 607.4836750975663, 589.7268695213564, 617.6180812142297, 603.5783583390864, 599.7037170633893, 603.8087675304085, 649.9313244777413, 588.2463279765618, 638.4685068391265, 611.2580967862943, 637.421943401637, 621.1299963854806, 591.8926883816252, 583.5864293679192, 606.1657572522845, 656.3426232576753, 625.6637389284845, 598.4456045648764, 630.5142560957363, 610.6707109474588, 607.4432818060959], 'episode_lengths': [300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300]}, 'sampler_perf': {'mean_raw_obs_processing_ms': 0.34054384452797193, 'mean_inference_ms': 0.8364510221606176, 'mean_action_processing_ms': 0.21899213523915542, 'mean_env_wait_ms': 0.3332731499742183, 'mean_env_render_ms': 0.0}, 'num_faulty_episodes': 0, 'connector_metrics': {'ObsPreprocessorConnector_ms': 0.0053632259368896484, 'StateBufferConnector_ms': 0.0042836666107177734, 'ViewRequirementAgentConnector_ms': 0.12398958206176758}, 'num_episodes': 14, 'episode_return_max': 656.3426232576753, 'episode_return_min': 554.0001613174567, 'episode_return_mean': 603.4171706591706}, 'episode_reward_max': 656.3426232576753, 'episode_reward_min': 554.0001613174567, 'episode_reward_mean': 603.4171706591706, 'episode_len_mean': 300.0, 'episodes_this_iter': 14, 'episodes_timesteps_total': 30000, 'policy_reward_min': {}, 'policy_reward_max': {}, 'policy_reward_mean': {}, 'hist_stats': {'episode_reward': [557.9595628590914, 598.6723485242048, 576.7905364410914, 623.9128949566787, 593.9106186498275, 569.062897996119, 590.2013885065847, 589.5060260096283, 597.8507385469798, 603.3792850540186, 569.6136375997933, 644.0588800895705, 593.328203558257, 570.5709290327773, 571.9257168448096, 588.9998994945554, 598.7589479373542, 596.5288003010861, 570.1350830746087, 568.0480647727866, 620.0024518546635, 576.4197241751825, 601.7157758808075, 584.2411923236373, 604.9850591253866, 554.0001613174567, 611.4221957176887, 584.6438986226336, 576.1361015052117, 589.8023402153758, 626.6456881142497, 572.008304005685, 591.9557121248316, 612.8474983675018, 609.5166032129912, 603.8943007676796, 636.1130024943114, 611.9740266708382, 602.5425770692818, 628.1434802365283, 610.6536786191289, 609.0682140526151, 578.0150787062627, 625.5863911032764, 588.5307544495234, 622.7886128417579, 621.1413480448987, 632.7839459952713, 610.1124621743139, 592.1165754452259, 611.1495018386091, 583.7511119350513, 617.5665974881094, 616.9876188074414, 568.6591477732395, 619.8716461880304, 594.7810736546629, 611.3439781196436, 593.3561105262047, 591.1017537228189, 625.1183979513283, 632.7680447687559, 614.8452452281645, 627.5385971308539, 596.5194669359113, 592.5112024770766, 573.9262997589445, 630.0462283258863, 587.5262928094561, 587.3002309029821, 649.5510743605195, 647.6793445327738, 587.8095600275365, 601.3975749074691, 584.5722420902252, 595.9262462625873, 606.7030501305279, 625.70331948732, 623.5837370558404, 607.4836750975663, 589.7268695213564, 617.6180812142297, 603.5783583390864, 599.7037170633893, 603.8087675304085, 649.9313244777413, 588.2463279765618, 638.4685068391265, 611.2580967862943, 637.421943401637, 621.1299963854806, 591.8926883816252, 583.5864293679192, 606.1657572522845, 656.3426232576753, 625.6637389284845, 598.4456045648764, 630.5142560957363, 610.6707109474588, 607.4432818060959], 'episode_lengths': [300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300]}, 'sampler_perf': {'mean_raw_obs_processing_ms': 0.34054384452797193, 'mean_inference_ms': 0.8364510221606176, 'mean_action_processing_ms': 0.21899213523915542, 'mean_env_wait_ms': 0.3332731499742183, 'mean_env_render_ms': 0.0}, 'num_faulty_episodes': 0, 'connector_metrics': {'ObsPreprocessorConnector_ms': 0.0053632259368896484, 'StateBufferConnector_ms': 0.0042836666107177734, 'ViewRequirementAgentConnector_ms': 0.12398958206176758}, 'num_episodes': 14, 'episode_return_max': 656.3426232576753, 'episode_return_min': 554.0001613174567, 'episode_return_mean': 603.4171706591706, 'num_healthy_workers': 1, 'num_in_flight_async_reqs': 0, 'num_remote_worker_restarts': 0, 'num_agent_steps_sampled': 245760, 'num_agent_steps_trained': 245760, 'num_env_steps_sampled': 122880, 'num_env_steps_trained': 122880, 'num_env_steps_sampled_this_iter': 4096, 'num_env_steps_trained_this_iter': 4096, 'num_env_steps_sampled_throughput_per_sec': 312.59178580771174, 'num_env_steps_trained_throughput_per_sec': 312.59178580771174, 'timesteps_total': 122880, 'num_env_steps_sampled_lifetime': 122880, 'num_agent_steps_sampled_lifetime': 245760, 'num_steps_trained_this_iter': 4096, 'agent_timesteps_total': 245760, 'timers': {'training_iteration_time_ms': 13107.549, 'restore_workers_time_ms': 0.021, 'training_step_time_ms': 13107.482, 'sample_time_ms': 6865.865, 'load_time_ms': 0.603, 'load_throughput': 6788315.625, 'learn_time_ms': 6236.751, 'learn_throughput': 656.752, 'synch_weights_time_ms': 3.669}, 'counters': {'num_env_steps_sampled': 122880, 'num_env_steps_trained': 122880, 'num_agent_steps_sampled': 245760, 'num_agent_steps_trained': 245760}, 'done': False, 'episodes_total': 409, 'training_iteration': 30, 'trial_id': 'default', 'date': '2024-05-23_10-17-00', 'timestamp': 1716452220, 'time_this_iter_s': 13.10737919807434, 'time_total_s': 406.53645038604736, 'pid': 3351, 'hostname': 'LAPTOP-9AD2MD1C', 'node_ip': '172.23.87.14', 'config': {'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, '_fake_gpus': False, 'num_learner_workers': 0, 'num_gpus_per_learner_worker': 0, 'num_cpus_per_learner_worker': 1, 'local_gpu_idx': 0, 'custom_resources_per_worker': {}, 'placement_strategy': 'PACK', 'eager_tracing': True, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'torch_compile_learner': False, 'torch_compile_learner_what_to_compile': <TorchCompileWhatToCompile.FORWARD_TRAIN: 'forward_train'>, 'torch_compile_learner_dynamo_backend': 'inductor', 'torch_compile_learner_dynamo_mode': None, 'torch_compile_worker': False, 'torch_compile_worker_dynamo_backend': 'onnxrt', 'torch_compile_worker_dynamo_mode': None, 'enable_rl_module_and_learner': False, 'enable_env_runner_and_connector_v2': False, 'env': 'KeepTheDistance?dst=0&agent=2&spawn_area=100', 'env_config': {}, 'observation_space': None, 'action_space': None, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, '_is_atari': None, 'env_task_fn': None, 'render_env': False, 'action_mask_key': 'action_mask', 'env_runner_cls': None, 'num_envs_per_env_runner': 1, 'validate_env_runners_after_construction': True, 'sample_timeout_s': 60.0, '_env_to_module_connector': None, 'add_default_connectors_to_env_to_module_pipeline': True, '_module_to_env_connector': None, 'add_default_connectors_to_module_to_env_pipeline': True, 'episode_lookback_horizon': 1, 'rollout_fragment_length': 'auto', 'batch_mode': 'truncate_episodes', 'compress_observations': False, 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'enable_tf1_exec_eagerly': False, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'update_worker_filter_stats': True, 'use_worker_filter_stats': True, 'enable_connectors': True, 'sampler_perf_stats_ema_coef': None, 'gamma': 0.95, 'lr': 0.001, 'grad_clip': None, 'grad_clip_by': 'global_norm', 'train_batch_size': 4096, 'train_batch_size_per_learner': None, 'model': {'_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'fcnet_weights_initializer': None, 'fcnet_weights_initializer_config': None, 'fcnet_bias_initializer': None, 'fcnet_bias_initializer_config': None, 'conv_filters': None, 'conv_activation': 'relu', 'conv_kernel_initializer': None, 'conv_kernel_initializer_config': None, 'conv_bias_initializer': None, 'conv_bias_initializer_config': None, 'conv_transpose_kernel_initializer': None, 'conv_transpose_kernel_initializer_config': None, 'conv_transpose_bias_initializer': None, 'conv_transpose_bias_initializer_config': None, 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'post_fcnet_weights_initializer': None, 'post_fcnet_weights_initializer_config': None, 'post_fcnet_bias_initializer': None, 'post_fcnet_bias_initializer_config': None, 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, 'lstm_weights_initializer': None, 'lstm_weights_initializer_config': None, 'lstm_bias_initializer': None, 'lstm_bias_initializer_config': None, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'encoder_latent_dim': None, 'always_check_shapes': False, 'lstm_use_prev_action_reward': -1, '_use_default_native_models': -1}, '_learner_connector': None, 'add_default_connectors_to_learner_pipeline': True, 'optimizer': {}, 'max_requests_in_flight_per_sampler_worker': 2, '_learner_class': None, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'algorithm_config_overrides_per_module': {}, '_per_module_overrides': {}, 'count_steps_by': 'env_steps', 'policy_map_capacity': 100, 'policy_mapping_fn': <function AlgorithmConfig.DEFAULT_POLICY_MAPPING_FN at 0x7fb8255f62a0>, 'policies_to_train': None, 'policy_states_are_swappable': False, 'observation_fn': None, 'input_config': {}, 'actions_in_input_normalized': False, 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_config': {}, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'offline_sampling': False, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_sample_timeout_s': 120.0, 'evaluation_parallel_to_training': False, 'evaluation_force_reset_envs_before_iteration': True, 'evaluation_config': None, 'off_policy_estimation_methods': {}, 'ope_split_batch_by_episode': True, 'evaluation_num_env_runners': 0, 'always_attach_evaluation_results': True, 'in_evaluation': False, 'sync_filters_on_rollout_workers_timeout_s': 10.0, 'keep_per_episode_custom_metrics': False, 'metrics_episode_collection_timeout_s': 60.0, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_iteration': None, 'min_train_timesteps_per_iteration': 0, 'min_sample_timesteps_per_iteration': 0, 'export_native_model_files': False, 'checkpoint_trainable_policies_only': False, 'logger_creator': None, 'logger_config': None, 'log_level': 'WARN', 'log_sys_usage': True, 'fake_sampler': False, 'seed': None, '_run_training_always_in_thread': False, '_evaluation_parallel_to_training_wo_thread': False, 'ignore_env_runner_failures': False, 'recreate_failed_env_runners': False, 'max_num_env_runner_restarts': 1000, 'delay_between_env_runner_restarts_s': 60.0, 'restart_failed_sub_environments': False, 'num_consecutive_env_runner_failures_tolerance': 100, 'env_runner_health_probe_timeout_s': 30, 'env_runner_restore_timeout_s': 1800, '_model_config_dict': {}, '_rl_module_spec': None, '_AlgorithmConfig__prior_exploration_config': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_initialize_loss_from_dummy_batch': False, 'simple_optimizer': False, 'policy_map_cache': -1, 'worker_cls': -1, 'synchronize_filters': -1, 'enable_async_evaluation': -1, 'custom_async_evaluation_function': -1, '_enable_rl_module_api': -1, 'auto_wrap_old_gym_envs': -1, 'disable_env_checking': -1, 'replay_sequence_length': None, '_disable_execution_plan_api': -1, 'lr_schedule': None, 'use_critic': True, 'use_gae': True, 'use_kl_loss': True, 'kl_coeff': 0.2, 'kl_target': 0.01, 'sgd_minibatch_size': 256, 'mini_batch_size_per_learner': None, 'num_sgd_iter': 30, 'shuffle_sequences': True, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.0, 'entropy_coeff_schedule': None, 'clip_param': 0.3, 'vf_clip_param': 10.0, 'vf_share_layers': -1, 'lambda': 1.0, 'input': 'sampler', 'policies': {'default_policy': (None, None, None, None)}, 'callbacks': <class 'ray.rllib.algorithms.callbacks.DefaultCallbacks'>, 'create_env_on_driver': False, 'custom_eval_function': None, 'framework': 'torch', 'num_cpus_for_driver': 1, 'num_workers': 1}, 'time_since_restore': 406.53645038604736, 'iterations_since_restore': 30, 'perf': {'cpu_util_percent': 29.76, 'ram_util_percent': 87.492}})'.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainingResult(checkpoint=Checkpoint(filesystem=local, path=/mnt/c/Users/nicol/Desktop/Università/tesi/experiments/RL_experiments/algos/KeepTheDistance?dst=0&agent=2&spawn_area=100), metrics={'custom_metrics': {}, 'episode_media': {}, 'info': {'learner': {'default_policy': {'learner_stats': {'allreduce_latency': 0.0, 'grad_gnorm': 5.732406087343891, 'cur_kl_coeff': 0.6750000000000002, 'cur_lr': 0.0010000000000000005, 'total_loss': 2.0669429610628867, 'policy_loss': -0.009641030859590198, 'vf_loss': 2.0618812701043985, 'vf_explained_var': 0.41663303778817257, 'kl': 0.021781805181126438, 'entropy': 3.2973313165207703, 'entropy_coeff': 0.0}, 'model': {}, 'custom_metrics': {}, 'num_agent_steps_trained': 256.0, 'num_grad_updates_lifetime': 28320.5, 'diff_num_grad_updates_vs_sampler_policy': 479.5}}, 'num_env_steps_sampled': 122880, 'num_env_steps_trained': 122880, 'num_agent_steps_sampled': 245760, 'num_agent_steps_trained': 245760}, 'sampler_results': {'episode_reward_max': 656.3426232576753, 'episode_reward_min': 554.0001613174567, 'episode_reward_mean': 603.4171706591706, 'episode_len_mean': 300.0, 'episode_media': {}, 'episodes_this_iter': 14, 'episodes_timesteps_total': 30000, 'policy_reward_min': {}, 'policy_reward_max': {}, 'policy_reward_mean': {}, 'custom_metrics': {}, 'hist_stats': {'episode_reward': [557.9595628590914, 598.6723485242048, 576.7905364410914, 623.9128949566787, 593.9106186498275, 569.062897996119, 590.2013885065847, 589.5060260096283, 597.8507385469798, 603.3792850540186, 569.6136375997933, 644.0588800895705, 593.328203558257, 570.5709290327773, 571.9257168448096, 588.9998994945554, 598.7589479373542, 596.5288003010861, 570.1350830746087, 568.0480647727866, 620.0024518546635, 576.4197241751825, 601.7157758808075, 584.2411923236373, 604.9850591253866, 554.0001613174567, 611.4221957176887, 584.6438986226336, 576.1361015052117, 589.8023402153758, 626.6456881142497, 572.008304005685, 591.9557121248316, 612.8474983675018, 609.5166032129912, 603.8943007676796, 636.1130024943114, 611.9740266708382, 602.5425770692818, 628.1434802365283, 610.6536786191289, 609.0682140526151, 578.0150787062627, 625.5863911032764, 588.5307544495234, 622.7886128417579, 621.1413480448987, 632.7839459952713, 610.1124621743139, 592.1165754452259, 611.1495018386091, 583.7511119350513, 617.5665974881094, 616.9876188074414, 568.6591477732395, 619.8716461880304, 594.7810736546629, 611.3439781196436, 593.3561105262047, 591.1017537228189, 625.1183979513283, 632.7680447687559, 614.8452452281645, 627.5385971308539, 596.5194669359113, 592.5112024770766, 573.9262997589445, 630.0462283258863, 587.5262928094561, 587.3002309029821, 649.5510743605195, 647.6793445327738, 587.8095600275365, 601.3975749074691, 584.5722420902252, 595.9262462625873, 606.7030501305279, 625.70331948732, 623.5837370558404, 607.4836750975663, 589.7268695213564, 617.6180812142297, 603.5783583390864, 599.7037170633893, 603.8087675304085, 649.9313244777413, 588.2463279765618, 638.4685068391265, 611.2580967862943, 637.421943401637, 621.1299963854806, 591.8926883816252, 583.5864293679192, 606.1657572522845, 656.3426232576753, 625.6637389284845, 598.4456045648764, 630.5142560957363, 610.6707109474588, 607.4432818060959], 'episode_lengths': [300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300]}, 'sampler_perf': {'mean_raw_obs_processing_ms': 0.34054384452797193, 'mean_inference_ms': 0.8364510221606176, 'mean_action_processing_ms': 0.21899213523915542, 'mean_env_wait_ms': 0.3332731499742183, 'mean_env_render_ms': 0.0}, 'num_faulty_episodes': 0, 'connector_metrics': {'ObsPreprocessorConnector_ms': 0.0053632259368896484, 'StateBufferConnector_ms': 0.0042836666107177734, 'ViewRequirementAgentConnector_ms': 0.12398958206176758}, 'num_episodes': 14, 'episode_return_max': 656.3426232576753, 'episode_return_min': 554.0001613174567, 'episode_return_mean': 603.4171706591706}, 'env_runner_results': {'episode_reward_max': 656.3426232576753, 'episode_reward_min': 554.0001613174567, 'episode_reward_mean': 603.4171706591706, 'episode_len_mean': 300.0, 'episode_media': {}, 'episodes_this_iter': 14, 'episodes_timesteps_total': 30000, 'policy_reward_min': {}, 'policy_reward_max': {}, 'policy_reward_mean': {}, 'custom_metrics': {}, 'hist_stats': {'episode_reward': [557.9595628590914, 598.6723485242048, 576.7905364410914, 623.9128949566787, 593.9106186498275, 569.062897996119, 590.2013885065847, 589.5060260096283, 597.8507385469798, 603.3792850540186, 569.6136375997933, 644.0588800895705, 593.328203558257, 570.5709290327773, 571.9257168448096, 588.9998994945554, 598.7589479373542, 596.5288003010861, 570.1350830746087, 568.0480647727866, 620.0024518546635, 576.4197241751825, 601.7157758808075, 584.2411923236373, 604.9850591253866, 554.0001613174567, 611.4221957176887, 584.6438986226336, 576.1361015052117, 589.8023402153758, 626.6456881142497, 572.008304005685, 591.9557121248316, 612.8474983675018, 609.5166032129912, 603.8943007676796, 636.1130024943114, 611.9740266708382, 602.5425770692818, 628.1434802365283, 610.6536786191289, 609.0682140526151, 578.0150787062627, 625.5863911032764, 588.5307544495234, 622.7886128417579, 621.1413480448987, 632.7839459952713, 610.1124621743139, 592.1165754452259, 611.1495018386091, 583.7511119350513, 617.5665974881094, 616.9876188074414, 568.6591477732395, 619.8716461880304, 594.7810736546629, 611.3439781196436, 593.3561105262047, 591.1017537228189, 625.1183979513283, 632.7680447687559, 614.8452452281645, 627.5385971308539, 596.5194669359113, 592.5112024770766, 573.9262997589445, 630.0462283258863, 587.5262928094561, 587.3002309029821, 649.5510743605195, 647.6793445327738, 587.8095600275365, 601.3975749074691, 584.5722420902252, 595.9262462625873, 606.7030501305279, 625.70331948732, 623.5837370558404, 607.4836750975663, 589.7268695213564, 617.6180812142297, 603.5783583390864, 599.7037170633893, 603.8087675304085, 649.9313244777413, 588.2463279765618, 638.4685068391265, 611.2580967862943, 637.421943401637, 621.1299963854806, 591.8926883816252, 583.5864293679192, 606.1657572522845, 656.3426232576753, 625.6637389284845, 598.4456045648764, 630.5142560957363, 610.6707109474588, 607.4432818060959], 'episode_lengths': [300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300]}, 'sampler_perf': {'mean_raw_obs_processing_ms': 0.34054384452797193, 'mean_inference_ms': 0.8364510221606176, 'mean_action_processing_ms': 0.21899213523915542, 'mean_env_wait_ms': 0.3332731499742183, 'mean_env_render_ms': 0.0}, 'num_faulty_episodes': 0, 'connector_metrics': {'ObsPreprocessorConnector_ms': 0.0053632259368896484, 'StateBufferConnector_ms': 0.0042836666107177734, 'ViewRequirementAgentConnector_ms': 0.12398958206176758}, 'num_episodes': 14, 'episode_return_max': 656.3426232576753, 'episode_return_min': 554.0001613174567, 'episode_return_mean': 603.4171706591706}, 'episode_reward_max': 656.3426232576753, 'episode_reward_min': 554.0001613174567, 'episode_reward_mean': 603.4171706591706, 'episode_len_mean': 300.0, 'episodes_this_iter': 14, 'episodes_timesteps_total': 30000, 'policy_reward_min': {}, 'policy_reward_max': {}, 'policy_reward_mean': {}, 'hist_stats': {'episode_reward': [557.9595628590914, 598.6723485242048, 576.7905364410914, 623.9128949566787, 593.9106186498275, 569.062897996119, 590.2013885065847, 589.5060260096283, 597.8507385469798, 603.3792850540186, 569.6136375997933, 644.0588800895705, 593.328203558257, 570.5709290327773, 571.9257168448096, 588.9998994945554, 598.7589479373542, 596.5288003010861, 570.1350830746087, 568.0480647727866, 620.0024518546635, 576.4197241751825, 601.7157758808075, 584.2411923236373, 604.9850591253866, 554.0001613174567, 611.4221957176887, 584.6438986226336, 576.1361015052117, 589.8023402153758, 626.6456881142497, 572.008304005685, 591.9557121248316, 612.8474983675018, 609.5166032129912, 603.8943007676796, 636.1130024943114, 611.9740266708382, 602.5425770692818, 628.1434802365283, 610.6536786191289, 609.0682140526151, 578.0150787062627, 625.5863911032764, 588.5307544495234, 622.7886128417579, 621.1413480448987, 632.7839459952713, 610.1124621743139, 592.1165754452259, 611.1495018386091, 583.7511119350513, 617.5665974881094, 616.9876188074414, 568.6591477732395, 619.8716461880304, 594.7810736546629, 611.3439781196436, 593.3561105262047, 591.1017537228189, 625.1183979513283, 632.7680447687559, 614.8452452281645, 627.5385971308539, 596.5194669359113, 592.5112024770766, 573.9262997589445, 630.0462283258863, 587.5262928094561, 587.3002309029821, 649.5510743605195, 647.6793445327738, 587.8095600275365, 601.3975749074691, 584.5722420902252, 595.9262462625873, 606.7030501305279, 625.70331948732, 623.5837370558404, 607.4836750975663, 589.7268695213564, 617.6180812142297, 603.5783583390864, 599.7037170633893, 603.8087675304085, 649.9313244777413, 588.2463279765618, 638.4685068391265, 611.2580967862943, 637.421943401637, 621.1299963854806, 591.8926883816252, 583.5864293679192, 606.1657572522845, 656.3426232576753, 625.6637389284845, 598.4456045648764, 630.5142560957363, 610.6707109474588, 607.4432818060959], 'episode_lengths': [300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300]}, 'sampler_perf': {'mean_raw_obs_processing_ms': 0.34054384452797193, 'mean_inference_ms': 0.8364510221606176, 'mean_action_processing_ms': 0.21899213523915542, 'mean_env_wait_ms': 0.3332731499742183, 'mean_env_render_ms': 0.0}, 'num_faulty_episodes': 0, 'connector_metrics': {'ObsPreprocessorConnector_ms': 0.0053632259368896484, 'StateBufferConnector_ms': 0.0042836666107177734, 'ViewRequirementAgentConnector_ms': 0.12398958206176758}, 'num_episodes': 14, 'episode_return_max': 656.3426232576753, 'episode_return_min': 554.0001613174567, 'episode_return_mean': 603.4171706591706, 'num_healthy_workers': 1, 'num_in_flight_async_reqs': 0, 'num_remote_worker_restarts': 0, 'num_agent_steps_sampled': 245760, 'num_agent_steps_trained': 245760, 'num_env_steps_sampled': 122880, 'num_env_steps_trained': 122880, 'num_env_steps_sampled_this_iter': 4096, 'num_env_steps_trained_this_iter': 4096, 'num_env_steps_sampled_throughput_per_sec': 312.59178580771174, 'num_env_steps_trained_throughput_per_sec': 312.59178580771174, 'timesteps_total': 122880, 'num_env_steps_sampled_lifetime': 122880, 'num_agent_steps_sampled_lifetime': 245760, 'num_steps_trained_this_iter': 4096, 'agent_timesteps_total': 245760, 'timers': {'training_iteration_time_ms': 13107.549, 'restore_workers_time_ms': 0.021, 'training_step_time_ms': 13107.482, 'sample_time_ms': 6865.865, 'load_time_ms': 0.603, 'load_throughput': 6788315.625, 'learn_time_ms': 6236.751, 'learn_throughput': 656.752, 'synch_weights_time_ms': 3.669}, 'counters': {'num_env_steps_sampled': 122880, 'num_env_steps_trained': 122880, 'num_agent_steps_sampled': 245760, 'num_agent_steps_trained': 245760}, 'done': False, 'episodes_total': 409, 'training_iteration': 30, 'trial_id': 'default', 'date': '2024-05-23_10-17-00', 'timestamp': 1716452220, 'time_this_iter_s': 13.10737919807434, 'time_total_s': 406.53645038604736, 'pid': 3351, 'hostname': 'LAPTOP-9AD2MD1C', 'node_ip': '172.23.87.14', 'config': {'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, '_fake_gpus': False, 'num_learner_workers': 0, 'num_gpus_per_learner_worker': 0, 'num_cpus_per_learner_worker': 1, 'local_gpu_idx': 0, 'custom_resources_per_worker': {}, 'placement_strategy': 'PACK', 'eager_tracing': True, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'torch_compile_learner': False, 'torch_compile_learner_what_to_compile': <TorchCompileWhatToCompile.FORWARD_TRAIN: 'forward_train'>, 'torch_compile_learner_dynamo_backend': 'inductor', 'torch_compile_learner_dynamo_mode': None, 'torch_compile_worker': False, 'torch_compile_worker_dynamo_backend': 'onnxrt', 'torch_compile_worker_dynamo_mode': None, 'enable_rl_module_and_learner': False, 'enable_env_runner_and_connector_v2': False, 'env': 'KeepTheDistance?dst=0&agent=2&spawn_area=100', 'env_config': {}, 'observation_space': None, 'action_space': None, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, '_is_atari': None, 'env_task_fn': None, 'render_env': False, 'action_mask_key': 'action_mask', 'env_runner_cls': None, 'num_envs_per_env_runner': 1, 'validate_env_runners_after_construction': True, 'sample_timeout_s': 60.0, '_env_to_module_connector': None, 'add_default_connectors_to_env_to_module_pipeline': True, '_module_to_env_connector': None, 'add_default_connectors_to_module_to_env_pipeline': True, 'episode_lookback_horizon': 1, 'rollout_fragment_length': 'auto', 'batch_mode': 'truncate_episodes', 'compress_observations': False, 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'enable_tf1_exec_eagerly': False, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'update_worker_filter_stats': True, 'use_worker_filter_stats': True, 'enable_connectors': True, 'sampler_perf_stats_ema_coef': None, 'gamma': 0.95, 'lr': 0.001, 'grad_clip': None, 'grad_clip_by': 'global_norm', 'train_batch_size': 4096, 'train_batch_size_per_learner': None, 'model': {'_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'fcnet_weights_initializer': None, 'fcnet_weights_initializer_config': None, 'fcnet_bias_initializer': None, 'fcnet_bias_initializer_config': None, 'conv_filters': None, 'conv_activation': 'relu', 'conv_kernel_initializer': None, 'conv_kernel_initializer_config': None, 'conv_bias_initializer': None, 'conv_bias_initializer_config': None, 'conv_transpose_kernel_initializer': None, 'conv_transpose_kernel_initializer_config': None, 'conv_transpose_bias_initializer': None, 'conv_transpose_bias_initializer_config': None, 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'post_fcnet_weights_initializer': None, 'post_fcnet_weights_initializer_config': None, 'post_fcnet_bias_initializer': None, 'post_fcnet_bias_initializer_config': None, 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, 'lstm_weights_initializer': None, 'lstm_weights_initializer_config': None, 'lstm_bias_initializer': None, 'lstm_bias_initializer_config': None, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'encoder_latent_dim': None, 'always_check_shapes': False, 'lstm_use_prev_action_reward': -1, '_use_default_native_models': -1}, '_learner_connector': None, 'add_default_connectors_to_learner_pipeline': True, 'optimizer': {}, 'max_requests_in_flight_per_sampler_worker': 2, '_learner_class': None, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'algorithm_config_overrides_per_module': {}, '_per_module_overrides': {}, 'count_steps_by': 'env_steps', 'policy_map_capacity': 100, 'policy_mapping_fn': <function AlgorithmConfig.DEFAULT_POLICY_MAPPING_FN at 0x7fb8255f62a0>, 'policies_to_train': None, 'policy_states_are_swappable': False, 'observation_fn': None, 'input_config': {}, 'actions_in_input_normalized': False, 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_config': {}, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'offline_sampling': False, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_sample_timeout_s': 120.0, 'evaluation_parallel_to_training': False, 'evaluation_force_reset_envs_before_iteration': True, 'evaluation_config': None, 'off_policy_estimation_methods': {}, 'ope_split_batch_by_episode': True, 'evaluation_num_env_runners': 0, 'always_attach_evaluation_results': True, 'in_evaluation': False, 'sync_filters_on_rollout_workers_timeout_s': 10.0, 'keep_per_episode_custom_metrics': False, 'metrics_episode_collection_timeout_s': 60.0, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_iteration': None, 'min_train_timesteps_per_iteration': 0, 'min_sample_timesteps_per_iteration': 0, 'export_native_model_files': False, 'checkpoint_trainable_policies_only': False, 'logger_creator': None, 'logger_config': None, 'log_level': 'WARN', 'log_sys_usage': True, 'fake_sampler': False, 'seed': None, '_run_training_always_in_thread': False, '_evaluation_parallel_to_training_wo_thread': False, 'ignore_env_runner_failures': False, 'recreate_failed_env_runners': False, 'max_num_env_runner_restarts': 1000, 'delay_between_env_runner_restarts_s': 60.0, 'restart_failed_sub_environments': False, 'num_consecutive_env_runner_failures_tolerance': 100, 'env_runner_health_probe_timeout_s': 30, 'env_runner_restore_timeout_s': 1800, '_model_config_dict': {}, '_rl_module_spec': None, '_AlgorithmConfig__prior_exploration_config': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_initialize_loss_from_dummy_batch': False, 'simple_optimizer': False, 'policy_map_cache': -1, 'worker_cls': -1, 'synchronize_filters': -1, 'enable_async_evaluation': -1, 'custom_async_evaluation_function': -1, '_enable_rl_module_api': -1, 'auto_wrap_old_gym_envs': -1, 'disable_env_checking': -1, 'replay_sequence_length': None, '_disable_execution_plan_api': -1, 'lr_schedule': None, 'use_critic': True, 'use_gae': True, 'use_kl_loss': True, 'kl_coeff': 0.2, 'kl_target': 0.01, 'sgd_minibatch_size': 256, 'mini_batch_size_per_learner': None, 'num_sgd_iter': 30, 'shuffle_sequences': True, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.0, 'entropy_coeff_schedule': None, 'clip_param': 0.3, 'vf_clip_param': 10.0, 'vf_share_layers': -1, 'lambda': 1.0, 'input': 'sampler', 'policies': {'default_policy': (None, None, None, None)}, 'callbacks': <class 'ray.rllib.algorithms.callbacks.DefaultCallbacks'>, 'create_env_on_driver': False, 'custom_eval_function': None, 'framework': 'torch', 'num_cpus_for_driver': 1, 'num_workers': 1}, 'time_since_restore': 406.53645038604736, 'iterations_since_restore': 30, 'perf': {'cpu_util_percent': 29.76, 'ram_util_percent': 87.492}})"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "save_algo(algo, \"KeepTheDistance?dst=0&agent=2&spawn_area=100\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tianEnv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
