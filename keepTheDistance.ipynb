{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Keep average distance\n",
    "\n",
    "the agents goal is to position close to each others at a distance previously defined\n",
    "\n",
    "challenges:\n",
    "- deal with continuous space environment\n",
    "- limited vision of an agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ipycanvas import Canvas, hold_canvas\n",
    "\n",
    "class CanvasWithBorders(Canvas):\n",
    "    def clear(self):\n",
    "        super().clear()\n",
    "        border_color = 'black'  # You can customize the border color here\n",
    "        border_width = 1  # You can customize the border width here\n",
    "        \n",
    "        self.fill_style = border_color\n",
    "        # Draw top border\n",
    "        self.fill_rect(0, 0, self.width, border_width)\n",
    "        # Draw bottom border\n",
    "        self.fill_rect(0, self.height - border_width, self.width, border_width)\n",
    "        # Draw left border\n",
    "        self.fill_rect(0, 0, border_width, self.height)\n",
    "        # Draw right border\n",
    "        self.fill_rect(self.width - border_width, 0, border_width, self.height)\n",
    "\n",
    "import os\n",
    "from ray.rllib.algorithms.algorithm import Algorithm\n",
    "\n",
    "def save_algo(algo, name):\n",
    "    base_dir = os.path.join(os.getcwd(), \"algos\")\n",
    "    subfolder_path = os.path.join(base_dir, name)\n",
    "    os.makedirs(subfolder_path, exist_ok=True)\n",
    "    path_to_checkpoint  = algo.save(subfolder_path)\n",
    "    print(f\"An Algorithm checkpoint has been created inside directory: '{path_to_checkpoint}'.\")\n",
    "\n",
    "def load_algo(name):\n",
    "    base_dir = os.path.join(os.getcwd(), \"algos\")\n",
    "    subfolder_path = os.path.join(base_dir, name)\n",
    "    if not os.path.exists(subfolder_path):\n",
    "        raise FileNotFoundError(f\"The specified subfolder '{subfolder_path}' does not exist.\")\n",
    "    \n",
    "    return Algorithm.from_checkpoint(subfolder_path)\n",
    "\n",
    "#save_algo(algo, \"KeepTheDistance_dst=0_agent=2_100x100train\")\n",
    "#algo2 = load_algo(\"KeepTheDistance_dst=0_agent=2_100x100train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import random as rnd\n",
    "\n",
    "def simulate_episode(env, policy, steps, sleep_between_frames=0.3, print_info=True):\n",
    "    obs, _ = env.reset()\n",
    "    env.render()\n",
    "\n",
    "    for i in range(steps):\n",
    "        if print_info:\n",
    "            print(f\"obs: \", obs)\n",
    "        actions = policy.compute_actions(obs)\n",
    "        #actions = {agent: np.array([rnd.random()*2-1, rnd.random()*2-1, 1.0], np.float32) for agent in obs.keys()}\n",
    "        #actions = {agent: env.action_space.sample() for agent in obs.keys()}\n",
    "        obs, reward, _, _, _ = env.step(actions)\n",
    "        env.render()\n",
    "        if print_info:\n",
    "            print(f\"action: \", actions)\n",
    "            print(f\"reward: \", reward, \"\\n\")\n",
    "        time.sleep(sleep_between_frames)\n",
    "\n",
    "def simulate_random_episode(env, steps, sleep_between_frames=0.3, print_info=True):\n",
    "    obs, _ = env.reset()\n",
    "    env.render()\n",
    "\n",
    "    for i in range(steps):\n",
    "        if print_info:\n",
    "            print(f\"obs: \", obs)\n",
    "        actions = {agent: np.array([rnd.random()*2-1, rnd.random()*2-1, rnd.random()], np.float32) for agent in obs.keys()}\n",
    "        obs, reward, _, _, _ = env.step(actions)\n",
    "        env.render()\n",
    "        if print_info:\n",
    "            print(f\"action: \", actions)\n",
    "            print(f\"reward: \", reward, \"\\n\")\n",
    "        time.sleep(sleep_between_frames)\n",
    "\n",
    "def ppo_result_format(result):\n",
    "    return (f\"iteration [{result['training_iteration']}] => \" +\n",
    "          f\"episode_reward_mean: {result['sampler_results']['episode_reward_mean']}, \" +\n",
    "          f\"episode_len_mean: {result['sampler_results']['episode_len_mean']}, \" +\n",
    "          f\"agent_steps_trained: {result['info']['num_agent_steps_trained']}, \" +\n",
    "          f\"env_steps_trained: {result['info']['num_env_steps_trained']}, \" + \n",
    "          f\"entropy: {result['info']['learner']['default_policy']['learner_stats']['entropy']}, \" +\n",
    "          f\"learning_rate: {result['info']['learner']['default_policy']['learner_stats']['cur_lr']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### environment definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Set\n",
    "from ray.rllib.env.multi_agent_env import MultiAgentEnv\n",
    "import random as rnd\n",
    "from gymnasium.spaces import Discrete, Box, Dict, Tuple\n",
    "from gymnasium.spaces.utils import flatten, flatten_space\n",
    "import numpy as np\n",
    "from IPython.display import clear_output\n",
    "import math\n",
    "from ipycanvas import Canvas, hold_canvas\n",
    "\n",
    "class EnvironmentConfiguration: \n",
    "    def __init__(self, n_agents, target_distance, speed, spawn_area=100, visible_nbrs=1, max_steps=None, spawn_area_schedule=None):\n",
    "        self.n_agents = n_agents\n",
    "        self.visible_nbrs = visible_nbrs\n",
    "        self.target_distance = target_distance\n",
    "        self.max_steps = max_steps\n",
    "        self.speed = speed\n",
    "        self.spawn_area = spawn_area\n",
    "        self.spawn_area_schedule = spawn_area_schedule\n",
    "\n",
    "class KeepTheDistance(MultiAgentEnv):\n",
    "\n",
    "    canvas = None\n",
    "    CANVAS_WIDTH, CANVAS_HEIGHT = 300.0, 300.0\n",
    "\n",
    "    def __init__(self, config: EnvironmentConfiguration):\n",
    "        assert config.n_agents > config.visible_nbrs # just base case implemented \n",
    "             \n",
    "        self.n_agents = config.n_agents\n",
    "        self.visible_nbrs = config.visible_nbrs\n",
    "        self.target_distance = config.target_distance\n",
    "        self.max_steps = config.max_steps\n",
    "        self.speed = config.speed\n",
    "        self.spawn_area = config.spawn_area\n",
    "        self.spawn_area_schedule = config.spawn_area_schedule\n",
    "        if self.spawn_area_schedule != None:\n",
    "            self.spawn_area_schedule_index = 0\n",
    "            self.n_reset = 0\n",
    "            self.spawn_area = self.spawn_area_schedule[0][1]\n",
    "        \n",
    "        self.agents_ids = ['agent-' + str(i) for i in range(self.n_agents)]\n",
    "        self.agent_colors = {agent: self.rgb_to_hex(rnd.randint(0, 255), rnd.randint(0, 255), rnd.randint(0, 255)) for agent in self.agents_ids}\n",
    "        self.observation_space = self.observation_space('agent-0')\n",
    "        self.action_space = self.action_space(\"\")\n",
    "\n",
    "    def unflatten_observation_space(self, agent):\n",
    "        #distance_vector = Box(low=-np.inf, high=np.inf, shape=(2,1), dtype=np.float32)\n",
    "        #obs_space = Dict({\"nbr-1\": distance_vector})\n",
    "        direction = Box(low=-1, high=1, shape=(2,1), dtype=np.float32)\n",
    "        distance = Box(low=-np.inf, high=np.inf, shape=(1,1), dtype=np.float32)\n",
    "        return Dict({f\"nbr-{i}\": Dict({'direction': direction, 'distance': distance}) for i in range(self.visible_nbrs)})\n",
    "\n",
    "    def observation_space(self, agent):\n",
    "        return flatten_space(self.unflatten_observation_space(agent))\n",
    "\n",
    "    def action_space(self, agent):\n",
    "        direction = Box(low=-1.0, high=1.0, shape=(2,1), dtype=np.float32)\n",
    "        speed = Box(0.0, 1.0, dtype=np.float32)\n",
    "        return flatten_space(Tuple([direction, speed]))\n",
    "    \n",
    "    def __get_random_point(self, max_x, max_y, min_x=0, min_y=0):\n",
    "        return (rnd.randint(min_x, max_x-1), rnd.randint(min_y, max_y-1))\n",
    "    \n",
    "    def __get_observation(self, agent):\n",
    "        distance_vectors = [self.__distance_vector_between(agent, nbr) \n",
    "                            for nbr in self.__get_n_closest_neighbours(agent, self.visible_nbrs)]\n",
    "\n",
    "        obs = {\n",
    "            f\"nbr-{i}\": {\n",
    "                \"direction\": self.__compute_unit_vector(distance_vectors[i]),\n",
    "                \"distance\": self.__compute_norm(distance_vectors[i])\n",
    "            }\n",
    "            for i in range(len(distance_vectors))\n",
    "            }\n",
    "        return flatten(self.unflatten_observation_space(agent), obs)\n",
    "\n",
    "    def rgb_to_hex(self, r, g, b):\n",
    "        return f'#{r:02x}{g:02x}{b:02x}'\n",
    "\n",
    "    def __distance_vector_between(self, agent1, agent2):\n",
    "        agent1_pos = self.agents_pos[agent1]\n",
    "        agent2_pos = self.agents_pos[agent2]\n",
    "        return (agent1_pos[0]-agent2_pos[0], agent1_pos[1]-agent2_pos[1])\n",
    "    \n",
    "    def __distance_between(self, agent1, agent2):\n",
    "        return self.__compute_norm(self.__distance_vector_between(agent1, agent2))\n",
    "\n",
    "    def __compute_norm(self, vector):\n",
    "        return math.sqrt(math.pow(vector[0], 2) + math.pow(vector[1], 2))\n",
    "    \n",
    "    def __compute_unit_vector(self, vector):\n",
    "        norm = self.__compute_norm(vector)\n",
    "        if norm == 0:\n",
    "            return [0,0]\n",
    "        return [vector[0]/norm, vector[1]/norm]\n",
    "\n",
    "    def __total_distance_from_closest_neighbours(self, agent):\n",
    "        return sum([abs(self.__distance_between(agent, nbr) - self.target_distance) for nbr in self.__get_n_closest_neighbours(agent, self.visible_nbrs)])\n",
    "\n",
    "    def __get_local_reward(self, agent, action):\n",
    "        last_action = self.last_actions[agent]\n",
    "        self.last_actions[agent] = action\n",
    "\n",
    "        newDistance = self.__total_distance_from_closest_neighbours(agent)\n",
    "        reward_1 = self.last_step_distances[agent] - newDistance\n",
    "        self.last_step_distances[agent] = newDistance\n",
    "\n",
    "        closest_nbrs = self.__get_n_closest_neighbours(agent, self.visible_nbrs)\n",
    "        reward_2 = sum([100 if abs(self.__distance_between(agent, nbr) - self.target_distance) < 0.5 else 0 for nbr in closest_nbrs])\n",
    "\n",
    "        reward_3 = 0 if last_action[0] * action[0] > 0 else -10\n",
    "        reward_3 += 9 if last_action[1] * action[1] > 0 else -10 \n",
    "\n",
    "        reward_4 = -action[2]*10\n",
    "        return reward_1 + reward_2 #+ reward_3# working for two agents using value for reward_2 equals to one\n",
    "\n",
    "    def __get_global_reward(self):\n",
    "        return 0\n",
    "    \n",
    "    def __get_other_agents(self, agent):\n",
    "        return [other for other in self.agents_ids if other != agent]\n",
    "\n",
    "    def __get_n_closest_neighbours(self, agent, n=1):\n",
    "        distances = {other: self.__distance_between(agent, other) for other in self.__get_other_agents(agent)}\n",
    "        return [neighbour[0] for neighbour in sorted(list(distances.items()), key=lambda d: d[1])[:n]]\n",
    "        # return {neighbour[0]: neighbour[1] for neighbour in sorted(list(dst.items()), key=lambda d: d[0])[:n]}\n",
    "\n",
    "    def __update_agent_position(self, agent, action):\n",
    "        unit_movement = self.__compute_unit_vector([action[0], action[1]])\n",
    "        self.agents_pos[agent] = (self.agents_pos[agent][0] + unit_movement[0]*action[2]*self.speed, \n",
    "                                 self.agents_pos[agent][1] + unit_movement[1]*action[2]*self.speed)\n",
    "\n",
    "    def reset(self, seed=None, options=None):\n",
    "        if self.spawn_area_schedule != None:\n",
    "            self.n_reset += 1\n",
    "            if (self.spawn_area_schedule_index < len(self.spawn_area_schedule)-1 and \n",
    "                self.n_reset >= self.spawn_area_schedule[self.spawn_area_schedule_index+1][0]):\n",
    "                self.spawn_area_schedule_index += 1\n",
    "                self.spawn_area = self.spawn_area_schedule[self.spawn_area_schedule_index][1]\n",
    "\n",
    "        self.steps = 0\n",
    "        self.agents_pos = {agent: self.__get_random_point(max_x=self.spawn_area, max_y=self.spawn_area) for agent in self.agents_ids}\n",
    "        self.last_step_distances = {agent: self.__total_distance_from_closest_neighbours(agent) for agent in self.agents_ids}\n",
    "        self.last_actions = {agent: [0]*3 for agent in self.agents_ids}\n",
    "        return {agent: self.__get_observation(agent) for agent in self.agents_ids}, {}\n",
    "     \n",
    "    def step(self, actions):\n",
    "        self.steps += 1\n",
    "        observations, rewards, terminated, truncated, infos = {}, {}, {}, {}, {}\n",
    "\n",
    "        for agent, action in actions.items():\n",
    "            self.__update_agent_position(agent, action)\n",
    "\n",
    "        for agent, action in actions.items():\n",
    "            observations[agent] = self.__get_observation(agent)\n",
    "            rewards[agent] = self.__get_local_reward(agent, action) + self.__get_global_reward()\n",
    "            terminated[agent] = False\n",
    "            truncated[agent] = False\n",
    "            infos[agent] = {}\n",
    "\n",
    "        truncated['__all__'] = False\n",
    "        if self.max_steps != None and self.steps == self.max_steps:\n",
    "            terminated['__all__'] = True\n",
    "        else:\n",
    "            terminated['__all__'] = False\n",
    "\n",
    "        return observations, rewards, terminated, truncated, infos\n",
    "     \n",
    "    def rgb_to_hex(self, r, g, b):\n",
    "        return f'#{r:02x}{g:02x}{b:02x}'\n",
    "\n",
    "    def render(self):\n",
    "        pass\n",
    "\n",
    "    def get_agent_ids(self):\n",
    "       return self.agents\n",
    "\n",
    "\n",
    "class RenderableKeepTheDistance(KeepTheDistance):\n",
    "    def render(self):\n",
    "        if self.canvas is None:\n",
    "            self.canvas = CanvasWithBorders(width=self.CANVAS_WIDTH, height=self.CANVAS_HEIGHT)\n",
    "            display(self.canvas)\n",
    "        \n",
    "        with hold_canvas():\n",
    "            agent_size = max(self.CANVAS_WIDTH/float(self.spawn_area),1)\n",
    "            top_left = (0.0,0.0)\n",
    "            bottom_right = (self.spawn_area, self.spawn_area)\n",
    "            self.canvas.clear()\n",
    "\n",
    "            for agent in self.agents_ids:\n",
    "                raw_pos = self.agents_pos[agent]\n",
    "                color = self.agent_colors[agent]\n",
    "                \n",
    "                agent_pos_in_frame = [((raw_pos[0]-top_left[0])/(bottom_right[0]-top_left[0]))*self.CANVAS_WIDTH,\n",
    "                            ((raw_pos[1]-top_left[1])/(bottom_right[1]-top_left[1]))*self.CANVAS_HEIGHT,]\n",
    "\n",
    "                self.canvas.fill_style = color\n",
    "                self.canvas.fill_circle(\n",
    "                    agent_pos_in_frame[0],\n",
    "                    agent_pos_in_frame[1],\n",
    "                    agent_size/2.0\n",
    "                )\n",
    "                \n",
    "                self.canvas.stroke_style = \"black\"\n",
    "                self.canvas.stroke_circle(\n",
    "                    agent_pos_in_frame[0],\n",
    "                    agent_pos_in_frame[1],\n",
    "                    agent_size/2.0\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5c504bbed6914630afdfdc963d8db736",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "CanvasWithBorders(height=300, width=300)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "obs:  {'agent-0': array([-0.8, -0.6,  5. ], dtype=float32), 'agent-1': array([0.8, 0.6, 5. ], dtype=float32)}\n",
      "action:  {'agent-0': array([ 0.25117123, -0.52546006,  0.9467585 ], dtype=float32), 'agent-1': array([ 0.9007992 , -0.38765645,  0.7313749 ], dtype=float32)}\n",
      "reward:  {'agent-0': -0.5576289789726632, 'agent-1': -0.5576289789726632} \n",
      "\n",
      "obs:  {'agent-0': array([-0.76714414, -0.6414748 ,  5.557629  ], dtype=float32), 'agent-1': array([0.76714414, 0.6414748 , 5.557629  ], dtype=float32)}\n",
      "action:  {'agent-0': array([0.42556387, 0.35005745, 0.15835695], dtype=float32), 'agent-1': array([-0.44827724, -0.9752154 ,  0.10878799], dtype=float32)}\n",
      "reward:  {'agent-0': 0.2564201741511427, 'agent-1': 0.2564201741511427} \n",
      "\n",
      "obs:  {'agent-0': array([-0.77261025, -0.6348806 ,  5.301209  ], dtype=float32), 'agent-1': array([0.77261025, 0.6348806 , 5.301209  ], dtype=float32)}\n",
      "action:  {'agent-0': array([-0.66467696, -0.25630164,  0.24431604], dtype=float32), 'agent-1': array([-0.53505605,  0.05762007,  0.11425567], dtype=float32)}\n",
      "reward:  {'agent-0': -0.15192849185673296, 'agent-1': -0.15192849185673296} \n",
      "\n",
      "obs:  {'agent-0': array([-0.7720556 , -0.63555497,  5.4531374 ], dtype=float32), 'agent-1': array([0.7720556 , 0.63555497, 5.4531374 ], dtype=float32)}\n",
      "action:  {'agent-0': array([-0.04430339,  0.06773406,  0.7302989 ], dtype=float32), 'agent-1': array([ 0.8270413, -0.8397571,  0.800262 ], dtype=float32)}\n",
      "reward:  {'agent-0': -0.20037106363488544, 'agent-1': -0.20037106363488544} \n",
      "\n",
      "obs:  {'agent-0': array([-0.91472715, -0.40407205,  5.653508  ], dtype=float32), 'agent-1': array([0.91472715, 0.40407205, 5.653508  ], dtype=float32)}\n",
      "action:  {'agent-0': array([0.5039696, 0.7245785, 0.9401075], dtype=float32), 'agent-1': array([ 0.34986487, -0.7338109 ,  0.6085944 ], dtype=float32)}\n",
      "reward:  {'agent-0': 0.6631189553035606, 'agent-1': 0.6631189553035606} \n",
      "\n",
      "obs:  {'agent-0': array([-0.98119295, -0.19302954,  4.9903893 ], dtype=float32), 'agent-1': array([0.98119295, 0.19302954, 4.9903893 ], dtype=float32)}\n",
      "action:  {'agent-0': array([-0.45584822, -0.53110534,  0.7045422 ], dtype=float32), 'agent-1': array([-0.9358964 , -0.24709103,  0.7619192 ], dtype=float32)}\n",
      "reward:  {'agent-0': 0.19127240046687444, 'agent-1': 0.19127240046687444} \n",
      "\n",
      "obs:  {'agent-0': array([-0.96241134, -0.27159595,  4.799117  ], dtype=float32), 'agent-1': array([0.96241134, 0.27159595, 4.799117  ], dtype=float32)}\n",
      "action:  {'agent-0': array([0.01682643, 0.92239046, 0.74517775], dtype=float32), 'agent-1': array([-0.9746824 , -0.17969035,  0.4846529 ], dtype=float32)}\n",
      "reward:  {'agent-0': 0.6438813761474558, 'agent-1': 0.6438813761474558} \n",
      "\n",
      "obs:  {'agent-0': array([-0.9935688 , -0.11323021,  4.155236  ], dtype=float32), 'agent-1': array([0.9935688 , 0.11323021, 4.155236  ], dtype=float32)}\n",
      "action:  {'agent-0': array([ 0.34443292, -0.2669714 ,  0.8054334 ], dtype=float32), 'agent-1': array([ 0.46966398, -0.3302122 ,  0.70731676], dtype=float32)}\n",
      "reward:  {'agent-0': 0.046753030357867154, 'agent-1': 0.046753030357867154} \n",
      "\n",
      "obs:  {'agent-0': array([-0.99076366, -0.13559991,  4.1084824 ], dtype=float32), 'agent-1': array([0.99076366, 0.13559991, 4.1084824 ], dtype=float32)}\n",
      "action:  {'agent-0': array([-0.12417866, -0.74013203,  0.854903  ], dtype=float32), 'agent-1': array([0.93419474, 0.04781356, 0.38724414], dtype=float32)}\n",
      "reward:  {'agent-0': -0.70449870945742, 'agent-1': -0.70449870945742} \n",
      "\n",
      "obs:  {'agent-0': array([-0.95548487, -0.29504007,  4.812981  ], dtype=float32), 'agent-1': array([0.95548487, 0.29504007, 4.812981  ], dtype=float32)}\n",
      "action:  {'agent-0': array([ 0.31337997, -0.6596009 ,  0.40684438], dtype=float32), 'agent-1': array([-0.14867972, -0.7637892 ,  0.40803796], dtype=float32)}\n",
      "reward:  {'agent-0': 0.25086041750851695, 'agent-1': 0.25086041750851695} \n",
      "\n",
      "obs:  {'agent-0': array([-0.9526653, -0.304021 ,  4.562121 ], dtype=float32), 'agent-1': array([0.9526653, 0.304021 , 4.562121 ], dtype=float32)}\n",
      "action:  {'agent-0': array([ 0.00504582, -0.9976265 ,  0.5058301 ], dtype=float32), 'agent-1': array([-0.3135929 ,  0.41622958,  0.47751817], dtype=float32)}\n",
      "reward:  {'agent-0': -0.08818032919174801, 'agent-1': -0.08818032919174801} \n",
      "\n",
      "obs:  {'agent-0': array([-0.8722602 , -0.48904207,  4.6503015 ], dtype=float32), 'agent-1': array([0.8722602 , 0.48904207, 4.6503015 ], dtype=float32)}\n",
      "action:  {'agent-0': array([ 0.4076577, -0.8298745,  0.4043847], dtype=float32), 'agent-1': array([ 0.02537857, -0.7386074 ,  0.99860525], dtype=float32)}\n",
      "reward:  {'agent-0': 0.40853103996895257, 'agent-1': 0.40853103996895257} \n",
      "\n",
      "obs:  {'agent-0': array([-0.92232007, -0.38642684,  4.2417703 ], dtype=float32), 'agent-1': array([0.92232007, 0.38642684, 4.2417703 ], dtype=float32)}\n",
      "action:  {'agent-0': array([ 0.41288793, -0.32999235,  0.46682742], dtype=float32), 'agent-1': array([-0.10243722, -0.5681685 ,  0.13647087], dtype=float32)}\n",
      "reward:  {'agent-0': 0.28691403631009793, 'agent-1': 0.28691403631009793} \n",
      "\n",
      "obs:  {'agent-0': array([-0.8909015, -0.4541965,  3.9548562], dtype=float32), 'agent-1': array([0.8909015, 0.4541965, 3.9548562], dtype=float32)}\n",
      "action:  {'agent-0': array([-0.18580167, -0.38837415,  0.4174685 ], dtype=float32), 'agent-1': array([-0.34957117, -0.42945197,  0.07102823], dtype=float32)}\n",
      "reward:  {'agent-0': -0.27257788865742816, 'agent-1': -0.27257788865742816} \n",
      "\n",
      "obs:  {'agent-0': array([-0.86546886, -0.5009628 ,  4.227434  ], dtype=float32), 'agent-1': array([0.86546886, 0.5009628 , 4.227434  ], dtype=float32)}\n",
      "action:  {'agent-0': array([-0.04380057,  0.34036756,  0.00453994], dtype=float32), 'agent-1': array([0.2791278 , 0.4599443 , 0.19690715], dtype=float32)}\n",
      "reward:  {'agent-0': -0.17191607165270906, 'agent-1': -0.17191607165270906} \n",
      "\n",
      "obs:  {'agent-0': array([-0.8550011, -0.5186262,  4.39935  ], dtype=float32), 'agent-1': array([0.8550011, 0.5186262, 4.39935  ], dtype=float32)}\n",
      "action:  {'agent-0': array([ 0.8877103, -0.1266858,  0.9492579], dtype=float32), 'agent-1': array([-0.62518144, -0.21955732,  0.99884367], dtype=float32)}\n",
      "reward:  {'agent-0': 1.5925755802905592, 'agent-1': 1.5925755802905592} \n",
      "\n",
      "obs:  {'agent-0': array([-0.6695571, -0.7427606,  2.8067746], dtype=float32), 'agent-1': array([0.6695571, 0.7427606, 2.8067746], dtype=float32)}\n",
      "action:  {'agent-0': array([-0.89867115,  0.97770405,  0.35214326], dtype=float32), 'agent-1': array([0.6818526 , 0.38558725, 0.4120361 ], dtype=float32)}\n",
      "reward:  {'agent-0': -0.3941554192884853, 'agent-1': -0.3941554192884853} \n",
      "\n",
      "obs:  {'agent-0': array([-0.7736062 , -0.63366663,  3.2009299 ], dtype=float32), 'agent-1': array([0.7736062 , 0.63366663, 3.2009299 ], dtype=float32)}\n",
      "action:  {'agent-0': array([0.48283026, 0.17358173, 0.49998653], dtype=float32), 'agent-1': array([-0.21438472,  0.37340263,  0.69944954], dtype=float32)}\n",
      "reward:  {'agent-0': 0.2298648227624418, 'agent-1': 0.2298648227624418} \n",
      "\n",
      "obs:  {'agent-0': array([-0.5578781, -0.8299229,  2.971065 ], dtype=float32), 'agent-1': array([0.5578781, 0.8299229, 2.971065 ], dtype=float32)}\n",
      "action:  {'agent-0': array([-0.14425604,  0.01528847,  0.13922857], dtype=float32), 'agent-1': array([-0.05931919, -0.60776514,  0.25260255], dtype=float32)}\n",
      "reward:  {'agent-0': 0.14680448453292394, 'agent-1': 0.14680448453292394} \n",
      "\n",
      "obs:  {'agent-0': array([-0.62721103, -0.77884936,  2.8242607 ], dtype=float32), 'agent-1': array([0.62721103, 0.77884936, 2.8242607 ], dtype=float32)}\n",
      "action:  {'agent-0': array([0.04343826, 0.76164794, 0.6814117 ], dtype=float32), 'agent-1': array([ 0.5356208 , -0.21723594,  0.10837878], dtype=float32)}\n",
      "reward:  {'agent-0': 0.46918139202335984, 'agent-1': 0.46918139202335984} \n",
      "\n",
      "obs:  {'agent-0': array([-0.7783352, -0.627849 ,  2.3550792], dtype=float32), 'agent-1': array([0.7783352, 0.627849 , 2.3550792], dtype=float32)}\n",
      "action:  {'agent-0': array([-0.9382806 , -0.87387466,  0.21546371], dtype=float32), 'agent-1': array([-0.00343275,  0.22618462,  0.7314397 ], dtype=float32)}\n",
      "reward:  {'agent-0': -0.722834121583011, 'agent-1': -0.722834121583011} \n",
      "\n",
      "obs:  {'agent-0': array([-0.6431671 , -0.76572585,  3.0779133 ], dtype=float32), 'agent-1': array([0.6431671 , 0.76572585, 3.0779133 ], dtype=float32)}\n",
      "action:  {'agent-0': array([-0.98712945,  0.22344796,  0.6615556 ], dtype=float32), 'agent-1': array([-0.21616296, -0.48317513,  0.96961784], dtype=float32)}\n",
      "reward:  {'agent-0': 0.48458074832951015, 'agent-1': 0.48458074832951015} \n",
      "\n",
      "obs:  {'agent-0': array([-0.85946417, -0.511196  ,  2.5933325 ], dtype=float32), 'agent-1': array([0.85946417, 0.511196  , 2.5933325 ], dtype=float32)}\n",
      "action:  {'agent-0': array([ 0.34391955, -0.8701039 ,  0.3896649 ], dtype=float32), 'agent-1': array([-0.75088054, -0.3125714 ,  0.49295527], dtype=float32)}\n",
      "reward:  {'agent-0': 0.37870339560861144, 'agent-1': 0.37870339560861144} \n",
      "\n",
      "obs:  {'agent-0': array([-0.73625886, -0.67669994,  2.2146292 ], dtype=float32), 'agent-1': array([0.73625886, 0.67669994, 2.2146292 ], dtype=float32)}\n",
      "action:  {'agent-0': array([-0.6249481 ,  0.5069435 ,  0.82174927], dtype=float32), 'agent-1': array([ 0.782245  , -0.28216657,  0.7108546 ], dtype=float32)}\n",
      "reward:  {'agent-0': -0.8144954589580191, 'agent-1': -0.8144954589580191} \n",
      "\n",
      "obs:  {'agent-0': array([-0.9697212 , -0.24421458,  3.0291247 ], dtype=float32), 'agent-1': array([0.9697212 , 0.24421458, 3.0291247 ], dtype=float32)}\n",
      "action:  {'agent-0': array([0.09571373, 0.1326032 , 0.61224705], dtype=float32), 'agent-1': array([0.3280565 , 0.07868557, 0.8274107 ], dtype=float32)}\n",
      "reward:  {'agent-0': -0.38255697545024114, 'agent-1': -0.38255697545024114} \n",
      "\n",
      "obs:  {'agent-0': array([-0.99178886, -0.12788598,  3.4116817 ], dtype=float32), 'agent-1': array([0.99178886, 0.12788598, 3.4116817 ], dtype=float32)}\n",
      "action:  {'agent-0': array([ 0.8717722 , -0.47424886,  0.9542268 ], dtype=float32), 'agent-1': array([-0.6652885 , -0.8179873 ,  0.49957284], dtype=float32)}\n",
      "reward:  {'agent-0': 1.1250532083288594, 'agent-1': 1.1250532083288594} \n",
      "\n",
      "obs:  {'agent-0': array([-0.9753343 , -0.22073293,  2.2866285 ], dtype=float32), 'agent-1': array([0.9753343 , 0.22073293, 2.2866285 ], dtype=float32)}\n",
      "action:  {'agent-0': array([-0.69374377,  0.7456231 ,  0.03560326], dtype=float32), 'agent-1': array([-0.84330094, -0.92987263,  0.2989226 ], dtype=float32)}\n",
      "reward:  {'agent-0': 0.21691206987721445, 'agent-1': 0.21691206987721445} \n",
      "\n",
      "obs:  {'agent-0': array([-0.9922461 , -0.12428865,  2.0697165 ], dtype=float32), 'agent-1': array([0.9922461 , 0.12428865, 2.0697165 ], dtype=float32)}\n",
      "action:  {'agent-0': array([ 0.51355106, -0.42825803,  0.30401006], dtype=float32), 'agent-1': array([0.73835355, 0.19627532, 0.3684228 ], dtype=float32)}\n",
      "reward:  {'agent-0': -0.17412152756884147, 'agent-1': -0.17412152756884147} \n",
      "\n",
      "obs:  {'agent-0': array([-0.9698762 , -0.24359827,  2.2438378 ], dtype=float32), 'agent-1': array([0.9698762 , 0.24359827, 2.2438378 ], dtype=float32)}\n",
      "action:  {'agent-0': array([0.01212196, 0.13320872, 0.2838327 ], dtype=float32), 'agent-1': array([0.90228885, 0.15083341, 0.16346708], dtype=float32)}\n",
      "reward:  {'agent-0': -0.08614321511243528, 'agent-1': -0.08614321511243528} \n",
      "\n",
      "obs:  {'agent-0': array([-0.9921765 , -0.12484336,  2.329981  ], dtype=float32), 'agent-1': array([0.9921765 , 0.12484336, 2.329981  ], dtype=float32)}\n",
      "action:  {'agent-0': array([0.5716281 , 0.8152664 , 0.11251044], dtype=float32), 'agent-1': array([-0.8303022,  0.6428866,  0.8494014], dtype=float32)}\n",
      "reward:  {'agent-0': 0.5982220701478933, 'agent-1': 0.5982220701478933} \n",
      "\n"
     ]
    }
   ],
   "source": [
    "env_config = EnvironmentConfiguration(n_agents=2, visible_nbrs=1, target_distance=0, max_steps=100, speed=1, spawn_area=100, \n",
    "                         spawn_area_schedule=[[0,5],[3,10],[4,100],[5,1000]])\n",
    "env = RenderableKeepTheDistance(env_config)\n",
    "for i in range(1):\n",
    "    obs, _ = env.reset()\n",
    "    env.render()\n",
    "    simulate_random_episode(env, 30, sleep_between_frames=0.3)\n",
    "    #time.sleep(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "08e0666d15e44541975d10cb21dc2c4c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "CanvasWithBorders(height=300, width=300)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'agent-0': array([ 0.8       , -0.6       , 10.        ,  0.6950221 ,  0.71898836,\n",
      "       41.725292  ], dtype=float32), 'agent-1': array([-0.503871  , -0.8637789 , 41.677334  , -0.6950221 , -0.71898836,\n",
      "       41.725292  ], dtype=float32), 'agent-2': array([-0.8      ,  0.6      , 10.       ,  0.503871 ,  0.8637789,\n",
      "       41.677334 ], dtype=float32)}\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "env_config = EnvironmentConfiguration(n_agents=3, visible_nbrs=2, target_distance=0, max_steps=100, speed=1, spawn_area=50)\n",
    "env = RenderableKeepTheDistance(env_config)\n",
    "obs, _ = env.reset()\n",
    "env.render()\n",
    "print(obs)\n",
    "\n",
    "for i in range(0):\n",
    "    print(obs)\n",
    "    #actions = {agent: np.array([rnd.random()*2-1, rnd.random()*2-1, 1.0], np.float32) for agent in obs.keys()}\n",
    "    actions = {'agent-0': np.array([-1.0, -1.0, 1.0], np.float32),\n",
    "               'agent-1': np.array([0.0, 0.0, 1], np.float32)}\n",
    "    #actions = {agent: env.action_space.sample() for agent in obs.keys()}\n",
    "    obs, reward, _, _, _ = env.step(actions)\n",
    "    print(actions)\n",
    "    print(reward, \"\\n\")\n",
    "    env.render()\n",
    "    time.sleep(0.3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## policy training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ray\n",
    "ray.shutdown()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KeepTheDistance?dst=0&agent=2&spawn_area=100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ray.tune.registry import register_env\n",
    "\n",
    "env_config = EnvironmentConfiguration(n_agents=2, target_distance=0, max_steps=500, speed=1, spawn_area=100)\n",
    "register_env(\"KeepTheDistance?dst=0&agent=2&spawn_area=100\", lambda _: KeepTheDistance(env_config))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-23 17:56:02,620\tWARNING deprecation.py:50 -- DeprecationWarning: `_enable_new_api_stack` has been deprecated. Use `AlgorithmConfig._enable_new_api_stack` instead. This will raise an error in the future!\n",
      "/home/nicolo/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/rllib/algorithms/algorithm.py:521: RayDeprecationWarning: This API is deprecated and may be removed in future Ray releases. You could suppress this warning by setting env variable PYTHONWARNINGS=\"ignore::DeprecationWarning\"\n",
      "`UnifiedLogger` will be removed in Ray 2.7.\n",
      "  return UnifiedLogger(config, logdir, loggers=None)\n",
      "/home/nicolo/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/tune/logger/unified.py:53: RayDeprecationWarning: This API is deprecated and may be removed in future Ray releases. You could suppress this warning by setting env variable PYTHONWARNINGS=\"ignore::DeprecationWarning\"\n",
      "The `JsonLogger interface is deprecated in favor of the `ray.tune.json.JsonLoggerCallback` interface and will be removed in Ray 2.7.\n",
      "  self._loggers.append(cls(self.config, self.logdir, self.trial))\n",
      "/home/nicolo/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/tune/logger/unified.py:53: RayDeprecationWarning: This API is deprecated and may be removed in future Ray releases. You could suppress this warning by setting env variable PYTHONWARNINGS=\"ignore::DeprecationWarning\"\n",
      "The `CSVLogger interface is deprecated in favor of the `ray.tune.csv.CSVLoggerCallback` interface and will be removed in Ray 2.7.\n",
      "  self._loggers.append(cls(self.config, self.logdir, self.trial))\n",
      "/home/nicolo/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/tune/logger/unified.py:53: RayDeprecationWarning: This API is deprecated and may be removed in future Ray releases. You could suppress this warning by setting env variable PYTHONWARNINGS=\"ignore::DeprecationWarning\"\n",
      "The `TBXLogger interface is deprecated in favor of the `ray.tune.tensorboardx.TBXLoggerCallback` interface and will be removed in Ray 2.7.\n",
      "  self._loggers.append(cls(self.config, self.logdir, self.trial))\n",
      "2024-05-23 17:56:02,683\tWARNING deprecation.py:50 -- DeprecationWarning: `WorkerSet(num_workers=... OR local_worker=...)` has been deprecated. Use `EnvRunnerGroup(num_env_runners=... AND local_env_runner=...)` instead. This will raise an error in the future!\n",
      "2024-05-23 17:56:02,684\tWARNING deprecation.py:50 -- DeprecationWarning: `max_num_worker_restarts` has been deprecated. Use `AlgorithmConfig.max_num_env_runner_restarts` instead. This will raise an error in the future!\n",
      "2024-05-23 17:56:05,845\tINFO worker.py:1749 -- Started a local Ray instance.\n",
      "2024-05-23 17:56:12,313\tWARNING util.py:61 -- Install gputil for GPU system monitoring.\n"
     ]
    }
   ],
   "source": [
    "algo = load_algo(\"KeepTheDistance?dst=0&agent=2&spawn_area=100\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration [1] => episode_reward_mean: 52.81617410101117, episode_len_mean: 500.0, agent_steps_trained: 16384, env_steps_trained: 8192, entropy: 4.2788884340475, learning_rate: 0.0010000000000000005\n",
      "iteration [2] => episode_reward_mean: 37.30362596776846, episode_len_mean: 500.0, agent_steps_trained: 32768, env_steps_trained: 16384, entropy: 4.1835497113565605, learning_rate: 0.0010000000000000005\n",
      "iteration [3] => episode_reward_mean: 500.36734738975593, episode_len_mean: 500.0, agent_steps_trained: 49152, env_steps_trained: 24576, entropy: 4.085292186463873, learning_rate: 0.0010000000000000005\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b4a37bfe4c3a43439ee73e0d77dc35d0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "CanvasWithBorders(height=300, width=300)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "obs:  {'agent-0': array([-0.8337123 , -0.55219907, 92.358     ], dtype=float32), 'agent-1': array([ 0.8337123 ,  0.55219907, 92.358     ], dtype=float32)}\n",
      "action:  {'agent-0': array([ 1.        , -0.7243967 ,  0.55002356], dtype=float32), 'agent-1': array([0.04272294, 0.31985784, 0.76082504], dtype=float32)}\n",
      "reward:  {'agent-0': -0.31360999963919767, 'agent-1': -0.31360999963919767} \n",
      "\n",
      "obs:  {'agent-0': array([-0.8271713 , -0.56194985, 92.67161   ], dtype=float32), 'agent-1': array([ 0.8271713 ,  0.56194985, 92.67161   ], dtype=float32)}\n",
      "action:  {'agent-0': array([0.9998113, 1.       , 1.       ], dtype=float32), 'agent-1': array([ 1.        , -1.        ,  0.86758304], dtype=float32)}\n",
      "reward:  {'agent-0': 0.813647346045542, 'agent-1': 0.813647346045542} \n",
      "\n",
      "obs:  {'agent-0': array([-0.8334795, -0.5525504, 91.85796  ], dtype=float32), 'agent-1': array([ 0.8334795,  0.5525504, 91.85796  ], dtype=float32)}\n",
      "action:  {'agent-0': array([-0.635647 ,  0.8625028,  0.7280216], dtype=float32), 'agent-1': array([-0.13217771, -1.        ,  0.        ], dtype=float32)}\n",
      "reward:  {'agent-0': -0.039040684882039045, 'agent-1': -0.039040684882039045} \n",
      "\n",
      "obs:  {'agent-0': array([-0.8378254 , -0.54593825, 91.897     ], dtype=float32), 'agent-1': array([ 0.8378254 ,  0.54593825, 91.897     ], dtype=float32)}\n",
      "action:  {'agent-0': array([ 0.48943746, -1.        ,  0.        ], dtype=float32), 'agent-1': array([-0.9420241,  1.       ,  1.       ], dtype=float32)}\n",
      "reward:  {'agent-0': 0.17182568035006796, 'agent-1': 0.17182568035006796} \n",
      "\n",
      "obs:  {'agent-0': array([-0.8319194 , -0.55489653, 91.725174  ], dtype=float32), 'agent-1': array([ 0.8319194 ,  0.55489653, 91.725174  ], dtype=float32)}\n",
      "action:  {'agent-0': array([0.6145942, 0.8776264, 0.7233784], dtype=float32), 'agent-1': array([-0.2937826,  0.6263068,  0.       ], dtype=float32)}\n",
      "reward:  {'agent-0': 0.6736176302156309, 'agent-1': 0.6736176302156309} \n",
      "\n",
      "obs:  {'agent-0': array([-0.83351684, -0.5524941 , 91.05156   ], dtype=float32), 'agent-1': array([ 0.83351684,  0.5524941 , 91.05156   ], dtype=float32)}\n",
      "action:  {'agent-0': array([-0.08082533,  1.        ,  0.7608579 ], dtype=float32), 'agent-1': array([ 0.8472878, -1.       ,  1.       ], dtype=float32)}\n",
      "reward:  {'agent-0': 0.2354621175748548, 'agent-1': 0.2354621175748548} \n",
      "\n",
      "obs:  {'agent-0': array([-0.84347105, -0.53717464, 90.81609   ], dtype=float32), 'agent-1': array([ 0.84347105,  0.53717464, 90.81609   ], dtype=float32)}\n",
      "action:  {'agent-0': array([0.7088809 , 0.45120347, 0.        ], dtype=float32), 'agent-1': array([-0.18308163, -0.04964715,  0.        ], dtype=float32)}\n",
      "reward:  {'agent-0': 0.0, 'agent-1': 0.0} \n",
      "\n",
      "obs:  {'agent-0': array([-0.84347105, -0.53717464, 90.81609   ], dtype=float32), 'agent-1': array([ 0.84347105,  0.53717464, 90.81609   ], dtype=float32)}\n",
      "action:  {'agent-0': array([-0.55584633, -0.79159606,  0.4885666 ], dtype=float32), 'agent-1': array([-1.        , -0.04835582,  0.09389633], dtype=float32)}\n",
      "reward:  {'agent-0': -0.37035153754482053, 'agent-1': -0.37035153754482053} \n",
      "\n",
      "obs:  {'agent-0': array([-0.84209573, -0.53932804, 91.18645   ], dtype=float32), 'agent-1': array([ 0.84209573,  0.53932804, 91.18645   ], dtype=float32)}\n",
      "action:  {'agent-0': array([-0.2084471 , -0.69108534,  0.9219406 ], dtype=float32), 'agent-1': array([ 1.        , -0.35514653,  0.53189456], dtype=float32)}\n",
      "reward:  {'agent-0': -1.026486029908753, 'agent-1': -1.026486029908753} \n",
      "\n",
      "obs:  {'agent-0': array([-0.8410444 , -0.54096603, 92.21294   ], dtype=float32), 'agent-1': array([ 0.8410444 ,  0.54096603, 92.21294   ], dtype=float32)}\n",
      "action:  {'agent-0': array([-0.12092173, -0.17963034,  0.5042404 ], dtype=float32), 'agent-1': array([-1.       ,  0.6282567,  0.545049 ], dtype=float32)}\n",
      "reward:  {'agent-0': -0.2343976992836616, 'agent-1': -0.2343976992836616} \n",
      "\n",
      "obs:  {'agent-0': array([-0.83696556, -0.5472555 , 92.447334  ], dtype=float32), 'agent-1': array([ 0.83696556,  0.5472555 , 92.447334  ], dtype=float32)}\n",
      "action:  {'agent-0': array([-0.6376203 ,  0.00486577,  0.6532626 ], dtype=float32), 'agent-1': array([-0.11263371, -0.558023  ,  1.        ], dtype=float32)}\n",
      "reward:  {'agent-0': 0.15177300412652528, 'agent-1': 0.15177300412652528} \n",
      "\n",
      "obs:  {'agent-0': array([-0.84327596, -0.5374809 , 92.29556   ], dtype=float32), 'agent-1': array([ 0.84327596,  0.5374809 , 92.29556   ], dtype=float32)}\n",
      "action:  {'agent-0': array([-0.5352697 , -1.        ,  0.60001844], dtype=float32), 'agent-1': array([ 0.02072573, -0.03453594,  0.5163514 ], dtype=float32)}\n",
      "reward:  {'agent-0': -0.5094674206152092, 'agent-1': -0.5094674206152092} \n",
      "\n",
      "obs:  {'agent-0': array([-0.84456074, -0.53545976, 92.80502   ], dtype=float32), 'agent-1': array([ 0.84456074,  0.53545976, 92.80502   ], dtype=float32)}\n",
      "action:  {'agent-0': array([ 0.7351116 , -1.        ,  0.11554879], dtype=float32), 'agent-1': array([-0.1219635 , -0.64514583,  0.41681933], dtype=float32)}\n",
      "reward:  {'agent-0': 0.2924537222315564, 'agent-1': 0.2924537222315564} \n",
      "\n",
      "obs:  {'agent-0': array([-0.8456539, -0.5337317, 92.51257  ], dtype=float32), 'agent-1': array([ 0.8456539,  0.5337317, 92.51257  ], dtype=float32)}\n",
      "action:  {'agent-0': array([-1.        , -0.09804714,  0.6763183 ], dtype=float32), 'agent-1': array([-0.5187092 ,  0.45556068,  0.9514531 ], dtype=float32)}\n",
      "reward:  {'agent-0': -0.3369827000920509, 'agent-1': -0.3369827000920509} \n",
      "\n",
      "obs:  {'agent-0': array([-0.8421346, -0.5392674, 92.849556 ], dtype=float32), 'agent-1': array([ 0.8421346,  0.5392674, 92.849556 ], dtype=float32)}\n",
      "action:  {'agent-0': array([-1.       , -0.876043 ,  0.7259754], dtype=float32), 'agent-1': array([-1.,  1.,  1.], dtype=float32)}\n",
      "reward:  {'agent-0': -0.50998704333837, 'agent-1': -0.50998704333837} \n",
      "\n",
      "obs:  {'agent-0': array([-0.8358094, -0.5490197, 93.35954  ], dtype=float32), 'agent-1': array([ 0.8358094,  0.5490197, 93.35954  ], dtype=float32)}\n",
      "action:  {'agent-0': array([-0.3704183 , -0.6263844 ,  0.66532457], dtype=float32), 'agent-1': array([ 0.15676117, -1.        ,  0.8002446 ], dtype=float32)}\n",
      "reward:  {'agent-0': -0.2680216755358913, 'agent-1': -0.2680216755358913} \n",
      "\n",
      "obs:  {'agent-0': array([-0.8383576 , -0.54512066, 93.62756   ], dtype=float32), 'agent-1': array([ 0.8383576 ,  0.54512066, 93.62756   ], dtype=float32)}\n",
      "action:  {'agent-0': array([-0.46516013, -0.37249583,  0.        ], dtype=float32), 'agent-1': array([-0.8420545 ,  0.17324376,  0.03183165], dtype=float32)}\n",
      "reward:  {'agent-0': 0.02263938714509095, 'agent-1': 0.02263938714509095} \n",
      "\n",
      "obs:  {'agent-0': array([-0.8382273 , -0.54532105, 93.60493   ], dtype=float32), 'agent-1': array([ 0.8382273 ,  0.54532105, 93.60493   ], dtype=float32)}\n",
      "action:  {'agent-0': array([-0.8549643 ,  1.        ,  0.49928716], dtype=float32), 'agent-1': array([-1.        , -0.26761347,  0.74602073], dtype=float32)}\n",
      "reward:  {'agent-0': 0.6438534632583384, 'agent-1': 0.6438534632583384} \n",
      "\n",
      "obs:  {'agent-0': array([-0.8397708 , -0.54294103, 92.961075  ], dtype=float32), 'agent-1': array([ 0.8397708 ,  0.54294103, 92.961075  ], dtype=float32)}\n",
      "action:  {'agent-0': array([0.8996415 , 0.39939106, 0.44434702], dtype=float32), 'agent-1': array([0.54182756, 0.72137713, 0.9707774 ], dtype=float32)}\n",
      "reward:  {'agent-0': -0.47296755887035147, 'agent-1': -0.47296755887035147} \n",
      "\n",
      "obs:  {'agent-0': array([-0.8374131 , -0.54657054, 93.43404   ], dtype=float32), 'agent-1': array([ 0.8374131 ,  0.54657054, 93.43404   ], dtype=float32)}\n",
      "action:  {'agent-0': array([ 0.327595 , -0.5883192,  0.5016973], dtype=float32), 'agent-1': array([-1.        , -0.20420235,  1.        ], dtype=float32)}\n",
      "reward:  {'agent-0': 0.8905755417395795, 'agent-1': 0.8905755417395795} \n",
      "\n",
      "obs:  {'agent-0': array([-0.83224714, -0.55440485, 92.543465  ], dtype=float32), 'agent-1': array([ 0.83224714,  0.55440485, 92.543465  ], dtype=float32)}\n",
      "action:  {'agent-0': array([ 1.        , -0.18336248,  0.02552724], dtype=float32), 'agent-1': array([-0.704995, -1.      ,  0.      ], dtype=float32)}\n",
      "reward:  {'agent-0': 0.018342412287623233, 'agent-1': 0.018342412287623233} \n",
      "\n",
      "obs:  {'agent-0': array([-0.83214074, -0.55456454, 92.52512   ], dtype=float32), 'agent-1': array([ 0.83214074,  0.55456454, 92.52512   ], dtype=float32)}\n",
      "action:  {'agent-0': array([ 0.06118798, -0.55727446,  0.54338413], dtype=float32), 'agent-1': array([-1.        ,  0.49863768,  0.75899136], dtype=float32)}\n",
      "reward:  {'agent-0': 0.12015765318807325, 'agent-1': 0.12015765318807325} \n",
      "\n",
      "obs:  {'agent-0': array([-0.82523036, -0.56479627, 92.40496   ], dtype=float32), 'agent-1': array([ 0.82523036,  0.56479627, 92.40496   ], dtype=float32)}\n",
      "action:  {'agent-0': array([-0.10569644, -0.05493146,  1.        ], dtype=float32), 'agent-1': array([-0.45325756,  0.92949104,  0.20638126], dtype=float32)}\n",
      "reward:  {'agent-0': -1.0228601262207064, 'agent-1': -1.0228601262207064} \n",
      "\n",
      "obs:  {'agent-0': array([-0.82472485, -0.5655342 , 93.427826  ], dtype=float32), 'agent-1': array([ 0.82472485,  0.5655342 , 93.427826  ], dtype=float32)}\n",
      "action:  {'agent-0': array([1.       , 1.       , 0.8409224], dtype=float32), 'agent-1': array([ 0.07411528, -0.89412177,  0.433792  ], dtype=float32)}\n",
      "reward:  {'agent-0': 1.0400848149666757, 'agent-1': 1.0400848149666757} \n",
      "\n",
      "obs:  {'agent-0': array([-0.82796115, -0.5607854 , 92.38774   ], dtype=float32), 'agent-1': array([ 0.82796115,  0.5607854 , 92.38774   ], dtype=float32)}\n",
      "action:  {'agent-0': array([ 0.10608339, -1.        ,  0.02369347], dtype=float32), 'agent-1': array([-0.9865189,  1.       ,  0.       ], dtype=float32)}\n",
      "reward:  {'agent-0': -0.011145726306935444, 'agent-1': -0.011145726306935444} \n",
      "\n",
      "obs:  {'agent-0': array([-0.82783425, -0.5609728 , 92.39889   ], dtype=float32), 'agent-1': array([ 0.82783425,  0.5609728 , 92.39889   ], dtype=float32)}\n",
      "action:  {'agent-0': array([ 0.01743913, -0.77523124,  0.20802206], dtype=float32), 'agent-1': array([-0.8649385,  1.       ,  0.       ], dtype=float32)}\n",
      "reward:  {'agent-0': -0.11295741680646643, 'agent-1': -0.11295741680646643} \n",
      "\n",
      "obs:  {'agent-0': array([-0.82677287, -0.5625359 , 92.51184   ], dtype=float32), 'agent-1': array([ 0.82677287,  0.5625359 , 92.51184   ], dtype=float32)}\n",
      "action:  {'agent-0': array([-0.514462  ,  0.40477967,  0.5247358 ], dtype=float32), 'agent-1': array([-0.29396218, -0.9911451 ,  0.13285288], dtype=float32)}\n",
      "reward:  {'agent-0': -0.057390017032773244, 'agent-1': -0.057390017032773244} \n",
      "\n",
      "obs:  {'agent-0': array([-0.8303072, -0.557306 , 92.56923  ], dtype=float32), 'agent-1': array([ 0.8303072,  0.557306 , 92.56923  ], dtype=float32)}\n",
      "action:  {'agent-0': array([-0.52408385, -1.        ,  0.59550047], dtype=float32), 'agent-1': array([-1.        ,  1.        ,  0.39237586], dtype=float32)}\n",
      "reward:  {'agent-0': -0.45013516188295455, 'agent-1': -0.45013516188295455} \n",
      "\n",
      "obs:  {'agent-0': array([-0.8262782, -0.5632622, 93.01936  ], dtype=float32), 'agent-1': array([ 0.8262782,  0.5632622, 93.01936  ], dtype=float32)}\n",
      "action:  {'agent-0': array([-0.8946519 , -0.00733149,  0.7848068 ], dtype=float32), 'agent-1': array([ 0.8734714 , -0.2064063 ,  0.54456204], dtype=float32)}\n",
      "reward:  {'agent-0': -1.023169405401049, 'agent-1': -1.023169405401049} \n",
      "\n",
      "obs:  {'agent-0': array([-0.8312688, -0.5558707, 94.042534 ], dtype=float32), 'agent-1': array([ 0.8312688,  0.5558707, 94.042534 ], dtype=float32)}\n",
      "action:  {'agent-0': array([-0.17181629, -0.34249735,  0.34037557], dtype=float32), 'agent-1': array([-1.        , -0.29138917,  0.0460068 ], dtype=float32)}\n",
      "reward:  {'agent-0': -0.2522931817909466, 'agent-1': -0.2522931817909466} \n",
      "\n",
      "obs:  {'agent-0': array([-0.8301948, -0.5574734, 94.29483  ], dtype=float32), 'agent-1': array([ 0.8301948,  0.5574734, 94.29483  ], dtype=float32)}\n",
      "action:  {'agent-0': array([-0.67962426, -0.91668034,  0.5918649 ], dtype=float32), 'agent-1': array([-0.1383043 ,  0.15606213,  1.        ], dtype=float32)}\n",
      "reward:  {'agent-0': -0.4317482057873434, 'agent-1': -0.4317482057873434} \n",
      "\n",
      "obs:  {'agent-0': array([-0.8231304, -0.5678524, 94.72658  ], dtype=float32), 'agent-1': array([ 0.8231304,  0.5678524, 94.72658  ], dtype=float32)}\n",
      "action:  {'agent-0': array([0.487615  , 1.        , 0.44605315], dtype=float32), 'agent-1': array([-0.2337398 ,  0.21405959,  1.        ], dtype=float32)}\n",
      "reward:  {'agent-0': 0.6090751023899514, 'agent-1': 0.6090751023899514} \n",
      "\n",
      "obs:  {'agent-0': array([-0.81854445, -0.5744432 , 94.1175    ], dtype=float32), 'agent-1': array([ 0.81854445,  0.5744432 , 94.1175    ], dtype=float32)}\n",
      "action:  {'agent-0': array([0.9349228 , 0.94291854, 0.930042  ], dtype=float32), 'agent-1': array([0.04533148, 1.        , 0.3171474 ], dtype=float32)}\n",
      "reward:  {'agent-0': 0.7215987961377124, 'agent-1': 0.7215987961377124} \n",
      "\n",
      "obs:  {'agent-0': array([-0.8180111, -0.5752024, 93.395905 ], dtype=float32), 'agent-1': array([ 0.8180111,  0.5752024, 93.395905 ], dtype=float32)}\n",
      "action:  {'agent-0': array([-1.        ,  0.07551289,  0.01066956], dtype=float32), 'agent-1': array([0.05996346, 0.67612934, 0.45027035], dtype=float32)}\n",
      "reward:  {'agent-0': -0.29936963682268924, 'agent-1': -0.29936963682268924} \n",
      "\n",
      "obs:  {'agent-0': array([-0.81593555, -0.5781429 , 93.695274  ], dtype=float32), 'agent-1': array([ 0.81593555,  0.5781429 , 93.695274  ], dtype=float32)}\n",
      "action:  {'agent-0': array([ 0.95271707, -1.        ,  0.        ], dtype=float32), 'agent-1': array([-0.6116257, -1.       ,  1.       ], dtype=float32)}\n",
      "reward:  {'agent-0': 0.9180978771543096, 'agent-1': 0.9180978771543096} \n",
      "\n",
      "obs:  {'agent-0': array([-0.8183859 , -0.57466906, 92.777176  ], dtype=float32), 'agent-1': array([ 0.8183859 ,  0.57466906, 92.777176  ], dtype=float32)}\n",
      "action:  {'agent-0': array([ 0.31260204, -0.54911464,  0.28457624], dtype=float32), 'agent-1': array([-0.17934781,  1.        ,  0.18225831], dtype=float32)}\n",
      "reward:  {'agent-0': -0.10474684075360585, 'agent-1': -0.10474684075360585} \n",
      "\n",
      "obs:  {'agent-0': array([-0.8156008, -0.578615 , 92.88192  ], dtype=float32), 'agent-1': array([ 0.8156008,  0.578615 , 92.88192  ], dtype=float32)}\n",
      "action:  {'agent-0': array([-0.59004956,  1.        ,  0.060399  ], dtype=float32), 'agent-1': array([-0.4200651, -1.       ,  0.7832072], dtype=float32)}\n",
      "reward:  {'agent-0': 0.6690493932616306, 'agent-1': 0.6690493932616306} \n",
      "\n",
      "obs:  {'agent-0': array([-0.81856185, -0.5744184 , 92.212875  ], dtype=float32), 'agent-1': array([ 0.81856185,  0.5744184 , 92.212875  ], dtype=float32)}\n",
      "action:  {'agent-0': array([ 0.12923408, -0.48154932,  0.38420886], dtype=float32), 'agent-1': array([-0.25313812,  1.        ,  0.        ], dtype=float32)}\n",
      "reward:  {'agent-0': -0.13234159766223286, 'agent-1': -0.13234159766223286} \n",
      "\n",
      "obs:  {'agent-0': array([-0.81631035, -0.5776136 , 92.345215  ], dtype=float32), 'agent-1': array([ 0.81631035,  0.5776136 , 92.345215  ], dtype=float32)}\n",
      "action:  {'agent-0': array([0.308347  , 0.30795145, 1.        ], dtype=float32), 'agent-1': array([-0.8175911 , -0.35327458,  0.        ], dtype=float32)}\n",
      "reward:  {'agent-0': 0.985606421403304, 'agent-1': 0.985606421403304} \n",
      "\n",
      "obs:  {'agent-0': array([-0.8173721, -0.5761101, 91.35961  ], dtype=float32), 'agent-1': array([ 0.8173721,  0.5761101, 91.35961  ], dtype=float32)}\n",
      "action:  {'agent-0': array([-0.37916356,  0.5416049 ,  0.43469808], dtype=float32), 'agent-1': array([1.        , 1.        , 0.31693226], dtype=float32)}\n",
      "reward:  {'agent-0': -0.3116915408161418, 'agent-1': -0.3116915408161418} \n",
      "\n",
      "obs:  {'agent-0': array([-0.8197571 , -0.57271135, 91.6713    ], dtype=float32), 'agent-1': array([ 0.8197571 ,  0.57271135, 91.6713    ], dtype=float32)}\n",
      "action:  {'agent-0': array([ 0.15331829, -1.        ,  0.58074814], dtype=float32), 'agent-1': array([-0.9874064,  0.6591201,  1.       ], dtype=float32)}\n",
      "reward:  {'agent-0': 0.09571082283437704, 'agent-1': 0.09571082283437704} \n",
      "\n",
      "obs:  {'agent-0': array([-0.8105705 , -0.58564115, 91.57559   ], dtype=float32), 'agent-1': array([ 0.8105705 ,  0.58564115, 91.57559   ], dtype=float32)}\n",
      "action:  {'agent-0': array([0.6392057 , 0.07062471, 0.76802886], dtype=float32), 'agent-1': array([ 1.        , -0.06167871,  0.54375345], dtype=float32)}\n",
      "reward:  {'agent-0': 0.24785515103701528, 'agent-1': 0.24785515103701528} \n",
      "\n",
      "obs:  {'agent-0': array([-0.8103541, -0.5859404, 91.327736 ], dtype=float32), 'agent-1': array([ 0.8103541,  0.5859404, 91.327736 ], dtype=float32)}\n",
      "action:  {'agent-0': array([-0.76124775,  1.        ,  0.        ], dtype=float32), 'agent-1': array([1., 1., 0.], dtype=float32)}\n",
      "reward:  {'agent-0': 0.0, 'agent-1': 0.0} \n",
      "\n",
      "obs:  {'agent-0': array([-0.8103541, -0.5859404, 91.327736 ], dtype=float32), 'agent-1': array([ 0.8103541,  0.5859404, 91.327736 ], dtype=float32)}\n",
      "action:  {'agent-0': array([-0.21906596,  1.        ,  0.        ], dtype=float32), 'agent-1': array([-1.       , -0.3890798,  0.8233119], dtype=float32)}\n",
      "reward:  {'agent-0': 0.7964540962215807, 'agent-1': 0.7964540962215807} \n",
      "\n",
      "obs:  {'agent-0': array([-0.80900794, -0.5877977 , 90.53128   ], dtype=float32), 'agent-1': array([ 0.80900794,  0.5877977 , 90.53128   ], dtype=float32)}\n",
      "action:  {'agent-0': array([-0.51104987,  1.        ,  0.38368887], dtype=float32), 'agent-1': array([-0.5184447, -1.       ,  0.6101133], dtype=float32)}\n",
      "reward:  {'agent-0': 0.6027646012531989, 'agent-1': 0.6027646012531989} \n",
      "\n",
      "obs:  {'agent-0': array([-0.81324947, -0.5819152 , 89.92851   ], dtype=float32), 'agent-1': array([ 0.81324947,  0.5819152 , 89.92851   ], dtype=float32)}\n",
      "action:  {'agent-0': array([ 0.41555786, -0.8923714 ,  0.4330395 ], dtype=float32), 'agent-1': array([-0.3489185 , -0.01825428,  1.        ], dtype=float32)}\n",
      "reward:  {'agent-0': 0.7575577836147289, 'agent-1': 0.7575577836147289} \n",
      "\n",
      "obs:  {'agent-0': array([-0.80690926, -0.59067535, 89.17096   ], dtype=float32), 'agent-1': array([ 0.80690926,  0.59067535, 89.17096   ], dtype=float32)}\n",
      "action:  {'agent-0': array([0.05346596, 1.        , 0.6508193 ], dtype=float32), 'agent-1': array([-0.9783274,  1.       ,  0.       ], dtype=float32)}\n",
      "reward:  {'agent-0': 0.41048213904508657, 'agent-1': 0.41048213904508657} \n",
      "\n",
      "obs:  {'agent-0': array([-0.81024945, -0.58608514, 88.760475  ], dtype=float32), 'agent-1': array([ 0.81024945,  0.58608514, 88.760475  ], dtype=float32)}\n",
      "action:  {'agent-0': array([-0.3574723 , -0.3847227 ,  0.55832136], dtype=float32), 'agent-1': array([ 0.89128685, -0.94126904,  0.8862248 ], dtype=float32)}\n",
      "reward:  {'agent-0': -0.667523012365919, 'agent-1': -0.667523012365919} \n",
      "\n",
      "obs:  {'agent-0': array([-0.8152649, -0.5790882, 89.428    ], dtype=float32), 'agent-1': array([ 0.8152649,  0.5790882, 89.428    ], dtype=float32)}\n",
      "action:  {'agent-0': array([-0.8026944 , -0.13801318,  0.        ], dtype=float32), 'agent-1': array([0.17523777, 0.71686006, 1.        ], dtype=float32)}\n",
      "reward:  {'agent-0': -0.7584919615822798, 'agent-1': -0.7584919615822798} \n",
      "\n",
      "obs:  {'agent-0': array([-0.8110413 , -0.58498895, 90.18649   ], dtype=float32), 'agent-1': array([ 0.8110413 ,  0.58498895, 90.18649   ], dtype=float32)}\n",
      "action:  {'agent-0': array([ 0.26307487, -0.39447904,  0.31503314], dtype=float32), 'agent-1': array([-0.22848684, -0.3832299 ,  0.43031383], dtype=float32)}\n",
      "reward:  {'agent-0': 0.3832639153908417, 'agent-1': 0.3832639153908417} \n",
      "\n"
     ]
    }
   ],
   "source": [
    "from ray.rllib.algorithms.ppo import PPOConfig\n",
    "from ray.tune.logger import pretty_print\n",
    "\n",
    "from gymnasium.wrappers.time_limit import TimeLimit\n",
    "\n",
    "trainin_steps = 30\n",
    "\n",
    "algo = (\n",
    "    PPOConfig()\n",
    "    .training(gamma = 0.95, \n",
    "              lr = 0.001,\n",
    "              train_batch_size = 4096*2, \n",
    "              sgd_minibatch_size = 256, \n",
    "              num_sgd_iter = 30,\n",
    "              #entropy_coeff=0.005,\n",
    "              )\n",
    "    .env_runners(num_env_runners=1)\n",
    "    .resources(num_gpus=0)\n",
    "    .environment(env=\"KeepTheDistance?dst=0&agent=2&spawn_area=100\")\n",
    "    .build()\n",
    ")\n",
    "clear_output()\n",
    "\n",
    "out = \"\"\n",
    "for i in range(trainin_steps):\n",
    "    result = algo.train()\n",
    "    clear_output()\n",
    "    out += ppo_result_format(result) + \"\\n\"\n",
    "    print(out)\n",
    "    simulate_episode(RenderableKeepTheDistance(env_config), algo, 50, sleep_between_frames=0.08, print_info=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3bbf86ca5ff74da487ebf89baa653890",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "CanvasWithBorders(height=300, width=300)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "env_config_2 = EnvironmentConfiguration(n_agents=2, target_distance=0, max_steps=500, speed=1, spawn_area=500)\n",
    "simulate_episode(RenderableKeepTheDistance(env_config_2), algo, 200, sleep_between_frames=0.03, print_info=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "An Algorithm checkpoint has been created inside directory: 'TrainingResult(checkpoint=Checkpoint(filesystem=local, path=/mnt/c/Users/nicol/Desktop/Universit/tesi/experiments/RL_experiments/algos/KeepTheDistance?dst=0&agent=2&spawn_area=100), metrics={'custom_metrics': {}, 'episode_media': {}, 'info': {'learner': {'default_policy': {'learner_stats': {'allreduce_latency': 0.0, 'grad_gnorm': 5.732406087343891, 'cur_kl_coeff': 0.6750000000000002, 'cur_lr': 0.0010000000000000005, 'total_loss': 2.0669429610628867, 'policy_loss': -0.009641030859590198, 'vf_loss': 2.0618812701043985, 'vf_explained_var': 0.41663303778817257, 'kl': 0.021781805181126438, 'entropy': 3.2973313165207703, 'entropy_coeff': 0.0}, 'model': {}, 'custom_metrics': {}, 'num_agent_steps_trained': 256.0, 'num_grad_updates_lifetime': 28320.5, 'diff_num_grad_updates_vs_sampler_policy': 479.5}}, 'num_env_steps_sampled': 122880, 'num_env_steps_trained': 122880, 'num_agent_steps_sampled': 245760, 'num_agent_steps_trained': 245760}, 'sampler_results': {'episode_reward_max': 656.3426232576753, 'episode_reward_min': 554.0001613174567, 'episode_reward_mean': 603.4171706591706, 'episode_len_mean': 300.0, 'episode_media': {}, 'episodes_this_iter': 14, 'episodes_timesteps_total': 30000, 'policy_reward_min': {}, 'policy_reward_max': {}, 'policy_reward_mean': {}, 'custom_metrics': {}, 'hist_stats': {'episode_reward': [557.9595628590914, 598.6723485242048, 576.7905364410914, 623.9128949566787, 593.9106186498275, 569.062897996119, 590.2013885065847, 589.5060260096283, 597.8507385469798, 603.3792850540186, 569.6136375997933, 644.0588800895705, 593.328203558257, 570.5709290327773, 571.9257168448096, 588.9998994945554, 598.7589479373542, 596.5288003010861, 570.1350830746087, 568.0480647727866, 620.0024518546635, 576.4197241751825, 601.7157758808075, 584.2411923236373, 604.9850591253866, 554.0001613174567, 611.4221957176887, 584.6438986226336, 576.1361015052117, 589.8023402153758, 626.6456881142497, 572.008304005685, 591.9557121248316, 612.8474983675018, 609.5166032129912, 603.8943007676796, 636.1130024943114, 611.9740266708382, 602.5425770692818, 628.1434802365283, 610.6536786191289, 609.0682140526151, 578.0150787062627, 625.5863911032764, 588.5307544495234, 622.7886128417579, 621.1413480448987, 632.7839459952713, 610.1124621743139, 592.1165754452259, 611.1495018386091, 583.7511119350513, 617.5665974881094, 616.9876188074414, 568.6591477732395, 619.8716461880304, 594.7810736546629, 611.3439781196436, 593.3561105262047, 591.1017537228189, 625.1183979513283, 632.7680447687559, 614.8452452281645, 627.5385971308539, 596.5194669359113, 592.5112024770766, 573.9262997589445, 630.0462283258863, 587.5262928094561, 587.3002309029821, 649.5510743605195, 647.6793445327738, 587.8095600275365, 601.3975749074691, 584.5722420902252, 595.9262462625873, 606.7030501305279, 625.70331948732, 623.5837370558404, 607.4836750975663, 589.7268695213564, 617.6180812142297, 603.5783583390864, 599.7037170633893, 603.8087675304085, 649.9313244777413, 588.2463279765618, 638.4685068391265, 611.2580967862943, 637.421943401637, 621.1299963854806, 591.8926883816252, 583.5864293679192, 606.1657572522845, 656.3426232576753, 625.6637389284845, 598.4456045648764, 630.5142560957363, 610.6707109474588, 607.4432818060959], 'episode_lengths': [300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300]}, 'sampler_perf': {'mean_raw_obs_processing_ms': 0.34054384452797193, 'mean_inference_ms': 0.8364510221606176, 'mean_action_processing_ms': 0.21899213523915542, 'mean_env_wait_ms': 0.3332731499742183, 'mean_env_render_ms': 0.0}, 'num_faulty_episodes': 0, 'connector_metrics': {'ObsPreprocessorConnector_ms': 0.0053632259368896484, 'StateBufferConnector_ms': 0.0042836666107177734, 'ViewRequirementAgentConnector_ms': 0.12398958206176758}, 'num_episodes': 14, 'episode_return_max': 656.3426232576753, 'episode_return_min': 554.0001613174567, 'episode_return_mean': 603.4171706591706}, 'env_runner_results': {'episode_reward_max': 656.3426232576753, 'episode_reward_min': 554.0001613174567, 'episode_reward_mean': 603.4171706591706, 'episode_len_mean': 300.0, 'episode_media': {}, 'episodes_this_iter': 14, 'episodes_timesteps_total': 30000, 'policy_reward_min': {}, 'policy_reward_max': {}, 'policy_reward_mean': {}, 'custom_metrics': {}, 'hist_stats': {'episode_reward': [557.9595628590914, 598.6723485242048, 576.7905364410914, 623.9128949566787, 593.9106186498275, 569.062897996119, 590.2013885065847, 589.5060260096283, 597.8507385469798, 603.3792850540186, 569.6136375997933, 644.0588800895705, 593.328203558257, 570.5709290327773, 571.9257168448096, 588.9998994945554, 598.7589479373542, 596.5288003010861, 570.1350830746087, 568.0480647727866, 620.0024518546635, 576.4197241751825, 601.7157758808075, 584.2411923236373, 604.9850591253866, 554.0001613174567, 611.4221957176887, 584.6438986226336, 576.1361015052117, 589.8023402153758, 626.6456881142497, 572.008304005685, 591.9557121248316, 612.8474983675018, 609.5166032129912, 603.8943007676796, 636.1130024943114, 611.9740266708382, 602.5425770692818, 628.1434802365283, 610.6536786191289, 609.0682140526151, 578.0150787062627, 625.5863911032764, 588.5307544495234, 622.7886128417579, 621.1413480448987, 632.7839459952713, 610.1124621743139, 592.1165754452259, 611.1495018386091, 583.7511119350513, 617.5665974881094, 616.9876188074414, 568.6591477732395, 619.8716461880304, 594.7810736546629, 611.3439781196436, 593.3561105262047, 591.1017537228189, 625.1183979513283, 632.7680447687559, 614.8452452281645, 627.5385971308539, 596.5194669359113, 592.5112024770766, 573.9262997589445, 630.0462283258863, 587.5262928094561, 587.3002309029821, 649.5510743605195, 647.6793445327738, 587.8095600275365, 601.3975749074691, 584.5722420902252, 595.9262462625873, 606.7030501305279, 625.70331948732, 623.5837370558404, 607.4836750975663, 589.7268695213564, 617.6180812142297, 603.5783583390864, 599.7037170633893, 603.8087675304085, 649.9313244777413, 588.2463279765618, 638.4685068391265, 611.2580967862943, 637.421943401637, 621.1299963854806, 591.8926883816252, 583.5864293679192, 606.1657572522845, 656.3426232576753, 625.6637389284845, 598.4456045648764, 630.5142560957363, 610.6707109474588, 607.4432818060959], 'episode_lengths': [300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300]}, 'sampler_perf': {'mean_raw_obs_processing_ms': 0.34054384452797193, 'mean_inference_ms': 0.8364510221606176, 'mean_action_processing_ms': 0.21899213523915542, 'mean_env_wait_ms': 0.3332731499742183, 'mean_env_render_ms': 0.0}, 'num_faulty_episodes': 0, 'connector_metrics': {'ObsPreprocessorConnector_ms': 0.0053632259368896484, 'StateBufferConnector_ms': 0.0042836666107177734, 'ViewRequirementAgentConnector_ms': 0.12398958206176758}, 'num_episodes': 14, 'episode_return_max': 656.3426232576753, 'episode_return_min': 554.0001613174567, 'episode_return_mean': 603.4171706591706}, 'episode_reward_max': 656.3426232576753, 'episode_reward_min': 554.0001613174567, 'episode_reward_mean': 603.4171706591706, 'episode_len_mean': 300.0, 'episodes_this_iter': 14, 'episodes_timesteps_total': 30000, 'policy_reward_min': {}, 'policy_reward_max': {}, 'policy_reward_mean': {}, 'hist_stats': {'episode_reward': [557.9595628590914, 598.6723485242048, 576.7905364410914, 623.9128949566787, 593.9106186498275, 569.062897996119, 590.2013885065847, 589.5060260096283, 597.8507385469798, 603.3792850540186, 569.6136375997933, 644.0588800895705, 593.328203558257, 570.5709290327773, 571.9257168448096, 588.9998994945554, 598.7589479373542, 596.5288003010861, 570.1350830746087, 568.0480647727866, 620.0024518546635, 576.4197241751825, 601.7157758808075, 584.2411923236373, 604.9850591253866, 554.0001613174567, 611.4221957176887, 584.6438986226336, 576.1361015052117, 589.8023402153758, 626.6456881142497, 572.008304005685, 591.9557121248316, 612.8474983675018, 609.5166032129912, 603.8943007676796, 636.1130024943114, 611.9740266708382, 602.5425770692818, 628.1434802365283, 610.6536786191289, 609.0682140526151, 578.0150787062627, 625.5863911032764, 588.5307544495234, 622.7886128417579, 621.1413480448987, 632.7839459952713, 610.1124621743139, 592.1165754452259, 611.1495018386091, 583.7511119350513, 617.5665974881094, 616.9876188074414, 568.6591477732395, 619.8716461880304, 594.7810736546629, 611.3439781196436, 593.3561105262047, 591.1017537228189, 625.1183979513283, 632.7680447687559, 614.8452452281645, 627.5385971308539, 596.5194669359113, 592.5112024770766, 573.9262997589445, 630.0462283258863, 587.5262928094561, 587.3002309029821, 649.5510743605195, 647.6793445327738, 587.8095600275365, 601.3975749074691, 584.5722420902252, 595.9262462625873, 606.7030501305279, 625.70331948732, 623.5837370558404, 607.4836750975663, 589.7268695213564, 617.6180812142297, 603.5783583390864, 599.7037170633893, 603.8087675304085, 649.9313244777413, 588.2463279765618, 638.4685068391265, 611.2580967862943, 637.421943401637, 621.1299963854806, 591.8926883816252, 583.5864293679192, 606.1657572522845, 656.3426232576753, 625.6637389284845, 598.4456045648764, 630.5142560957363, 610.6707109474588, 607.4432818060959], 'episode_lengths': [300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300]}, 'sampler_perf': {'mean_raw_obs_processing_ms': 0.34054384452797193, 'mean_inference_ms': 0.8364510221606176, 'mean_action_processing_ms': 0.21899213523915542, 'mean_env_wait_ms': 0.3332731499742183, 'mean_env_render_ms': 0.0}, 'num_faulty_episodes': 0, 'connector_metrics': {'ObsPreprocessorConnector_ms': 0.0053632259368896484, 'StateBufferConnector_ms': 0.0042836666107177734, 'ViewRequirementAgentConnector_ms': 0.12398958206176758}, 'num_episodes': 14, 'episode_return_max': 656.3426232576753, 'episode_return_min': 554.0001613174567, 'episode_return_mean': 603.4171706591706, 'num_healthy_workers': 1, 'num_in_flight_async_reqs': 0, 'num_remote_worker_restarts': 0, 'num_agent_steps_sampled': 245760, 'num_agent_steps_trained': 245760, 'num_env_steps_sampled': 122880, 'num_env_steps_trained': 122880, 'num_env_steps_sampled_this_iter': 4096, 'num_env_steps_trained_this_iter': 4096, 'num_env_steps_sampled_throughput_per_sec': 312.59178580771174, 'num_env_steps_trained_throughput_per_sec': 312.59178580771174, 'timesteps_total': 122880, 'num_env_steps_sampled_lifetime': 122880, 'num_agent_steps_sampled_lifetime': 245760, 'num_steps_trained_this_iter': 4096, 'agent_timesteps_total': 245760, 'timers': {'training_iteration_time_ms': 13107.549, 'restore_workers_time_ms': 0.021, 'training_step_time_ms': 13107.482, 'sample_time_ms': 6865.865, 'load_time_ms': 0.603, 'load_throughput': 6788315.625, 'learn_time_ms': 6236.751, 'learn_throughput': 656.752, 'synch_weights_time_ms': 3.669}, 'counters': {'num_env_steps_sampled': 122880, 'num_env_steps_trained': 122880, 'num_agent_steps_sampled': 245760, 'num_agent_steps_trained': 245760}, 'done': False, 'episodes_total': 409, 'training_iteration': 30, 'trial_id': 'default', 'date': '2024-05-23_10-17-00', 'timestamp': 1716452220, 'time_this_iter_s': 13.10737919807434, 'time_total_s': 406.53645038604736, 'pid': 3351, 'hostname': 'LAPTOP-9AD2MD1C', 'node_ip': '172.23.87.14', 'config': {'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, '_fake_gpus': False, 'num_learner_workers': 0, 'num_gpus_per_learner_worker': 0, 'num_cpus_per_learner_worker': 1, 'local_gpu_idx': 0, 'custom_resources_per_worker': {}, 'placement_strategy': 'PACK', 'eager_tracing': True, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'torch_compile_learner': False, 'torch_compile_learner_what_to_compile': <TorchCompileWhatToCompile.FORWARD_TRAIN: 'forward_train'>, 'torch_compile_learner_dynamo_backend': 'inductor', 'torch_compile_learner_dynamo_mode': None, 'torch_compile_worker': False, 'torch_compile_worker_dynamo_backend': 'onnxrt', 'torch_compile_worker_dynamo_mode': None, 'enable_rl_module_and_learner': False, 'enable_env_runner_and_connector_v2': False, 'env': 'KeepTheDistance?dst=0&agent=2&spawn_area=100', 'env_config': {}, 'observation_space': None, 'action_space': None, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, '_is_atari': None, 'env_task_fn': None, 'render_env': False, 'action_mask_key': 'action_mask', 'env_runner_cls': None, 'num_envs_per_env_runner': 1, 'validate_env_runners_after_construction': True, 'sample_timeout_s': 60.0, '_env_to_module_connector': None, 'add_default_connectors_to_env_to_module_pipeline': True, '_module_to_env_connector': None, 'add_default_connectors_to_module_to_env_pipeline': True, 'episode_lookback_horizon': 1, 'rollout_fragment_length': 'auto', 'batch_mode': 'truncate_episodes', 'compress_observations': False, 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'enable_tf1_exec_eagerly': False, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'update_worker_filter_stats': True, 'use_worker_filter_stats': True, 'enable_connectors': True, 'sampler_perf_stats_ema_coef': None, 'gamma': 0.95, 'lr': 0.001, 'grad_clip': None, 'grad_clip_by': 'global_norm', 'train_batch_size': 4096, 'train_batch_size_per_learner': None, 'model': {'_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'fcnet_weights_initializer': None, 'fcnet_weights_initializer_config': None, 'fcnet_bias_initializer': None, 'fcnet_bias_initializer_config': None, 'conv_filters': None, 'conv_activation': 'relu', 'conv_kernel_initializer': None, 'conv_kernel_initializer_config': None, 'conv_bias_initializer': None, 'conv_bias_initializer_config': None, 'conv_transpose_kernel_initializer': None, 'conv_transpose_kernel_initializer_config': None, 'conv_transpose_bias_initializer': None, 'conv_transpose_bias_initializer_config': None, 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'post_fcnet_weights_initializer': None, 'post_fcnet_weights_initializer_config': None, 'post_fcnet_bias_initializer': None, 'post_fcnet_bias_initializer_config': None, 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, 'lstm_weights_initializer': None, 'lstm_weights_initializer_config': None, 'lstm_bias_initializer': None, 'lstm_bias_initializer_config': None, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'encoder_latent_dim': None, 'always_check_shapes': False, 'lstm_use_prev_action_reward': -1, '_use_default_native_models': -1}, '_learner_connector': None, 'add_default_connectors_to_learner_pipeline': True, 'optimizer': {}, 'max_requests_in_flight_per_sampler_worker': 2, '_learner_class': None, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'algorithm_config_overrides_per_module': {}, '_per_module_overrides': {}, 'count_steps_by': 'env_steps', 'policy_map_capacity': 100, 'policy_mapping_fn': <function AlgorithmConfig.DEFAULT_POLICY_MAPPING_FN at 0x7fb8255f62a0>, 'policies_to_train': None, 'policy_states_are_swappable': False, 'observation_fn': None, 'input_config': {}, 'actions_in_input_normalized': False, 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_config': {}, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'offline_sampling': False, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_sample_timeout_s': 120.0, 'evaluation_parallel_to_training': False, 'evaluation_force_reset_envs_before_iteration': True, 'evaluation_config': None, 'off_policy_estimation_methods': {}, 'ope_split_batch_by_episode': True, 'evaluation_num_env_runners': 0, 'always_attach_evaluation_results': True, 'in_evaluation': False, 'sync_filters_on_rollout_workers_timeout_s': 10.0, 'keep_per_episode_custom_metrics': False, 'metrics_episode_collection_timeout_s': 60.0, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_iteration': None, 'min_train_timesteps_per_iteration': 0, 'min_sample_timesteps_per_iteration': 0, 'export_native_model_files': False, 'checkpoint_trainable_policies_only': False, 'logger_creator': None, 'logger_config': None, 'log_level': 'WARN', 'log_sys_usage': True, 'fake_sampler': False, 'seed': None, '_run_training_always_in_thread': False, '_evaluation_parallel_to_training_wo_thread': False, 'ignore_env_runner_failures': False, 'recreate_failed_env_runners': False, 'max_num_env_runner_restarts': 1000, 'delay_between_env_runner_restarts_s': 60.0, 'restart_failed_sub_environments': False, 'num_consecutive_env_runner_failures_tolerance': 100, 'env_runner_health_probe_timeout_s': 30, 'env_runner_restore_timeout_s': 1800, '_model_config_dict': {}, '_rl_module_spec': None, '_AlgorithmConfig__prior_exploration_config': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_initialize_loss_from_dummy_batch': False, 'simple_optimizer': False, 'policy_map_cache': -1, 'worker_cls': -1, 'synchronize_filters': -1, 'enable_async_evaluation': -1, 'custom_async_evaluation_function': -1, '_enable_rl_module_api': -1, 'auto_wrap_old_gym_envs': -1, 'disable_env_checking': -1, 'replay_sequence_length': None, '_disable_execution_plan_api': -1, 'lr_schedule': None, 'use_critic': True, 'use_gae': True, 'use_kl_loss': True, 'kl_coeff': 0.2, 'kl_target': 0.01, 'sgd_minibatch_size': 256, 'mini_batch_size_per_learner': None, 'num_sgd_iter': 30, 'shuffle_sequences': True, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.0, 'entropy_coeff_schedule': None, 'clip_param': 0.3, 'vf_clip_param': 10.0, 'vf_share_layers': -1, 'lambda': 1.0, 'input': 'sampler', 'policies': {'default_policy': (None, None, None, None)}, 'callbacks': <class 'ray.rllib.algorithms.callbacks.DefaultCallbacks'>, 'create_env_on_driver': False, 'custom_eval_function': None, 'framework': 'torch', 'num_cpus_for_driver': 1, 'num_workers': 1}, 'time_since_restore': 406.53645038604736, 'iterations_since_restore': 30, 'perf': {'cpu_util_percent': 29.76, 'ram_util_percent': 87.492}})'.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainingResult(checkpoint=Checkpoint(filesystem=local, path=/mnt/c/Users/nicol/Desktop/Universit/tesi/experiments/RL_experiments/algos/KeepTheDistance?dst=0&agent=2&spawn_area=100), metrics={'custom_metrics': {}, 'episode_media': {}, 'info': {'learner': {'default_policy': {'learner_stats': {'allreduce_latency': 0.0, 'grad_gnorm': 5.732406087343891, 'cur_kl_coeff': 0.6750000000000002, 'cur_lr': 0.0010000000000000005, 'total_loss': 2.0669429610628867, 'policy_loss': -0.009641030859590198, 'vf_loss': 2.0618812701043985, 'vf_explained_var': 0.41663303778817257, 'kl': 0.021781805181126438, 'entropy': 3.2973313165207703, 'entropy_coeff': 0.0}, 'model': {}, 'custom_metrics': {}, 'num_agent_steps_trained': 256.0, 'num_grad_updates_lifetime': 28320.5, 'diff_num_grad_updates_vs_sampler_policy': 479.5}}, 'num_env_steps_sampled': 122880, 'num_env_steps_trained': 122880, 'num_agent_steps_sampled': 245760, 'num_agent_steps_trained': 245760}, 'sampler_results': {'episode_reward_max': 656.3426232576753, 'episode_reward_min': 554.0001613174567, 'episode_reward_mean': 603.4171706591706, 'episode_len_mean': 300.0, 'episode_media': {}, 'episodes_this_iter': 14, 'episodes_timesteps_total': 30000, 'policy_reward_min': {}, 'policy_reward_max': {}, 'policy_reward_mean': {}, 'custom_metrics': {}, 'hist_stats': {'episode_reward': [557.9595628590914, 598.6723485242048, 576.7905364410914, 623.9128949566787, 593.9106186498275, 569.062897996119, 590.2013885065847, 589.5060260096283, 597.8507385469798, 603.3792850540186, 569.6136375997933, 644.0588800895705, 593.328203558257, 570.5709290327773, 571.9257168448096, 588.9998994945554, 598.7589479373542, 596.5288003010861, 570.1350830746087, 568.0480647727866, 620.0024518546635, 576.4197241751825, 601.7157758808075, 584.2411923236373, 604.9850591253866, 554.0001613174567, 611.4221957176887, 584.6438986226336, 576.1361015052117, 589.8023402153758, 626.6456881142497, 572.008304005685, 591.9557121248316, 612.8474983675018, 609.5166032129912, 603.8943007676796, 636.1130024943114, 611.9740266708382, 602.5425770692818, 628.1434802365283, 610.6536786191289, 609.0682140526151, 578.0150787062627, 625.5863911032764, 588.5307544495234, 622.7886128417579, 621.1413480448987, 632.7839459952713, 610.1124621743139, 592.1165754452259, 611.1495018386091, 583.7511119350513, 617.5665974881094, 616.9876188074414, 568.6591477732395, 619.8716461880304, 594.7810736546629, 611.3439781196436, 593.3561105262047, 591.1017537228189, 625.1183979513283, 632.7680447687559, 614.8452452281645, 627.5385971308539, 596.5194669359113, 592.5112024770766, 573.9262997589445, 630.0462283258863, 587.5262928094561, 587.3002309029821, 649.5510743605195, 647.6793445327738, 587.8095600275365, 601.3975749074691, 584.5722420902252, 595.9262462625873, 606.7030501305279, 625.70331948732, 623.5837370558404, 607.4836750975663, 589.7268695213564, 617.6180812142297, 603.5783583390864, 599.7037170633893, 603.8087675304085, 649.9313244777413, 588.2463279765618, 638.4685068391265, 611.2580967862943, 637.421943401637, 621.1299963854806, 591.8926883816252, 583.5864293679192, 606.1657572522845, 656.3426232576753, 625.6637389284845, 598.4456045648764, 630.5142560957363, 610.6707109474588, 607.4432818060959], 'episode_lengths': [300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300]}, 'sampler_perf': {'mean_raw_obs_processing_ms': 0.34054384452797193, 'mean_inference_ms': 0.8364510221606176, 'mean_action_processing_ms': 0.21899213523915542, 'mean_env_wait_ms': 0.3332731499742183, 'mean_env_render_ms': 0.0}, 'num_faulty_episodes': 0, 'connector_metrics': {'ObsPreprocessorConnector_ms': 0.0053632259368896484, 'StateBufferConnector_ms': 0.0042836666107177734, 'ViewRequirementAgentConnector_ms': 0.12398958206176758}, 'num_episodes': 14, 'episode_return_max': 656.3426232576753, 'episode_return_min': 554.0001613174567, 'episode_return_mean': 603.4171706591706}, 'env_runner_results': {'episode_reward_max': 656.3426232576753, 'episode_reward_min': 554.0001613174567, 'episode_reward_mean': 603.4171706591706, 'episode_len_mean': 300.0, 'episode_media': {}, 'episodes_this_iter': 14, 'episodes_timesteps_total': 30000, 'policy_reward_min': {}, 'policy_reward_max': {}, 'policy_reward_mean': {}, 'custom_metrics': {}, 'hist_stats': {'episode_reward': [557.9595628590914, 598.6723485242048, 576.7905364410914, 623.9128949566787, 593.9106186498275, 569.062897996119, 590.2013885065847, 589.5060260096283, 597.8507385469798, 603.3792850540186, 569.6136375997933, 644.0588800895705, 593.328203558257, 570.5709290327773, 571.9257168448096, 588.9998994945554, 598.7589479373542, 596.5288003010861, 570.1350830746087, 568.0480647727866, 620.0024518546635, 576.4197241751825, 601.7157758808075, 584.2411923236373, 604.9850591253866, 554.0001613174567, 611.4221957176887, 584.6438986226336, 576.1361015052117, 589.8023402153758, 626.6456881142497, 572.008304005685, 591.9557121248316, 612.8474983675018, 609.5166032129912, 603.8943007676796, 636.1130024943114, 611.9740266708382, 602.5425770692818, 628.1434802365283, 610.6536786191289, 609.0682140526151, 578.0150787062627, 625.5863911032764, 588.5307544495234, 622.7886128417579, 621.1413480448987, 632.7839459952713, 610.1124621743139, 592.1165754452259, 611.1495018386091, 583.7511119350513, 617.5665974881094, 616.9876188074414, 568.6591477732395, 619.8716461880304, 594.7810736546629, 611.3439781196436, 593.3561105262047, 591.1017537228189, 625.1183979513283, 632.7680447687559, 614.8452452281645, 627.5385971308539, 596.5194669359113, 592.5112024770766, 573.9262997589445, 630.0462283258863, 587.5262928094561, 587.3002309029821, 649.5510743605195, 647.6793445327738, 587.8095600275365, 601.3975749074691, 584.5722420902252, 595.9262462625873, 606.7030501305279, 625.70331948732, 623.5837370558404, 607.4836750975663, 589.7268695213564, 617.6180812142297, 603.5783583390864, 599.7037170633893, 603.8087675304085, 649.9313244777413, 588.2463279765618, 638.4685068391265, 611.2580967862943, 637.421943401637, 621.1299963854806, 591.8926883816252, 583.5864293679192, 606.1657572522845, 656.3426232576753, 625.6637389284845, 598.4456045648764, 630.5142560957363, 610.6707109474588, 607.4432818060959], 'episode_lengths': [300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300]}, 'sampler_perf': {'mean_raw_obs_processing_ms': 0.34054384452797193, 'mean_inference_ms': 0.8364510221606176, 'mean_action_processing_ms': 0.21899213523915542, 'mean_env_wait_ms': 0.3332731499742183, 'mean_env_render_ms': 0.0}, 'num_faulty_episodes': 0, 'connector_metrics': {'ObsPreprocessorConnector_ms': 0.0053632259368896484, 'StateBufferConnector_ms': 0.0042836666107177734, 'ViewRequirementAgentConnector_ms': 0.12398958206176758}, 'num_episodes': 14, 'episode_return_max': 656.3426232576753, 'episode_return_min': 554.0001613174567, 'episode_return_mean': 603.4171706591706}, 'episode_reward_max': 656.3426232576753, 'episode_reward_min': 554.0001613174567, 'episode_reward_mean': 603.4171706591706, 'episode_len_mean': 300.0, 'episodes_this_iter': 14, 'episodes_timesteps_total': 30000, 'policy_reward_min': {}, 'policy_reward_max': {}, 'policy_reward_mean': {}, 'hist_stats': {'episode_reward': [557.9595628590914, 598.6723485242048, 576.7905364410914, 623.9128949566787, 593.9106186498275, 569.062897996119, 590.2013885065847, 589.5060260096283, 597.8507385469798, 603.3792850540186, 569.6136375997933, 644.0588800895705, 593.328203558257, 570.5709290327773, 571.9257168448096, 588.9998994945554, 598.7589479373542, 596.5288003010861, 570.1350830746087, 568.0480647727866, 620.0024518546635, 576.4197241751825, 601.7157758808075, 584.2411923236373, 604.9850591253866, 554.0001613174567, 611.4221957176887, 584.6438986226336, 576.1361015052117, 589.8023402153758, 626.6456881142497, 572.008304005685, 591.9557121248316, 612.8474983675018, 609.5166032129912, 603.8943007676796, 636.1130024943114, 611.9740266708382, 602.5425770692818, 628.1434802365283, 610.6536786191289, 609.0682140526151, 578.0150787062627, 625.5863911032764, 588.5307544495234, 622.7886128417579, 621.1413480448987, 632.7839459952713, 610.1124621743139, 592.1165754452259, 611.1495018386091, 583.7511119350513, 617.5665974881094, 616.9876188074414, 568.6591477732395, 619.8716461880304, 594.7810736546629, 611.3439781196436, 593.3561105262047, 591.1017537228189, 625.1183979513283, 632.7680447687559, 614.8452452281645, 627.5385971308539, 596.5194669359113, 592.5112024770766, 573.9262997589445, 630.0462283258863, 587.5262928094561, 587.3002309029821, 649.5510743605195, 647.6793445327738, 587.8095600275365, 601.3975749074691, 584.5722420902252, 595.9262462625873, 606.7030501305279, 625.70331948732, 623.5837370558404, 607.4836750975663, 589.7268695213564, 617.6180812142297, 603.5783583390864, 599.7037170633893, 603.8087675304085, 649.9313244777413, 588.2463279765618, 638.4685068391265, 611.2580967862943, 637.421943401637, 621.1299963854806, 591.8926883816252, 583.5864293679192, 606.1657572522845, 656.3426232576753, 625.6637389284845, 598.4456045648764, 630.5142560957363, 610.6707109474588, 607.4432818060959], 'episode_lengths': [300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300]}, 'sampler_perf': {'mean_raw_obs_processing_ms': 0.34054384452797193, 'mean_inference_ms': 0.8364510221606176, 'mean_action_processing_ms': 0.21899213523915542, 'mean_env_wait_ms': 0.3332731499742183, 'mean_env_render_ms': 0.0}, 'num_faulty_episodes': 0, 'connector_metrics': {'ObsPreprocessorConnector_ms': 0.0053632259368896484, 'StateBufferConnector_ms': 0.0042836666107177734, 'ViewRequirementAgentConnector_ms': 0.12398958206176758}, 'num_episodes': 14, 'episode_return_max': 656.3426232576753, 'episode_return_min': 554.0001613174567, 'episode_return_mean': 603.4171706591706, 'num_healthy_workers': 1, 'num_in_flight_async_reqs': 0, 'num_remote_worker_restarts': 0, 'num_agent_steps_sampled': 245760, 'num_agent_steps_trained': 245760, 'num_env_steps_sampled': 122880, 'num_env_steps_trained': 122880, 'num_env_steps_sampled_this_iter': 4096, 'num_env_steps_trained_this_iter': 4096, 'num_env_steps_sampled_throughput_per_sec': 312.59178580771174, 'num_env_steps_trained_throughput_per_sec': 312.59178580771174, 'timesteps_total': 122880, 'num_env_steps_sampled_lifetime': 122880, 'num_agent_steps_sampled_lifetime': 245760, 'num_steps_trained_this_iter': 4096, 'agent_timesteps_total': 245760, 'timers': {'training_iteration_time_ms': 13107.549, 'restore_workers_time_ms': 0.021, 'training_step_time_ms': 13107.482, 'sample_time_ms': 6865.865, 'load_time_ms': 0.603, 'load_throughput': 6788315.625, 'learn_time_ms': 6236.751, 'learn_throughput': 656.752, 'synch_weights_time_ms': 3.669}, 'counters': {'num_env_steps_sampled': 122880, 'num_env_steps_trained': 122880, 'num_agent_steps_sampled': 245760, 'num_agent_steps_trained': 245760}, 'done': False, 'episodes_total': 409, 'training_iteration': 30, 'trial_id': 'default', 'date': '2024-05-23_10-17-00', 'timestamp': 1716452220, 'time_this_iter_s': 13.10737919807434, 'time_total_s': 406.53645038604736, 'pid': 3351, 'hostname': 'LAPTOP-9AD2MD1C', 'node_ip': '172.23.87.14', 'config': {'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, '_fake_gpus': False, 'num_learner_workers': 0, 'num_gpus_per_learner_worker': 0, 'num_cpus_per_learner_worker': 1, 'local_gpu_idx': 0, 'custom_resources_per_worker': {}, 'placement_strategy': 'PACK', 'eager_tracing': True, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'torch_compile_learner': False, 'torch_compile_learner_what_to_compile': <TorchCompileWhatToCompile.FORWARD_TRAIN: 'forward_train'>, 'torch_compile_learner_dynamo_backend': 'inductor', 'torch_compile_learner_dynamo_mode': None, 'torch_compile_worker': False, 'torch_compile_worker_dynamo_backend': 'onnxrt', 'torch_compile_worker_dynamo_mode': None, 'enable_rl_module_and_learner': False, 'enable_env_runner_and_connector_v2': False, 'env': 'KeepTheDistance?dst=0&agent=2&spawn_area=100', 'env_config': {}, 'observation_space': None, 'action_space': None, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, '_is_atari': None, 'env_task_fn': None, 'render_env': False, 'action_mask_key': 'action_mask', 'env_runner_cls': None, 'num_envs_per_env_runner': 1, 'validate_env_runners_after_construction': True, 'sample_timeout_s': 60.0, '_env_to_module_connector': None, 'add_default_connectors_to_env_to_module_pipeline': True, '_module_to_env_connector': None, 'add_default_connectors_to_module_to_env_pipeline': True, 'episode_lookback_horizon': 1, 'rollout_fragment_length': 'auto', 'batch_mode': 'truncate_episodes', 'compress_observations': False, 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'enable_tf1_exec_eagerly': False, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'update_worker_filter_stats': True, 'use_worker_filter_stats': True, 'enable_connectors': True, 'sampler_perf_stats_ema_coef': None, 'gamma': 0.95, 'lr': 0.001, 'grad_clip': None, 'grad_clip_by': 'global_norm', 'train_batch_size': 4096, 'train_batch_size_per_learner': None, 'model': {'_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'fcnet_weights_initializer': None, 'fcnet_weights_initializer_config': None, 'fcnet_bias_initializer': None, 'fcnet_bias_initializer_config': None, 'conv_filters': None, 'conv_activation': 'relu', 'conv_kernel_initializer': None, 'conv_kernel_initializer_config': None, 'conv_bias_initializer': None, 'conv_bias_initializer_config': None, 'conv_transpose_kernel_initializer': None, 'conv_transpose_kernel_initializer_config': None, 'conv_transpose_bias_initializer': None, 'conv_transpose_bias_initializer_config': None, 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'post_fcnet_weights_initializer': None, 'post_fcnet_weights_initializer_config': None, 'post_fcnet_bias_initializer': None, 'post_fcnet_bias_initializer_config': None, 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, 'lstm_weights_initializer': None, 'lstm_weights_initializer_config': None, 'lstm_bias_initializer': None, 'lstm_bias_initializer_config': None, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'encoder_latent_dim': None, 'always_check_shapes': False, 'lstm_use_prev_action_reward': -1, '_use_default_native_models': -1}, '_learner_connector': None, 'add_default_connectors_to_learner_pipeline': True, 'optimizer': {}, 'max_requests_in_flight_per_sampler_worker': 2, '_learner_class': None, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'algorithm_config_overrides_per_module': {}, '_per_module_overrides': {}, 'count_steps_by': 'env_steps', 'policy_map_capacity': 100, 'policy_mapping_fn': <function AlgorithmConfig.DEFAULT_POLICY_MAPPING_FN at 0x7fb8255f62a0>, 'policies_to_train': None, 'policy_states_are_swappable': False, 'observation_fn': None, 'input_config': {}, 'actions_in_input_normalized': False, 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_config': {}, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'offline_sampling': False, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_sample_timeout_s': 120.0, 'evaluation_parallel_to_training': False, 'evaluation_force_reset_envs_before_iteration': True, 'evaluation_config': None, 'off_policy_estimation_methods': {}, 'ope_split_batch_by_episode': True, 'evaluation_num_env_runners': 0, 'always_attach_evaluation_results': True, 'in_evaluation': False, 'sync_filters_on_rollout_workers_timeout_s': 10.0, 'keep_per_episode_custom_metrics': False, 'metrics_episode_collection_timeout_s': 60.0, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_iteration': None, 'min_train_timesteps_per_iteration': 0, 'min_sample_timesteps_per_iteration': 0, 'export_native_model_files': False, 'checkpoint_trainable_policies_only': False, 'logger_creator': None, 'logger_config': None, 'log_level': 'WARN', 'log_sys_usage': True, 'fake_sampler': False, 'seed': None, '_run_training_always_in_thread': False, '_evaluation_parallel_to_training_wo_thread': False, 'ignore_env_runner_failures': False, 'recreate_failed_env_runners': False, 'max_num_env_runner_restarts': 1000, 'delay_between_env_runner_restarts_s': 60.0, 'restart_failed_sub_environments': False, 'num_consecutive_env_runner_failures_tolerance': 100, 'env_runner_health_probe_timeout_s': 30, 'env_runner_restore_timeout_s': 1800, '_model_config_dict': {}, '_rl_module_spec': None, '_AlgorithmConfig__prior_exploration_config': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_initialize_loss_from_dummy_batch': False, 'simple_optimizer': False, 'policy_map_cache': -1, 'worker_cls': -1, 'synchronize_filters': -1, 'enable_async_evaluation': -1, 'custom_async_evaluation_function': -1, '_enable_rl_module_api': -1, 'auto_wrap_old_gym_envs': -1, 'disable_env_checking': -1, 'replay_sequence_length': None, '_disable_execution_plan_api': -1, 'lr_schedule': None, 'use_critic': True, 'use_gae': True, 'use_kl_loss': True, 'kl_coeff': 0.2, 'kl_target': 0.01, 'sgd_minibatch_size': 256, 'mini_batch_size_per_learner': None, 'num_sgd_iter': 30, 'shuffle_sequences': True, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.0, 'entropy_coeff_schedule': None, 'clip_param': 0.3, 'vf_clip_param': 10.0, 'vf_share_layers': -1, 'lambda': 1.0, 'input': 'sampler', 'policies': {'default_policy': (None, None, None, None)}, 'callbacks': <class 'ray.rllib.algorithms.callbacks.DefaultCallbacks'>, 'create_env_on_driver': False, 'custom_eval_function': None, 'framework': 'torch', 'num_cpus_for_driver': 1, 'num_workers': 1}, 'time_since_restore': 406.53645038604736, 'iterations_since_restore': 30, 'perf': {'cpu_util_percent': 29.76, 'ram_util_percent': 87.492}})"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "save_algo(algo, \"KeepTheDistance?dst=0&agent=2&spawn_area=100\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KeepTheDistance?dst=0&agent=3&visible_nbrs=2&spawn_area=100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ray.tune.registry import register_env\n",
    "env_config = EnvironmentConfiguration(n_agents=3, visible_nbrs=2, target_distance=0, max_steps=300, speed=1, spawn_area=100)\n",
    "register_env(\"KeepTheDistance?dst=0&agent=3&visible_nbrs=2&spawn_area=100\", lambda _: KeepTheDistance(env_config))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-24 10:13:10,842\tWARNING deprecation.py:50 -- DeprecationWarning: `_enable_new_api_stack` has been deprecated. Use `AlgorithmConfig._enable_new_api_stack` instead. This will raise an error in the future!\n",
      "/home/nicolo/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/rllib/algorithms/algorithm.py:521: RayDeprecationWarning: This API is deprecated and may be removed in future Ray releases. You could suppress this warning by setting env variable PYTHONWARNINGS=\"ignore::DeprecationWarning\"\n",
      "`UnifiedLogger` will be removed in Ray 2.7.\n",
      "  return UnifiedLogger(config, logdir, loggers=None)\n",
      "/home/nicolo/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/tune/logger/unified.py:53: RayDeprecationWarning: This API is deprecated and may be removed in future Ray releases. You could suppress this warning by setting env variable PYTHONWARNINGS=\"ignore::DeprecationWarning\"\n",
      "The `JsonLogger interface is deprecated in favor of the `ray.tune.json.JsonLoggerCallback` interface and will be removed in Ray 2.7.\n",
      "  self._loggers.append(cls(self.config, self.logdir, self.trial))\n",
      "/home/nicolo/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/tune/logger/unified.py:53: RayDeprecationWarning: This API is deprecated and may be removed in future Ray releases. You could suppress this warning by setting env variable PYTHONWARNINGS=\"ignore::DeprecationWarning\"\n",
      "The `CSVLogger interface is deprecated in favor of the `ray.tune.csv.CSVLoggerCallback` interface and will be removed in Ray 2.7.\n",
      "  self._loggers.append(cls(self.config, self.logdir, self.trial))\n",
      "/home/nicolo/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/tune/logger/unified.py:53: RayDeprecationWarning: This API is deprecated and may be removed in future Ray releases. You could suppress this warning by setting env variable PYTHONWARNINGS=\"ignore::DeprecationWarning\"\n",
      "The `TBXLogger interface is deprecated in favor of the `ray.tune.tensorboardx.TBXLoggerCallback` interface and will be removed in Ray 2.7.\n",
      "  self._loggers.append(cls(self.config, self.logdir, self.trial))\n",
      "2024-05-24 10:13:10,913\tWARNING deprecation.py:50 -- DeprecationWarning: `WorkerSet(num_workers=... OR local_worker=...)` has been deprecated. Use `EnvRunnerGroup(num_env_runners=... AND local_env_runner=...)` instead. This will raise an error in the future!\n",
      "2024-05-24 10:13:10,914\tWARNING deprecation.py:50 -- DeprecationWarning: `max_num_worker_restarts` has been deprecated. Use `AlgorithmConfig.max_num_env_runner_restarts` instead. This will raise an error in the future!\n",
      "2024-05-24 10:13:14,302\tINFO worker.py:1749 -- Started a local Ray instance.\n",
      "2024-05-24 10:13:23,121\tINFO trainable.py:161 -- Trainable.setup took 12.209 seconds. If your trainable is slow to initialize, consider setting reuse_actors=True to reduce actor creation overheads.\n",
      "2024-05-24 10:13:23,124\tWARNING util.py:61 -- Install gputil for GPU system monitoring.\n"
     ]
    }
   ],
   "source": [
    "algo = load_algo(\"KeepTheDistance?dst=0&agent=3&visible_nbrs=2&spawn_area=100\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "816cbb37135841c39e70355852f091a1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "CanvasWithBorders(height=300, width=300)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration [1] => episode_reward_mean: -16085.280054435147, episode_len_mean: 1000.0, agent_steps_trained: 12288, env_steps_trained: 4096, entropy: 4.240592692130142, learning_rate: 0.0010000000000000002\n",
      "iteration [2] => episode_reward_mean: -16432.439549192502, episode_len_mean: 1000.0, agent_steps_trained: 24576, env_steps_trained: 8192, entropy: 4.213702567583985, learning_rate: 0.0010000000000000002\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[39], line 26\u001b[0m\n\u001b[1;32m     24\u001b[0m env_show \u001b[38;5;241m=\u001b[39m RenderableKeepTheDistance(env_config)\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(trainin_steps):\n\u001b[0;32m---> 26\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43malgo\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     27\u001b[0m     simulate_episode(env_show, algo, \u001b[38;5;241m150\u001b[39m, sleep_between_frames\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.03\u001b[39m, print_info\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m     28\u001b[0m     \u001b[38;5;28mprint\u001b[39m(ppo_result_format(result))\n",
      "File \u001b[0;32m~/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/tune/trainable/trainable.py:328\u001b[0m, in \u001b[0;36mTrainable.train\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    326\u001b[0m start \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m    327\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 328\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    329\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    330\u001b[0m     skipped \u001b[38;5;241m=\u001b[39m skip_exceptions(e)\n",
      "File \u001b[0;32m~/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/rllib/algorithms/algorithm.py:873\u001b[0m, in \u001b[0;36mAlgorithm.step\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    863\u001b[0m     (\n\u001b[1;32m    864\u001b[0m         train_results,\n\u001b[1;32m    865\u001b[0m         eval_results,\n\u001b[1;32m    866\u001b[0m         train_iter_ctx,\n\u001b[1;32m    867\u001b[0m     ) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_run_one_training_iteration_and_evaluation_in_parallel()\n\u001b[1;32m    869\u001b[0m \u001b[38;5;66;03m# - No evaluation necessary, just run the next training iteration.\u001b[39;00m\n\u001b[1;32m    870\u001b[0m \u001b[38;5;66;03m# - We have to evaluate in this training iteration, but no parallelism ->\u001b[39;00m\n\u001b[1;32m    871\u001b[0m \u001b[38;5;66;03m#   evaluate after the training iteration is entirely done.\u001b[39;00m\n\u001b[1;32m    872\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 873\u001b[0m     train_results, train_iter_ctx \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_one_training_iteration\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    875\u001b[0m \u001b[38;5;66;03m# Sequential: Train (already done above), then evaluate.\u001b[39;00m\n\u001b[1;32m    876\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m evaluate_this_iter \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mevaluation_parallel_to_training:\n",
      "File \u001b[0;32m~/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/rllib/algorithms/algorithm.py:3156\u001b[0m, in \u001b[0;36mAlgorithm._run_one_training_iteration\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   3154\u001b[0m             \u001b[38;5;66;03m# Try to train one step.\u001b[39;00m\n\u001b[1;32m   3155\u001b[0m             \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_timers[TRAINING_STEP_TIMER]:\n\u001b[0;32m-> 3156\u001b[0m                 results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3158\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m results, train_iter_ctx\n",
      "File \u001b[0;32m~/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/rllib/algorithms/ppo/ppo.py:428\u001b[0m, in \u001b[0;36mPPO.training_step\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    424\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_training_step_new_api_stack()\n\u001b[1;32m    425\u001b[0m \u001b[38;5;66;03m# Old and hybrid API stacks (Policy, RolloutWorker, Connector, maybe RLModule,\u001b[39;00m\n\u001b[1;32m    426\u001b[0m \u001b[38;5;66;03m# maybe Learner).\u001b[39;00m\n\u001b[1;32m    427\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 428\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_training_step_old_and_hybrid_api_stacks\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/rllib/algorithms/ppo/ppo.py:562\u001b[0m, in \u001b[0;36mPPO._training_step_old_and_hybrid_api_stacks\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    557\u001b[0m     train_batch \u001b[38;5;241m=\u001b[39m synchronous_parallel_sample(\n\u001b[1;32m    558\u001b[0m         worker_set\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mworkers,\n\u001b[1;32m    559\u001b[0m         max_agent_steps\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mtotal_train_batch_size,\n\u001b[1;32m    560\u001b[0m     )\n\u001b[1;32m    561\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 562\u001b[0m     train_batch \u001b[38;5;241m=\u001b[39m \u001b[43msynchronous_parallel_sample\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    563\u001b[0m \u001b[43m        \u001b[49m\u001b[43mworker_set\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mworkers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    564\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_env_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtotal_train_batch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    565\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    566\u001b[0m train_batch \u001b[38;5;241m=\u001b[39m train_batch\u001b[38;5;241m.\u001b[39mas_multi_agent()\n\u001b[1;32m    567\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_counters[NUM_AGENT_STEPS_SAMPLED] \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m train_batch\u001b[38;5;241m.\u001b[39magent_steps()\n",
      "File \u001b[0;32m~/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/rllib/execution/rollout_ops.py:97\u001b[0m, in \u001b[0;36msynchronous_parallel_sample\u001b[0;34m(worker_set, max_agent_steps, max_env_steps, concat, sample_timeout_s, _uses_new_env_runners, _return_metrics)\u001b[0m\n\u001b[1;32m     94\u001b[0m         stats_dicts \u001b[38;5;241m=\u001b[39m [worker_set\u001b[38;5;241m.\u001b[39mlocal_worker()\u001b[38;5;241m.\u001b[39mget_metrics()]\n\u001b[1;32m     95\u001b[0m \u001b[38;5;66;03m# Loop over remote workers' `sample()` method in parallel.\u001b[39;00m\n\u001b[1;32m     96\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 97\u001b[0m     sampled_data \u001b[38;5;241m=\u001b[39m \u001b[43mworker_set\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforeach_worker\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     98\u001b[0m \u001b[43m        \u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     99\u001b[0m \u001b[43m            \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mw\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mw\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msample\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    100\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m_return_metrics\u001b[49m\n\u001b[1;32m    101\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mw\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mw\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msample\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mw\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_metrics\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    102\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    103\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlocal_worker\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    104\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout_seconds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample_timeout_s\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    105\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    106\u001b[0m     \u001b[38;5;66;03m# Nothing was returned (maybe all workers are stalling) or no healthy\u001b[39;00m\n\u001b[1;32m    107\u001b[0m     \u001b[38;5;66;03m# remote workers left: Break.\u001b[39;00m\n\u001b[1;32m    108\u001b[0m     \u001b[38;5;66;03m# There is no point staying in this loop, since we will not be able to\u001b[39;00m\n\u001b[1;32m    109\u001b[0m     \u001b[38;5;66;03m# get any new samples if we don't have any healthy remote workers left.\u001b[39;00m\n\u001b[1;32m    110\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m sampled_data \u001b[38;5;129;01mor\u001b[39;00m worker_set\u001b[38;5;241m.\u001b[39mnum_healthy_remote_workers() \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m~/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/rllib/env/env_runner_group.py:840\u001b[0m, in \u001b[0;36mEnvRunnerGroup.foreach_worker\u001b[0;34m(self, func, local_worker, healthy_only, remote_worker_ids, timeout_seconds, return_obj_refs, mark_healthy)\u001b[0m\n\u001b[1;32m    837\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_worker_manager\u001b[38;5;241m.\u001b[39mactor_ids():\n\u001b[1;32m    838\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m local_result\n\u001b[0;32m--> 840\u001b[0m remote_results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_worker_manager\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforeach_actor\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    841\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    842\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhealthy_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhealthy_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    843\u001b[0m \u001b[43m    \u001b[49m\u001b[43mremote_actor_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mremote_worker_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    844\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout_seconds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout_seconds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    845\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_obj_refs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_obj_refs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    846\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmark_healthy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmark_healthy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    847\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    849\u001b[0m _handle_remote_call_result_errors(\n\u001b[1;32m    850\u001b[0m     remote_results, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ignore_env_runner_failures\n\u001b[1;32m    851\u001b[0m )\n\u001b[1;32m    853\u001b[0m \u001b[38;5;66;03m# With application errors handled, return good results.\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/rllib/utils/actor_manager.py:622\u001b[0m, in \u001b[0;36mFaultTolerantActorManager.foreach_actor\u001b[0;34m(self, func, healthy_only, remote_actor_ids, timeout_seconds, return_obj_refs, mark_healthy)\u001b[0m\n\u001b[1;32m    616\u001b[0m remote_calls \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_actors(\n\u001b[1;32m    617\u001b[0m     func\u001b[38;5;241m=\u001b[39mfunc,\n\u001b[1;32m    618\u001b[0m     remote_actor_ids\u001b[38;5;241m=\u001b[39mremote_actor_ids,\n\u001b[1;32m    619\u001b[0m )\n\u001b[1;32m    621\u001b[0m \u001b[38;5;66;03m# Collect remote request results (if available given timeout and/or errors).\u001b[39;00m\n\u001b[0;32m--> 622\u001b[0m _, remote_results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fetch_result\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    623\u001b[0m \u001b[43m    \u001b[49m\u001b[43mremote_actor_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mremote_actor_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    624\u001b[0m \u001b[43m    \u001b[49m\u001b[43mremote_calls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mremote_calls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    625\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtags\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mremote_calls\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    626\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout_seconds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout_seconds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    627\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_obj_refs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_obj_refs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    628\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmark_healthy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmark_healthy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    629\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    631\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m remote_results\n",
      "File \u001b[0;32m~/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/rllib/utils/actor_manager.py:476\u001b[0m, in \u001b[0;36mFaultTolerantActorManager._fetch_result\u001b[0;34m(self, remote_actor_ids, remote_calls, tags, timeout_seconds, return_obj_refs, mark_healthy)\u001b[0m\n\u001b[1;32m    473\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m remote_calls:\n\u001b[1;32m    474\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [], RemoteCallResults()\n\u001b[0;32m--> 476\u001b[0m ready, _ \u001b[38;5;241m=\u001b[39m \u001b[43mray\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    477\u001b[0m \u001b[43m    \u001b[49m\u001b[43mremote_calls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    478\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_returns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mremote_calls\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    479\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    480\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# Make sure remote results are fetched locally in parallel.\u001b[39;49;00m\n\u001b[1;32m    481\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfetch_local\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mreturn_obj_refs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    482\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    484\u001b[0m \u001b[38;5;66;03m# Remote data should already be fetched to local object store at this point.\u001b[39;00m\n\u001b[1;32m    485\u001b[0m remote_results \u001b[38;5;241m=\u001b[39m RemoteCallResults()\n",
      "File \u001b[0;32m~/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/_private/auto_init_hook.py:21\u001b[0m, in \u001b[0;36mwrap_auto_init.<locals>.auto_init_wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(fn)\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mauto_init_wrapper\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     20\u001b[0m     auto_init_ray()\n\u001b[0;32m---> 21\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/_private/client_mode_hook.py:103\u001b[0m, in \u001b[0;36mclient_mode_hook.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    101\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m func\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minit\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m is_client_mode_enabled_by_default:\n\u001b[1;32m    102\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(ray, func\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m)(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m--> 103\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/_private/worker.py:2854\u001b[0m, in \u001b[0;36mwait\u001b[0;34m(ray_waitables, num_returns, timeout, fetch_local)\u001b[0m\n\u001b[1;32m   2852\u001b[0m timeout \u001b[38;5;241m=\u001b[39m timeout \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;241m10\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m6\u001b[39m\n\u001b[1;32m   2853\u001b[0m timeout_milliseconds \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(timeout \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m1000\u001b[39m)\n\u001b[0;32m-> 2854\u001b[0m ready_ids, remaining_ids \u001b[38;5;241m=\u001b[39m \u001b[43mworker\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcore_worker\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2855\u001b[0m \u001b[43m    \u001b[49m\u001b[43mray_waitables\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2856\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_returns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2857\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout_milliseconds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2858\u001b[0m \u001b[43m    \u001b[49m\u001b[43mworker\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcurrent_task_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2859\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfetch_local\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2860\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2861\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m ready_ids, remaining_ids\n",
      "File \u001b[0;32mpython/ray/_raylet.pyx:3812\u001b[0m, in \u001b[0;36mray._raylet.CoreWorker.wait\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpython/ray/_raylet.pyx:571\u001b[0m, in \u001b[0;36mray._raylet.check_status\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from ray.rllib.algorithms.ppo import PPOConfig\n",
    "from ray.tune.logger import pretty_print\n",
    "\n",
    "from gymnasium.wrappers.time_limit import TimeLimit\n",
    "\n",
    "trainin_steps = 40\n",
    "\n",
    "algo = (\n",
    "    PPOConfig()\n",
    "    .training(gamma = 0.95, \n",
    "              lr = 0.001,\n",
    "              train_batch_size = 4092, \n",
    "              sgd_minibatch_size = 128, \n",
    "              num_sgd_iter = 30,\n",
    "              #entropy_coeff=0.005,\n",
    "              )\n",
    "    .env_runners(num_env_runners=1)\n",
    "    .resources(num_gpus=0)\n",
    "    .environment(env=\"KeepTheDistance?dst=0&agent=3&visible_nbrs=2&spawn_area=100\")\n",
    "    .build()\n",
    ")\n",
    "clear_output()\n",
    "\n",
    "env_show = RenderableKeepTheDistance(env_config)\n",
    "for i in range(trainin_steps):\n",
    "    result = algo.train()\n",
    "    simulate_episode(env_show, algo, 150, sleep_between_frames=0.03, print_info=False)\n",
    "    print(ppo_result_format(result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "79effdd57d3540f0b7889af0abfd794f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "CanvasWithBorders(height=300, width=300)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "env_config_2 = EnvironmentConfiguration(n_agents=3, visible_nbrs=2, target_distance=0, max_steps=500, speed=1, spawn_area=100)\n",
    "simulate_episode(RenderableKeepTheDistance(env_config_2), algo, 300, sleep_between_frames=0.01, print_info=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "An Algorithm checkpoint has been created inside directory: 'TrainingResult(checkpoint=Checkpoint(filesystem=local, path=/mnt/c/Users/nicol/Desktop/Universit/tesi/experiments/RL_experiments/algos/KeepTheDistance?dst=0&agent=3&visible_nbrs=2&spawn_area=100), metrics={'custom_metrics': {}, 'episode_media': {}, 'info': {'learner': {'default_policy': {'learner_stats': {'allreduce_latency': 0.0, 'grad_gnorm': 26.797518886129062, 'cur_kl_coeff': 5.473673629760742, 'cur_lr': 0.0010000000000000002, 'total_loss': 4.241659853690201, 'policy_loss': 0.0008611866208310757, 'vf_loss': 4.17278758486112, 'vf_explained_var': 0.026051732442445224, 'kl': 0.012425122178001438, 'entropy': 5.3370947420597075, 'entropy_coeff': 0.0}, 'model': {}, 'custom_metrics': {}, 'num_agent_steps_trained': 256.0, 'num_grad_updates_lifetime': 56880.5, 'diff_num_grad_updates_vs_sampler_policy': 719.5}}, 'num_env_steps_sampled': 163840, 'num_env_steps_trained': 163840, 'num_agent_steps_sampled': 491520, 'num_agent_steps_trained': 491520}, 'sampler_results': {'episode_reward_max': 26042.054873470093, 'episode_reward_min': -98.5686833518256, 'episode_reward_mean': 1930.20330570609, 'episode_len_mean': 300.0, 'episode_media': {}, 'episodes_this_iter': 14, 'episodes_timesteps_total': 30000, 'policy_reward_min': {}, 'policy_reward_max': {}, 'policy_reward_mean': {}, 'custom_metrics': {}, 'hist_stats': {'episode_reward': [-92.27266819856116, 25.73075983931291, 20.15284682220323, 4.749758335196418, 10.090455927671556, 70.70791792123086, 96.00401067477735, 63.07796837442251, 63.78997120042281, 33.44352543766661, -46.465104402316875, -0.5203327196826706, 18318.744591704028, 37.14122664199385, 21.13643332215554, 87.87719936101234, 71.45022590217889, -9.101330754919871, -98.5686833518256, 95.33530645806059, 55.4811982213292, 18.30271494299572, -51.26575824571279, 42.082533480452554, -22.445596925520377, 68.74256746797532, 33.05435525796253, -14.032245291742242, 79.26786278759919, 33.07432484402226, -38.11927735077796, 26.551249438526455, 39.52130742704978, 97.97914468484055, 50.10658463112787, 27.31634486537763, 11540.808485968026, 55.878779664342275, 39.91017477423976, 23326.82285928509, -8.006498538129037, 18704.947979701745, -8.362829241117481, 97.42257175376655, 51.19154584835426, 67.80124618096765, 41.902646135463215, -4.600723926245507, 35.53484550023718, 10580.234960158623, -18.71622448426686, 39.01829619121443, 84.04304932371977, -1.4464490134462125, 60.037719617980514, 23.785667828333658, 42.26677431276764, 34.40088252853897, 26042.054873470093, 41.77314813414468, 45.79029879641874, 32.55142799204754, -11.965768183545151, 42.00753025786594, 12293.034255574681, 43.79904868980046, -25.863541249120757, 60.73670702197117, 47.89978373191765, 61.30928534284479, 20899.3740854774, -14.818305615857966, -8.893512470663119, 14833.645636660005, 41.397579473451756, 22821.41310938688, 7.491018013864107, 59.11961027045365, 63.7071514643104, 5.028932524212735, 33.37970701764043, 14.878160110483378, 98.08106277634619, -11.823124640004565, -0.665076458756527, 38.628779430559916, 14.463635071025323, 85.03423308275264, 155.99019409589116, 1.79278515037403, 10570.213268957787, 9.93113203795204, 70.76003720368493, 26.41514838991324, -14.33627414254741, 13.77353032658921, 454.51181271704314, 95.00891723474588, 33.989863382171976, -53.28672419852414], 'episode_lengths': [300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300]}, 'sampler_perf': {'mean_raw_obs_processing_ms': 0.6170270567527532, 'mean_inference_ms': 1.07689741151318, 'mean_action_processing_ms': 0.3920014103326924, 'mean_env_wait_ms': 0.7321249918278235, 'mean_env_render_ms': 0.0}, 'num_faulty_episodes': 0, 'connector_metrics': {'ObsPreprocessorConnector_ms': 0.008563756942749023, 'StateBufferConnector_ms': 0.00696110725402832, 'ViewRequirementAgentConnector_ms': 0.23952293395996094}, 'num_episodes': 14, 'episode_return_max': 26042.054873470093, 'episode_return_min': -98.5686833518256, 'episode_return_mean': 1930.20330570609}, 'env_runner_results': {'episode_reward_max': 26042.054873470093, 'episode_reward_min': -98.5686833518256, 'episode_reward_mean': 1930.20330570609, 'episode_len_mean': 300.0, 'episode_media': {}, 'episodes_this_iter': 14, 'episodes_timesteps_total': 30000, 'policy_reward_min': {}, 'policy_reward_max': {}, 'policy_reward_mean': {}, 'custom_metrics': {}, 'hist_stats': {'episode_reward': [-92.27266819856116, 25.73075983931291, 20.15284682220323, 4.749758335196418, 10.090455927671556, 70.70791792123086, 96.00401067477735, 63.07796837442251, 63.78997120042281, 33.44352543766661, -46.465104402316875, -0.5203327196826706, 18318.744591704028, 37.14122664199385, 21.13643332215554, 87.87719936101234, 71.45022590217889, -9.101330754919871, -98.5686833518256, 95.33530645806059, 55.4811982213292, 18.30271494299572, -51.26575824571279, 42.082533480452554, -22.445596925520377, 68.74256746797532, 33.05435525796253, -14.032245291742242, 79.26786278759919, 33.07432484402226, -38.11927735077796, 26.551249438526455, 39.52130742704978, 97.97914468484055, 50.10658463112787, 27.31634486537763, 11540.808485968026, 55.878779664342275, 39.91017477423976, 23326.82285928509, -8.006498538129037, 18704.947979701745, -8.362829241117481, 97.42257175376655, 51.19154584835426, 67.80124618096765, 41.902646135463215, -4.600723926245507, 35.53484550023718, 10580.234960158623, -18.71622448426686, 39.01829619121443, 84.04304932371977, -1.4464490134462125, 60.037719617980514, 23.785667828333658, 42.26677431276764, 34.40088252853897, 26042.054873470093, 41.77314813414468, 45.79029879641874, 32.55142799204754, -11.965768183545151, 42.00753025786594, 12293.034255574681, 43.79904868980046, -25.863541249120757, 60.73670702197117, 47.89978373191765, 61.30928534284479, 20899.3740854774, -14.818305615857966, -8.893512470663119, 14833.645636660005, 41.397579473451756, 22821.41310938688, 7.491018013864107, 59.11961027045365, 63.7071514643104, 5.028932524212735, 33.37970701764043, 14.878160110483378, 98.08106277634619, -11.823124640004565, -0.665076458756527, 38.628779430559916, 14.463635071025323, 85.03423308275264, 155.99019409589116, 1.79278515037403, 10570.213268957787, 9.93113203795204, 70.76003720368493, 26.41514838991324, -14.33627414254741, 13.77353032658921, 454.51181271704314, 95.00891723474588, 33.989863382171976, -53.28672419852414], 'episode_lengths': [300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300]}, 'sampler_perf': {'mean_raw_obs_processing_ms': 0.6170270567527532, 'mean_inference_ms': 1.07689741151318, 'mean_action_processing_ms': 0.3920014103326924, 'mean_env_wait_ms': 0.7321249918278235, 'mean_env_render_ms': 0.0}, 'num_faulty_episodes': 0, 'connector_metrics': {'ObsPreprocessorConnector_ms': 0.008563756942749023, 'StateBufferConnector_ms': 0.00696110725402832, 'ViewRequirementAgentConnector_ms': 0.23952293395996094}, 'num_episodes': 14, 'episode_return_max': 26042.054873470093, 'episode_return_min': -98.5686833518256, 'episode_return_mean': 1930.20330570609}, 'episode_reward_max': 26042.054873470093, 'episode_reward_min': -98.5686833518256, 'episode_reward_mean': 1930.20330570609, 'episode_len_mean': 300.0, 'episodes_this_iter': 14, 'episodes_timesteps_total': 30000, 'policy_reward_min': {}, 'policy_reward_max': {}, 'policy_reward_mean': {}, 'hist_stats': {'episode_reward': [-92.27266819856116, 25.73075983931291, 20.15284682220323, 4.749758335196418, 10.090455927671556, 70.70791792123086, 96.00401067477735, 63.07796837442251, 63.78997120042281, 33.44352543766661, -46.465104402316875, -0.5203327196826706, 18318.744591704028, 37.14122664199385, 21.13643332215554, 87.87719936101234, 71.45022590217889, -9.101330754919871, -98.5686833518256, 95.33530645806059, 55.4811982213292, 18.30271494299572, -51.26575824571279, 42.082533480452554, -22.445596925520377, 68.74256746797532, 33.05435525796253, -14.032245291742242, 79.26786278759919, 33.07432484402226, -38.11927735077796, 26.551249438526455, 39.52130742704978, 97.97914468484055, 50.10658463112787, 27.31634486537763, 11540.808485968026, 55.878779664342275, 39.91017477423976, 23326.82285928509, -8.006498538129037, 18704.947979701745, -8.362829241117481, 97.42257175376655, 51.19154584835426, 67.80124618096765, 41.902646135463215, -4.600723926245507, 35.53484550023718, 10580.234960158623, -18.71622448426686, 39.01829619121443, 84.04304932371977, -1.4464490134462125, 60.037719617980514, 23.785667828333658, 42.26677431276764, 34.40088252853897, 26042.054873470093, 41.77314813414468, 45.79029879641874, 32.55142799204754, -11.965768183545151, 42.00753025786594, 12293.034255574681, 43.79904868980046, -25.863541249120757, 60.73670702197117, 47.89978373191765, 61.30928534284479, 20899.3740854774, -14.818305615857966, -8.893512470663119, 14833.645636660005, 41.397579473451756, 22821.41310938688, 7.491018013864107, 59.11961027045365, 63.7071514643104, 5.028932524212735, 33.37970701764043, 14.878160110483378, 98.08106277634619, -11.823124640004565, -0.665076458756527, 38.628779430559916, 14.463635071025323, 85.03423308275264, 155.99019409589116, 1.79278515037403, 10570.213268957787, 9.93113203795204, 70.76003720368493, 26.41514838991324, -14.33627414254741, 13.77353032658921, 454.51181271704314, 95.00891723474588, 33.989863382171976, -53.28672419852414], 'episode_lengths': [300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300]}, 'sampler_perf': {'mean_raw_obs_processing_ms': 0.6170270567527532, 'mean_inference_ms': 1.07689741151318, 'mean_action_processing_ms': 0.3920014103326924, 'mean_env_wait_ms': 0.7321249918278235, 'mean_env_render_ms': 0.0}, 'num_faulty_episodes': 0, 'connector_metrics': {'ObsPreprocessorConnector_ms': 0.008563756942749023, 'StateBufferConnector_ms': 0.00696110725402832, 'ViewRequirementAgentConnector_ms': 0.23952293395996094}, 'num_episodes': 14, 'episode_return_max': 26042.054873470093, 'episode_return_min': -98.5686833518256, 'episode_return_mean': 1930.20330570609, 'num_healthy_workers': 1, 'num_in_flight_async_reqs': 0, 'num_remote_worker_restarts': 0, 'num_agent_steps_sampled': 491520, 'num_agent_steps_trained': 491520, 'num_env_steps_sampled': 163840, 'num_env_steps_trained': 163840, 'num_env_steps_sampled_this_iter': 4096, 'num_env_steps_trained_this_iter': 4096, 'num_env_steps_sampled_throughput_per_sec': 174.64544401360163, 'num_env_steps_trained_throughput_per_sec': 174.64544401360163, 'timesteps_total': 163840, 'num_env_steps_sampled_lifetime': 163840, 'num_agent_steps_sampled_lifetime': 491520, 'num_steps_trained_this_iter': 4096, 'agent_timesteps_total': 491520, 'timers': {'training_iteration_time_ms': 23517.072, 'restore_workers_time_ms': 0.02, 'training_step_time_ms': 23517.014, 'sample_time_ms': 11617.955, 'load_time_ms': 0.833, 'load_throughput': 4919920.153, 'learn_time_ms': 11892.561, 'learn_throughput': 344.417, 'synch_weights_time_ms': 4.667}, 'counters': {'num_env_steps_sampled': 163840, 'num_env_steps_trained': 163840, 'num_agent_steps_sampled': 491520, 'num_agent_steps_trained': 491520}, 'done': False, 'episodes_total': 546, 'training_iteration': 40, 'trial_id': 'default', 'date': '2024-05-23_18-19-04', 'timestamp': 1716481144, 'time_this_iter_s': 23.458407163619995, 'time_total_s': 933.9117562770844, 'pid': 13426, 'hostname': 'LAPTOP-9AD2MD1C', 'node_ip': '172.23.84.145', 'config': {'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, '_fake_gpus': False, 'num_learner_workers': 0, 'num_gpus_per_learner_worker': 0, 'num_cpus_per_learner_worker': 1, 'local_gpu_idx': 0, 'custom_resources_per_worker': {}, 'placement_strategy': 'PACK', 'eager_tracing': True, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'torch_compile_learner': False, 'torch_compile_learner_what_to_compile': <TorchCompileWhatToCompile.FORWARD_TRAIN: 'forward_train'>, 'torch_compile_learner_dynamo_backend': 'inductor', 'torch_compile_learner_dynamo_mode': None, 'torch_compile_worker': False, 'torch_compile_worker_dynamo_backend': 'onnxrt', 'torch_compile_worker_dynamo_mode': None, 'enable_rl_module_and_learner': False, 'enable_env_runner_and_connector_v2': False, 'env': 'KeepTheDistance?dst=0&agent=3&visible_nbrs=2&spawn_area=100', 'env_config': {}, 'observation_space': None, 'action_space': None, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, '_is_atari': None, 'env_task_fn': None, 'render_env': False, 'action_mask_key': 'action_mask', 'env_runner_cls': None, 'num_envs_per_env_runner': 1, 'validate_env_runners_after_construction': True, 'sample_timeout_s': 60.0, '_env_to_module_connector': None, 'add_default_connectors_to_env_to_module_pipeline': True, '_module_to_env_connector': None, 'add_default_connectors_to_module_to_env_pipeline': True, 'episode_lookback_horizon': 1, 'rollout_fragment_length': 'auto', 'batch_mode': 'truncate_episodes', 'compress_observations': False, 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'enable_tf1_exec_eagerly': False, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'update_worker_filter_stats': True, 'use_worker_filter_stats': True, 'enable_connectors': True, 'sampler_perf_stats_ema_coef': None, 'gamma': 0.95, 'lr': 0.001, 'grad_clip': None, 'grad_clip_by': 'global_norm', 'train_batch_size': 4096, 'train_batch_size_per_learner': None, 'model': {'_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'fcnet_weights_initializer': None, 'fcnet_weights_initializer_config': None, 'fcnet_bias_initializer': None, 'fcnet_bias_initializer_config': None, 'conv_filters': None, 'conv_activation': 'relu', 'conv_kernel_initializer': None, 'conv_kernel_initializer_config': None, 'conv_bias_initializer': None, 'conv_bias_initializer_config': None, 'conv_transpose_kernel_initializer': None, 'conv_transpose_kernel_initializer_config': None, 'conv_transpose_bias_initializer': None, 'conv_transpose_bias_initializer_config': None, 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'post_fcnet_weights_initializer': None, 'post_fcnet_weights_initializer_config': None, 'post_fcnet_bias_initializer': None, 'post_fcnet_bias_initializer_config': None, 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, 'lstm_weights_initializer': None, 'lstm_weights_initializer_config': None, 'lstm_bias_initializer': None, 'lstm_bias_initializer_config': None, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'encoder_latent_dim': None, 'always_check_shapes': False, 'lstm_use_prev_action_reward': -1, '_use_default_native_models': -1}, '_learner_connector': None, 'add_default_connectors_to_learner_pipeline': True, 'optimizer': {}, 'max_requests_in_flight_per_sampler_worker': 2, '_learner_class': None, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'algorithm_config_overrides_per_module': {}, '_per_module_overrides': {}, 'count_steps_by': 'env_steps', 'policy_map_capacity': 100, 'policy_mapping_fn': <function AlgorithmConfig.DEFAULT_POLICY_MAPPING_FN at 0x7f326e0fc220>, 'policies_to_train': None, 'policy_states_are_swappable': False, 'observation_fn': None, 'input_config': {}, 'actions_in_input_normalized': False, 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_config': {}, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'offline_sampling': False, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_sample_timeout_s': 120.0, 'evaluation_parallel_to_training': False, 'evaluation_force_reset_envs_before_iteration': True, 'evaluation_config': None, 'off_policy_estimation_methods': {}, 'ope_split_batch_by_episode': True, 'evaluation_num_env_runners': 0, 'always_attach_evaluation_results': True, 'in_evaluation': False, 'sync_filters_on_rollout_workers_timeout_s': 10.0, 'keep_per_episode_custom_metrics': False, 'metrics_episode_collection_timeout_s': 60.0, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_iteration': None, 'min_train_timesteps_per_iteration': 0, 'min_sample_timesteps_per_iteration': 0, 'export_native_model_files': False, 'checkpoint_trainable_policies_only': False, 'logger_creator': None, 'logger_config': None, 'log_level': 'WARN', 'log_sys_usage': True, 'fake_sampler': False, 'seed': None, '_run_training_always_in_thread': False, '_evaluation_parallel_to_training_wo_thread': False, 'ignore_env_runner_failures': False, 'recreate_failed_env_runners': False, 'max_num_env_runner_restarts': 1000, 'delay_between_env_runner_restarts_s': 60.0, 'restart_failed_sub_environments': False, 'num_consecutive_env_runner_failures_tolerance': 100, 'env_runner_health_probe_timeout_s': 30, 'env_runner_restore_timeout_s': 1800, '_model_config_dict': {}, '_rl_module_spec': None, '_AlgorithmConfig__prior_exploration_config': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_initialize_loss_from_dummy_batch': False, 'simple_optimizer': False, 'policy_map_cache': -1, 'worker_cls': -1, 'synchronize_filters': -1, 'enable_async_evaluation': -1, 'custom_async_evaluation_function': -1, '_enable_rl_module_api': -1, 'auto_wrap_old_gym_envs': -1, 'disable_env_checking': -1, 'replay_sequence_length': None, '_disable_execution_plan_api': -1, 'lr_schedule': None, 'use_critic': True, 'use_gae': True, 'use_kl_loss': True, 'kl_coeff': 0.2, 'kl_target': 0.01, 'sgd_minibatch_size': 256, 'mini_batch_size_per_learner': None, 'num_sgd_iter': 30, 'shuffle_sequences': True, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.0, 'entropy_coeff_schedule': None, 'clip_param': 0.3, 'vf_clip_param': 10.0, 'vf_share_layers': -1, 'lambda': 1.0, 'input': 'sampler', 'policies': {'default_policy': (None, None, None, None)}, 'callbacks': <class 'ray.rllib.algorithms.callbacks.DefaultCallbacks'>, 'create_env_on_driver': False, 'custom_eval_function': None, 'framework': 'torch', 'num_cpus_for_driver': 1, 'num_workers': 1}, 'time_since_restore': 933.9117562770844, 'iterations_since_restore': 40, 'perf': {'cpu_util_percent': 35.35609756097562, 'ram_util_percent': 79.00731707317074}})'.\n"
     ]
    }
   ],
   "source": [
    "save_algo(algo, \"KeepTheDistance?dst=0&agent=3&visible_nbrs=2&spawn_area=100\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KeepTheDistance?dst=0&agent=4&visible_nbrs=3&spawn_area=100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ray.tune.registry import register_env\n",
    "\n",
    "train_batch_size = 4096\n",
    "reset_per_batch = train_batch_size/300\n",
    "\n",
    "spawn_area_schedule = [[0,10],[4,30],[9,50],[18,100]]\n",
    "#spawn_area_schedule = [[0,10],[4,30],[10,50],[18,100]]\n",
    "\n",
    "env_config = EnvironmentConfiguration(n_agents=4, visible_nbrs=3, target_distance=0, max_steps=1000, speed=1, spawn_area=20, \n",
    "                                      spawn_area_schedule=[[schedule[0]*train_batch_size, schedule[1]] for schedule in spawn_area_schedule])\n",
    "env_config_show = EnvironmentConfiguration(n_agents=4, visible_nbrs=3, target_distance=0, max_steps=300, speed=1, spawn_area=20, \n",
    "                                      spawn_area_schedule=spawn_area_schedule)\n",
    "register_env(\"KeepTheDistance?dst=0&agent=4&visible_nbrs=3&spawn_area=100\", lambda _: KeepTheDistance(env_config))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "The specified subfolder '/mnt/c/Users/nicol/Desktop/Universit/tesi/experiments/RL_experiments/algos/KeepTheDistance?dst=0&agent=4&visible_nbrs=3&spawn_area=100' does not exist.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[101], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m algo \u001b[38;5;241m=\u001b[39m \u001b[43mload_algo\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mKeepTheDistance?dst=0&agent=4&visible_nbrs=3&spawn_area=100\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[1], line 33\u001b[0m, in \u001b[0;36mload_algo\u001b[0;34m(name)\u001b[0m\n\u001b[1;32m     31\u001b[0m subfolder_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(base_dir, name)\n\u001b[1;32m     32\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexists(subfolder_path):\n\u001b[0;32m---> 33\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe specified subfolder \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00msubfolder_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m does not exist.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     35\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m Algorithm\u001b[38;5;241m.\u001b[39mfrom_checkpoint(subfolder_path)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: The specified subfolder '/mnt/c/Users/nicol/Desktop/Universit/tesi/experiments/RL_experiments/algos/KeepTheDistance?dst=0&agent=4&visible_nbrs=3&spawn_area=100' does not exist."
     ]
    }
   ],
   "source": [
    "algo = load_algo(\"KeepTheDistance?dst=0&agent=4&visible_nbrs=3&spawn_area=100\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c9e5141cfd324caa90526592c7cad488",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "CanvasWithBorders(height=300, width=300)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration [1] => episode_reward_mean: 241.53260623370483, episode_len_mean: 1000.0, agent_steps_trained: 16384, env_steps_trained: 4096, entropy: 4.262171379725138, learning_rate: 0.0010000000000000005\n",
      "iteration [2] => episode_reward_mean: 8972.364221824693, episode_len_mean: 1000.0, agent_steps_trained: 32768, env_steps_trained: 8192, entropy: 4.166322618474563, learning_rate: 0.0010000000000000005\n",
      "iteration [3] => episode_reward_mean: 20690.61975089599, episode_len_mean: 1000.0, agent_steps_trained: 49152, env_steps_trained: 12288, entropy: 4.057953937600057, learning_rate: 0.0010000000000000005\n",
      "iteration [4] => episode_reward_mean: 29882.30053861677, episode_len_mean: 1000.0, agent_steps_trained: 65536, env_steps_trained: 16384, entropy: 4.133171391238769, learning_rate: 0.0010000000000000005\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[105], line 26\u001b[0m\n\u001b[1;32m     24\u001b[0m env_to_show \u001b[38;5;241m=\u001b[39m RenderableKeepTheDistance(env_config_show)\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(trainin_steps):\n\u001b[0;32m---> 26\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43malgo\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     27\u001b[0m     \u001b[38;5;66;03m#clear_output()\u001b[39;00m\n\u001b[1;32m     28\u001b[0m     \u001b[38;5;66;03m#print(out)\u001b[39;00m\n\u001b[1;32m     29\u001b[0m     simulate_episode(env_to_show, algo, \u001b[38;5;241m150\u001b[39m, sleep_between_frames\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.03\u001b[39m, print_info\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "File \u001b[0;32m~/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/tune/trainable/trainable.py:328\u001b[0m, in \u001b[0;36mTrainable.train\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    326\u001b[0m start \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m    327\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 328\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    329\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    330\u001b[0m     skipped \u001b[38;5;241m=\u001b[39m skip_exceptions(e)\n",
      "File \u001b[0;32m~/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/rllib/algorithms/algorithm.py:873\u001b[0m, in \u001b[0;36mAlgorithm.step\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    863\u001b[0m     (\n\u001b[1;32m    864\u001b[0m         train_results,\n\u001b[1;32m    865\u001b[0m         eval_results,\n\u001b[1;32m    866\u001b[0m         train_iter_ctx,\n\u001b[1;32m    867\u001b[0m     ) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_run_one_training_iteration_and_evaluation_in_parallel()\n\u001b[1;32m    869\u001b[0m \u001b[38;5;66;03m# - No evaluation necessary, just run the next training iteration.\u001b[39;00m\n\u001b[1;32m    870\u001b[0m \u001b[38;5;66;03m# - We have to evaluate in this training iteration, but no parallelism ->\u001b[39;00m\n\u001b[1;32m    871\u001b[0m \u001b[38;5;66;03m#   evaluate after the training iteration is entirely done.\u001b[39;00m\n\u001b[1;32m    872\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 873\u001b[0m     train_results, train_iter_ctx \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_one_training_iteration\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    875\u001b[0m \u001b[38;5;66;03m# Sequential: Train (already done above), then evaluate.\u001b[39;00m\n\u001b[1;32m    876\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m evaluate_this_iter \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mevaluation_parallel_to_training:\n",
      "File \u001b[0;32m~/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/rllib/algorithms/algorithm.py:3156\u001b[0m, in \u001b[0;36mAlgorithm._run_one_training_iteration\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   3154\u001b[0m             \u001b[38;5;66;03m# Try to train one step.\u001b[39;00m\n\u001b[1;32m   3155\u001b[0m             \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_timers[TRAINING_STEP_TIMER]:\n\u001b[0;32m-> 3156\u001b[0m                 results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3158\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m results, train_iter_ctx\n",
      "File \u001b[0;32m~/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/rllib/algorithms/ppo/ppo.py:428\u001b[0m, in \u001b[0;36mPPO.training_step\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    424\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_training_step_new_api_stack()\n\u001b[1;32m    425\u001b[0m \u001b[38;5;66;03m# Old and hybrid API stacks (Policy, RolloutWorker, Connector, maybe RLModule,\u001b[39;00m\n\u001b[1;32m    426\u001b[0m \u001b[38;5;66;03m# maybe Learner).\u001b[39;00m\n\u001b[1;32m    427\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 428\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_training_step_old_and_hybrid_api_stacks\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/rllib/algorithms/ppo/ppo.py:587\u001b[0m, in \u001b[0;36mPPO._training_step_old_and_hybrid_api_stacks\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    585\u001b[0m     train_results \u001b[38;5;241m=\u001b[39m train_one_step(\u001b[38;5;28mself\u001b[39m, train_batch)\n\u001b[1;32m    586\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 587\u001b[0m     train_results \u001b[38;5;241m=\u001b[39m \u001b[43mmulti_gpu_train_one_step\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    589\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39menable_rl_module_and_learner:\n\u001b[1;32m    590\u001b[0m     \u001b[38;5;66;03m# The train results's loss keys are pids to their loss values. But we also\u001b[39;00m\n\u001b[1;32m    591\u001b[0m     \u001b[38;5;66;03m# return a total_loss key at the same level as the pid keys. So we need to\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    594\u001b[0m     \u001b[38;5;66;03m#  passing medium to infer which policies to update. We could use\u001b[39;00m\n\u001b[1;32m    595\u001b[0m     \u001b[38;5;66;03m#  policies_to_train variable that is given by the user to infer this.\u001b[39;00m\n\u001b[1;32m    596\u001b[0m     policies_to_update \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m(train_results\u001b[38;5;241m.\u001b[39mkeys()) \u001b[38;5;241m-\u001b[39m {ALL_MODULES}\n",
      "File \u001b[0;32m~/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/rllib/execution/train_ops.py:176\u001b[0m, in \u001b[0;36mmulti_gpu_train_one_step\u001b[0;34m(algorithm, train_batch)\u001b[0m\n\u001b[1;32m    171\u001b[0m         permutation \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mpermutation(num_batches)\n\u001b[1;32m    172\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m batch_index \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_batches):\n\u001b[1;32m    173\u001b[0m             \u001b[38;5;66;03m# Learn on the pre-loaded data in the buffer.\u001b[39;00m\n\u001b[1;32m    174\u001b[0m             \u001b[38;5;66;03m# Note: For minibatch SGD, the data is an offset into\u001b[39;00m\n\u001b[1;32m    175\u001b[0m             \u001b[38;5;66;03m# the pre-loaded entire train batch.\u001b[39;00m\n\u001b[0;32m--> 176\u001b[0m             results \u001b[38;5;241m=\u001b[39m \u001b[43mpolicy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearn_on_loaded_batch\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    177\u001b[0m \u001b[43m                \u001b[49m\u001b[43mpermutation\u001b[49m\u001b[43m[\u001b[49m\u001b[43mbatch_index\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mper_device_batch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\n\u001b[1;32m    178\u001b[0m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    180\u001b[0m             learner_info_builder\u001b[38;5;241m.\u001b[39madd_learn_on_batch_results(results, policy_id)\n\u001b[1;32m    182\u001b[0m \u001b[38;5;66;03m# Tower reduce and finalize results.\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/rllib/policy/torch_policy_v2.py:838\u001b[0m, in \u001b[0;36mTorchPolicyV2.learn_on_loaded_batch\u001b[0;34m(self, offset, buffer_index)\u001b[0m\n\u001b[1;32m    835\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    836\u001b[0m         batch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_loaded_batches[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m][offset : offset \u001b[38;5;241m+\u001b[39m device_batch_size]\n\u001b[0;32m--> 838\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearn_on_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    840\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevices) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m    841\u001b[0m     \u001b[38;5;66;03m# Copy weights of main model (tower-0) to all other towers.\u001b[39;00m\n\u001b[1;32m    842\u001b[0m     state_dict \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mstate_dict()\n",
      "File \u001b[0;32m~/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/rllib/utils/threading.py:24\u001b[0m, in \u001b[0;36mwith_lock.<locals>.wrapper\u001b[0;34m(self, *a, **k)\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     23\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[0;32m---> 24\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mk\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     26\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas no attribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_lock\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m e\u001b[38;5;241m.\u001b[39margs[\u001b[38;5;241m0\u001b[39m]:\n",
      "File \u001b[0;32m~/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/rllib/policy/torch_policy_v2.py:715\u001b[0m, in \u001b[0;36mTorchPolicyV2.learn_on_batch\u001b[0;34m(self, postprocessed_batch)\u001b[0m\n\u001b[1;32m    709\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallbacks\u001b[38;5;241m.\u001b[39mon_learn_on_batch(\n\u001b[1;32m    710\u001b[0m     policy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m, train_batch\u001b[38;5;241m=\u001b[39mpostprocessed_batch, result\u001b[38;5;241m=\u001b[39mlearn_stats\n\u001b[1;32m    711\u001b[0m )\n\u001b[1;32m    713\u001b[0m \u001b[38;5;66;03m# Compute gradients (will calculate all losses and `backward()`\u001b[39;00m\n\u001b[1;32m    714\u001b[0m \u001b[38;5;66;03m# them to get the grads).\u001b[39;00m\n\u001b[0;32m--> 715\u001b[0m grads, fetches \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_gradients\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpostprocessed_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    717\u001b[0m \u001b[38;5;66;03m# Step the optimizers.\u001b[39;00m\n\u001b[1;32m    718\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply_gradients(_directStepOptimizerSingleton)\n",
      "File \u001b[0;32m~/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/rllib/utils/threading.py:24\u001b[0m, in \u001b[0;36mwith_lock.<locals>.wrapper\u001b[0;34m(self, *a, **k)\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     23\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[0;32m---> 24\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mk\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     26\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas no attribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_lock\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m e\u001b[38;5;241m.\u001b[39margs[\u001b[38;5;241m0\u001b[39m]:\n",
      "File \u001b[0;32m~/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/rllib/policy/torch_policy_v2.py:933\u001b[0m, in \u001b[0;36mTorchPolicyV2.compute_gradients\u001b[0;34m(self, postprocessed_batch)\u001b[0m\n\u001b[1;32m    930\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lazy_tensor_dict(postprocessed_batch, device\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevices[\u001b[38;5;241m0\u001b[39m])\n\u001b[1;32m    932\u001b[0m \u001b[38;5;66;03m# Do the (maybe parallelized) gradient calculation step.\u001b[39;00m\n\u001b[0;32m--> 933\u001b[0m tower_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_multi_gpu_parallel_grad_calc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mpostprocessed_batch\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    935\u001b[0m all_grads, grad_info \u001b[38;5;241m=\u001b[39m tower_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    937\u001b[0m grad_info[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mallreduce_latency\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_optimizers)\n",
      "File \u001b[0;32m~/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/rllib/policy/torch_policy_v2.py:1429\u001b[0m, in \u001b[0;36mTorchPolicyV2._multi_gpu_parallel_grad_calc\u001b[0;34m(self, sample_batches)\u001b[0m\n\u001b[1;32m   1425\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevices) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_fake_gpus\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[1;32m   1426\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m shard_idx, (model, sample_batch, device) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\n\u001b[1;32m   1427\u001b[0m         \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_gpu_towers, sample_batches, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevices)\n\u001b[1;32m   1428\u001b[0m     ):\n\u001b[0;32m-> 1429\u001b[0m         \u001b[43m_worker\u001b[49m\u001b[43m(\u001b[49m\u001b[43mshard_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_batch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1430\u001b[0m         \u001b[38;5;66;03m# Raise errors right away for better debugging.\u001b[39;00m\n\u001b[1;32m   1431\u001b[0m         last_result \u001b[38;5;241m=\u001b[39m results[\u001b[38;5;28mlen\u001b[39m(results) \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m]\n",
      "File \u001b[0;32m~/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/rllib/policy/torch_policy_v2.py:1348\u001b[0m, in \u001b[0;36mTorchPolicyV2._multi_gpu_parallel_grad_calc.<locals>._worker\u001b[0;34m(shard_idx, model, sample_batch, device)\u001b[0m\n\u001b[1;32m   1343\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1344\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m NullContextManager() \u001b[38;5;28;01mif\u001b[39;00m device\u001b[38;5;241m.\u001b[39mtype \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mdevice(  \u001b[38;5;66;03m# noqa: E501\u001b[39;00m\n\u001b[1;32m   1345\u001b[0m         device\n\u001b[1;32m   1346\u001b[0m     ):\n\u001b[1;32m   1347\u001b[0m         loss_out \u001b[38;5;241m=\u001b[39m force_list(\n\u001b[0;32m-> 1348\u001b[0m             \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdist_class\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1349\u001b[0m         )\n\u001b[1;32m   1351\u001b[0m         \u001b[38;5;66;03m# Call Model's custom-loss with Policy loss outputs and\u001b[39;00m\n\u001b[1;32m   1352\u001b[0m         \u001b[38;5;66;03m# train_batch.\u001b[39;00m\n\u001b[1;32m   1353\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(model, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcustom_loss\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m~/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/rllib/algorithms/ppo/ppo_torch_policy.py:84\u001b[0m, in \u001b[0;36mPPOTorchPolicy.loss\u001b[0;34m(self, model, dist_class, train_batch)\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;129m@override\u001b[39m(TorchPolicyV2)\n\u001b[1;32m     67\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mloss\u001b[39m(\n\u001b[1;32m     68\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     71\u001b[0m     train_batch: SampleBatch,\n\u001b[1;32m     72\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Union[TensorType, List[TensorType]]:\n\u001b[1;32m     73\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Compute loss for Proximal Policy Objective.\u001b[39;00m\n\u001b[1;32m     74\u001b[0m \n\u001b[1;32m     75\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     81\u001b[0m \u001b[38;5;124;03m        The PPO loss tensor given the input batch.\u001b[39;00m\n\u001b[1;32m     82\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 84\u001b[0m     logits, state \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     85\u001b[0m     curr_action_dist \u001b[38;5;241m=\u001b[39m dist_class(logits, model)\n\u001b[1;32m     87\u001b[0m     \u001b[38;5;66;03m# RNN case: Mask away 0-padded chunks at end of time axis.\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/rllib/models/modelv2.py:244\u001b[0m, in \u001b[0;36mModelV2.__call__\u001b[0;34m(self, input_dict, state, seq_lens)\u001b[0m\n\u001b[1;32m    238\u001b[0m     restored[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mobs_flat\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m input_dict[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mobs\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m    239\u001b[0m \u001b[38;5;66;03m# Input to this Model went through a Preprocessor.\u001b[39;00m\n\u001b[1;32m    240\u001b[0m \u001b[38;5;66;03m# Generate extra keys: \"obs_flat\" (vs \"obs\", which will hold the\u001b[39;00m\n\u001b[1;32m    241\u001b[0m \u001b[38;5;66;03m# original obs).\u001b[39;00m\n\u001b[1;32m    242\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    243\u001b[0m     restored[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mobs\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m restore_original_dimensions(\n\u001b[0;32m--> 244\u001b[0m         \u001b[43minput_dict\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mobs\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobs_space, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mframework\n\u001b[1;32m    245\u001b[0m     )\n\u001b[1;32m    246\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    247\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(input_dict[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mobs\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mshape) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m2\u001b[39m:\n",
      "File \u001b[0;32m~/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/rllib/policy/sample_batch.py:953\u001b[0m, in \u001b[0;36mSampleBatch.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    951\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_interceptor \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    952\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m key \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mintercepted_values:\n\u001b[0;32m--> 953\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mintercepted_values[key] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_interceptor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    954\u001b[0m     value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mintercepted_values[key]\n\u001b[1;32m    955\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m value\n",
      "File \u001b[0;32m~/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/rllib/utils/torch_utils.py:276\u001b[0m, in \u001b[0;36mconvert_to_torch_tensor\u001b[0;34m(x, device)\u001b[0m\n\u001b[1;32m    272\u001b[0m         tensor \u001b[38;5;241m=\u001b[39m tensor\u001b[38;5;241m.\u001b[39mfloat()\n\u001b[1;32m    274\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m tensor \u001b[38;5;28;01mif\u001b[39;00m device \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m tensor\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m--> 276\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtree\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap_structure\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmapping\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/tianEnv/lib/python3.11/site-packages/tree/__init__.py:435\u001b[0m, in \u001b[0;36mmap_structure\u001b[0;34m(func, *structures, **kwargs)\u001b[0m\n\u001b[1;32m    432\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m other \u001b[38;5;129;01min\u001b[39;00m structures[\u001b[38;5;241m1\u001b[39m:]:\n\u001b[1;32m    433\u001b[0m   assert_same_structure(structures[\u001b[38;5;241m0\u001b[39m], other, check_types\u001b[38;5;241m=\u001b[39mcheck_types)\n\u001b[1;32m    434\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m unflatten_as(structures[\u001b[38;5;241m0\u001b[39m],\n\u001b[0;32m--> 435\u001b[0m                     \u001b[43m[\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mzip\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mmap\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mflatten\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstructures\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m)\n",
      "File \u001b[0;32m~/anaconda3/envs/tianEnv/lib/python3.11/site-packages/tree/__init__.py:435\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    432\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m other \u001b[38;5;129;01min\u001b[39;00m structures[\u001b[38;5;241m1\u001b[39m:]:\n\u001b[1;32m    433\u001b[0m   assert_same_structure(structures[\u001b[38;5;241m0\u001b[39m], other, check_types\u001b[38;5;241m=\u001b[39mcheck_types)\n\u001b[1;32m    434\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m unflatten_as(structures[\u001b[38;5;241m0\u001b[39m],\n\u001b[0;32m--> 435\u001b[0m                     [\u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m args \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mmap\u001b[39m(flatten, structures))])\n",
      "File \u001b[0;32m~/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/rllib/utils/torch_utils.py:265\u001b[0m, in \u001b[0;36mconvert_to_torch_tensor.<locals>.mapping\u001b[0;34m(item)\u001b[0m\n\u001b[1;32m    262\u001b[0m             tensor \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mfrom_numpy(item)\n\u001b[1;32m    263\u001b[0m     \u001b[38;5;66;03m# Already numpy: Wrap as torch tensor.\u001b[39;00m\n\u001b[1;32m    264\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 265\u001b[0m         tensor \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_numpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mitem\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    266\u001b[0m \u001b[38;5;66;03m# Everything else: Convert to numpy, then wrap as torch tensor.\u001b[39;00m\n\u001b[1;32m    267\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    268\u001b[0m     tensor \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mfrom_numpy(np\u001b[38;5;241m.\u001b[39masarray(item))\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from ray.rllib.algorithms.ppo import PPOConfig\n",
    "from ray.tune.logger import pretty_print\n",
    "\n",
    "from gymnasium.wrappers.time_limit import TimeLimit\n",
    "\n",
    "trainin_steps = 40\n",
    "\n",
    "algo = (\n",
    "    PPOConfig()\n",
    "    .training(gamma = 0.95, \n",
    "              lr = 0.001,\n",
    "              train_batch_size = train_batch_size, \n",
    "              sgd_minibatch_size = 256, \n",
    "              num_sgd_iter = 30,\n",
    "              #entropy_coeff=0.005,\n",
    "              )\n",
    "    .env_runners(num_env_runners=1)\n",
    "    .resources(num_gpus=0)\n",
    "    .environment(env=\"KeepTheDistance?dst=0&agent=4&visible_nbrs=3&spawn_area=100\")\n",
    "    .build()\n",
    ")\n",
    "clear_output()\n",
    "\n",
    "env_to_show = RenderableKeepTheDistance(env_config_show)\n",
    "for i in range(trainin_steps):\n",
    "    result = algo.train()\n",
    "    #clear_output()\n",
    "    #print(out)\n",
    "    simulate_episode(env_to_show, algo, 150, sleep_between_frames=0.03, print_info=False)\n",
    "    print(ppo_result_format(result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5066be2dfd4447a39660264441ee19cb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "CanvasWithBorders(height=300, width=300)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "env_config_2 = EnvironmentConfiguration(n_agents=4, visible_nbrs=3, target_distance=0, max_steps=500, speed=1, spawn_area=100)\n",
    "simulate_episode(RenderableKeepTheDistance(env_config_2), algo, 300, sleep_between_frames=0.03, print_info=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_algo(algo, \"KeepTheDistance?dst=0&agent=3&visible_nbrs=2&spawn_area=100\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tianEnv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
