{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Keep average distance\n",
    "\n",
    "the agents goal is to position close to each others at a distance previously defined\n",
    "\n",
    "challenges:\n",
    "- deal with continuous space environment\n",
    "- limited vision of an agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ipycanvas import Canvas, hold_canvas\n",
    "\n",
    "class CanvasWithBorders(Canvas):\n",
    "    def clear(self):\n",
    "        super().clear()\n",
    "        border_color = 'black'  # You can customize the border color here\n",
    "        border_width = 1  # You can customize the border width here\n",
    "        \n",
    "        self.fill_style = border_color\n",
    "        # Draw top border\n",
    "        self.fill_rect(0, 0, self.width, border_width)\n",
    "        # Draw bottom border\n",
    "        self.fill_rect(0, self.height - border_width, self.width, border_width)\n",
    "        # Draw left border\n",
    "        self.fill_rect(0, 0, border_width, self.height)\n",
    "        # Draw right border\n",
    "        self.fill_rect(self.width - border_width, 0, border_width, self.height)\n",
    "\n",
    "import os\n",
    "from ray.rllib.algorithms.algorithm import Algorithm\n",
    "\n",
    "def save_algo(algo, name):\n",
    "    base_dir = os.path.join(os.getcwd(), \"algos\")\n",
    "    subfolder_path = os.path.join(base_dir, name)\n",
    "    os.makedirs(subfolder_path, exist_ok=True)\n",
    "    path_to_checkpoint  = algo.save(subfolder_path)\n",
    "    print(f\"An Algorithm checkpoint has been created inside directory: '{path_to_checkpoint}'.\")\n",
    "\n",
    "def load_algo(name):\n",
    "    base_dir = os.path.join(os.getcwd(), \"algos\")\n",
    "    subfolder_path = os.path.join(base_dir, name)\n",
    "    if not os.path.exists(subfolder_path):\n",
    "        raise FileNotFoundError(f\"The specified subfolder '{subfolder_path}' does not exist.\")\n",
    "    \n",
    "    return Algorithm.from_checkpoint(subfolder_path)\n",
    "\n",
    "#save_algo(algo, \"KeepTheDistance_dst=0_agent=2_100x100train\")\n",
    "#algo2 = load_algo(\"KeepTheDistance_dst=0_agent=2_100x100train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random as rnd\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "class Vector2D():\n",
    "    def __init__(self, x, y):\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "    \n",
    "    def __str__(self):\n",
    "        return f\"({self.x}, {self.y})\"\n",
    "\n",
    "    def to_np_array(self):\n",
    "        return np.array([self.x, self.y], dtype=np.float32)\n",
    "\n",
    "    def get_random_point(max_x, max_y, min_x=0, min_y=0):\n",
    "        return Vector2D(rnd.randint(min_x, max_x-1), rnd.randint(min_y, max_y-1))\n",
    "    \n",
    "    def distance_vector(v1, v2):\n",
    "        return Vector2D(v1.x-v2.x, v1.y-v2.y)\n",
    "    \n",
    "    def distance(v1, v2):\n",
    "        distance_vector = Vector2D.distance_vector(v1, v2)\n",
    "        return Vector2D.norm(distance_vector)\n",
    "\n",
    "    def norm(v):\n",
    "        return math.sqrt(math.pow(v.x, 2) + math.pow(v.y, 2))\n",
    "\n",
    "    def unit_vector(v):\n",
    "        norm = Vector2D.norm(v)\n",
    "        if norm == 0:\n",
    "            return Vector2D(0,0)\n",
    "        return Vector2D(v.x/norm, v.y/norm)\n",
    "\n",
    "    def similarity(v1, v2):\n",
    "        x1, y1 = v1.x, v1.y\n",
    "        x2, y2 = v2.x, v2.y\n",
    "        v1_value = math.degrees(math.atan2(y1, x1))/180\n",
    "        v2_value = math.degrees(math.atan2(y2, x2))/180\n",
    "        diff = abs(v1_value-v2_value)\n",
    "        similarity = 1 - math.pow(diff, 0.9)\n",
    "        return similarity\n",
    "    \n",
    "    def sum(v1, v2):\n",
    "        return Vector2D(v1.x+v2.x, v1.y+v2.y)\n",
    "    \n",
    "    def mul(v, n):\n",
    "        return Vector2D(v.x*n, v.y*n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import random as rnd\n",
    "\n",
    "def simulate_episode(env, policy, steps, sleep_between_frames=0.3, print_info=True):\n",
    "    obs, _ = env.reset()\n",
    "    env.render()\n",
    "\n",
    "    for i in range(steps):\n",
    "        if print_info:\n",
    "            print(f\"obs: \", obs)\n",
    "        actions = policy.compute_actions(obs)\n",
    "        #actions = {agent: np.array([rnd.random()*2-1, rnd.random()*2-1, 1.0], np.float32) for agent in obs.keys()}\n",
    "        #actions = {agent: env.action_space.sample() for agent in obs.keys()}\n",
    "        obs, reward, _, _, _ = env.step(actions)\n",
    "        env.render()\n",
    "        if print_info:\n",
    "            print(f\"action: \", actions)\n",
    "            print(f\"reward: \", reward, \"\\n\")\n",
    "        time.sleep(sleep_between_frames)\n",
    "\n",
    "def simulate_random_episode(env, steps, sleep_between_frames=0.3, print_info=True):\n",
    "    obs, _ = env.reset()\n",
    "    env.render()\n",
    "\n",
    "    action_space = env.action_space\n",
    "    for i in range(steps):\n",
    "        if print_info:\n",
    "            print(f\"obs: \", obs)\n",
    "        actions = {agent: action_space.sample() for agent in obs.keys()}\n",
    "        obs, reward, _, _, _ = env.step(actions)\n",
    "        env.render()\n",
    "        if print_info:\n",
    "            print(f\"action: \", actions)\n",
    "            print(f\"reward: \", reward, \"\\n\")\n",
    "        time.sleep(sleep_between_frames)\n",
    "\n",
    "def ppo_result_format(result):\n",
    "    return (f\"iteration [{result['training_iteration']}] => \" +\n",
    "          f\"episode_reward_mean: {result['sampler_results']['episode_reward_mean']}, \" +\n",
    "          f\"episode_len_mean: {result['sampler_results']['episode_len_mean']}, \" +\n",
    "          f\"agent_steps_trained: {result['info']['num_agent_steps_trained']}, \" +\n",
    "          f\"env_steps_trained: {result['info']['num_env_steps_trained']}, \" + \n",
    "          f\"entropy: {result['info']['learner']['default_policy']['learner_stats']['entropy']}, \" +\n",
    "          f\"learning_rate: {result['info']['learner']['default_policy']['learner_stats']['cur_lr']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### environment definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Set\n",
    "from ray.rllib.env.multi_agent_env import MultiAgentEnv\n",
    "import random as rnd\n",
    "from gymnasium.spaces import Discrete, Box, Dict, Tuple, MultiDiscrete\n",
    "from gymnasium.spaces.utils import flatten, flatten_space\n",
    "import numpy as np\n",
    "from IPython.display import clear_output\n",
    "import math\n",
    "from ipycanvas import Canvas, hold_canvas\n",
    "\n",
    "class EnvironmentConfiguration: \n",
    "    def __init__(self, n_agents, target_distance, speed, spawn_area=100, visible_nbrs=1, max_steps=None, spawn_area_schedule=None):\n",
    "        self.n_agents = n_agents\n",
    "        self.visible_nbrs = visible_nbrs\n",
    "        self.target_distance = target_distance\n",
    "        self.max_steps = max_steps\n",
    "        self.speed = speed\n",
    "        self.spawn_area = spawn_area\n",
    "        self.spawn_area_schedule = spawn_area_schedule\n",
    "\n",
    "class KeepTheDistance(MultiAgentEnv):\n",
    "\n",
    "    canvas = None\n",
    "    CANVAS_WIDTH, CANVAS_HEIGHT = 300.0, 300.0\n",
    "\n",
    "    def __init__(self, config: EnvironmentConfiguration):\n",
    "        assert config.n_agents > config.visible_nbrs # just base case implemented \n",
    "             \n",
    "        self.n_agents = config.n_agents\n",
    "        self.visible_nbrs = config.visible_nbrs\n",
    "        self.target_distance = config.target_distance\n",
    "        self.max_steps = config.max_steps\n",
    "        self.speed = config.speed\n",
    "        self.spawn_area = config.spawn_area\n",
    "        self.spawn_area_schedule = config.spawn_area_schedule\n",
    "        if self.spawn_area_schedule != None:\n",
    "            self.spawn_area_schedule_index = 0\n",
    "            self.n_reset = 0\n",
    "            self.spawn_area = self.spawn_area_schedule[0][1]\n",
    "        \n",
    "        self.agents_ids = ['agent-' + str(i) for i in range(self.n_agents)]\n",
    "        self.agent_colors = {agent: self.rgb_to_hex(rnd.randint(0, 255), rnd.randint(0, 255), rnd.randint(0, 255)) for agent in self.agents_ids}\n",
    "        self.observation_space = self.observation_space('agent-0')\n",
    "        self.action_space = self.action_space(\"\")\n",
    "\n",
    "    def unflatten_observation_space(self, agent):\n",
    "        #distance_vector = Box(low=-np.inf, high=np.inf, shape=(2,1), dtype=np.float32)\n",
    "        #obs_space = Dict({\"nbr-1\": distance_vector})\n",
    "        direction = Box(low=-1, high=1, shape=(2,1), dtype=np.float32)\n",
    "        distance = Box(low=-np.inf, high=np.inf, shape=(1,1), dtype=np.float32)\n",
    "        return Dict({f\"nbr-{i}\": Dict({'direction': direction, 'distance': distance}) for i in range(self.visible_nbrs)})\n",
    "\n",
    "    def observation_space(self, agent):\n",
    "        return flatten_space(self.unflatten_observation_space(agent))\n",
    "\n",
    "    def action_space(self, agent):\n",
    "        direction = Box(low=-1.0, high=1.0, shape=(2,1), dtype=np.float32)\n",
    "        speed = Box(0.0, 1.0, dtype=np.float32)\n",
    "        return flatten_space(Tuple([direction, speed]))\n",
    "    \n",
    "    def __get_observation(self, agent):\n",
    "        distance_vectors = [Vector2D.distance_vector(self.agents_pos[agent], self.agents_pos[nbr])  \n",
    "                            for nbr in self.__get_n_closest_neighbours(agent, self.visible_nbrs)]\n",
    "\n",
    "        obs = {\n",
    "            f\"nbr-{i}\": {\n",
    "                \"direction\": Vector2D.unit_vector(distance_vectors[i]).to_np_array(),\n",
    "                \"distance\": np.log(1 + Vector2D.norm(distance_vectors[i])) #1 - np.exp(-alpha * x)\n",
    "            }\n",
    "            for i in range(len(distance_vectors))\n",
    "            }\n",
    "        return flatten(self.unflatten_observation_space(agent), obs)\n",
    "\n",
    "    def rgb_to_hex(self, r, g, b):\n",
    "        return f'#{r:02x}{g:02x}{b:02x}'\n",
    "\n",
    "    def __total_distance_from_closest_neighbours(self, agent):\n",
    "        return sum([abs(Vector2D.distance(self.agents_pos[agent], self.agents_pos[nbr]) - self.target_distance) for nbr in self.__get_n_closest_neighbours(agent, self.visible_nbrs)])\n",
    "\n",
    "    def __get_local_reward(self, agent, action):\n",
    "        last_action = self.last_actions[agent]\n",
    "        self.last_actions[agent] = action\n",
    "\n",
    "        # r0: negative of the distance from the closest neighbours\n",
    "        reward_0 = -self.__total_distance_from_closest_neighbours(agent)\n",
    "\n",
    "        # r1: improvement of the distance from the closest neighbours\n",
    "        newDistance = self.__total_distance_from_closest_neighbours(agent)\n",
    "        reward_1 = self.last_step_distances[agent] - newDistance\n",
    "        self.last_step_distances[agent] = newDistance\n",
    "\n",
    "        # r2: bonus if the agent is very close to the target distance\n",
    "        closest_nbrs = self.__get_n_closest_neighbours(agent, self.visible_nbrs)\n",
    "        #reward_2 = sum([100 if abs(Vector2D.distance(self.agents_pos[agent], self.agents_pos[nbr]) - self.target_distance) < 0.5 else 0 for nbr in closest_nbrs])\n",
    "        reward_2 = sum([max(0, 1 - (abs(Vector2D.distance(self.agents_pos[agent], self.agents_pos[nbr]) - self.target_distance) / self.target_distance)) for nbr in closest_nbrs])\n",
    "\n",
    "\n",
    "        # r3: penalize rapid changes of direction\n",
    "        reward_3 = -(1-Vector2D.similarity(Vector2D(action[0],action[1]), Vector2D(last_action[0],last_action[1])))\n",
    "\n",
    "        #reward_4 = -action[2]*10\n",
    "        return reward_1 + reward_2 # + reward_3 #+ reward_3# working for two agents using value for reward_2 equals to one\n",
    "\n",
    "    def __get_global_reward(self):\n",
    "        return 0\n",
    "    \n",
    "    def __get_other_agents(self, agent):\n",
    "        return [other for other in self.agents_ids if other != agent]\n",
    "\n",
    "    def __get_n_closest_neighbours(self, agent, n=1):\n",
    "        distances = {other: Vector2D.distance(self.agents_pos[agent], self.agents_pos[other]) for other in self.__get_other_agents(agent)}\n",
    "        return [neighbour[0] for neighbour in sorted(list(distances.items()), key=lambda d: d[1])[:n]]\n",
    "        # return {neighbour[0]: neighbour[1] for neighbour in sorted(list(dst.items()), key=lambda d: d[0])[:n]}\n",
    "\n",
    "    def __update_agent_position(self, agent, action):\n",
    "        unit_movement = Vector2D(action[0], action[1])\n",
    "        self.agents_pos[agent] = Vector2D.sum(self.agents_pos[agent], Vector2D.mul(unit_movement, action[2]*self.speed))\n",
    "\n",
    "    def reset(self, seed=None, options=None):\n",
    "        if self.spawn_area_schedule != None:\n",
    "            self.n_reset += 1\n",
    "            if (self.spawn_area_schedule_index < len(self.spawn_area_schedule)-1 and \n",
    "                self.n_reset >= self.spawn_area_schedule[self.spawn_area_schedule_index+1][0]):\n",
    "                self.spawn_area_schedule_index += 1\n",
    "                self.spawn_area = self.spawn_area_schedule[self.spawn_area_schedule_index][1]\n",
    "\n",
    "        self.steps = 0\n",
    "        self.agents_pos = {agent: Vector2D.get_random_point(max_x=self.spawn_area, max_y=self.spawn_area) for agent in self.agents_ids}\n",
    "        self.last_step_distances = {agent: self.__total_distance_from_closest_neighbours(agent) for agent in self.agents_ids}\n",
    "        self.last_actions = {agent: [0]*3 for agent in self.agents_ids}\n",
    "        return {agent: self.__get_observation(agent) for agent in self.agents_ids}, {}\n",
    "     \n",
    "    def step(self, actions):\n",
    "        self.steps += 1\n",
    "        observations, rewards, terminated, truncated, infos = {}, {}, {}, {}, {}\n",
    "\n",
    "        for agent, action in actions.items():\n",
    "            self.__update_agent_position(agent, action)\n",
    "\n",
    "        for agent, action in actions.items():\n",
    "            observations[agent] = self.__get_observation(agent)\n",
    "            rewards[agent] = self.__get_local_reward(agent, action) + self.__get_global_reward()\n",
    "            terminated[agent] = False\n",
    "            truncated[agent] = False\n",
    "            infos[agent] = {}\n",
    "\n",
    "        truncated['__all__'] = False\n",
    "        if self.max_steps != None and self.steps == self.max_steps:\n",
    "            terminated['__all__'] = True\n",
    "        else:\n",
    "            terminated['__all__'] = False\n",
    "\n",
    "        return observations, rewards, terminated, truncated, infos\n",
    "     \n",
    "    def rgb_to_hex(self, r, g, b):\n",
    "        return f'#{r:02x}{g:02x}{b:02x}'\n",
    "\n",
    "    def render(self):\n",
    "        pass\n",
    "\n",
    "    def get_agent_ids(self):\n",
    "       return self.agents\n",
    "\n",
    "\n",
    "class RenderableKeepTheDistance(KeepTheDistance):\n",
    "    def render(self):\n",
    "        if self.canvas is None:\n",
    "            self.canvas = CanvasWithBorders(width=self.CANVAS_WIDTH, height=self.CANVAS_HEIGHT)\n",
    "            display(self.canvas)\n",
    "        \n",
    "        with hold_canvas():\n",
    "            agent_size = max(self.CANVAS_WIDTH/float(self.spawn_area),1)\n",
    "            top_left = (0.0,0.0)\n",
    "            bottom_right = (self.spawn_area, self.spawn_area)\n",
    "            self.canvas.clear()\n",
    "\n",
    "            for agent in self.agents_ids:\n",
    "                raw_pos = self.agents_pos[agent].to_np_array()\n",
    "                color = self.agent_colors[agent]\n",
    "                \n",
    "                agent_pos_in_frame = [((raw_pos[0]-top_left[0])/(bottom_right[0]-top_left[0]))*self.CANVAS_WIDTH,\n",
    "                            ((raw_pos[1]-top_left[1])/(bottom_right[1]-top_left[1]))*self.CANVAS_HEIGHT,]\n",
    "\n",
    "                self.canvas.fill_style = color\n",
    "                self.canvas.fill_circle(\n",
    "                    agent_pos_in_frame[0],\n",
    "                    agent_pos_in_frame[1],\n",
    "                    agent_size/2.0\n",
    "                )\n",
    "                \n",
    "                self.canvas.stroke_style = \"black\"\n",
    "                self.canvas.stroke_circle(\n",
    "                    agent_pos_in_frame[0],\n",
    "                    agent_pos_in_frame[1],\n",
    "                    agent_size/2.0\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'agent-0': array([0.70710677, 0.70710677, 1.3424541 ], dtype=float32), 'agent-1': array([-0.70710677, -0.70710677,  1.3424541 ], dtype=float32)}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1de2df30970f48eb836cd7d4568f823b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "CanvasWithBorders(height=300, width=300)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "obs:  {'agent-0': array([-1.       ,  0.       ,  0.6931472], dtype=float32), 'agent-1': array([1.       , 0.       , 0.6931472], dtype=float32)}\n",
      "action:  {'agent-0': array([-0.45745727, -0.8009053 ,  0.04903754], dtype=float32), 'agent-1': array([0.6539487 , 0.5224657 , 0.59678525], dtype=float32)}\n",
      "reward:  {'agent-0': -0.45566929521952426, 'agent-1': -0.45566929521952426} \n",
      "\n",
      "obs:  {'agent-0': array([-0.9704811 , -0.2411772 ,  0.89839935], dtype=float32), 'agent-1': array([0.9704811 , 0.2411772 , 0.89839935], dtype=float32)}\n",
      "action:  {'agent-0': array([-0.48482028,  0.10999227,  0.2594751 ], dtype=float32), 'agent-1': array([-0.5568653 , -0.5819053 ,  0.08382403], dtype=float32)}\n",
      "reward:  {'agent-0': -0.061060199976091134, 'agent-1': -0.061060199976091134} \n",
      "\n",
      "obs:  {'agent-0': array([-0.9835766 , -0.18049121,  0.9229602 ], dtype=float32), 'agent-1': array([0.9835766 , 0.18049121, 0.9229602 ], dtype=float32)}\n",
      "action:  {'agent-0': array([-0.9076892 ,  0.15017396,  0.3761028 ], dtype=float32), 'agent-1': array([-0.29385248,  0.78879476,  0.35324332], dtype=float32)}\n",
      "reward:  {'agent-0': -0.2823709142169415, 'agent-1': -0.2823709142169415} \n",
      "\n",
      "obs:  {'agent-0': array([-0.96125966, -0.27564442,  1.0292981 ], dtype=float32), 'agent-1': array([0.96125966, 0.27564442, 1.0292981 ], dtype=float32)}\n",
      "action:  {'agent-0': array([0.91293854, 0.5791823 , 0.65548056], dtype=float32), 'agent-1': array([-0.70737   , -0.60080415,  0.2996317 ], dtype=float32)}\n",
      "reward:  {'agent-0': 0.877853248124027, 'agent-1': 0.877853248124027} \n",
      "\n",
      "obs:  {'agent-0': array([-0.99760276,  0.06920049,  0.65297455], dtype=float32), 'agent-1': array([ 0.99760276, -0.06920049,  0.65297455], dtype=float32)}\n",
      "action:  {'agent-0': array([0.7699532 , 0.88605106, 0.25466415], dtype=float32), 'agent-1': array([0.57001275, 0.98081493, 0.49026892], dtype=float32)}\n",
      "reward:  {'agent-0': -0.09929336683741152, 'agent-1': -0.09929336683741152} \n",
      "\n",
      "obs:  {'agent-0': array([-0.982243 , -0.1876132,  0.7033651], dtype=float32), 'agent-1': array([0.982243 , 0.1876132, 0.7033651], dtype=float32)}\n",
      "action:  {'agent-0': array([-0.94537294, -0.6443997 ,  0.35350215], dtype=float32), 'agent-1': array([-0.90504366,  0.02907705,  0.95200354], dtype=float32)}\n",
      "reward:  {'agent-0': 0.3683212708312549, 'agent-1': 0.3683212708312549} \n",
      "\n",
      "obs:  {'agent-0': array([-0.728291  , -0.68526804,  0.50211936], dtype=float32), 'agent-1': array([0.728291  , 0.68526804, 0.50211936], dtype=float32)}\n",
      "action:  {'agent-0': array([ 0.31047583, -0.3889742 ,  0.07767651], dtype=float32), 'agent-1': array([-0.38277435, -0.7160961 ,  0.94036597], dtype=float32)}\n",
      "reward:  {'agent-0': 100.435937736859, 'agent-1': 100.435937736859} \n",
      "\n",
      "obs:  {'agent-0': array([-0.42047384,  0.90730464,  0.19579826], dtype=float32), 'agent-1': array([ 0.42047384, -0.90730464,  0.19579826], dtype=float32)}\n",
      "action:  {'agent-0': array([ 0.7979421 , -0.33644477,  0.115529  ], dtype=float32), 'agent-1': array([-0.5177743, -0.2957633,  0.7437552], dtype=float32)}\n",
      "reward:  {'agent-0': -0.32376048464636564, 'agent-1': -0.32376048464636564} \n",
      "\n",
      "obs:  {'agent-0': array([0.7153926, 0.6987227, 0.4318097], dtype=float32), 'agent-1': array([-0.7153926, -0.6987227,  0.4318097], dtype=float32)}\n",
      "action:  {'agent-0': array([-0.87393975, -0.3952561 ,  0.19714388], dtype=float32), 'agent-1': array([-0.98193717, -0.24365671,  0.8133516 ], dtype=float32)}\n",
      "reward:  {'agent-0': -0.5883124743444514, 'agent-1': -0.5883124743444514} \n",
      "\n",
      "obs:  {'agent-0': array([0.89751077, 0.4409925 , 0.75534916], dtype=float32), 'agent-1': array([-0.89751077, -0.4409925 ,  0.75534916], dtype=float32)}\n",
      "action:  {'agent-0': array([ 0.6710632, -0.5892979,  0.7569639], dtype=float32), 'agent-1': array([0.8387263 , 0.31625357, 0.57299036], dtype=float32)}\n",
      "reward:  {'agent-0': 0.08020107667330367, 'agent-1': 0.08020107667330367} \n",
      "\n"
     ]
    }
   ],
   "source": [
    "env_config = EnvironmentConfiguration(n_agents=2, target_distance=0, max_steps=500, speed=1, spawn_area=10)\n",
    "env = RenderableKeepTheDistance(env_config)\n",
    "\n",
    "print(env.reset()[0])\n",
    "#env.render()\n",
    "simulate_random_episode(env, 10, print_info=True)\n",
    "#env.step({'agent-1': (1,1,1)})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## policy training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ray\n",
    "ray.shutdown()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KeepTheDistance?dst=0&agent=2&spawn_area=100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ray.tune.registry import register_env\n",
    "\n",
    "env_config = EnvironmentConfiguration(n_agents=2, target_distance=0, max_steps=300, speed=1, spawn_area=100)\n",
    "register_env(\"KeepTheDistance?dst=0&agent=2&spawn_area=100\", lambda _: KeepTheDistance(env_config))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-24 16:05:18,459\tWARNING deprecation.py:50 -- DeprecationWarning: `_enable_new_api_stack` has been deprecated. Use `AlgorithmConfig._enable_new_api_stack` instead. This will raise an error in the future!\n",
      "/home/nicolo/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/rllib/algorithms/algorithm.py:521: RayDeprecationWarning: This API is deprecated and may be removed in future Ray releases. You could suppress this warning by setting env variable PYTHONWARNINGS=\"ignore::DeprecationWarning\"\n",
      "`UnifiedLogger` will be removed in Ray 2.7.\n",
      "  return UnifiedLogger(config, logdir, loggers=None)\n",
      "/home/nicolo/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/tune/logger/unified.py:53: RayDeprecationWarning: This API is deprecated and may be removed in future Ray releases. You could suppress this warning by setting env variable PYTHONWARNINGS=\"ignore::DeprecationWarning\"\n",
      "The `JsonLogger interface is deprecated in favor of the `ray.tune.json.JsonLoggerCallback` interface and will be removed in Ray 2.7.\n",
      "  self._loggers.append(cls(self.config, self.logdir, self.trial))\n",
      "/home/nicolo/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/tune/logger/unified.py:53: RayDeprecationWarning: This API is deprecated and may be removed in future Ray releases. You could suppress this warning by setting env variable PYTHONWARNINGS=\"ignore::DeprecationWarning\"\n",
      "The `CSVLogger interface is deprecated in favor of the `ray.tune.csv.CSVLoggerCallback` interface and will be removed in Ray 2.7.\n",
      "  self._loggers.append(cls(self.config, self.logdir, self.trial))\n",
      "/home/nicolo/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/tune/logger/unified.py:53: RayDeprecationWarning: This API is deprecated and may be removed in future Ray releases. You could suppress this warning by setting env variable PYTHONWARNINGS=\"ignore::DeprecationWarning\"\n",
      "The `TBXLogger interface is deprecated in favor of the `ray.tune.tensorboardx.TBXLoggerCallback` interface and will be removed in Ray 2.7.\n",
      "  self._loggers.append(cls(self.config, self.logdir, self.trial))\n",
      "2024-05-24 16:05:18,533\tWARNING deprecation.py:50 -- DeprecationWarning: `WorkerSet(num_workers=... OR local_worker=...)` has been deprecated. Use `EnvRunnerGroup(num_env_runners=... AND local_env_runner=...)` instead. This will raise an error in the future!\n",
      "2024-05-24 16:05:18,534\tWARNING deprecation.py:50 -- DeprecationWarning: `max_num_worker_restarts` has been deprecated. Use `AlgorithmConfig.max_num_env_runner_restarts` instead. This will raise an error in the future!\n",
      "2024-05-24 16:05:21,873\tINFO worker.py:1749 -- Started a local Ray instance.\n",
      "2024-05-24 16:05:30,131\tINFO trainable.py:161 -- Trainable.setup took 11.599 seconds. If your trainable is slow to initialize, consider setting reuse_actors=True to reduce actor creation overheads.\n",
      "2024-05-24 16:05:30,133\tWARNING util.py:61 -- Install gputil for GPU system monitoring.\n"
     ]
    }
   ],
   "source": [
    "algo = load_algo(\"KeepTheDistance?dst=0&agent=2&spawn_area=100\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration [1] => episode_reward_mean: -6.524227662165874, episode_len_mean: 300.0, agent_steps_trained: 8192, env_steps_trained: 4096, entropy: 4.23718300635616, learning_rate: 0.0010000000000000005\n",
      "iteration [2] => episode_reward_mean: 54.45913761079322, episode_len_mean: 300.0, agent_steps_trained: 16384, env_steps_trained: 8192, entropy: 4.34562553614378, learning_rate: 0.0010000000000000005\n",
      "iteration [3] => episode_reward_mean: 50.75709731524839, episode_len_mean: 300.0, agent_steps_trained: 24576, env_steps_trained: 12288, entropy: 4.2624438298245275, learning_rate: 0.0010000000000000005\n",
      "iteration [4] => episode_reward_mean: 134.46876803369832, episode_len_mean: 300.0, agent_steps_trained: 32768, env_steps_trained: 16384, entropy: 4.18227636863788, learning_rate: 0.0010000000000000005\n",
      "iteration [5] => episode_reward_mean: 266.40354222345053, episode_len_mean: 300.0, agent_steps_trained: 40960, env_steps_trained: 20480, entropy: 4.103445679694414, learning_rate: 0.0010000000000000005\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5849b07a7b634edcac54b2c5cdf33a35",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "CanvasWithBorders(height=300, width=300)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "obs:  {'agent-0': array([0.0464614, 0.9989201, 3.7852457], dtype=float32), 'agent-1': array([-0.0464614, -0.9989201,  3.7852457], dtype=float32)}\n",
      "action:  {'agent-0': array([-0.26440948, -0.32545447,  0.28671905], dtype=float32), 'agent-1': array([-0.10532188, -0.31774217,  0.21871096], dtype=float32)}\n",
      "reward:  {'agent-0': -43.02027082455359, 'agent-1': -43.02027082455359} \n",
      "\n",
      "obs:  {'agent-0': array([0.04526294, 0.9989751 , 3.7846503 ], dtype=float32), 'agent-1': array([-0.04526294, -0.9989751 ,  3.7846503 ], dtype=float32)}\n",
      "action:  {'agent-0': array([-1.        ,  1.        ,  0.49738595], dtype=float32), 'agent-1': array([-0.51586145, -0.7581216 ,  0.28242958], dtype=float32)}\n",
      "reward:  {'agent-0': -43.7168073859859, 'agent-1': -43.7168073859859} \n",
      "\n",
      "obs:  {'agent-0': array([0.036497  , 0.99933374, 3.8003495 ], dtype=float32), 'agent-1': array([-0.036497  , -0.99933374,  3.8003495 ], dtype=float32)}\n",
      "action:  {'agent-0': array([1.        , 0.22029948, 0.10762587], dtype=float32), 'agent-1': array([-0.5403122 , -0.06378508,  0.28751034], dtype=float32)}\n",
      "reward:  {'agent-0': -43.769205574832164, 'agent-1': -43.769205574832164} \n",
      "\n",
      "obs:  {'agent-0': array([0.04246144, 0.9990981 , 3.8015206 ], dtype=float32), 'agent-1': array([-0.04246144, -0.9990981 ,  3.8015206 ], dtype=float32)}\n",
      "action:  {'agent-0': array([0.21241629, 0.78459287, 0.10969394], dtype=float32), 'agent-1': array([-1.        , -0.59350514,  0.        ], dtype=float32)}\n",
      "reward:  {'agent-0': -43.85618681548549, 'agent-1': -43.85618681548549} \n",
      "\n",
      "obs:  {'agent-0': array([0.04290853, 0.999079  , 3.8034616 ], dtype=float32), 'agent-1': array([-0.04290853, -0.999079  ,  3.8034616 ], dtype=float32)}\n",
      "action:  {'agent-0': array([ 1.        , -0.6819282 ,  0.33467653], dtype=float32), 'agent-1': array([-0.90117526, -0.67495966,  0.        ], dtype=float32)}\n",
      "reward:  {'agent-0': -43.64388910565892, 'agent-1': -43.64388910565892} \n",
      "\n",
      "obs:  {'agent-0': array([0.0507856 , 0.99870956, 3.7987175 ], dtype=float32), 'agent-1': array([-0.0507856 , -0.99870956,  3.7987175 ], dtype=float32)}\n",
      "action:  {'agent-0': array([ 1.        , -0.5407618 ,  0.31897068], dtype=float32), 'agent-1': array([-0.9181593 ,  0.26520264,  0.19966048], dtype=float32)}\n",
      "reward:  {'agent-0': -43.447281287266286, 'agent-1': -43.447281287266286} \n",
      "\n",
      "obs:  {'agent-0': array([0.06257634, 0.9980402 , 3.794304  ], dtype=float32), 'agent-1': array([-0.06257634, -0.9980402 ,  3.794304  ], dtype=float32)}\n",
      "action:  {'agent-0': array([-1.        , -0.82952636,  0.04718271], dtype=float32), 'agent-1': array([-1.        ,  0.8371333 ,  0.85956156], dtype=float32)}\n",
      "reward:  {'agent-0': -42.74951337810861, 'agent-1': -42.74951337810861} \n",
      "\n",
      "obs:  {'agent-0': array([0.08260095, 0.9965827 , 3.7784805 ], dtype=float32), 'agent-1': array([-0.08260095, -0.9965827 ,  3.7784805 ], dtype=float32)}\n",
      "action:  {'agent-0': array([ 0.8782482 , -1.        ,  0.08198205], dtype=float32), 'agent-1': array([ 0.21015263, -0.6781365 ,  0.4958407 ], dtype=float32)}\n",
      "reward:  {'agent-0': -43.000283012493085, 'agent-1': -43.000283012493085} \n",
      "\n",
      "obs:  {'agent-0': array([0.08137037, 0.99668396, 3.7841961 ], dtype=float32), 'agent-1': array([-0.08137037, -0.99668396,  3.7841961 ], dtype=float32)}\n",
      "action:  {'agent-0': array([0.15070343, 0.70902514, 0.        ], dtype=float32), 'agent-1': array([1.        , 0.5235059 , 0.04507446], dtype=float32)}\n",
      "reward:  {'agent-0': -42.97311830509669, 'agent-1': -42.97311830509669} \n",
      "\n",
      "obs:  {'agent-0': array([0.08037291, 0.99676484, 3.7835784 ], dtype=float32), 'agent-1': array([-0.08037291, -0.99676484,  3.7835784 ], dtype=float32)}\n",
      "action:  {'agent-0': array([-0.5872085 , -1.        ,  0.20288616], dtype=float32), 'agent-1': array([ 0.60593545, -0.14286876,  0.        ], dtype=float32)}\n",
      "reward:  {'agent-0': -42.76143587404419, 'agent-1': -42.76143587404419} \n",
      "\n",
      "obs:  {'agent-0': array([0.07798471, 0.99695456, 3.778753  ], dtype=float32), 'agent-1': array([-0.07798471, -0.99695456,  3.778753  ], dtype=float32)}\n",
      "action:  {'agent-0': array([1.        , 1.        , 0.11804006], dtype=float32), 'agent-1': array([-1.        ,  0.35331917,  0.        ], dtype=float32)}\n",
      "reward:  {'agent-0': -42.888458943847915, 'agent-1': -42.888458943847915} \n",
      "\n",
      "obs:  {'agent-0': array([0.080506 , 0.9967541, 3.7816515], dtype=float32), 'agent-1': array([-0.080506 , -0.9967541,  3.7816515], dtype=float32)}\n",
      "action:  {'agent-0': array([ 1.        , -0.81702554,  0.        ], dtype=float32), 'agent-1': array([0.74125004, 0.3885156 , 0.01318303], dtype=float32)}\n",
      "reward:  {'agent-0': -42.88256807327297, 'agent-1': -42.88256807327297} \n",
      "\n",
      "obs:  {'agent-0': array([0.08028918, 0.99677163, 3.7815173 ], dtype=float32), 'agent-1': array([-0.08028918, -0.99677163,  3.7815173 ], dtype=float32)}\n",
      "action:  {'agent-0': array([-1.        ,  0.04807222,  0.19708914], dtype=float32), 'agent-1': array([-1.        ,  0.17642856,  0.16682744], dtype=float32)}\n",
      "reward:  {'agent-0': -42.860253724902165, 'agent-1': -42.860253724902165} \n",
      "\n",
      "obs:  {'agent-0': array([0.07962492, 0.9968249 , 3.7810085 ], dtype=float32), 'agent-1': array([-0.07962492, -0.9968249 ,  3.7810085 ], dtype=float32)}\n",
      "action:  {'agent-0': array([ 0.5899923 , -0.05275518,  0.        ], dtype=float32), 'agent-1': array([-1.        ,  0.08364594,  0.        ], dtype=float32)}\n",
      "reward:  {'agent-0': -42.860253724902165, 'agent-1': -42.860253724902165} \n",
      "\n",
      "obs:  {'agent-0': array([0.07962492, 0.9968249 , 3.7810085 ], dtype=float32), 'agent-1': array([-0.07962492, -0.9968249 ,  3.7810085 ], dtype=float32)}\n",
      "action:  {'agent-0': array([-0.3694179 , -0.02277684,  0.8056093 ], dtype=float32), 'agent-1': array([-1.       ,  0.8453367,  1.       ], dtype=float32)}\n",
      "reward:  {'agent-0': -42.062267111925, 'agent-1': -42.062267111925} \n",
      "\n",
      "obs:  {'agent-0': array([0.09783443, 0.9952027 , 3.7626472 ], dtype=float32), 'agent-1': array([-0.09783443, -0.9952027 ,  3.7626472 ], dtype=float32)}\n",
      "action:  {'agent-0': array([ 1.        , -0.87819076,  0.2619846 ], dtype=float32), 'agent-1': array([0.43001628, 0.2069068 , 0.        ], dtype=float32)}\n",
      "reward:  {'agent-0': -41.85988775202764, 'agent-1': -41.85988775202764} \n",
      "\n",
      "obs:  {'agent-0': array([0.10456604, 0.9945179 , 3.7579365 ], dtype=float32), 'agent-1': array([-0.10456604, -0.9945179 ,  3.7579365 ], dtype=float32)}\n",
      "action:  {'agent-0': array([-0.37152022, -1.        ,  0.        ], dtype=float32), 'agent-1': array([0.32309008, 0.98542047, 0.66183376], dtype=float32)}\n",
      "reward:  {'agent-0': -41.18917231172327, 'agent-1': -41.18917231172327} \n",
      "\n",
      "obs:  {'agent-0': array([0.1010773, 0.9948786, 3.7421637], dtype=float32), 'agent-1': array([-0.1010773, -0.9948786,  3.7421637], dtype=float32)}\n",
      "action:  {'agent-0': array([1.        , 0.62736607, 1.        ], dtype=float32), 'agent-1': array([-1.        ,  0.29481506,  0.        ], dtype=float32)}\n",
      "reward:  {'agent-0': -41.92475140983133, 'agent-1': -41.92475140983133} \n",
      "\n",
      "obs:  {'agent-0': array([0.12315614, 0.9923873 , 3.7594485 ], dtype=float32), 'agent-1': array([-0.12315614, -0.9923873 ,  3.7594485 ], dtype=float32)}\n",
      "action:  {'agent-0': array([ 0.29878998, -0.8394503 ,  0.20091665], dtype=float32), 'agent-1': array([0.22016668, 0.8815576 , 1.        ], dtype=float32)}\n",
      "reward:  {'agent-0': -40.8628183857858, 'agent-1': -40.8628183857858} \n",
      "\n",
      "obs:  {'agent-0': array([0.12243786, 0.99247617, 3.7343981 ], dtype=float32), 'agent-1': array([-0.12243786, -0.99247617,  3.7343981 ], dtype=float32)}\n",
      "action:  {'agent-0': array([0.963835, 1.      , 1.      ], dtype=float32), 'agent-1': array([-0.0404281,  1.       ,  1.       ], dtype=float32)}\n",
      "reward:  {'agent-0': -40.99789556001445, 'agent-1': -40.99789556001445} \n",
      "\n",
      "obs:  {'agent-0': array([0.14652993, 0.98920625, 3.7376194 ], dtype=float32), 'agent-1': array([-0.14652993, -0.98920625,  3.7376194 ], dtype=float32)}\n",
      "action:  {'agent-0': array([-0.10141122,  0.65417755,  0.20482445], dtype=float32), 'agent-1': array([-0.59237385,  1.        ,  0.5398481 ], dtype=float32)}\n",
      "reward:  {'agent-0': -40.64178794664536, 'agent-1': -40.64178794664536} \n",
      "\n",
      "obs:  {'agent-0': array([0.1551713 , 0.98788756, 3.7291043 ], dtype=float32), 'agent-1': array([-0.1551713 , -0.98788756,  3.7291043 ], dtype=float32)}\n",
      "action:  {'agent-0': array([1.       , 0.6253841, 0.       ], dtype=float32), 'agent-1': array([-0.55219984, -0.02268648,  0.        ], dtype=float32)}\n",
      "reward:  {'agent-0': -40.64178794664536, 'agent-1': -40.64178794664536} \n",
      "\n",
      "obs:  {'agent-0': array([0.1551713 , 0.98788756, 3.7291043 ], dtype=float32), 'agent-1': array([-0.1551713 , -0.98788756,  3.7291043 ], dtype=float32)}\n",
      "action:  {'agent-0': array([ 1.        , -0.14803326,  0.7581694 ], dtype=float32), 'agent-1': array([-0.52275074,  0.31510222,  0.02833959], dtype=float32)}\n",
      "reward:  {'agent-0': -40.64956703513637, 'agent-1': -40.64956703513637} \n",
      "\n",
      "obs:  {'agent-0': array([0.17415741, 0.98471785, 3.729291  ], dtype=float32), 'agent-1': array([-0.17415741, -0.98471785,  3.729291  ], dtype=float32)}\n",
      "action:  {'agent-0': array([-0.41665316, -0.17776877,  1.        ], dtype=float32), 'agent-1': array([1.        , 0.11612499, 0.39620867], dtype=float32)}\n",
      "reward:  {'agent-0': -40.294837959536046, 'agent-1': -40.294837959536046} \n",
      "\n",
      "obs:  {'agent-0': array([0.15551773, 0.9878331 , 3.7207375 ], dtype=float32), 'agent-1': array([-0.15551773, -0.9878331 ,  3.7207375 ], dtype=float32)}\n",
      "action:  {'agent-0': array([ 1.        , -0.00910395,  0.596975  ], dtype=float32), 'agent-1': array([1.        , 0.06547034, 0.1337882 ], dtype=float32)}\n",
      "reward:  {'agent-0': -40.35546946245682, 'agent-1': -40.35546946245682} \n",
      "\n",
      "obs:  {'agent-0': array([0.16676174, 0.9859972 , 3.7222047 ], dtype=float32), 'agent-1': array([-0.16676174, -0.9859972 ,  3.7222047 ], dtype=float32)}\n",
      "action:  {'agent-0': array([ 0.3068534 , -0.17128116,  0.31943768], dtype=float32), 'agent-1': array([-1.        , -0.5687018 ,  0.80725396], dtype=float32)}\n",
      "reward:  {'agent-0': -40.913467407323694, 'agent-1': -40.913467407323694} \n",
      "\n",
      "obs:  {'agent-0': array([0.18661393, 0.9824333 , 3.7356071 ], dtype=float32), 'agent-1': array([-0.18661393, -0.9824333 ,  3.7356071 ], dtype=float32)}\n",
      "action:  {'agent-0': array([-0.45085198,  0.51588404,  0.374089  ], dtype=float32), 'agent-1': array([0.25170386, 0.4857806 , 0.        ], dtype=float32)}\n",
      "reward:  {'agent-0': -41.07208506206378, 'agent-1': -41.07208506206378} \n",
      "\n",
      "obs:  {'agent-0': array([0.18178682, 0.98333794, 3.7393844 ], dtype=float32), 'agent-1': array([-0.18178682, -0.98333794,  3.7393844 ], dtype=float32)}\n",
      "action:  {'agent-0': array([ 0.09907043, -0.63913476,  0.        ], dtype=float32), 'agent-1': array([-0.4681201, -1.       ,  0.       ], dtype=float32)}\n",
      "reward:  {'agent-0': -41.07208506206378, 'agent-1': -41.07208506206378} \n",
      "\n",
      "obs:  {'agent-0': array([0.18178682, 0.98333794, 3.7393844 ], dtype=float32), 'agent-1': array([-0.18178682, -0.98333794,  3.7393844 ], dtype=float32)}\n",
      "action:  {'agent-0': array([ 1.        , -1.        ,  0.81847584], dtype=float32), 'agent-1': array([-0.95450544,  0.11972952,  0.16740435], dtype=float32)}\n",
      "reward:  {'agent-0': -40.44073017896124, 'agent-1': -40.44073017896124} \n",
      "\n",
      "obs:  {'agent-0': array([0.20881493, 0.97795516, 3.7242641 ], dtype=float32), 'agent-1': array([-0.20881493, -0.97795516,  3.7242641 ], dtype=float32)}\n",
      "action:  {'agent-0': array([-0.22399628,  1.        ,  0.00365335], dtype=float32), 'agent-1': array([-1.        ,  0.04583716,  0.        ], dtype=float32)}\n",
      "reward:  {'agent-0': -40.4441321384312, 'agent-1': -40.4441321384312} \n",
      "\n",
      "obs:  {'agent-0': array([0.20877713, 0.97796327, 3.7243464 ], dtype=float32), 'agent-1': array([-0.20877713, -0.97796327,  3.7243464 ], dtype=float32)}\n",
      "action:  {'agent-0': array([-1.        , -1.        ,  0.63337517], dtype=float32), 'agent-1': array([-0.4814943,  1.       ,  0.       ], dtype=float32)}\n",
      "reward:  {'agent-0': -39.69546997227063, 'agent-1': -39.69546997227063} \n",
      "\n",
      "obs:  {'agent-0': array([0.19675884, 0.98045194, 3.7061167 ], dtype=float32), 'agent-1': array([-0.19675884, -0.98045194,  3.7061167 ], dtype=float32)}\n",
      "action:  {'agent-0': array([-0.46229118, -0.5016445 ,  0.        ], dtype=float32), 'agent-1': array([-0.31885552,  0.02047873,  0.39195323], dtype=float32)}\n",
      "reward:  {'agent-0': -39.71238434301402, 'agent-1': -39.71238434301402} \n",
      "\n",
      "obs:  {'agent-0': array([0.19982208, 0.9798322 , 3.7065322 ], dtype=float32), 'agent-1': array([-0.19982208, -0.9798322 ,  3.7065322 ], dtype=float32)}\n",
      "action:  {'agent-0': array([0.57680583, 0.44484174, 0.39187312], dtype=float32), 'agent-1': array([-0.6222067 , -1.        ,  0.26226938], dtype=float32)}\n",
      "reward:  {'agent-0': -40.219020538403605, 'agent-1': -40.219020538403605} \n",
      "\n",
      "obs:  {'agent-0': array([0.20698245, 0.9783447 , 3.7188997 ], dtype=float32), 'agent-1': array([-0.20698245, -0.9783447 ,  3.7188997 ], dtype=float32)}\n",
      "action:  {'agent-0': array([-0.8669702, -1.       ,  0.684633 ], dtype=float32), 'agent-1': array([-0.08926386,  0.5104859 ,  0.        ], dtype=float32)}\n",
      "reward:  {'agent-0': -39.428801662717895, 'agent-1': -39.428801662717895} \n",
      "\n",
      "obs:  {'agent-0': array([0.19607686, 0.98058856, 3.6995425 ], dtype=float32), 'agent-1': array([-0.19607686, -0.98058856,  3.6995425 ], dtype=float32)}\n",
      "action:  {'agent-0': array([0.9751253, 1.       , 0.5275622], dtype=float32), 'agent-1': array([-1.        ,  0.09067369,  0.4927296 ], dtype=float32)}\n",
      "reward:  {'agent-0': -40.10973589488662, 'agent-1': -40.10973589488662} \n",
      "\n",
      "obs:  {'agent-0': array([0.21785843, 0.9759804 , 3.716245  ], dtype=float32), 'agent-1': array([-0.21785843, -0.9759804 ,  3.716245  ], dtype=float32)}\n",
      "action:  {'agent-0': array([-0.02110022, -0.47827226,  0.42588168], dtype=float32), 'agent-1': array([0.09651732, 1.        , 0.43858793], dtype=float32)}\n",
      "reward:  {'agent-0': -39.47181005840334, 'agent-1': -39.47181005840334} \n",
      "\n",
      "obs:  {'agent-0': array([0.22007926, 0.975482  , 3.7006056 ], dtype=float32), 'agent-1': array([-0.22007926, -0.975482  ,  3.7006056 ], dtype=float32)}\n",
      "action:  {'agent-0': array([1.        , 0.39540362, 0.52308047], dtype=float32), 'agent-1': array([-1.        ,  0.69656456,  0.18918791], dtype=float32)}\n",
      "reward:  {'agent-0': -39.70756560778822, 'agent-1': -39.70756560778822} \n",
      "\n",
      "obs:  {'agent-0': array([0.23671043, 0.97158027, 3.706414  ], dtype=float32), 'agent-1': array([-0.23671043, -0.97158027,  3.706414  ], dtype=float32)}\n",
      "action:  {'agent-0': array([ 0.6687684 , -0.73219955,  0.74524164], dtype=float32), 'agent-1': array([-0.7314771,  0.4017384,  0.       ], dtype=float32)}\n",
      "reward:  {'agent-0': -39.30016997206799, 'agent-1': -39.30016997206799} \n",
      "\n",
      "obs:  {'agent-0': array([0.25184596, 0.96776736, 3.6963556 ], dtype=float32), 'agent-1': array([-0.25184596, -0.96776736,  3.6963556 ], dtype=float32)}\n",
      "action:  {'agent-0': array([-1., -1.,  0.], dtype=float32), 'agent-1': array([ 0.83912086, -0.5528439 ,  0.        ], dtype=float32)}\n",
      "reward:  {'agent-0': -39.30016997206799, 'agent-1': -39.30016997206799} \n",
      "\n",
      "obs:  {'agent-0': array([0.25184596, 0.96776736, 3.6963556 ], dtype=float32), 'agent-1': array([-0.25184596, -0.96776736,  3.6963556 ], dtype=float32)}\n",
      "action:  {'agent-0': array([-0.7613368 ,  1.        ,  0.75901276], dtype=float32), 'agent-1': array([-0.625846  , -0.22160816,  0.5485118 ], dtype=float32)}\n",
      "reward:  {'agent-0': -40.09578793974622, 'agent-1': -40.09578793974622} \n",
      "\n",
      "obs:  {'agent-0': array([0.24099809, 0.97052556, 3.7159057 ], dtype=float32), 'agent-1': array([-0.24099809, -0.97052556,  3.7159057 ], dtype=float32)}\n",
      "action:  {'agent-0': array([ 0.65506923, -0.36520898,  0.09346589], dtype=float32), 'agent-1': array([ 1., -1.,  0.], dtype=float32)}\n",
      "reward:  {'agent-0': -40.07747204716724, 'agent-1': -40.07747204716724} \n",
      "\n",
      "obs:  {'agent-0': array([0.24263594, 0.9701174 , 3.7154598 ], dtype=float32), 'agent-1': array([-0.24263594, -0.9701174 ,  3.7154598 ], dtype=float32)}\n",
      "action:  {'agent-0': array([1.        , 0.32791662, 0.2747226 ], dtype=float32), 'agent-1': array([-1.        ,  0.75686693,  0.82122064], dtype=float32)}\n",
      "reward:  {'agent-0': -39.84563828670302, 'agent-1': -39.84563828670302} \n",
      "\n",
      "obs:  {'agent-0': array([0.27155238, 0.9624237 , 3.7098    ], dtype=float32), 'agent-1': array([-0.27155238, -0.9624237 ,  3.7098    ], dtype=float32)}\n",
      "action:  {'agent-0': array([-0.86059225, -0.9174906 ,  0.5966429 ], dtype=float32), 'agent-1': array([ 1.       , -0.7304158,  0.5147127], dtype=float32)}\n",
      "reward:  {'agent-0': -39.412699165894374, 'agent-1': -39.412699165894374} \n",
      "\n",
      "obs:  {'agent-0': array([0.24844782, 0.9686453 , 3.6991441 ], dtype=float32), 'agent-1': array([-0.24844782, -0.9686453 ,  3.6991441 ], dtype=float32)}\n",
      "action:  {'agent-0': array([ 1.       , -0.5964138,  0.       ], dtype=float32), 'agent-1': array([ 1.        , -0.65360504,  0.07894087], dtype=float32)}\n",
      "reward:  {'agent-0': -39.44316589927713, 'agent-1': -39.44316589927713} \n",
      "\n",
      "obs:  {'agent-0': array([0.24625453, 0.9692052 , 3.6998978 ], dtype=float32), 'agent-1': array([-0.24625453, -0.9692052 ,  3.6998978 ], dtype=float32)}\n",
      "action:  {'agent-0': array([-0.2882545 , -0.8920748 ,  0.46753654], dtype=float32), 'agent-1': array([-0.9363768 ,  0.18926394,  0.        ], dtype=float32)}\n",
      "reward:  {'agent-0': -39.00575453128925, 'agent-1': -39.00575453128925} \n",
      "\n",
      "obs:  {'agent-0': array([0.24556093, 0.96938115, 3.6890233 ], dtype=float32), 'agent-1': array([-0.24556093, -0.96938115,  3.6890233 ], dtype=float32)}\n",
      "action:  {'agent-0': array([0.65723634, 0.47314382, 0.3606839 ], dtype=float32), 'agent-1': array([ 0.9930247 , -0.63570404,  1.        ], dtype=float32)}\n",
      "reward:  {'agent-0': -39.61272544127329, 'agent-1': -39.61272544127329} \n",
      "\n",
      "obs:  {'agent-0': array([0.22271426, 0.9748838 , 3.7040815 ], dtype=float32), 'agent-1': array([-0.22271426, -0.9748838 ,  3.7040815 ], dtype=float32)}\n",
      "action:  {'agent-0': array([ 1.        , -0.71831906,  0.06194395], dtype=float32), 'agent-1': array([-1.        ,  0.90020835,  0.52853525], dtype=float32)}\n",
      "reward:  {'agent-0': -39.24310720806343, 'agent-1': -39.24310720806343} \n",
      "\n",
      "obs:  {'agent-0': array([0.23985864, 0.97080785, 3.6949387 ], dtype=float32), 'agent-1': array([-0.23985864, -0.97080785,  3.6949387 ], dtype=float32)}\n",
      "action:  {'agent-0': array([-0.9819097 , -0.1992935 ,  0.13991043], dtype=float32), 'agent-1': array([0.7631147 , 0.883669  , 0.02311933], dtype=float32)}\n",
      "reward:  {'agent-0': -39.159267479398274, 'agent-1': -39.159267479398274} \n",
      "\n",
      "obs:  {'agent-0': array([0.23641342, 0.97165257, 3.6928532 ], dtype=float32), 'agent-1': array([-0.23641342, -0.97165257,  3.6928532 ], dtype=float32)}\n",
      "action:  {'agent-0': array([ 0.68855906, -0.12358493,  0.03132245], dtype=float32), 'agent-1': array([-0.4317994 ,  0.07734871,  0.16261584], dtype=float32)}\n",
      "reward:  {'agent-0': -39.16509440222204, 'agent-1': -39.16509440222204} \n",
      "\n",
      "obs:  {'agent-0': array([0.23872177, 0.971088  , 3.6929984 ], dtype=float32), 'agent-1': array([-0.23872177, -0.971088  ,  3.6929984 ], dtype=float32)}\n",
      "action:  {'agent-0': array([-0.38659495, -1.        ,  1.        ], dtype=float32), 'agent-1': array([-0.10004115, -0.00427616,  0.96971595], dtype=float32)}\n",
      "reward:  {'agent-0': -38.128928080497396, 'agent-1': -38.128928080497396} \n",
      "\n",
      "obs:  {'agent-0': array([0.23761427, 0.9713596 , 3.666862  ], dtype=float32), 'agent-1': array([-0.23761427, -0.9713596 ,  3.666862  ], dtype=float32)}\n",
      "action:  {'agent-0': array([1.        , 0.04895616, 1.        ], dtype=float32), 'agent-1': array([-1.       ,  0.2099936,  0.7983685], dtype=float32)}\n",
      "reward:  {'agent-0': -38.48191078661808, 'agent-1': -38.48191078661808} \n",
      "\n",
      "obs:  {'agent-0': array([0.28216752, 0.9593651 , 3.6758425 ], dtype=float32), 'agent-1': array([-0.28216752, -0.9593651 ,  3.6758425 ], dtype=float32)}\n",
      "action:  {'agent-0': array([-0.46776354,  0.712286  ,  1.        ], dtype=float32), 'agent-1': array([0.33180475, 1.        , 1.        ], dtype=float32)}\n",
      "reward:  {'agent-0': -37.986468640741336, 'agent-1': -37.986468640741336} \n",
      "\n",
      "obs:  {'agent-0': array([0.26479897, 0.9643036 , 3.6632147 ], dtype=float32), 'agent-1': array([-0.26479897, -0.9643036 ,  3.6632147 ], dtype=float32)}\n",
      "action:  {'agent-0': array([-1.        ,  0.111799  ,  0.27268925], dtype=float32), 'agent-1': array([ 1.        , -0.02074182,  0.16925904], dtype=float32)}\n",
      "reward:  {'agent-0': -37.90472289210259, 'agent-1': -37.90472289210259} \n",
      "\n",
      "obs:  {'agent-0': array([0.25371057, 0.96728015, 3.6611156 ], dtype=float32), 'agent-1': array([-0.25371057, -0.96728015,  3.6611156 ], dtype=float32)}\n",
      "action:  {'agent-0': array([0.94634986, 0.73561   , 0.33258158], dtype=float32), 'agent-1': array([-0.08813024, -1.        ,  0.        ], dtype=float32)}\n",
      "reward:  {'agent-0': -38.22198926606911, 'agent-1': -38.22198926606911} \n",
      "\n",
      "obs:  {'agent-0': array([0.25983912, 0.9656519 , 3.6692376 ], dtype=float32), 'agent-1': array([-0.25983912, -0.9656519 ,  3.6692376 ], dtype=float32)}\n",
      "action:  {'agent-0': array([-0.8918604 ,  0.10985398,  0.25893983], dtype=float32), 'agent-1': array([0.33889866, 0.9438851 , 0.88574326], dtype=float32)}\n",
      "reward:  {'agent-0': -37.30536049586871, 'agent-1': -37.30536049586871} \n",
      "\n",
      "obs:  {'agent-0': array([0.25198665, 0.9677307 , 3.6455898 ], dtype=float32), 'agent-1': array([-0.25198665, -0.9677307 ,  3.6455898 ], dtype=float32)}\n",
      "action:  {'agent-0': array([0.82625437, 1.        , 0.98522764], dtype=float32), 'agent-1': array([0.8216672 , 0.98858535, 0.1554982 ], dtype=float32)}\n",
      "reward:  {'agent-0': -38.28566556284138, 'agent-1': -38.28566556284138} \n",
      "\n",
      "obs:  {'agent-0': array([0.2634598 , 0.96467036, 3.6708598 ], dtype=float32), 'agent-1': array([-0.2634598 , -0.96467036,  3.6708598 ], dtype=float32)}\n",
      "action:  {'agent-0': array([ 1., -1.,  0.], dtype=float32), 'agent-1': array([-0.71764994,  1.        ,  0.        ], dtype=float32)}\n",
      "reward:  {'agent-0': -38.28566556284138, 'agent-1': -38.28566556284138} \n",
      "\n",
      "obs:  {'agent-0': array([0.2634598 , 0.96467036, 3.6708598 ], dtype=float32), 'agent-1': array([-0.2634598 , -0.96467036,  3.6708598 ], dtype=float32)}\n",
      "action:  {'agent-0': array([-1.        , -0.28559202,  0.33627966], dtype=float32), 'agent-1': array([0.5842923 , 0.59671855, 0.        ], dtype=float32)}\n",
      "reward:  {'agent-0': -38.10559746753889, 'agent-1': -38.10559746753889} \n",
      "\n",
      "obs:  {'agent-0': array([0.25587982, 0.9667086 , 3.6662657 ], dtype=float32), 'agent-1': array([-0.25587982, -0.9667086 ,  3.6662657 ], dtype=float32)}\n",
      "action:  {'agent-0': array([-0.01432335,  0.01495707,  0.4495193 ], dtype=float32), 'agent-1': array([-1.        , -0.9397818 ,  0.45846978], dtype=float32)}\n",
      "reward:  {'agent-0': -38.64564705861738, 'agent-1': -38.64564705861738} \n",
      "\n",
      "obs:  {'agent-0': array([0.2640009, 0.9645224, 3.6799812], dtype=float32), 'agent-1': array([-0.2640009, -0.9645224,  3.6799812], dtype=float32)}\n",
      "action:  {'agent-0': array([-1.        ,  0.32548606,  0.2701239 ], dtype=float32), 'agent-1': array([0.6607342, 1.       , 0.       ], dtype=float32)}\n",
      "reward:  {'agent-0': -38.66017776696391, 'agent-1': -38.66017776696391} \n",
      "\n",
      "obs:  {'agent-0': array([0.25691453, 0.9664341 , 3.6803477 ], dtype=float32), 'agent-1': array([-0.25691453, -0.9664341 ,  3.6803477 ], dtype=float32)}\n",
      "action:  {'agent-0': array([-0.16824186, -1.        ,  0.05440149], dtype=float32), 'agent-1': array([0.18469584, 0.5413728 , 0.2698313 ], dtype=float32)}\n",
      "reward:  {'agent-0': -38.45127145199832, 'agent-1': -38.45127145199832} \n",
      "\n",
      "obs:  {'agent-0': array([0.2567762, 0.9664709, 3.6750662], dtype=float32), 'agent-1': array([-0.2567762, -0.9664709,  3.6750662], dtype=float32)}\n",
      "action:  {'agent-0': array([-0.8855037, -1.       ,  0.       ], dtype=float32), 'agent-1': array([-0.5919966,  1.       ,  0.873129 ], dtype=float32)}\n",
      "reward:  {'agent-0': -37.747081832260754, 'agent-1': -37.747081832260754} \n",
      "\n",
      "obs:  {'agent-0': array([0.27525997, 0.9613698 , 3.6570554 ], dtype=float32), 'agent-1': array([-0.27525997, -0.9613698 ,  3.6570554 ], dtype=float32)}\n",
      "action:  {'agent-0': array([ 1.       , -0.8030769,  0.       ], dtype=float32), 'agent-1': array([-0.5507393,  1.       ,  0.       ], dtype=float32)}\n",
      "reward:  {'agent-0': -37.747081832260754, 'agent-1': -37.747081832260754} \n",
      "\n",
      "obs:  {'agent-0': array([0.27525997, 0.9613698 , 3.6570554 ], dtype=float32), 'agent-1': array([-0.27525997, -0.9613698 ,  3.6570554 ], dtype=float32)}\n",
      "action:  {'agent-0': array([-1.        ,  1.        ,  0.24383694], dtype=float32), 'agent-1': array([-1.        , -0.45357096,  0.9466882 ], dtype=float32)}\n",
      "reward:  {'agent-0': -38.59088485684822, 'agent-1': -38.59088485684822} \n",
      "\n",
      "obs:  {'agent-0': array([0.28745422, 0.95779437, 3.6785989 ], dtype=float32), 'agent-1': array([-0.28745422, -0.95779437,  3.6785989 ], dtype=float32)}\n",
      "action:  {'agent-0': array([0.45157433, 0.16109979, 0.40488437], dtype=float32), 'agent-1': array([-0.23762393,  1.        ,  0.        ], dtype=float32)}\n",
      "reward:  {'agent-0': -38.70623136629962, 'agent-1': -38.70623136629962} \n",
      "\n",
      "obs:  {'agent-0': array([0.29132125, 0.9566253 , 3.681508  ], dtype=float32), 'agent-1': array([-0.29132125, -0.9566253 ,  3.681508  ], dtype=float32)}\n",
      "action:  {'agent-0': array([-0.13584405,  0.65159106,  0.        ], dtype=float32), 'agent-1': array([ 1.       , -0.5658952,  0.       ], dtype=float32)}\n",
      "reward:  {'agent-0': -38.70623136629962, 'agent-1': -38.70623136629962} \n",
      "\n",
      "obs:  {'agent-0': array([0.29132125, 0.9566253 , 3.681508  ], dtype=float32), 'agent-1': array([-0.29132125, -0.9566253 ,  3.681508  ], dtype=float32)}\n",
      "action:  {'agent-0': array([-0.5963092, -1.       ,  0.6531574], dtype=float32), 'agent-1': array([-1.        ,  1.        ,  0.01620713], dtype=float32)}\n",
      "reward:  {'agent-0': -37.95750297743838, 'agent-1': -37.95750297743838} \n",
      "\n",
      "obs:  {'agent-0': array([0.28723362, 0.9578605 , 3.6624713 ], dtype=float32), 'agent-1': array([-0.28723362, -0.9578605 ,  3.6624713 ], dtype=float32)}\n",
      "action:  {'agent-0': array([ 0.3224455, -0.7518442,  0.1016984], dtype=float32), 'agent-1': array([-0.22533739,  0.5814247 ,  0.67737514], dtype=float32)}\n",
      "reward:  {'agent-0': -37.561580758863, 'agent-1': -37.561580758863} \n",
      "\n",
      "obs:  {'agent-0': array([0.29519793, 0.9554361 , 3.6522565 ], dtype=float32), 'agent-1': array([-0.29519793, -0.9554361 ,  3.6522565 ], dtype=float32)}\n",
      "action:  {'agent-0': array([ 1.        , -0.78580946,  0.        ], dtype=float32), 'agent-1': array([-0.29589784,  1.        ,  0.21385098], dtype=float32)}\n",
      "reward:  {'agent-0': -37.37614368411248, 'agent-1': -37.37614368411248} \n",
      "\n",
      "obs:  {'agent-0': array([0.29835552, 0.9544548 , 3.647436  ], dtype=float32), 'agent-1': array([-0.29835552, -0.9544548 ,  3.647436  ], dtype=float32)}\n",
      "action:  {'agent-0': array([-0.06672823,  1.        ,  0.57649755], dtype=float32), 'agent-1': array([0.12829673, 0.09921825, 1.        ], dtype=float32)}\n",
      "reward:  {'agent-0': -37.78313335001257, 'agent-1': -37.78313335001257} \n",
      "\n",
      "obs:  {'agent-0': array([0.29072797, 0.95680577, 3.6579854 ], dtype=float32), 'agent-1': array([-0.29072797, -0.95680577,  3.6579854 ], dtype=float32)}\n",
      "action:  {'agent-0': array([-0.6063897 , -0.49092066,  0.        ], dtype=float32), 'agent-1': array([-1.,  1.,  0.], dtype=float32)}\n",
      "reward:  {'agent-0': -37.78313335001257, 'agent-1': -37.78313335001257} \n",
      "\n",
      "obs:  {'agent-0': array([0.29072797, 0.95680577, 3.6579854 ], dtype=float32), 'agent-1': array([-0.29072797, -0.95680577,  3.6579854 ], dtype=float32)}\n",
      "action:  {'agent-0': array([ 0.71817064, -0.82557094,  0.5498275 ], dtype=float32), 'agent-1': array([-0.7763163,  0.290699 ,  0.       ], dtype=float32)}\n",
      "reward:  {'agent-0': -37.4670865167725, 'agent-1': -37.4670865167725} \n",
      "\n",
      "obs:  {'agent-0': array([0.30371946, 0.95276153, 3.649803  ], dtype=float32), 'agent-1': array([-0.30371946, -0.95276153,  3.649803  ], dtype=float32)}\n",
      "action:  {'agent-0': array([ 0.6042608 , -0.581241  ,  0.39157224], dtype=float32), 'agent-1': array([ 0.33056343, -1.        ,  0.28169656], dtype=float32)}\n",
      "reward:  {'agent-0': -37.562403950582855, 'agent-1': -37.562403950582855} \n",
      "\n",
      "obs:  {'agent-0': array([0.3067689, 0.951784 , 3.6522777], dtype=float32), 'agent-1': array([-0.3067689, -0.951784 ,  3.6522777], dtype=float32)}\n",
      "action:  {'agent-0': array([ 0.6683152 , -0.7950189 ,  0.63878703], dtype=float32), 'agent-1': array([ 1., -1.,  1.], dtype=float32)}\n",
      "reward:  {'agent-0': -37.86142647406454, 'agent-1': -37.86142647406454} \n",
      "\n",
      "obs:  {'agent-0': array([0.2892096, 0.9572658, 3.6600022], dtype=float32), 'agent-1': array([-0.2892096, -0.9572658,  3.6600022], dtype=float32)}\n",
      "action:  {'agent-0': array([ 1.        , -0.6125856 ,  0.18687499], dtype=float32), 'agent-1': array([-1.        ,  0.01118267,  0.17060071], dtype=float32)}\n",
      "reward:  {'agent-0': -37.85526675447844, 'agent-1': -37.85526675447844} \n",
      "\n",
      "obs:  {'agent-0': array([0.2986999, 0.9543471, 3.6598437], dtype=float32), 'agent-1': array([-0.2986999, -0.9543471,  3.6598437], dtype=float32)}\n",
      "action:  {'agent-0': array([ 0.2467984, -0.8032874,  0.       ], dtype=float32), 'agent-1': array([-0.3008889 ,  0.2789812 ,  0.23465964], dtype=float32)}\n",
      "reward:  {'agent-0': -37.81397990612804, 'agent-1': -37.81397990612804} \n",
      "\n",
      "obs:  {'agent-0': array([0.30089322, 0.95365787, 3.6587806 ], dtype=float32), 'agent-1': array([-0.30089322, -0.95365787,  3.6587806 ], dtype=float32)}\n",
      "action:  {'agent-0': array([-1.        , -0.24536622,  0.        ], dtype=float32), 'agent-1': array([-1.        ,  0.23065138,  0.        ], dtype=float32)}\n",
      "reward:  {'agent-0': -37.81397990612804, 'agent-1': -37.81397990612804} \n",
      "\n",
      "obs:  {'agent-0': array([0.30089322, 0.95365787, 3.6587806 ], dtype=float32), 'agent-1': array([-0.30089322, -0.95365787,  3.6587806 ], dtype=float32)}\n",
      "action:  {'agent-0': array([-1.       ,  0.5542846,  0.       ], dtype=float32), 'agent-1': array([-0.790123, -1.      ,  0.      ], dtype=float32)}\n",
      "reward:  {'agent-0': -37.81397990612804, 'agent-1': -37.81397990612804} \n",
      "\n",
      "obs:  {'agent-0': array([0.30089322, 0.95365787, 3.6587806 ], dtype=float32), 'agent-1': array([-0.30089322, -0.95365787,  3.6587806 ], dtype=float32)}\n",
      "action:  {'agent-0': array([ 0.2808113 , -1.        ,  0.07059222], dtype=float32), 'agent-1': array([-1.       , -1.       ,  0.8746004], dtype=float32)}\n",
      "reward:  {'agent-0': -38.85465978805991, 'agent-1': -38.85465978805991} \n",
      "\n",
      "obs:  {'agent-0': array([0.31585383, 0.94880784, 3.6852393 ], dtype=float32), 'agent-1': array([-0.31585383, -0.94880784,  3.6852393 ], dtype=float32)}\n",
      "action:  {'agent-0': array([-1.        , -0.48721308,  0.06000435], dtype=float32), 'agent-1': array([0.20435047, 0.70883393, 0.65327203], dtype=float32)}\n",
      "reward:  {'agent-0': -38.326457530584555, 'agent-1': -38.326457530584555} \n",
      "\n",
      "obs:  {'agent-0': array([0.31515807, 0.94903916, 3.6718974 ], dtype=float32), 'agent-1': array([-0.31515807, -0.94903916,  3.6718974 ], dtype=float32)}\n",
      "action:  {'agent-0': array([-0.9127607 , -1.        ,  0.34461713], dtype=float32), 'agent-1': array([-0.28165054, -0.0676747 ,  0.        ], dtype=float32)}\n",
      "reward:  {'agent-0': -37.90074427892518, 'agent-1': -37.90074427892518} \n",
      "\n",
      "obs:  {'agent-0': array([0.31039864, 0.95060647, 3.6610134 ], dtype=float32), 'agent-1': array([-0.31039864, -0.95060647,  3.6610134 ], dtype=float32)}\n",
      "action:  {'agent-0': array([0.92226267, 0.60844374, 0.27705735], dtype=float32), 'agent-1': array([-1.       ,  0.4320891,  0.7509062], dtype=float32)}\n",
      "reward:  {'agent-0': -38.07822011065617, 'agent-1': -38.07822011065617} \n",
      "\n",
      "obs:  {'agent-0': array([0.33538243, 0.94208205, 3.6655653 ], dtype=float32), 'agent-1': array([-0.33538243, -0.94208205,  3.6655653 ], dtype=float32)}\n",
      "action:  {'agent-0': array([-0.705937  , -1.        ,  0.71325684], dtype=float32), 'agent-1': array([-0.17729509, -1.        ,  0.02137497], dtype=float32)}\n",
      "reward:  {'agent-0': -37.259576564804846, 'agent-1': -37.259576564804846} \n",
      "\n",
      "obs:  {'agent-0': array([0.32933924, 0.94421166, 3.644394  ], dtype=float32), 'agent-1': array([-0.32933924, -0.94421166,  3.644394  ], dtype=float32)}\n",
      "action:  {'agent-0': array([-0.40573353, -0.65483534,  0.13516805], dtype=float32), 'agent-1': array([ 1.        , -0.16383672,  0.7371902 ], dtype=float32)}\n",
      "reward:  {'agent-0': -37.03696227025636, 'agent-1': -37.03696227025636} \n",
      "\n",
      "obs:  {'agent-0': array([0.30993387, 0.9507581 , 3.6385584 ], dtype=float32), 'agent-1': array([-0.30993387, -0.9507581 ,  3.6385584 ], dtype=float32)}\n",
      "action:  {'agent-0': array([ 0.4476204, -0.8889232,  0.       ], dtype=float32), 'agent-1': array([ 0.61275864, -0.12470984,  1.        ], dtype=float32)}\n",
      "reward:  {'agent-0': -36.970836345704434, 'agent-1': -36.970836345704434} \n",
      "\n",
      "obs:  {'agent-0': array([0.2939141, 0.9558318, 3.6368184], dtype=float32), 'agent-1': array([-0.2939141, -0.9558318,  3.6368184], dtype=float32)}\n",
      "action:  {'agent-0': array([ 0.6128938 , -1.        ,  0.36484426], dtype=float32), 'agent-1': array([-1.        , -0.40290302,  0.        ], dtype=float32)}\n",
      "reward:  {'agent-0': -36.68923292821337, 'agent-1': -36.68923292821337} \n",
      "\n",
      "obs:  {'agent-0': array([0.30226472, 0.953224  , 3.6293745 ], dtype=float32), 'agent-1': array([-0.30226472, -0.953224  ,  3.6293745 ], dtype=float32)}\n",
      "action:  {'agent-0': array([0.70868444, 0.483382  , 0.        ], dtype=float32), 'agent-1': array([-0.03677332, -1.        ,  0.6970084 ], dtype=float32)}\n",
      "reward:  {'agent-0': -37.36184972139987, 'agent-1': -37.36184972139987} \n",
      "\n",
      "obs:  {'agent-0': array([0.29750916, 0.95471895, 3.6470635 ], dtype=float32), 'agent-1': array([-0.29750916, -0.95471895,  3.6470635 ], dtype=float32)}\n",
      "action:  {'agent-0': array([-0.14056182, -0.68358696,  0.43676212], dtype=float32), 'agent-1': array([1.        , 0.57996833, 0.5870027 ], dtype=float32)}\n",
      "reward:  {'agent-0': -36.561389487893685, 'agent-1': -36.561389487893685} \n",
      "\n",
      "obs:  {'agent-0': array([0.2862883 , 0.95814353, 3.6259766 ], dtype=float32), 'agent-1': array([-0.2862883 , -0.95814353,  3.6259766 ], dtype=float32)}\n",
      "action:  {'agent-0': array([-0.53059435, -0.86772364,  0.22200054], dtype=float32), 'agent-1': array([-1.        , -0.28474236,  0.        ], dtype=float32)}\n",
      "reward:  {'agent-0': -36.34314068564179, 'agent-1': -36.34314068564179} \n",
      "\n",
      "obs:  {'agent-0': array([0.2847664 , 0.95859694, 3.6201491 ], dtype=float32), 'agent-1': array([-0.2847664 , -0.95859694,  3.6201491 ], dtype=float32)}\n",
      "action:  {'agent-0': array([-1.        , -0.45044988,  0.23587936], dtype=float32), 'agent-1': array([ 1.        , -1.        ,  0.33970422], dtype=float32)}\n",
      "reward:  {'agent-0': -36.40826990371298, 'agent-1': -36.40826990371298} \n",
      "\n",
      "obs:  {'agent-0': array([0.26844785, 0.9632942 , 3.6218917 ], dtype=float32), 'agent-1': array([-0.26844785, -0.9632942 ,  3.6218917 ], dtype=float32)}\n",
      "action:  {'agent-0': array([0.36768723, 0.9544356 , 0.45562482], dtype=float32), 'agent-1': array([-1.,  1.,  1.], dtype=float32)}\n",
      "reward:  {'agent-0': -36.19980759863932, 'agent-1': -36.19980759863932} \n",
      "\n",
      "obs:  {'agent-0': array([0.30224606, 0.95322996, 3.6163037 ], dtype=float32), 'agent-1': array([-0.30224606, -0.95322996,  3.6163037 ], dtype=float32)}\n",
      "action:  {'agent-0': array([0.3857093 , 0.3905828 , 0.03732595], dtype=float32), 'agent-1': array([0.48524368, 0.45470452, 0.8060593 ], dtype=float32)}\n",
      "reward:  {'agent-0': -35.75135368403794, 'agent-1': -35.75135368403794} \n",
      "\n",
      "obs:  {'agent-0': array([0.2954996, 0.9553429, 3.604175 ], dtype=float32), 'agent-1': array([-0.2954996, -0.9553429,  3.604175 ], dtype=float32)}\n",
      "action:  {'agent-0': array([-1.        ,  0.75159395,  0.35503945], dtype=float32), 'agent-1': array([-1.        , -0.8807659 ,  0.03740934], dtype=float32)}\n",
      "reward:  {'agent-0': -35.946038390911845, 'agent-1': -35.946038390911845} \n",
      "\n",
      "obs:  {'agent-0': array([0.28506285, 0.95850885, 3.6094584 ], dtype=float32), 'agent-1': array([-0.28506285, -0.95850885,  3.6094584 ], dtype=float32)}\n",
      "action:  {'agent-0': array([0.15548384, 0.290344  , 0.        ], dtype=float32), 'agent-1': array([ 1.        , -0.27111542,  0.6033084 ], dtype=float32)}\n",
      "reward:  {'agent-0': -35.93627092191065, 'agent-1': -35.93627092191065} \n",
      "\n",
      "obs:  {'agent-0': array([0.26835206, 0.9633209 , 3.609194  ], dtype=float32), 'agent-1': array([-0.26835206, -0.9633209 ,  3.609194  ], dtype=float32)}\n",
      "action:  {'agent-0': array([ 1.       , -0.4093917,  0.9566624], dtype=float32), 'agent-1': array([-0.13834989,  1.        ,  0.        ], dtype=float32)}\n",
      "reward:  {'agent-0': -35.83042094035897, 'agent-1': -35.83042094035897} \n",
      "\n",
      "obs:  {'agent-0': array([0.29584455, 0.9552361 , 3.6063242 ], dtype=float32), 'agent-1': array([-0.29584455, -0.9552361 ,  3.6063242 ], dtype=float32)}\n",
      "action:  {'agent-0': array([1.        , 0.40186858, 0.        ], dtype=float32), 'agent-1': array([0.37161505, 0.4736483 , 0.07650107], dtype=float32)}\n",
      "reward:  {'agent-0': -35.7874015641557, 'agent-1': -35.7874015641557} \n",
      "\n",
      "obs:  {'agent-0': array([0.2954058 , 0.95537186, 3.6051555 ], dtype=float32), 'agent-1': array([-0.2954058 , -0.95537186,  3.6051555 ], dtype=float32)}\n",
      "action:  {'agent-0': array([-0.02662635, -0.5368337 ,  0.        ], dtype=float32), 'agent-1': array([-0.4965179 ,  0.93341005,  0.5481473 ], dtype=float32)}\n",
      "reward:  {'agent-0': -35.381377371247964, 'agent-1': -35.381377371247964} \n",
      "\n",
      "obs:  {'agent-0': array([0.3064881, 0.9518745, 3.594057 ], dtype=float32), 'agent-1': array([-0.3064881, -0.9518745,  3.594057 ], dtype=float32)}\n",
      "action:  {'agent-0': array([-1.        , -0.30324614,  0.        ], dtype=float32), 'agent-1': array([1.        , 1.        , 0.88537264], dtype=float32)}\n",
      "reward:  {'agent-0': -34.27202135888693, 'agent-1': -34.27202135888693} \n",
      "\n",
      "obs:  {'agent-0': array([0.29057515, 0.9568522 , 3.56309   ], dtype=float32), 'agent-1': array([-0.29057515, -0.9568522 ,  3.56309   ], dtype=float32)}\n",
      "action:  {'agent-0': array([0.59677875, 0.8495002 , 1.        ], dtype=float32), 'agent-1': array([0.20813715, 0.5838826 , 0.2787745 ], dtype=float32)}\n",
      "reward:  {'agent-0': -35.08709072868234, 'agent-1': -35.08709072868234} \n",
      "\n",
      "obs:  {'agent-0': array([0.29917994, 0.9541967 , 3.585935  ], dtype=float32), 'agent-1': array([-0.29917994, -0.9541967 ,  3.585935  ], dtype=float32)}\n",
      "action:  {'agent-0': array([-0.45266652, -0.20925218,  0.14921433], dtype=float32), 'agent-1': array([-0.81601524, -0.07177097,  0.8907934 ], dtype=float32)}\n",
      "reward:  {'agent-0': -35.32099935880312, 'agent-1': -35.32099935880312} \n",
      "\n",
      "obs:  {'agent-0': array([0.3158662, 0.9488037, 3.592396 ], dtype=float32), 'agent-1': array([-0.3158662, -0.9488037,  3.592396 ], dtype=float32)}\n",
      "action:  {'agent-0': array([1., 1., 0.], dtype=float32), 'agent-1': array([-1.,  1.,  0.], dtype=float32)}\n",
      "reward:  {'agent-0': -35.32099935880312, 'agent-1': -35.32099935880312} \n",
      "\n",
      "obs:  {'agent-0': array([0.3158662, 0.9488037, 3.592396 ], dtype=float32), 'agent-1': array([-0.3158662, -0.9488037,  3.592396 ], dtype=float32)}\n",
      "action:  {'agent-0': array([ 1.       , -0.6003977,  1.       ], dtype=float32), 'agent-1': array([ 1.        , -0.05568445,  0.41603655], dtype=float32)}\n",
      "reward:  {'agent-0': -34.96553011216997, 'agent-1': -34.96553011216997} \n",
      "\n",
      "obs:  {'agent-0': array([0.3357785 , 0.94194096, 3.582561  ], dtype=float32), 'agent-1': array([-0.3357785 , -0.94194096,  3.582561  ], dtype=float32)}\n",
      "action:  {'agent-0': array([ 0.9441024 , -0.45599997,  0.05846885], dtype=float32), 'agent-1': array([1.        , 0.56668127, 0.28089646], dtype=float32)}\n",
      "reward:  {'agent-0': -34.71502035236564, 'agent-1': -34.71502035236564} \n",
      "\n",
      "obs:  {'agent-0': array([0.33170015, 0.9433849 , 3.5755713 ], dtype=float32), 'agent-1': array([-0.33170015, -0.9433849 ,  3.5755713 ], dtype=float32)}\n",
      "action:  {'agent-0': array([0.47097564, 1.        , 1.        ], dtype=float32), 'agent-1': array([0.2971418, 1.       , 0.       ], dtype=float32)}\n",
      "reward:  {'agent-0': -35.814804951286156, 'agent-1': -35.814804951286156} \n",
      "\n",
      "obs:  {'agent-0': array([0.33466476, 0.9423373 , 3.6059    ], dtype=float32), 'agent-1': array([-0.33466476, -0.9423373 ,  3.6059    ], dtype=float32)}\n",
      "action:  {'agent-0': array([-0.63551676, -0.23118848,  0.        ], dtype=float32), 'agent-1': array([0.5029359 , 1.        , 0.03175741], dtype=float32)}\n",
      "reward:  {'agent-0': -35.779533783449885, 'agent-1': -35.779533783449885} \n",
      "\n",
      "obs:  {'agent-0': array([0.33454826, 0.94237864, 3.6049416 ], dtype=float32), 'agent-1': array([-0.33454826, -0.94237864,  3.6049416 ], dtype=float32)}\n",
      "action:  {'agent-0': array([-1.        ,  0.80803955,  0.6095004 ], dtype=float32), 'agent-1': array([0.91929805, 0.5046674 , 0.        ], dtype=float32)}\n",
      "reward:  {'agent-0': -36.04732717897158, 'agent-1': -36.04732717897158} \n",
      "\n",
      "obs:  {'agent-0': array([0.3151546 , 0.94904035, 3.6121962 ], dtype=float32), 'agent-1': array([-0.3151546 , -0.94904035,  3.6121962 ], dtype=float32)}\n",
      "action:  {'agent-0': array([ 0.72777903, -0.73929965,  1.        ], dtype=float32), 'agent-1': array([0.79782724, 0.43098867, 1.        ], dtype=float32)}\n",
      "reward:  {'agent-0': -34.915909372741936, 'agent-1': -34.915909372741936} \n",
      "\n",
      "obs:  {'agent-0': array([0.32336068, 0.9462758 , 3.5811803 ], dtype=float32), 'agent-1': array([-0.32336068, -0.9462758 ,  3.5811803 ], dtype=float32)}\n",
      "action:  {'agent-0': array([1.        , 0.09740245, 1.        ], dtype=float32), 'agent-1': array([0.1537342, 0.5344145, 0.       ], dtype=float32)}\n",
      "reward:  {'agent-0': -35.34328012047472, 'agent-1': -35.34328012047472} \n",
      "\n",
      "obs:  {'agent-0': array([0.34774455, 0.93758935, 3.5930092 ], dtype=float32), 'agent-1': array([-0.34774455, -0.93758935,  3.5930092 ], dtype=float32)}\n",
      "action:  {'agent-0': array([0.4551816 , 0.40983558, 0.        ], dtype=float32), 'agent-1': array([1.        , 0.24952221, 0.17993587], dtype=float32)}\n",
      "reward:  {'agent-0': -35.238945072827306, 'agent-1': -35.238945072827306} \n",
      "\n",
      "obs:  {'agent-0': array([0.34366798, 0.9390912 , 3.5901344 ], dtype=float32), 'agent-1': array([-0.34366798, -0.9390912 ,  3.5901344 ], dtype=float32)}\n",
      "action:  {'agent-0': array([-0.72705686,  0.3165301 ,  1.        ], dtype=float32), 'agent-1': array([0.27269483, 1.        , 0.41059336], dtype=float32)}\n",
      "reward:  {'agent-0': -34.870452851077204, 'agent-1': -34.870452851077204} \n",
      "\n",
      "obs:  {'agent-0': array([0.32323852, 0.94631755, 3.5799139 ], dtype=float32), 'agent-1': array([-0.32323852, -0.94631755,  3.5799139 ], dtype=float32)}\n",
      "action:  {'agent-0': array([-0.36988354, -0.47005337,  0.58592683], dtype=float32), 'agent-1': array([0.8293829 , 0.7402139 , 0.20922643], dtype=float32)}\n",
      "reward:  {'agent-0': -34.33788943815415, 'agent-1': -34.33788943815415} \n",
      "\n",
      "obs:  {'agent-0': array([0.3168867, 0.9484634, 3.5649557], dtype=float32), 'agent-1': array([-0.3168867, -0.9484634,  3.5649557], dtype=float32)}\n",
      "action:  {'agent-0': array([-0.00555688, -0.7773626 ,  0.5291159 ], dtype=float32), 'agent-1': array([-0.6944114 ,  0.87491107,  0.5426557 ], dtype=float32)}\n",
      "reward:  {'agent-0': -33.62194932600417, 'agent-1': -33.62194932600417} \n",
      "\n",
      "obs:  {'agent-0': array([0.3347547, 0.9423053, 3.544488 ], dtype=float32), 'agent-1': array([-0.3347547, -0.9423053,  3.544488 ], dtype=float32)}\n",
      "action:  {'agent-0': array([1.        , 0.03036511, 0.5199182 ], dtype=float32), 'agent-1': array([ 1.       , -0.2672115,  0.       ], dtype=float32)}\n",
      "reward:  {'agent-0': -33.814344073117084, 'agent-1': -33.814344073117084} \n",
      "\n",
      "obs:  {'agent-0': array([0.3482257, 0.9374107, 3.5500295], dtype=float32), 'agent-1': array([-0.3482257, -0.9374107,  3.5500295], dtype=float32)}\n",
      "action:  {'agent-0': array([ 0.49412847, -1.        ,  0.50653213], dtype=float32), 'agent-1': array([-0.6109184, -0.3884977,  1.       ], dtype=float32)}\n",
      "reward:  {'agent-0': -34.014175461335356, 'agent-1': -34.014175461335356} \n",
      "\n",
      "obs:  {'agent-0': array([0.37149906, 0.9284333 , 3.555753  ], dtype=float32), 'agent-1': array([-0.37149906, -0.9284333 ,  3.555753  ], dtype=float32)}\n",
      "action:  {'agent-0': array([-0.42237276, -0.2737977 ,  0.        ], dtype=float32), 'agent-1': array([-1.        , -0.79364926,  0.07125032], dtype=float32)}\n",
      "reward:  {'agent-0': -34.093175604399875, 'agent-1': -34.093175604399875} \n",
      "\n",
      "obs:  {'agent-0': array([0.3727281, 0.9279406, 3.5580068], dtype=float32), 'agent-1': array([-0.3727281, -0.9279406,  3.5580068], dtype=float32)}\n",
      "action:  {'agent-0': array([0.16555333, 0.5591359 , 0.58903706], dtype=float32), 'agent-1': array([-0.93017536,  0.23737967,  1.        ], dtype=float32)}\n",
      "reward:  {'agent-0': -34.57379564048089, 'agent-1': -34.57379564048089} \n",
      "\n",
      "obs:  {'agent-0': array([0.3972713, 0.9177012, 3.5716093], dtype=float32), 'agent-1': array([-0.3972713, -0.9177012,  3.5716093], dtype=float32)}\n",
      "action:  {'agent-0': array([ 0.75394106, -0.1461159 ,  0.44546586], dtype=float32), 'agent-1': array([1.        , 0.22492695, 0.        ], dtype=float32)}\n",
      "reward:  {'agent-0': -34.6490988888342, 'agent-1': -34.6490988888342} \n",
      "\n",
      "obs:  {'agent-0': array([0.40610096, 0.9138282 , 3.5737238 ], dtype=float32), 'agent-1': array([-0.40610096, -0.9138282 ,  3.5737238 ], dtype=float32)}\n",
      "action:  {'agent-0': array([ 0.15698862, -0.6678175 ,  0.        ], dtype=float32), 'agent-1': array([-0.39821225,  0.51358855,  0.58959305], dtype=float32)}\n",
      "reward:  {'agent-0': -34.46938238629543, 'agent-1': -34.46938238629543} \n",
      "\n",
      "obs:  {'agent-0': array([0.41502964, 0.9098079 , 3.5686698 ], dtype=float32), 'agent-1': array([-0.41502964, -0.9098079 ,  3.5686698 ], dtype=float32)}\n",
      "action:  {'agent-0': array([-0.3999446 ,  1.        ,  0.78356194], dtype=float32), 'agent-1': array([-1.        ,  0.16279387,  1.        ], dtype=float32)}\n",
      "reward:  {'agent-0': -35.321036408577285, 'agent-1': -35.321036408577285} \n",
      "\n",
      "obs:  {'agent-0': array([0.4244619, 0.9054458, 3.592397 ], dtype=float32), 'agent-1': array([-0.4244619, -0.9054458,  3.592397 ], dtype=float32)}\n",
      "action:  {'agent-0': array([-1.       ,  0.7718481,  0.8694519], dtype=float32), 'agent-1': array([-0.7675331 ,  0.19588041,  0.46339077], dtype=float32)}\n",
      "reward:  {'agent-0': -35.635503083930836, 'agent-1': -35.635503083930836} \n",
      "\n",
      "obs:  {'agent-0': array([0.40629846, 0.9137404 , 3.6010177 ], dtype=float32), 'agent-1': array([-0.40629846, -0.9137404 ,  3.6010177 ], dtype=float32)}\n",
      "action:  {'agent-0': array([ 0.8564645 , -0.13783526,  0.7884295 ], dtype=float32), 'agent-1': array([ 0.07064509, -0.21259856,  0.27866888], dtype=float32)}\n",
      "reward:  {'agent-0': -35.86204165685334, 'agent-1': -35.86204165685334} \n",
      "\n",
      "obs:  {'agent-0': array([0.42201236, 0.90659004, 3.6071823 ], dtype=float32), 'agent-1': array([-0.42201236, -0.90659004,  3.6071823 ], dtype=float32)}\n",
      "action:  {'agent-0': array([-0.844497  , -0.07854766,  0.9380407 ], dtype=float32), 'agent-1': array([-0.94022536, -0.08765149,  1.        ], dtype=float32)}\n",
      "reward:  {'agent-0': -35.93741649421948, 'agent-1': -35.93741649421948} \n",
      "\n",
      "obs:  {'agent-0': array([0.42524698, 0.90507734, 3.609225  ], dtype=float32), 'agent-1': array([-0.42524698, -0.90507734,  3.609225  ], dtype=float32)}\n",
      "action:  {'agent-0': array([1.        , 0.3484192 , 0.53695774], dtype=float32), 'agent-1': array([0.352731  , 0.6624241 , 0.44028366], dtype=float32)}\n",
      "reward:  {'agent-0': -36.00718335266066, 'agent-1': -36.00718335266066} \n",
      "\n",
      "obs:  {'agent-0': array([0.43502247, 0.9004196 , 3.611112  ], dtype=float32), 'agent-1': array([-0.43502247, -0.9004196 ,  3.611112  ], dtype=float32)}\n",
      "action:  {'agent-0': array([-0.2152527 , -0.3386932 ,  0.34661904], dtype=float32), 'agent-1': array([0.90267396, 0.8467188 , 0.3480048 ], dtype=float32)}\n",
      "reward:  {'agent-0': -35.467454831839056, 'agent-1': -35.467454831839056} \n",
      "\n",
      "obs:  {'agent-0': array([0.43068182, 0.90250385, 3.5964203 ], dtype=float32), 'agent-1': array([-0.43068182, -0.90250385,  3.5964203 ], dtype=float32)}\n",
      "action:  {'agent-0': array([0.08674133, 0.268152  , 0.0783529 ], dtype=float32), 'agent-1': array([ 0.33511937, -0.16411239,  0.        ], dtype=float32)}\n",
      "reward:  {'agent-0': -35.48934409670344, 'agent-1': -35.48934409670344} \n",
      "\n",
      "obs:  {'agent-0': array([0.4306077, 0.9025392, 3.5970204], dtype=float32), 'agent-1': array([-0.4306077, -0.9025392,  3.5970204], dtype=float32)}\n",
      "action:  {'agent-0': array([-0.4073707 ,  0.46457934,  0.20242807], dtype=float32), 'agent-1': array([0.65554106, 0.81579876, 0.03726056], dtype=float32)}\n",
      "reward:  {'agent-0': -35.50097667876456, 'agent-1': -35.50097667876456} \n",
      "\n",
      "obs:  {'agent-0': array([0.42745572, 0.9040363 , 3.597339  ], dtype=float32), 'agent-1': array([-0.42745572, -0.9040363 ,  3.597339  ], dtype=float32)}\n",
      "action:  {'agent-0': array([ 1.        , -0.24258107,  0.8667671 ], dtype=float32), 'agent-1': array([-0.29158092,  1.        ,  0.        ], dtype=float32)}\n",
      "reward:  {'agent-0': -35.69208685810098, 'agent-1': -35.69208685810098} \n",
      "\n",
      "obs:  {'agent-0': array([0.44945154, 0.8933047 , 3.602561  ], dtype=float32), 'agent-1': array([-0.44945154, -0.8933047 ,  3.602561  ], dtype=float32)}\n",
      "action:  {'agent-0': array([0.12082458, 1.        , 0.29966193], dtype=float32), 'agent-1': array([-0.61631525,  0.48686647,  0.352149  ], dtype=float32)}\n",
      "reward:  {'agent-0': -35.92083495189166, 'agent-1': -35.92083495189166} \n",
      "\n",
      "obs:  {'agent-0': array([0.45363936, 0.89118534, 3.608776  ], dtype=float32), 'agent-1': array([-0.45363936, -0.89118534,  3.608776  ], dtype=float32)}\n",
      "action:  {'agent-0': array([0.92938876, 1.        , 0.47217417], dtype=float32), 'agent-1': array([-0.77166116,  1.        ,  0.58924544], dtype=float32)}\n",
      "reward:  {'agent-0': -36.231801711830464, 'agent-1': -36.231801711830464} \n",
      "\n",
      "obs:  {'agent-0': array([0.4744074, 0.8803054, 3.6171632], dtype=float32), 'agent-1': array([-0.4744074, -0.8803054,  3.6171632], dtype=float32)}\n",
      "action:  {'agent-0': array([-0.9806074 ,  1.        ,  0.57944703], dtype=float32), 'agent-1': array([-0.9877975 ,  0.19904709,  0.        ], dtype=float32)}\n",
      "reward:  {'agent-0': -36.4805640213418, 'agent-1': -36.4805640213418} \n",
      "\n",
      "obs:  {'agent-0': array([0.45559672, 0.8901863 , 3.6238225 ], dtype=float32), 'agent-1': array([-0.45559672, -0.8901863 ,  3.6238225 ], dtype=float32)}\n",
      "action:  {'agent-0': array([-1.        ,  0.25592327,  0.8738848 ], dtype=float32), 'agent-1': array([0.48263335, 1.        , 0.        ], dtype=float32)}\n",
      "reward:  {'agent-0': -36.29217887703978, 'agent-1': -36.29217887703978} \n",
      "\n",
      "obs:  {'agent-0': array([0.43388247, 0.90096945, 3.6187837 ], dtype=float32), 'agent-1': array([-0.43388247, -0.90096945,  3.6187837 ], dtype=float32)}\n",
      "action:  {'agent-0': array([0.07248771, 0.21546626, 0.31234533], dtype=float32), 'agent-1': array([-0.15540588,  0.195804  ,  0.6805893 ], dtype=float32)}\n",
      "reward:  {'agent-0': -36.28875018978035, 'agent-1': -36.28875018978035} \n",
      "\n",
      "obs:  {'agent-0': array([0.437462 , 0.8992369, 3.6186917], dtype=float32), 'agent-1': array([-0.437462 , -0.8992369,  3.6186917], dtype=float32)}\n",
      "action:  {'agent-0': array([-0.14061695,  1.        ,  0.35972238], dtype=float32), 'agent-1': array([ 0.48048294, -0.71821177,  0.72272277], dtype=float32)}\n",
      "reward:  {'agent-0': -36.91241372318629, 'agent-1': -36.91241372318629} \n",
      "\n",
      "obs:  {'agent-0': array([0.41929284, 0.90785104, 3.6352787 ], dtype=float32), 'agent-1': array([-0.41929284, -0.90785104,  3.6352787 ], dtype=float32)}\n",
      "action:  {'agent-0': array([-0.31283188,  0.15162647,  0.98789173], dtype=float32), 'agent-1': array([-1.       , -0.0549702,  0.2623024], dtype=float32)}\n",
      "reward:  {'agent-0': -37.042060094769646, 'agent-1': -37.042060094769646} \n",
      "\n",
      "obs:  {'agent-0': array([0.41656345, 0.9091066 , 3.6386924 ], dtype=float32), 'agent-1': array([-0.41656345, -0.9091066 ,  3.6386924 ], dtype=float32)}\n",
      "action:  {'agent-0': array([ 1.       , -0.4132312,  0.2882283], dtype=float32), 'agent-1': array([-0.74169195,  1.        ,  0.34547764], dtype=float32)}\n",
      "reward:  {'agent-0': -36.85294183633418, 'agent-1': -36.85294183633418} \n",
      "\n",
      "obs:  {'agent-0': array([0.43347517, 0.9011655 , 3.6337087 ], dtype=float32), 'agent-1': array([-0.43347517, -0.9011655 ,  3.6337087 ], dtype=float32)}\n",
      "action:  {'agent-0': array([-0.09830952, -0.7236564 ,  0.08883184], dtype=float32), 'agent-1': array([-1.       ,  1.       ,  0.8424487], dtype=float32)}\n",
      "reward:  {'agent-0': -36.41520632283414, 'agent-1': -36.41520632283414} \n",
      "\n",
      "obs:  {'agent-0': array([0.46158054, 0.8870983 , 3.6220772 ], dtype=float32), 'agent-1': array([-0.46158054, -0.8870983 ,  3.6220772 ], dtype=float32)}\n",
      "action:  {'agent-0': array([-1.       ,  0.5773382,  0.5109733], dtype=float32), 'agent-1': array([0.16965818, 0.72731364, 0.04459417], dtype=float32)}\n",
      "reward:  {'agent-0': -36.41342308017461, 'agent-1': -36.41342308017461} \n",
      "\n",
      "obs:  {'agent-0': array([0.4473628 , 0.89435256, 3.6220295 ], dtype=float32), 'agent-1': array([-0.4473628 , -0.89435256,  3.6220295 ], dtype=float32)}\n",
      "action:  {'agent-0': array([0.6542212 , 1.        , 0.34024388], dtype=float32), 'agent-1': array([-0.417799,  1.      ,  0.      ], dtype=float32)}\n",
      "reward:  {'agent-0': -36.8173315057699, 'agent-1': -36.8173315057699} \n",
      "\n",
      "obs:  {'agent-0': array([0.4485009, 0.8937824, 3.6327674], dtype=float32), 'agent-1': array([-0.4485009, -0.8937824,  3.6327674], dtype=float32)}\n",
      "action:  {'agent-0': array([-1.        ,  0.37616777,  0.26963407], dtype=float32), 'agent-1': array([-1.        , -0.13587701,  0.6907215 ], dtype=float32)}\n",
      "reward:  {'agent-0': -37.18184958881413, 'agent-1': -37.18184958881413} \n",
      "\n",
      "obs:  {'agent-0': array([0.45542905, 0.8902721 , 3.6423602 ], dtype=float32), 'agent-1': array([-0.45542905, -0.8902721 ,  3.6423602 ], dtype=float32)}\n",
      "action:  {'agent-0': array([1.        , 0.28360736, 0.34387553], dtype=float32), 'agent-1': array([1.        , 1.        , 0.12794405], dtype=float32)}\n",
      "reward:  {'agent-0': -37.25368044455852, 'agent-1': -37.25368044455852} \n",
      "\n",
      "obs:  {'agent-0': array([0.46034715, 0.887739  , 3.6442397 ], dtype=float32), 'agent-1': array([-0.46034715, -0.887739  ,  3.6442397 ], dtype=float32)}\n",
      "action:  {'agent-0': array([-0.15728515,  0.9274554 ,  0.        ], dtype=float32), 'agent-1': array([ 0.03185332, -0.87131643,  1.        ], dtype=float32)}\n",
      "reward:  {'agent-0': -38.0149434972449, 'agent-1': -38.0149434972449} \n",
      "\n",
      "obs:  {'agent-0': array([0.45029062, 0.89288205, 3.6639447 ], dtype=float32), 'agent-1': array([-0.45029062, -0.89288205,  3.6639447 ], dtype=float32)}\n",
      "action:  {'agent-0': array([ 0.39710355, -0.2334218 ,  0.6942768 ], dtype=float32), 'agent-1': array([1.        , 0.08632755, 0.38139036], dtype=float32)}\n",
      "reward:  {'agent-0': -37.79325502026078, 'agent-1': -37.79325502026078} \n",
      "\n",
      "obs:  {'agent-0': array([0.45013538, 0.8929603 , 3.6582463 ], dtype=float32), 'agent-1': array([-0.45013538, -0.8929603 ,  3.6582463 ], dtype=float32)}\n",
      "action:  {'agent-0': array([-0.9070615 ,  0.33308268,  0.        ], dtype=float32), 'agent-1': array([-0.77224284,  0.5316416 ,  0.80228555], dtype=float32)}\n",
      "reward:  {'agent-0': -37.698634410793375, 'agent-1': -37.698634410793375} \n",
      "\n",
      "obs:  {'agent-0': array([0.4676997, 0.8838874, 3.6558044], dtype=float32), 'agent-1': array([-0.4676997, -0.8838874,  3.6558044], dtype=float32)}\n",
      "action:  {'agent-0': array([-1.        , -0.66037005,  1.        ], dtype=float32), 'agent-1': array([-1.        ,  0.7028471 ,  0.38711017], dtype=float32)}\n",
      "reward:  {'agent-0': -36.58795823053893, 'agent-1': -36.58795823053893} \n",
      "\n",
      "obs:  {'agent-0': array([0.46514624, 0.8852339 , 3.6266837 ], dtype=float32), 'agent-1': array([-0.46514624, -0.8852339 ,  3.6266837 ], dtype=float32)}\n",
      "action:  {'agent-0': array([ 1.        , -0.77923036,  1.        ], dtype=float32), 'agent-1': array([0.91213274, 0.20845997, 0.9296322 ], dtype=float32)}\n",
      "reward:  {'agent-0': -35.80214884108955, 'agent-1': -35.80214884108955} \n",
      "\n",
      "obs:  {'agent-0': array([0.47960258, 0.8774858 , 3.6055562 ], dtype=float32), 'agent-1': array([-0.47960258, -0.8774858 ,  3.6055562 ], dtype=float32)}\n",
      "action:  {'agent-0': array([0.26166272, 1.        , 0.        ], dtype=float32), 'agent-1': array([-0.8820683, -1.       ,  0.6644802], dtype=float32)}\n",
      "reward:  {'agent-0': -36.66684582802274, 'agent-1': -36.66684582802274} \n",
      "\n",
      "obs:  {'agent-0': array([0.48427728, 0.8749146 , 3.6287804 ], dtype=float32), 'agent-1': array([-0.48427728, -0.8749146 ,  3.6287804 ], dtype=float32)}\n",
      "action:  {'agent-0': array([ 0.3915496 , -0.04278696,  0.        ], dtype=float32), 'agent-1': array([-0.7623    , -0.60535604,  0.08156544], dtype=float32)}\n",
      "reward:  {'agent-0': -36.74016944569358, 'agent-1': -36.74016944569358} \n",
      "\n",
      "obs:  {'agent-0': array([0.48500314, 0.87451243, 3.6307251 ], dtype=float32), 'agent-1': array([-0.48500314, -0.87451243,  3.6307251 ], dtype=float32)}\n",
      "action:  {'agent-0': array([-0.8735267 , -0.5836495 ,  0.67766154], dtype=float32), 'agent-1': array([-1.        ,  1.        ,  0.48718825], dtype=float32)}\n",
      "reward:  {'agent-0': -35.9189966796609, 'agent-1': -35.9189966796609} \n",
      "\n",
      "obs:  {'agent-0': array([0.4931744 , 0.86993045, 3.6087263 ], dtype=float32), 'agent-1': array([-0.4931744 , -0.86993045,  3.6087263 ], dtype=float32)}\n",
      "action:  {'agent-0': array([ 0.01603544, -0.16097832,  0.        ], dtype=float32), 'agent-1': array([1.        , 0.69145894, 1.        ], dtype=float32)}\n",
      "reward:  {'agent-0': -34.828317537723905, 'agent-1': -34.828317537723905} \n",
      "\n",
      "obs:  {'agent-0': array([0.47990632, 0.87731975, 3.5787387 ], dtype=float32), 'agent-1': array([-0.47990632, -0.87731975,  3.5787387 ], dtype=float32)}\n",
      "action:  {'agent-0': array([ 1.        , -0.77745944,  0.08852437], dtype=float32), 'agent-1': array([-1.        , -0.00139976,  0.9903879 ], dtype=float32)}\n",
      "reward:  {'agent-0': -35.30050555995602, 'agent-1': -35.30050555995602} \n",
      "\n",
      "obs:  {'agent-0': array([0.5040506, 0.8636741, 3.5918317], dtype=float32), 'agent-1': array([-0.5040506, -0.8636741,  3.5918317], dtype=float32)}\n",
      "action:  {'agent-0': array([-0.78137404, -0.30069357,  0.6241173 ], dtype=float32), 'agent-1': array([-1.        ,  0.6111578 ,  0.44910467], dtype=float32)}\n",
      "reward:  {'agent-0': -34.88249845411798, 'agent-1': -34.88249845411798} \n",
      "\n",
      "obs:  {'agent-0': array([0.5089853 , 0.86077523, 3.5802498 ], dtype=float32), 'agent-1': array([-0.5089853 , -0.86077523,  3.5802498 ], dtype=float32)}\n",
      "action:  {'agent-0': array([-0.6597344 ,  1.        ,  0.48215982], dtype=float32), 'agent-1': array([0.79502726, 0.5089456 , 0.916443  ], dtype=float32)}\n",
      "reward:  {'agent-0': -34.3753152117567, 'agent-1': -34.3753152117567} \n",
      "\n",
      "obs:  {'agent-0': array([0.48604596, 0.87393326, 3.5660143 ], dtype=float32), 'agent-1': array([-0.48604596, -0.87393326,  3.5660143 ], dtype=float32)}\n",
      "action:  {'agent-0': array([0.47192907, 0.9684322 , 0.12888587], dtype=float32), 'agent-1': array([-0.75887626,  1.        ,  1.        ], dtype=float32)}\n",
      "reward:  {'agent-0': -34.02803622911743, 'agent-1': -34.02803622911743} \n",
      "\n",
      "obs:  {'agent-0': array([0.5150954 , 0.85713285, 3.5561488 ], dtype=float32), 'agent-1': array([-0.5150954 , -0.85713285,  3.5561488 ], dtype=float32)}\n",
      "action:  {'agent-0': array([ 1.       , -0.2923913,  0.2379685], dtype=float32), 'agent-1': array([0.94193006, 1.        , 0.6348591 ], dtype=float32)}\n",
      "reward:  {'agent-0': -33.238835822623685, 'agent-1': -33.238835822623685} \n",
      "\n",
      "obs:  {'agent-0': array([0.51649404, 0.8562908 , 3.5333605 ], dtype=float32), 'agent-1': array([-0.51649404, -0.8562908 ,  3.5333605 ], dtype=float32)}\n",
      "action:  {'agent-0': array([-1.        , -1.        ,  0.03627545], dtype=float32), 'agent-1': array([1.        , 1.        , 0.09085724], dtype=float32)}\n",
      "reward:  {'agent-0': -33.064338224433634, 'agent-1': -33.064338224433634} \n",
      "\n",
      "obs:  {'agent-0': array([0.51537484, 0.8569649 , 3.528251  ], dtype=float32), 'agent-1': array([-0.51537484, -0.8569649 ,  3.528251  ], dtype=float32)}\n",
      "action:  {'agent-0': array([ 0.50303054, -1.        ,  0.36304924], dtype=float32), 'agent-1': array([-0.67744434,  0.80758476,  0.11359525], dtype=float32)}\n",
      "reward:  {'agent-0': -32.81156278230192, 'agent-1': -32.81156278230192} \n",
      "\n",
      "obs:  {'agent-0': array([0.5272564 , 0.84970623, 3.5208027 ], dtype=float32), 'agent-1': array([-0.5272564 , -0.84970623,  3.5208027 ], dtype=float32)}\n",
      "action:  {'agent-0': array([0.88168585, 0.6910548 , 0.5675621 ], dtype=float32), 'agent-1': array([0.03753948, 1.        , 0.9129896 ], dtype=float32)}\n",
      "reward:  {'agent-0': -32.621727768569734, 'agent-1': -32.621727768569734} \n",
      "\n",
      "obs:  {'agent-0': array([0.54461384, 0.83868694, 3.5151725 ], dtype=float32), 'agent-1': array([-0.54461384, -0.83868694,  3.5151725 ], dtype=float32)}\n",
      "action:  {'agent-0': array([0.01000166, 1.        , 0.2106624 ], dtype=float32), 'agent-1': array([-0.4861719,  1.       ,  0.7537441], dtype=float32)}\n",
      "reward:  {'agent-0': -32.37262446854192, 'agent-1': -32.37262446854192} \n",
      "\n",
      "obs:  {'agent-0': array([0.5601894 , 0.82836455, 3.507736  ], dtype=float32), 'agent-1': array([-0.5601894 , -0.82836455,  3.507736  ], dtype=float32)}\n",
      "action:  {'agent-0': array([-1.        ,  0.85393476,  0.60305345], dtype=float32), 'agent-1': array([ 0.69401956, -0.366615  ,  0.        ], dtype=float32)}\n",
      "reward:  {'agent-0': -32.47094542034375, 'agent-1': -32.47094542034375} \n",
      "\n",
      "obs:  {'agent-0': array([0.5399211 , 0.84171563, 3.5106778 ], dtype=float32), 'agent-1': array([-0.5399211 , -0.84171563,  3.5106778 ], dtype=float32)}\n",
      "action:  {'agent-0': array([ 0.3816073 , -0.9002432 ,  0.44121614], dtype=float32), 'agent-1': array([-1.       , -0.8395652,  1.       ], dtype=float32)}\n",
      "reward:  {'agent-0': -33.482398214235666, 'agent-1': -33.482398214235666} \n",
      "\n",
      "obs:  {'agent-0': array([0.55850595, 0.8295005 , 3.540449  ], dtype=float32), 'agent-1': array([-0.55850595, -0.8295005 ,  3.540449  ], dtype=float32)}\n",
      "action:  {'agent-0': array([-0.1221174,  0.8115932,  0.       ], dtype=float32), 'agent-1': array([-1.,  1.,  0.], dtype=float32)}\n",
      "reward:  {'agent-0': -33.482398214235666, 'agent-1': -33.482398214235666} \n",
      "\n",
      "obs:  {'agent-0': array([0.55850595, 0.8295005 , 3.540449  ], dtype=float32), 'agent-1': array([-0.55850595, -0.8295005 ,  3.540449  ], dtype=float32)}\n",
      "action:  {'agent-0': array([ 0.36108065, -0.5087563 ,  0.        ], dtype=float32), 'agent-1': array([-0.02422881,  1.        ,  0.        ], dtype=float32)}\n",
      "reward:  {'agent-0': -33.482398214235666, 'agent-1': -33.482398214235666} \n",
      "\n",
      "obs:  {'agent-0': array([0.55850595, 0.8295005 , 3.540449  ], dtype=float32), 'agent-1': array([-0.55850595, -0.8295005 ,  3.540449  ], dtype=float32)}\n",
      "action:  {'agent-0': array([1.        , 0.33301663, 0.        ], dtype=float32), 'agent-1': array([ 0.09950745, -1.        ,  0.        ], dtype=float32)}\n",
      "reward:  {'agent-0': -33.482398214235666, 'agent-1': -33.482398214235666} \n",
      "\n",
      "obs:  {'agent-0': array([0.55850595, 0.8295005 , 3.540449  ], dtype=float32), 'agent-1': array([-0.55850595, -0.8295005 ,  3.540449  ], dtype=float32)}\n",
      "action:  {'agent-0': array([0.5463592 , 0.07689023, 0.43916592], dtype=float32), 'agent-1': array([0.24015081, 0.00405967, 0.        ], dtype=float32)}\n",
      "reward:  {'agent-0': -33.644900082141795, 'agent-1': -33.644900082141795} \n",
      "\n",
      "obs:  {'agent-0': array([0.56294006, 0.82649773, 3.5451505 ], dtype=float32), 'agent-1': array([-0.56294006, -0.82649773,  3.5451505 ], dtype=float32)}\n",
      "action:  {'agent-0': array([-0.10770565,  0.29223692,  0.6864793 ], dtype=float32), 'agent-1': array([-0.04323465,  0.85170746,  0.77644134], dtype=float32)}\n",
      "reward:  {'agent-0': -33.24218692135527, 'agent-1': -33.24218692135527} \n",
      "\n",
      "obs:  {'agent-0': array([0.5685454 , 0.82265186, 3.5334585 ], dtype=float32), 'agent-1': array([-0.5685454 , -0.82265186,  3.5334585 ], dtype=float32)}\n",
      "action:  {'agent-0': array([-1.       ,  1.       ,  0.5156979], dtype=float32), 'agent-1': array([-1.        ,  0.79399526,  0.        ], dtype=float32)}\n",
      "reward:  {'agent-0': -33.38093971732, 'agent-1': -33.38093971732} \n",
      "\n",
      "obs:  {'agent-0': array([0.5507333, 0.8346813, 3.5375023], dtype=float32), 'agent-1': array([-0.5507333, -0.8346813,  3.5375023], dtype=float32)}\n",
      "action:  {'agent-0': array([-0.72769374, -0.8745023 ,  0.7499503 ], dtype=float32), 'agent-1': array([-0.49750257,  1.        ,  0.        ], dtype=float32)}\n",
      "reward:  {'agent-0': -32.53311076307897, 'agent-1': -32.53311076307897} \n",
      "\n",
      "obs:  {'agent-0': array([0.54831094, 0.83627456, 3.5125334 ], dtype=float32), 'agent-1': array([-0.54831094, -0.83627456,  3.5125334 ], dtype=float32)}\n",
      "action:  {'agent-0': array([ 0.9024049 , -0.4310429 ,  0.09664959], dtype=float32), 'agent-1': array([-0.32048738,  0.09834683,  0.        ], dtype=float32)}\n",
      "reward:  {'agent-0': -32.54623447186819, 'agent-1': -32.54623447186819} \n",
      "\n",
      "obs:  {'agent-0': array([0.5507696, 0.8346573, 3.5129247], dtype=float32), 'agent-1': array([-0.5507696, -0.8346573,  3.5129247], dtype=float32)}\n",
      "action:  {'agent-0': array([-0.9576957 ,  0.67898655,  0.13830575], dtype=float32), 'agent-1': array([-0.29436505, -0.7343844 ,  0.        ], dtype=float32)}\n",
      "reward:  {'agent-0': -32.55206764324375, 'agent-1': -32.55206764324375} \n",
      "\n",
      "obs:  {'agent-0': array([0.54660195, 0.83739257, 3.5130985 ], dtype=float32), 'agent-1': array([-0.54660195, -0.83739257,  3.5130985 ], dtype=float32)}\n",
      "action:  {'agent-0': array([1.        , 0.46668673, 0.00997251], dtype=float32), 'agent-1': array([ 1.       , -0.5495069,  0.9645539], dtype=float32)}\n",
      "reward:  {'agent-0': -32.496371233167245, 'agent-1': -32.496371233167245} \n",
      "\n",
      "obs:  {'agent-0': array([0.51816374, 0.8552814 , 3.5114372 ], dtype=float32), 'agent-1': array([-0.51816374, -0.8552814 ,  3.5114372 ], dtype=float32)}\n",
      "action:  {'agent-0': array([-1.        ,  0.19645655,  0.23938257], dtype=float32), 'agent-1': array([3.9100647e-04, 1.0000000e+00, 0.0000000e+00], dtype=float32)}\n",
      "reward:  {'agent-0': -32.413363985591154, 'agent-1': -32.413363985591154} \n",
      "\n",
      "obs:  {'agent-0': array([0.5121054, 0.8589226, 3.508956 ], dtype=float32), 'agent-1': array([-0.5121054, -0.8589226,  3.508956 ], dtype=float32)}\n",
      "action:  {'agent-0': array([ 0.9633157 , -0.48257607,  0.6785084 ], dtype=float32), 'agent-1': array([ 1.        , -0.57292056,  0.90253925], dtype=float32)}\n",
      "reward:  {'agent-0': -32.45027543996187, 'agent-1': -32.45027543996187} \n",
      "\n",
      "obs:  {'agent-0': array([0.50385207, 0.86379   , 3.51006   ], dtype=float32), 'agent-1': array([-0.50385207, -0.86379   ,  3.51006   ], dtype=float32)}\n",
      "action:  {'agent-0': array([ 0.960284  , -0.6916083 ,  0.84564865], dtype=float32), 'agent-1': array([1.        , 0.5337186 , 0.24920207], dtype=float32)}\n",
      "reward:  {'agent-0': -32.124983834059215, 'agent-1': -32.124983834059215} \n",
      "\n",
      "obs:  {'agent-0': array([0.52647495, 0.85019064, 3.5002878 ], dtype=float32), 'agent-1': array([-0.52647495, -0.85019064,  3.5002878 ], dtype=float32)}\n",
      "action:  {'agent-0': array([0.7653123, 1.       , 0.1224629], dtype=float32), 'agent-1': array([0.6311635 , 1.        , 0.11131048], dtype=float32)}\n",
      "reward:  {'agent-0': -32.14682351834377, 'agent-1': -32.14682351834377} \n",
      "\n",
      "obs:  {'agent-0': array([0.52684724, 0.84995997, 3.500947  ], dtype=float32), 'agent-1': array([-0.52684724, -0.84995997,  3.500947  ], dtype=float32)}\n",
      "action:  {'agent-0': array([-0.6159371 , -1.        ,  0.01002729], dtype=float32), 'agent-1': array([ 1.        , -0.41092116,  0.6964787 ], dtype=float32)}\n",
      "reward:  {'agent-0': -32.01998086364381, 'agent-1': -32.01998086364381} \n",
      "\n",
      "obs:  {'agent-0': array([0.50699   , 0.86195195, 3.4971128 ], dtype=float32), 'agent-1': array([-0.50699   , -0.86195195,  3.4971128 ], dtype=float32)}\n",
      "action:  {'agent-0': array([-0.4499085 ,  1.        ,  0.83444846], dtype=float32), 'agent-1': array([ 1.       , -1.       ,  0.9426596], dtype=float32)}\n",
      "reward:  {'agent-0': -32.946544444843404, 'agent-1': -32.946544444843404} \n",
      "\n",
      "obs:  {'agent-0': array([0.45272505, 0.8916502 , 3.524787  ], dtype=float32), 'agent-1': array([-0.45272505, -0.8916502 ,  3.524787  ], dtype=float32)}\n",
      "action:  {'agent-0': array([ 1.        , -1.        ,  0.28519598], dtype=float32), 'agent-1': array([-0.92636144, -1.        ,  0.83576137], dtype=float32)}\n",
      "reward:  {'agent-0': -33.924206542016805, 'agent-1': -33.924206542016805} \n",
      "\n",
      "obs:  {'agent-0': array([0.4709068 , 0.88218296, 3.5531802 ], dtype=float32), 'agent-1': array([-0.4709068 , -0.88218296,  3.5531802 ], dtype=float32)}\n",
      "action:  {'agent-0': array([ 0.3018855, -0.6946337,  0.6209398], dtype=float32), 'agent-1': array([-0.48147738,  0.4280318 ,  1.        ], dtype=float32)}\n",
      "reward:  {'agent-0': -33.49587519646381, 'agent-1': -33.49587519646381} \n",
      "\n",
      "obs:  {'agent-0': array([0.49689907, 0.86780834, 3.5408397 ], dtype=float32), 'agent-1': array([-0.49689907, -0.86780834,  3.5408397 ], dtype=float32)}\n",
      "action:  {'agent-0': array([ 1.        , -0.38839483,  0.        ], dtype=float32), 'agent-1': array([0.704291 , 0.4624884, 0.777037 ], dtype=float32)}\n",
      "reward:  {'agent-0': -32.91341145487464, 'agent-1': -32.91341145487464} \n",
      "\n",
      "obs:  {'agent-0': array([0.48906535, 0.87224716, 3.5238106 ], dtype=float32), 'agent-1': array([-0.48906535, -0.87224716,  3.5238106 ], dtype=float32)}\n",
      "action:  {'agent-0': array([-1.       , -1.       ,  0.4262005], dtype=float32), 'agent-1': array([ 0.23958528, -0.89742535,  0.33170545], dtype=float32)}\n",
      "reward:  {'agent-0': -32.556200813782006, 'agent-1': -32.556200813782006} \n",
      "\n",
      "obs:  {'agent-0': array([0.47889915, 0.8778699 , 3.5132217 ], dtype=float32), 'agent-1': array([-0.47889915, -0.8778699 ,  3.5132217 ], dtype=float32)}\n",
      "action:  {'agent-0': array([ 0.6855943 , -0.92467326,  0.5881207 ], dtype=float32), 'agent-1': array([ 0.01775217, -0.60012484,  0.08762699], dtype=float32)}\n",
      "reward:  {'agent-0': -32.32266181865603, 'agent-1': -32.32266181865603} \n",
      "\n",
      "obs:  {'agent-0': array([0.4947858, 0.869015 , 3.5062377], dtype=float32), 'agent-1': array([-0.4947858, -0.869015 ,  3.5062377], dtype=float32)}\n",
      "action:  {'agent-0': array([0.03482699, 0.86509085, 0.16586918], dtype=float32), 'agent-1': array([-0.36537284,  0.57409716,  0.5364264 ], dtype=float32)}\n",
      "reward:  {'agent-0': -32.2805907442416, 'agent-1': -32.2805907442416} \n",
      "\n",
      "obs:  {'agent-0': array([0.5016812, 0.8650526, 3.5049744], dtype=float32), 'agent-1': array([-0.5016812, -0.8650526,  3.5049744], dtype=float32)}\n",
      "action:  {'agent-0': array([-0.78212136, -0.18677604,  0.        ], dtype=float32), 'agent-1': array([-1.       ,  0.7341517,  0.3521282], dtype=float32)}\n",
      "reward:  {'agent-0': -32.23654298536785, 'agent-1': -32.23654298536785} \n",
      "\n",
      "obs:  {'agent-0': array([0.51329  , 0.8582153, 3.50365  ], dtype=float32), 'agent-1': array([-0.51329  , -0.8582153,  3.50365  ], dtype=float32)}\n",
      "action:  {'agent-0': array([-0.9014738, -0.5400168,  1.       ], dtype=float32), 'agent-1': array([0.60371685, 0.3765756 , 0.511273  ], dtype=float32)}\n",
      "reward:  {'agent-0': -30.993788382137335, 'agent-1': -30.993788382137335} \n",
      "\n",
      "obs:  {'agent-0': array([0.49482676, 0.8689917 , 3.4655418 ], dtype=float32), 'agent-1': array([-0.49482676, -0.8689917 ,  3.4655418 ], dtype=float32)}\n",
      "action:  {'agent-0': array([ 0.09862101, -1.        ,  0.54709965], dtype=float32), 'agent-1': array([0.63126695, 0.3275565 , 0.5433337 ], dtype=float32)}\n",
      "reward:  {'agent-0': -30.220877063848032, 'agent-1': -30.220877063848032} \n",
      "\n",
      "obs:  {'agent-0': array([0.49791813, 0.86722404, 3.441087  ], dtype=float32), 'agent-1': array([-0.49791813, -0.86722404,  3.441087  ], dtype=float32)}\n",
      "action:  {'agent-0': array([-1.        ,  0.23799229,  0.04726511], dtype=float32), 'agent-1': array([-0.7409744 , -0.49285752,  0.68192065], dtype=float32)}\n",
      "reward:  {'agent-0': -30.75097185573647, 'agent-1': -30.75097185573647} \n",
      "\n",
      "obs:  {'agent-0': array([0.50422937, 0.86356974, 3.4579234 ], dtype=float32), 'agent-1': array([-0.50422937, -0.86356974,  3.4579234 ], dtype=float32)}\n",
      "action:  {'agent-0': array([ 0.02358985, -0.4395579 ,  0.52702796], dtype=float32), 'agent-1': array([-1.        , -0.47719657,  0.2870468 ], dtype=float32)}\n",
      "reward:  {'agent-0': -30.821736611692206, 'agent-1': -30.821736611692206} \n",
      "\n",
      "obs:  {'agent-0': array([0.5127882 , 0.85851514, 3.4601495 ], dtype=float32), 'agent-1': array([-0.5127882 , -0.85851514,  3.4601495 ], dtype=float32)}\n",
      "action:  {'agent-0': array([0.64126706, 0.9327533 , 0.5462312 ], dtype=float32), 'agent-1': array([-1.        ,  0.15686071,  1.        ], dtype=float32)}\n",
      "reward:  {'agent-0': -31.83192963328237, 'agent-1': -31.83192963328237} \n",
      "\n",
      "obs:  {'agent-0': array([0.5389338, 0.8423481, 3.4914014], dtype=float32), 'agent-1': array([-0.5389338, -0.8423481,  3.4914014], dtype=float32)}\n",
      "action:  {'agent-0': array([-0.80954206, -0.6339848 ,  0.6028671 ], dtype=float32), 'agent-1': array([-0.79877913, -0.48778772,  1.        ], dtype=float32)}\n",
      "reward:  {'agent-0': -32.088982276609, 'agent-1': -32.088982276609} \n",
      "\n",
      "obs:  {'agent-0': array([0.5443001, 0.8388906, 3.4992003], dtype=float32), 'agent-1': array([-0.5443001, -0.8388906,  3.4992003], dtype=float32)}\n",
      "action:  {'agent-0': array([0.32586265, 0.55375886, 0.        ], dtype=float32), 'agent-1': array([-1.        , -0.06348181,  0.33307672], dtype=float32)}\n",
      "reward:  {'agent-0': -32.289125164890265, 'agent-1': -32.289125164890265} \n",
      "\n",
      "obs:  {'agent-0': array([0.5512417, 0.8343456, 3.5052307], dtype=float32), 'agent-1': array([-0.5512417, -0.8343456,  3.5052307], dtype=float32)}\n",
      "action:  {'agent-0': array([-0.6490614, -0.0619396,  0.5385106], dtype=float32), 'agent-1': array([0.3147719, 0.58736  , 0.       ], dtype=float32)}\n",
      "reward:  {'agent-0': -32.06978594281989, 'agent-1': -32.06978594281989} \n",
      "\n",
      "obs:  {'agent-0': array([0.544113  , 0.83901197, 3.49862   ], dtype=float32), 'agent-1': array([-0.544113  , -0.83901197,  3.49862   ], dtype=float32)}\n",
      "action:  {'agent-0': array([-0.8964637, -1.       ,  0.4587338], dtype=float32), 'agent-1': array([-1.       , -0.6815971,  0.       ], dtype=float32)}\n",
      "reward:  {'agent-0': -31.461287500176415, 'agent-1': -31.461287500176415} \n",
      "\n",
      "obs:  {'agent-0': array([0.5415655 , 0.84065855, 3.4800482 ], dtype=float32), 'agent-1': array([-0.5415655 , -0.84065855,  3.4800482 ], dtype=float32)}\n",
      "action:  {'agent-0': array([-1.        , -0.32371438,  0.11281919], dtype=float32), 'agent-1': array([ 1.        , -0.84218574,  0.71059644], dtype=float32)}\n",
      "reward:  {'agent-0': -31.503513174775208, 'agent-1': -31.503513174775208} \n",
      "\n",
      "obs:  {'agent-0': array([0.5147023 , 0.85736895, 3.4813483 ], dtype=float32), 'agent-1': array([-0.5147023 , -0.85736895,  3.4813483 ], dtype=float32)}\n",
      "action:  {'agent-0': array([ 0.21721089, -1.        ,  0.        ], dtype=float32), 'agent-1': array([ 1.        , -0.16747224,  0.        ], dtype=float32)}\n",
      "reward:  {'agent-0': -31.503513174775208, 'agent-1': -31.503513174775208} \n",
      "\n",
      "obs:  {'agent-0': array([0.5147023 , 0.85736895, 3.4813483 ], dtype=float32), 'agent-1': array([-0.5147023 , -0.85736895,  3.4813483 ], dtype=float32)}\n",
      "action:  {'agent-0': array([-0.8705626,  0.5085311,  0.6318853], dtype=float32), 'agent-1': array([ 0.7418835 , -1.        ,  0.94110763], dtype=float32)}\n",
      "reward:  {'agent-0': -31.98966881970516, 'agent-1': -31.98966881970516} \n",
      "\n",
      "obs:  {'agent-0': array([0.46785867, 0.8838033 , 3.4961944 ], dtype=float32), 'agent-1': array([-0.46785867, -0.8838033 ,  3.4961944 ], dtype=float32)}\n",
      "action:  {'agent-0': array([-0.91029835,  0.99811447,  0.40239555], dtype=float32), 'agent-1': array([ 0.32801104, -0.07833201,  0.3180781 ], dtype=float32)}\n",
      "reward:  {'agent-0': -32.15235966650379, 'agent-1': -32.15235966650379} \n",
      "\n",
      "obs:  {'agent-0': array([0.45085374, 0.89259785, 3.501114  ], dtype=float32), 'agent-1': array([-0.45085374, -0.89259785,  3.501114  ], dtype=float32)}\n",
      "action:  {'agent-0': array([-1.       , -0.9679808,  0.8226723], dtype=float32), 'agent-1': array([0.91907847, 1.        , 0.91594815], dtype=float32)}\n",
      "reward:  {'agent-0': -29.882061907271215, 'agent-1': -29.882061907271215} \n",
      "\n",
      "obs:  {'agent-0': array([0.42940512, 0.903112  , 3.4301755 ], dtype=float32), 'agent-1': array([-0.42940512, -0.903112  ,  3.4301755 ], dtype=float32)}\n",
      "action:  {'agent-0': array([-1.        ,  0.71886456,  0.79695153], dtype=float32), 'agent-1': array([0.26774943, 0.61681366, 0.        ], dtype=float32)}\n",
      "reward:  {'agent-0': -30.072750593004375, 'agent-1': -30.072750593004375} \n",
      "\n",
      "obs:  {'agent-0': array([0.4001815, 0.9164359, 3.4363313], dtype=float32), 'agent-1': array([-0.4001815, -0.9164359,  3.4363313], dtype=float32)}\n",
      "action:  {'agent-0': array([-0.8084674 , -0.9377661 ,  0.47952074], dtype=float32), 'agent-1': array([-0.89224917,  0.7864089 ,  0.        ], dtype=float32)}\n",
      "reward:  {'agent-0': -29.50602906715549, 'agent-1': -29.50602906715549} \n",
      "\n",
      "obs:  {'agent-0': array([0.3947289, 0.9187976, 3.4179244], dtype=float32), 'agent-1': array([-0.3947289, -0.9187976,  3.4179244], dtype=float32)}\n",
      "action:  {'agent-0': array([ 1.        , -0.43305004,  0.6882669 ], dtype=float32), 'agent-1': array([ 0.829067 , -0.5052748,  1.       ], dtype=float32)}\n",
      "reward:  {'agent-0': -29.641597312550786, 'agent-1': -29.641597312550786} \n",
      "\n",
      "obs:  {'agent-0': array([0.3881735 , 0.92158633, 3.4223585 ], dtype=float32), 'agent-1': array([-0.3881735 , -0.92158633,  3.4223585 ], dtype=float32)}\n",
      "action:  {'agent-0': array([0.43884444, 1.        , 0.54435086], dtype=float32), 'agent-1': array([-0.6207919 ,  1.        ,  0.24018896], dtype=float32)}\n",
      "reward:  {'agent-0': -30.07347093929389, 'agent-1': -30.07347093929389} \n",
      "\n",
      "obs:  {'agent-0': array([0.39550057, 0.91846573, 3.4363544 ], dtype=float32), 'agent-1': array([-0.39550057, -0.91846573,  3.4363544 ], dtype=float32)}\n",
      "action:  {'agent-0': array([ 1.        , -1.        ,  0.40674165], dtype=float32), 'agent-1': array([-0.65617955, -0.6212121 ,  1.        ], dtype=float32)}\n",
      "reward:  {'agent-0': -30.703784028653892, 'agent-1': -30.703784028653892} \n",
      "\n",
      "obs:  {'agent-0': array([0.42199996, 0.9065958 , 3.4564362 ], dtype=float32), 'agent-1': array([-0.42199996, -0.9065958 ,  3.4564362 ], dtype=float32)}\n",
      "action:  {'agent-0': array([-0.7280571 , -0.30929112,  0.23392335], dtype=float32), 'agent-1': array([-0.9683605 ,  1.        ,  0.37567404], dtype=float32)}\n",
      "reward:  {'agent-0': -30.381441065861782, 'agent-1': -30.381441065861782} \n",
      "\n",
      "obs:  {'agent-0': array([0.43284562, 0.90146804, 3.4462166 ], dtype=float32), 'agent-1': array([-0.43284562, -0.90146804,  3.4462166 ], dtype=float32)}\n",
      "action:  {'agent-0': array([ 1.        , -0.2837584 ,  0.29781514], dtype=float32), 'agent-1': array([ 0.2985965, -0.671216 ,  0.       ], dtype=float32)}\n",
      "reward:  {'agent-0': -30.435696952385197, 'agent-1': -30.435696952385197} \n",
      "\n",
      "obs:  {'agent-0': array([0.4418591, 0.8970845, 3.4479442], dtype=float32), 'agent-1': array([-0.4418591, -0.8970845,  3.4479442], dtype=float32)}\n",
      "action:  {'agent-0': array([-0.48300362, -0.86472976,  0.39890102], dtype=float32), 'agent-1': array([-0.618682, -1.      ,  0.      ], dtype=float32)}\n",
      "reward:  {'agent-0': -30.041128891793022, 'agent-1': -30.041128891793022} \n",
      "\n",
      "obs:  {'agent-0': array([0.441249 , 0.8973847, 3.435313 ], dtype=float32), 'agent-1': array([-0.441249 , -0.8973847,  3.435313 ], dtype=float32)}\n",
      "action:  {'agent-0': array([-1.        , -1.        ,  0.11172092], dtype=float32), 'agent-1': array([-0.8852054 , -0.6548903 ,  0.05253583], dtype=float32)}\n",
      "reward:  {'agent-0': -29.94298044215534, 'agent-1': -29.94298044215534} \n",
      "\n",
      "obs:  {'agent-0': array([0.44051737, 0.8977441 , 3.432146  ], dtype=float32), 'agent-1': array([-0.44051737, -0.8977441 ,  3.432146  ], dtype=float32)}\n",
      "action:  {'agent-0': array([-0.9378501 , -0.18686688,  0.        ], dtype=float32), 'agent-1': array([-0.31062484, -1.        ,  0.6253285 ], dtype=float32)}\n",
      "reward:  {'agent-0': -30.59009963842141, 'agent-1': -30.59009963842141} \n",
      "\n",
      "obs:  {'agent-0': array([0.43754828, 0.8991949 , 3.4528437 ], dtype=float32), 'agent-1': array([-0.43754828, -0.8991949 ,  3.4528437 ], dtype=float32)}\n",
      "action:  {'agent-0': array([ 1.       , -1.       ,  0.7553238], dtype=float32), 'agent-1': array([1.       , 0.5651556, 0.3457505], dtype=float32)}\n",
      "reward:  {'agent-0': -29.924698158858767, 'agent-1': -29.924698158858767} \n",
      "\n",
      "obs:  {'agent-0': array([0.46096435, 0.8874187 , 3.4315553 ], dtype=float32), 'agent-1': array([-0.46096435, -0.8874187 ,  3.4315553 ], dtype=float32)}\n",
      "action:  {'agent-0': array([-0.4086784, -0.5633099,  0.3481782], dtype=float32), 'agent-1': array([0.41209137, 1.        , 0.        ], dtype=float32)}\n",
      "reward:  {'agent-0': -29.68507645773947, 'agent-1': -29.68507645773947} \n",
      "\n",
      "obs:  {'agent-0': array([0.4598919, 0.8879749, 3.4237764], dtype=float32), 'agent-1': array([-0.4598919, -0.8879749,  3.4237764], dtype=float32)}\n",
      "action:  {'agent-0': array([-1.        , -0.17016804,  0.14072856], dtype=float32), 'agent-1': array([-0.77586514,  1.        ,  0.00995868], dtype=float32)}\n",
      "reward:  {'agent-0': -29.593979626608284, 'agent-1': -29.593979626608284} \n",
      "\n",
      "obs:  {'agent-0': array([0.4568133, 0.8895626, 3.4208033], dtype=float32), 'agent-1': array([-0.4568133, -0.8895626,  3.4208033], dtype=float32)}\n",
      "action:  {'agent-0': array([-0.2519877 , -1.        ,  0.59135765], dtype=float32), 'agent-1': array([0.69817114, 0.24308693, 0.        ], dtype=float32)}\n",
      "reward:  {'agent-0': -29.00018437699025, 'agent-1': -29.00018437699025} \n",
      "\n",
      "obs:  {'agent-0': array([0.4610284 , 0.88738537, 3.4012036 ], dtype=float32), 'agent-1': array([-0.4610284 , -0.88738537,  3.4012036 ], dtype=float32)}\n",
      "action:  {'agent-0': array([-0.8975001, -0.8413329,  0.       ], dtype=float32), 'agent-1': array([0.26029098, 0.44381607, 0.48511538], dtype=float32)}\n",
      "reward:  {'agent-0': -28.750916785881863, 'agent-1': -28.750916785881863} \n",
      "\n",
      "obs:  {'agent-0': array([0.46063358, 0.8875904 , 3.39286   ], dtype=float32), 'agent-1': array([-0.46063358, -0.8875904 ,  3.39286   ], dtype=float32)}\n",
      "action:  {'agent-0': array([ 0.16348767, -0.56602216,  0.8802193 ], dtype=float32), 'agent-1': array([-1.        ,  0.42323673,  0.04613054], dtype=float32)}\n",
      "reward:  {'agent-0': -28.381826355321568, 'agent-1': -28.381826355321568} \n",
      "\n",
      "obs:  {'agent-0': array([0.47331956, 0.8808908 , 3.3803763 ], dtype=float32), 'agent-1': array([-0.47331956, -0.8808908 ,  3.3803763 ], dtype=float32)}\n",
      "action:  {'agent-0': array([-1.        , -0.94146496,  0.        ], dtype=float32), 'agent-1': array([ 0.94934237, -0.6232894 ,  0.70180917], dtype=float32)}\n",
      "reward:  {'agent-0': -28.462877364093604, 'agent-1': -28.462877364093604} \n",
      "\n",
      "obs:  {'agent-0': array([0.44856378, 0.89375085, 3.383131  ], dtype=float32), 'agent-1': array([-0.44856378, -0.89375085,  3.383131  ], dtype=float32)}\n",
      "action:  {'agent-0': array([ 1., -1.,  1.], dtype=float32), 'agent-1': array([ 1.        , -0.80976003,  0.66463387], dtype=float32)}\n",
      "reward:  {'agent-0': -28.205125973720254, 'agent-1': -28.205125973720254} \n",
      "\n",
      "obs:  {'agent-0': array([0.46455324, 0.8855452 , 3.3743443 ], dtype=float32), 'agent-1': array([-0.46455324, -0.8855452 ,  3.3743443 ], dtype=float32)}\n",
      "action:  {'agent-0': array([-0.08811921, -0.46474105,  0.03220704], dtype=float32), 'agent-1': array([0.5944891 , 0.05860293, 0.        ], dtype=float32)}\n",
      "reward:  {'agent-0': -28.190553111324913, 'agent-1': -28.190553111324913} \n",
      "\n",
      "obs:  {'agent-0': array([0.4646927, 0.885472 , 3.373845 ], dtype=float32), 'agent-1': array([-0.4646927, -0.885472 ,  3.373845 ], dtype=float32)}\n",
      "action:  {'agent-0': array([ 1.        , -0.90827847,  0.        ], dtype=float32), 'agent-1': array([0.95541906, 0.42174137, 0.        ], dtype=float32)}\n",
      "reward:  {'agent-0': -28.190553111324913, 'agent-1': -28.190553111324913} \n",
      "\n",
      "obs:  {'agent-0': array([0.4646927, 0.885472 , 3.373845 ], dtype=float32), 'agent-1': array([-0.4646927, -0.885472 ,  3.373845 ], dtype=float32)}\n",
      "action:  {'agent-0': array([ 1.        , -0.3474741 ,  0.39050752], dtype=float32), 'agent-1': array([ 0.99952006, -1.        ,  0.6278579 ], dtype=float32)}\n",
      "reward:  {'agent-0': -28.519570854027236, 'agent-1': -28.519570854027236} \n",
      "\n",
      "obs:  {'agent-0': array([0.45101994, 0.8925139 , 3.3850534 ], dtype=float32), 'agent-1': array([-0.45101994, -0.8925139 ,  3.3850534 ], dtype=float32)}\n",
      "action:  {'agent-0': array([-0.00813913,  0.76127577,  0.55618197], dtype=float32), 'agent-1': array([-0.98402476, -0.7438416 ,  0.18671763], dtype=float32)}\n",
      "reward:  {'agent-0': -29.10240507420611, 'agent-1': -29.10240507420611} \n",
      "\n",
      "obs:  {'agent-0': array([0.4481452, 0.8939608, 3.4046052], dtype=float32), 'agent-1': array([-0.4481452, -0.8939608,  3.4046052], dtype=float32)}\n",
      "action:  {'agent-0': array([ 0.27076972, -0.32158315,  0.29095116], dtype=float32), 'agent-1': array([-1.        , -0.12666082,  0.06604302], dtype=float32)}\n",
      "reward:  {'agent-0': -29.09162482756571, 'agent-1': -29.09162482756571} \n",
      "\n",
      "obs:  {'agent-0': array([0.45328945, 0.8913634 , 3.4042468 ], dtype=float32), 'agent-1': array([-0.45328945, -0.8913634 ,  3.4042468 ], dtype=float32)}\n",
      "action:  {'agent-0': array([ 0.88922834, -1.        ,  0.93102324], dtype=float32), 'agent-1': array([-1.       , -1.       ,  0.5311924], dtype=float32)}\n",
      "reward:  {'agent-0': -29.384310752571952, 'agent-1': -29.384310752571952} \n",
      "\n",
      "obs:  {'agent-0': array([0.49502647, 0.8688779 , 3.4139264 ], dtype=float32), 'agent-1': array([-0.49502647, -0.8688779 ,  3.4139264 ], dtype=float32)}\n",
      "action:  {'agent-0': array([-0.4486432 ,  0.01364231,  1.        ], dtype=float32), 'agent-1': array([0.19613075, 1.        , 0.16720966], dtype=float32)}\n",
      "reward:  {'agent-0': -29.014573959668233, 'agent-1': -29.014573959668233} \n",
      "\n",
      "obs:  {'agent-0': array([0.4847417, 0.8746574, 3.401683 ], dtype=float32), 'agent-1': array([-0.4847417, -0.8746574,  3.401683 ], dtype=float32)}\n",
      "action:  {'agent-0': array([-0.3893813 , -0.9442029 ,  0.35970777], dtype=float32), 'agent-1': array([-1.        , -0.0224039 ,  0.18849123], dtype=float32)}\n",
      "reward:  {'agent-0': -28.745407039248935, 'agent-1': -28.745407039248935} \n",
      "\n",
      "obs:  {'agent-0': array([0.49096543, 0.87117904, 3.3926747 ], dtype=float32), 'agent-1': array([-0.49096543, -0.87117904,  3.3926747 ], dtype=float32)}\n",
      "action:  {'agent-0': array([-0.49723363,  0.3394251 ,  0.46216044], dtype=float32), 'agent-1': array([-0.993315  , -0.04986781,  0.65656245], dtype=float32)}\n",
      "reward:  {'agent-0': -29.11925864399661, 'agent-1': -29.11925864399661} \n",
      "\n",
      "obs:  {'agent-0': array([0.499167 , 0.8665058, 3.4051647], dtype=float32), 'agent-1': array([-0.499167 , -0.8665058,  3.4051647], dtype=float32)}\n",
      "action:  {'agent-0': array([ 1., -1.,  1.], dtype=float32), 'agent-1': array([-0.18817675,  0.5213896 ,  0.77430564], dtype=float32)}\n",
      "reward:  {'agent-0': -28.525142086400155, 'agent-1': -28.525142086400155} \n",
      "\n",
      "obs:  {'agent-0': array([0.54972833, 0.8353435 , 3.3852422 ], dtype=float32), 'agent-1': array([-0.54972833, -0.8353435 ,  3.3852422 ], dtype=float32)}\n",
      "action:  {'agent-0': array([1.        , 0.65232015, 0.8591206 ], dtype=float32), 'agent-1': array([-0.22489268,  0.06375515,  0.00771871], dtype=float32)}\n",
      "reward:  {'agent-0': -29.468983251654194, 'agent-1': -29.468983251654194} \n",
      "\n",
      "obs:  {'agent-0': array([0.5613338, 0.8275895, 3.4167092], dtype=float32), 'agent-1': array([-0.5613338, -0.8275895,  3.4167092], dtype=float32)}\n",
      "action:  {'agent-0': array([-0.84621185,  0.8956723 ,  0.        ], dtype=float32), 'agent-1': array([-0.9303756 ,  0.6295973 ,  0.00245085], dtype=float32)}\n",
      "reward:  {'agent-0': -29.468986328905533, 'agent-1': -29.468986328905533} \n",
      "\n",
      "obs:  {'agent-0': array([0.5614111 , 0.82753706, 3.4167094 ], dtype=float32), 'agent-1': array([-0.5614111 , -0.82753706,  3.4167094 ], dtype=float32)}\n",
      "action:  {'agent-0': array([-0.31150973, -0.4032979 ,  0.58101153], dtype=float32), 'agent-1': array([-1.       , -1.       ,  0.5661643], dtype=float32)}\n",
      "reward:  {'agent-0': -29.960132656330057, 'agent-1': -29.960132656330057} \n",
      "\n",
      "obs:  {'agent-0': array([0.5650639 , 0.82504714, 3.4327004 ], dtype=float32), 'agent-1': array([-0.5650639 , -0.82504714,  3.4327004 ], dtype=float32)}\n",
      "action:  {'agent-0': array([-1.        ,  0.34700227,  0.0123989 ], dtype=float32), 'agent-1': array([-0.6365451 ,  0.39193654,  0.79423183], dtype=float32)}\n",
      "reward:  {'agent-0': -29.991140375445188, 'agent-1': -29.991140375445188} \n",
      "\n",
      "obs:  {'agent-0': array([0.5809234, 0.8139582, 3.4337013], dtype=float32), 'agent-1': array([-0.5809234, -0.8139582,  3.4337013], dtype=float32)}\n",
      "action:  {'agent-0': array([0.3383317 , 0.20209217, 0.8118639 ], dtype=float32), 'agent-1': array([-1.        ,  1.        ,  0.26690584], dtype=float32)}\n",
      "reward:  {'agent-0': -30.226202000610087, 'agent-1': -30.226202000610087} \n",
      "\n",
      "obs:  {'agent-0': array([0.59432346, 0.8042261 , 3.4412575 ], dtype=float32), 'agent-1': array([-0.59432346, -0.8042261 ,  3.4412575 ], dtype=float32)}\n",
      "action:  {'agent-0': array([0.67266726, 0.5990077 , 0.2779361 ], dtype=float32), 'agent-1': array([ 1.        , -0.4731871 ,  0.43549466], dtype=float32)}\n",
      "reward:  {'agent-0': -30.381031918884332, 'agent-1': -30.381031918884332} \n",
      "\n",
      "obs:  {'agent-0': array([0.58311397, 0.8123904 , 3.4462037 ], dtype=float32), 'agent-1': array([-0.58311397, -0.8123904 ,  3.4462037 ], dtype=float32)}\n",
      "action:  {'agent-0': array([1.       , 0.848511 , 0.8928654], dtype=float32), 'agent-1': array([0.55975485, 1.        , 0.43023074], dtype=float32)}\n",
      "reward:  {'agent-0': -31.029052991670245, 'agent-1': -31.029052991670245} \n",
      "\n",
      "obs:  {'agent-0': array([0.59194994, 0.8059747 , 3.4666433 ], dtype=float32), 'agent-1': array([-0.59194994, -0.8059747 ,  3.4666433 ], dtype=float32)}\n",
      "action:  {'agent-0': array([0.6549442 , 0.13881242, 0.35544834], dtype=float32), 'agent-1': array([-0.25282872,  0.83576655,  0.7783073 ], dtype=float32)}\n",
      "reward:  {'agent-0': -30.80683650964545, 'agent-1': -30.80683650964545} \n",
      "\n",
      "obs:  {'agent-0': array([0.610164  , 0.79227513, 3.4596813 ], dtype=float32), 'agent-1': array([-0.610164  , -0.79227513,  3.4596813 ], dtype=float32)}\n",
      "action:  {'agent-0': array([1.        , 0.1332444 , 0.45217872], dtype=float32), 'agent-1': array([-1.        , -0.53198874,  0.34213787], dtype=float32)}\n",
      "reward:  {'agent-0': -31.48712117125749, 'agent-1': -31.48712117125749} \n",
      "\n",
      "obs:  {'agent-0': array([0.622208  , 0.78285193, 3.4808438 ], dtype=float32), 'agent-1': array([-0.622208  , -0.78285193,  3.4808438 ], dtype=float32)}\n",
      "action:  {'agent-0': array([-0.86691165,  0.46094728,  0.26961464], dtype=float32), 'agent-1': array([-1.        , -0.15794158,  0.        ], dtype=float32)}\n",
      "reward:  {'agent-0': -31.440060180662392, 'agent-1': -31.440060180662392} \n",
      "\n",
      "obs:  {'agent-0': array([0.61570513, 0.7879766 , 3.4793942 ], dtype=float32), 'agent-1': array([-0.61570513, -0.7879766 ,  3.4793942 ], dtype=float32)}\n",
      "action:  {'agent-0': array([ 1.       , -0.9870217,  0.677842 ], dtype=float32), 'agent-1': array([-1.       ,  1.       ,  0.6435038], dtype=float32)}\n",
      "reward:  {'agent-0': -31.27408825858154, 'agent-1': -31.27408825858154} \n",
      "\n",
      "obs:  {'agent-0': array([0.66122323, 0.75018924, 3.4742646 ], dtype=float32), 'agent-1': array([-0.66122323, -0.75018924,  3.4742646 ], dtype=float32)}\n",
      "action:  {'agent-0': array([ 0.23825562, -0.7319153 ,  0.        ], dtype=float32), 'agent-1': array([ 1.        , -0.06800902,  0.5361309 ], dtype=float32)}\n",
      "reward:  {'agent-0': -30.949875428075178, 'agent-1': -30.949875428075178} \n",
      "\n",
      "obs:  {'agent-0': array([0.6508272 , 0.75922585, 3.4641683 ], dtype=float32), 'agent-1': array([-0.6508272 , -0.75922585,  3.4641683 ], dtype=float32)}\n",
      "action:  {'agent-0': array([ 0.32041693, -1.        ,  0.        ], dtype=float32), 'agent-1': array([0.5912521, 0.4566295, 0.2560942], dtype=float32)}\n",
      "reward:  {'agent-0': -30.762570161792336, 'agent-1': -30.762570161792336} \n",
      "\n",
      "obs:  {'agent-0': array([0.6498679, 0.7600472, 3.4582887], dtype=float32), 'agent-1': array([-0.6498679, -0.7600472,  3.4582887], dtype=float32)}\n",
      "action:  {'agent-0': array([ 1.        , -1.        ,  0.92211026], dtype=float32), 'agent-1': array([0.3956808 , 1.        , 0.02727479], dtype=float32)}\n",
      "reward:  {'agent-0': -30.661210486490706, 'agent-1': -30.661210486490706} \n",
      "\n",
      "obs:  {'agent-0': array([0.6817384 , 0.73159605, 3.4550922 ], dtype=float32), 'agent-1': array([-0.6817384 , -0.73159605,  3.4550922 ], dtype=float32)}\n",
      "action:  {'agent-0': array([-0.6838565, -1.       ,  0.6341084], dtype=float32), 'agent-1': array([ 0.11059189, -0.79686457,  0.        ], dtype=float32)}\n",
      "reward:  {'agent-0': -29.90189213305406, 'agent-1': -29.90189213305406} \n",
      "\n",
      "obs:  {'agent-0': array([0.68454814, 0.72896767, 3.4308174 ], dtype=float32), 'agent-1': array([-0.68454814, -0.72896767,  3.4308174 ], dtype=float32)}\n",
      "action:  {'agent-0': array([-0.02465951,  0.74821925,  1.        ], dtype=float32), 'agent-1': array([0.21797681, 0.01282489, 1.        ], dtype=float32)}\n",
      "reward:  {'agent-0': -30.279517515752648, 'agent-1': -30.279517515752648} \n",
      "\n",
      "obs:  {'agent-0': array([0.6679977 , 0.74416333, 3.4429636 ], dtype=float32), 'agent-1': array([-0.6679977 , -0.74416333,  3.4429636 ], dtype=float32)}\n",
      "action:  {'agent-0': array([ 0.21932578, -0.886264  ,  0.94230783], dtype=float32), 'agent-1': array([-0.6600265, -0.3591392,  0.5081197], dtype=float32)}\n",
      "reward:  {'agent-0': -30.1676041504321, 'agent-1': -30.1676041504321} \n",
      "\n",
      "obs:  {'agent-0': array([0.6884436, 0.7252899, 3.4393792], dtype=float32), 'agent-1': array([-0.6884436, -0.7252899,  3.4393792], dtype=float32)}\n",
      "action:  {'agent-0': array([-1.        ,  0.25680864,  0.        ], dtype=float32), 'agent-1': array([ 1.        , -0.7291984 ,  0.29914138], dtype=float32)}\n",
      "reward:  {'agent-0': -30.122109615641712, 'agent-1': -30.122109615641712} \n",
      "\n",
      "obs:  {'agent-0': array([0.67955244, 0.73362696, 3.4379184 ], dtype=float32), 'agent-1': array([-0.67955244, -0.73362696,  3.4379184 ], dtype=float32)}\n",
      "action:  {'agent-0': array([0.50245893, 0.10456455, 0.        ], dtype=float32), 'agent-1': array([-0.8391874 , -0.8728021 ,  0.71443063], dtype=float32)}\n",
      "reward:  {'agent-0': -30.986991336740907, 'agent-1': -30.986991336740907} \n",
      "\n",
      "obs:  {'agent-0': array([0.6799335, 0.7332738, 3.4653294], dtype=float32), 'agent-1': array([-0.6799335, -0.7332738,  3.4653294], dtype=float32)}\n",
      "action:  {'agent-0': array([ 1.        , -1.        ,  0.47844866], dtype=float32), 'agent-1': array([0.597283 , 0.3609233, 0.5494746], dtype=float32)}\n",
      "reward:  {'agent-0': -30.598216452187287, 'agent-1': -30.598216452187287} \n",
      "\n",
      "obs:  {'agent-0': array([0.69348323, 0.72047275, 3.4531007 ], dtype=float32), 'agent-1': array([-0.69348323, -0.72047275,  3.4531007 ], dtype=float32)}\n",
      "action:  {'agent-0': array([-0.79649806,  1.        ,  0.750184  ], dtype=float32), 'agent-1': array([-1.        ,  0.41447103,  0.9549286 ], dtype=float32)}\n",
      "reward:  {'agent-0': -31.101406510963944, 'agent-1': -31.101406510963944} \n",
      "\n",
      "obs:  {'agent-0': array([0.6937551, 0.720211 , 3.4688997], dtype=float32), 'agent-1': array([-0.6937551, -0.720211 ,  3.4688997], dtype=float32)}\n",
      "action:  {'agent-0': array([ 1.        , -0.9489652 ,  0.46654508], dtype=float32), 'agent-1': array([-1.        ,  1.        ,  0.38813895], dtype=float32)}\n",
      "reward:  {'agent-0': -31.11878043954905, 'agent-1': -31.11878043954905} \n",
      "\n",
      "obs:  {'agent-0': array([0.72083294, 0.6931088 , 3.469441  ], dtype=float32), 'agent-1': array([-0.72083294, -0.6931088 ,  3.469441  ], dtype=float32)}\n",
      "action:  {'agent-0': array([0.9176669 , 0.00123191, 0.19429791], dtype=float32), 'agent-1': array([-0.1329009 , -0.95605856,  0.40286773], dtype=float32)}\n",
      "reward:  {'agent-0': -31.553244600742026, 'agent-1': -31.553244600742026} \n",
      "\n",
      "obs:  {'agent-0': array([0.7182553, 0.6957797, 3.482877 ], dtype=float32), 'agent-1': array([-0.7182553, -0.6957797,  3.482877 ], dtype=float32)}\n",
      "action:  {'agent-0': array([-1.       ,  0.0395695,  1.       ], dtype=float32), 'agent-1': array([-1.       , -1.       ,  0.5043641], dtype=float32)}\n",
      "reward:  {'agent-0': -31.58427516320734, 'agent-1': -31.58427516320734} \n",
      "\n",
      "obs:  {'agent-0': array([0.70185715, 0.71231776, 3.4838297 ], dtype=float32), 'agent-1': array([-0.70185715, -0.71231776,  3.4838297 ], dtype=float32)}\n",
      "action:  {'agent-0': array([-1.        , -0.5758408 ,  0.31084222], dtype=float32), 'agent-1': array([0.09262633, 1.        , 1.        ], dtype=float32)}\n",
      "reward:  {'agent-0': -30.466065988373742, 'agent-1': -30.466065988373742} \n",
      "\n",
      "obs:  {'agent-0': array([0.7143745, 0.6997636, 3.4489098], dtype=float32), 'agent-1': array([-0.7143745, -0.6997636,  3.4489098], dtype=float32)}\n",
      "action:  {'agent-0': array([-0.87825525, -1.        ,  0.70967954], dtype=float32), 'agent-1': array([ 1.        , -0.02794659,  0.35086137], dtype=float32)}\n",
      "reward:  {'agent-0': -29.280981679097756, 'agent-1': -29.280981679097756} \n",
      "\n",
      "obs:  {'agent-0': array([0.7100185, 0.704183 , 3.4105198], dtype=float32), 'agent-1': array([-0.7100185, -0.704183 ,  3.4105198], dtype=float32)}\n",
      "action:  {'agent-0': array([ 0.1897881 , -0.48322093,  0.07547677], dtype=float32), 'agent-1': array([-1.,  1.,  0.], dtype=float32)}\n",
      "reward:  {'agent-0': -29.265491596239865, 'agent-1': -29.265491596239865} \n",
      "\n",
      "obs:  {'agent-0': array([0.7108838, 0.7033095, 3.4100082], dtype=float32), 'agent-1': array([-0.7108838, -0.7033095,  3.4100082], dtype=float32)}\n",
      "action:  {'agent-0': array([-0.07617265, -1.        ,  0.13153955], dtype=float32), 'agent-1': array([-0.48333657,  0.41938615,  0.6453134 ], dtype=float32)}\n",
      "reward:  {'agent-0': -29.201492847900187, 'agent-1': -29.201492847900187} \n",
      "\n",
      "obs:  {'agent-0': array([0.72277975, 0.6910785 , 3.4078913 ], dtype=float32), 'agent-1': array([-0.72277975, -0.6910785 ,  3.4078913 ], dtype=float32)}\n",
      "action:  {'agent-0': array([0.36225533, 0.49693632, 0.60761285], dtype=float32), 'agent-1': array([ 0.49379897, -0.20426857,  0.70997334], dtype=float32)}\n",
      "reward:  {'agent-0': -29.418983268745194, 'agent-1': -29.418983268745194} \n",
      "\n",
      "obs:  {'agent-0': array([0.7130014, 0.7011627, 3.415067 ], dtype=float32), 'agent-1': array([-0.7130014, -0.7011627,  3.415067 ], dtype=float32)}\n",
      "action:  {'agent-0': array([-0.15327036,  0.17873716,  0.39611384], dtype=float32), 'agent-1': array([0.70714235, 0.54113257, 0.9060447 ], dtype=float32)}\n",
      "reward:  {'agent-0': -28.625391273355216, 'agent-1': -28.625391273355216} \n",
      "\n",
      "obs:  {'agent-0': array([0.7082649, 0.7059468, 3.3886318], dtype=float32), 'agent-1': array([-0.7082649, -0.7059468,  3.3886318], dtype=float32)}\n",
      "action:  {'agent-0': array([0.28184772, 0.10619044, 0.        ], dtype=float32), 'agent-1': array([ 1.        , -0.50425756,  0.82761025], dtype=float32)}\n",
      "reward:  {'agent-0': -28.347492873376115, 'agent-1': -28.347492873376115} \n",
      "\n",
      "obs:  {'agent-0': array([0.68601304, 0.72758925, 3.3792071 ], dtype=float32), 'agent-1': array([-0.68601304, -0.72758925,  3.3792071 ], dtype=float32)}\n",
      "action:  {'agent-0': array([-1., -1.,  0.], dtype=float32), 'agent-1': array([-1.        , -0.05359846,  1.        ], dtype=float32)}\n",
      "reward:  {'agent-0': -29.080710023584206, 'agent-1': -29.080710023584206} \n",
      "\n",
      "obs:  {'agent-0': array([0.7031035, 0.7110875, 3.4038842], dtype=float32), 'agent-1': array([-0.7031035, -0.7110875,  3.4038842], dtype=float32)}\n",
      "action:  {'agent-0': array([1.        , 0.0986656 , 0.79724365], dtype=float32), 'agent-1': array([0.30049384, 0.53289986, 0.09172165], dtype=float32)}\n",
      "reward:  {'agent-0': -29.64772670871588, 'agent-1': -29.64772670871588} \n",
      "\n",
      "obs:  {'agent-0': array([0.7156175, 0.6984924, 3.4225585], dtype=float32), 'agent-1': array([-0.7156175, -0.6984924,  3.4225585], dtype=float32)}\n",
      "action:  {'agent-0': array([ 0.11535704, -0.5417753 ,  0.        ], dtype=float32), 'agent-1': array([-0.2783996 , -0.07281375,  0.8591725 ], dtype=float32)}\n",
      "reward:  {'agent-0': -29.862845439129188, 'agent-1': -29.862845439129188} \n",
      "\n",
      "obs:  {'agent-0': array([0.7184722, 0.6955557, 3.429553 ], dtype=float32), 'agent-1': array([-0.7184722, -0.6955557,  3.429553 ], dtype=float32)}\n",
      "action:  {'agent-0': array([ 0.4835155 , -0.73398465,  0.32422552], dtype=float32), 'agent-1': array([-0.21638334,  1.        ,  0.        ], dtype=float32)}\n",
      "reward:  {'agent-0': -29.811268153748845, 'agent-1': -29.811268153748845} \n",
      "\n",
      "obs:  {'agent-0': array([0.7249739, 0.6887763, 3.4278805], dtype=float32), 'agent-1': array([-0.7249739, -0.6887763,  3.4278805], dtype=float32)}\n",
      "action:  {'agent-0': array([0.0811218, 1.       , 0.6837156], dtype=float32), 'agent-1': array([-1., -1.,  1.], dtype=float32)}\n",
      "reward:  {'agent-0': -31.739995022913156, 'agent-1': -31.739995022913156} \n",
      "\n",
      "obs:  {'agent-0': array([0.7141733 , 0.69996893, 3.4885974 ], dtype=float32), 'agent-1': array([-0.7141733 , -0.69996893,  3.4885974 ], dtype=float32)}\n",
      "action:  {'agent-0': array([-0.8836472 , -0.5488367 ,  0.07451242], dtype=float32), 'agent-1': array([-0.6701695,  1.       ,  0.7806829], dtype=float32)}\n",
      "reward:  {'agent-0': -31.504596227871676, 'agent-1': -31.504596227871676} \n",
      "\n",
      "obs:  {'agent-0': array([0.7340264, 0.679121 , 3.4813814], dtype=float32), 'agent-1': array([-0.7340264, -0.679121 ,  3.4813814], dtype=float32)}\n",
      "action:  {'agent-0': array([1.       , 1.       , 0.9373952], dtype=float32), 'agent-1': array([-0.1574046 , -0.65275854,  0.77405417], dtype=float32)}\n",
      "reward:  {'agent-0': -33.26358075963714, 'agent-1': -33.26358075963714} \n",
      "\n",
      "obs:  {'agent-0': array([0.7270546 , 0.68657964, 3.534083  ], dtype=float32), 'agent-1': array([-0.7270546 , -0.68657964,  3.534083  ], dtype=float32)}\n",
      "action:  {'agent-0': array([ 1.       , -0.7930643,  0.       ], dtype=float32), 'agent-1': array([0.3361001 , 0.02459133, 0.        ], dtype=float32)}\n",
      "reward:  {'agent-0': -33.26358075963714, 'agent-1': -33.26358075963714} \n",
      "\n",
      "obs:  {'agent-0': array([0.7270546 , 0.68657964, 3.534083  ], dtype=float32), 'agent-1': array([-0.7270546 , -0.68657964,  3.534083  ], dtype=float32)}\n",
      "action:  {'agent-0': array([ 0.7213832 , -1.        ,  0.18808356], dtype=float32), 'agent-1': array([-0.80580336,  0.94557476,  0.10417059], dtype=float32)}\n",
      "reward:  {'agent-0': -33.22843526666501, 'agent-1': -33.22843526666501} \n",
      "\n",
      "obs:  {'agent-0': array([0.734433 , 0.6786812, 3.5330567], dtype=float32), 'agent-1': array([-0.734433 , -0.6786812,  3.5330567], dtype=float32)}\n",
      "action:  {'agent-0': array([0.10941994, 0.68267775, 0.18053424], dtype=float32), 'agent-1': array([0.5202918 , 0.04040635, 0.        ], dtype=float32)}\n",
      "reward:  {'agent-0': -33.32667771992175, 'agent-1': -33.32667771992175} \n",
      "\n",
      "obs:  {'agent-0': array([0.73286074, 0.6803787 , 3.5359228 ], dtype=float32), 'agent-1': array([-0.73286074, -0.6803787 ,  3.5359228 ], dtype=float32)}\n",
      "action:  {'agent-0': array([ 1.        , -1.        ,  0.16602165], dtype=float32), 'agent-1': array([-1.        ,  0.25673676,  0.        ], dtype=float32)}\n",
      "reward:  {'agent-0': -33.33621657868571, 'agent-1': -33.33621657868571} \n",
      "\n",
      "obs:  {'agent-0': array([0.73763126, 0.67520374, 3.5362008 ], dtype=float32), 'agent-1': array([-0.73763126, -0.67520374,  3.5362008 ], dtype=float32)}\n",
      "action:  {'agent-0': array([0.3499285 , 0.07595921, 0.06264696], dtype=float32), 'agent-1': array([1.        , 0.61816514, 0.6592931 ], dtype=float32)}\n",
      "reward:  {'agent-0': -32.59437640333726, 'agent-1': -32.59437640333726} \n",
      "\n",
      "obs:  {'agent-0': array([0.73486495, 0.6782135 , 3.5143588 ], dtype=float32), 'agent-1': array([-0.73486495, -0.6782135 ,  3.5143588 ], dtype=float32)}\n",
      "action:  {'agent-0': array([-1.        ,  0.14646363,  0.        ], dtype=float32), 'agent-1': array([-0.19944084,  0.23347294,  0.34374925], dtype=float32)}\n",
      "reward:  {'agent-0': -32.59049688221323, 'agent-1': -32.59049688221323} \n",
      "\n",
      "obs:  {'agent-0': array([0.737056 , 0.6758316, 3.5142431], dtype=float32), 'agent-1': array([-0.737056 , -0.6758316,  3.5142431], dtype=float32)}\n",
      "action:  {'agent-0': array([-0.60089767,  0.37482035,  0.30473542], dtype=float32), 'agent-1': array([-1.,  1.,  0.], dtype=float32)}\n",
      "reward:  {'agent-0': -32.53338975680759, 'agent-1': -32.53338975680759} \n",
      "\n",
      "obs:  {'agent-0': array([0.73272127, 0.6805288 , 3.5125415 ], dtype=float32), 'agent-1': array([-0.73272127, -0.6805288 ,  3.5125415 ], dtype=float32)}\n",
      "action:  {'agent-0': array([ 0.9710593 , -0.01221734,  0.52182126], dtype=float32), 'agent-1': array([1.        , 0.23726392, 0.        ], dtype=float32)}\n",
      "reward:  {'agent-0': -32.90219170064496, 'agent-1': -32.90219170064496} \n",
      "\n",
      "obs:  {'agent-0': array([0.739909 , 0.672707 , 3.5234797], dtype=float32), 'agent-1': array([-0.739909 , -0.672707 ,  3.5234797], dtype=float32)}\n",
      "action:  {'agent-0': array([-0.35735744,  0.90616596,  0.        ], dtype=float32), 'agent-1': array([-0.44532508,  0.28247714,  0.5085491 ], dtype=float32)}\n",
      "reward:  {'agent-0': -32.974136276973404, 'agent-1': -32.974136276973404} \n",
      "\n",
      "obs:  {'agent-0': array([0.7451627, 0.6668827, 3.5255995], dtype=float32), 'agent-1': array([-0.7451627, -0.6668827,  3.5255995], dtype=float32)}\n",
      "action:  {'agent-0': array([ 0.76668   , -0.05379885,  0.7168053 ], dtype=float32), 'agent-1': array([-0.07297856, -0.55773336,  0.48637274], dtype=float32)}\n",
      "reward:  {'agent-0': -33.56598299195398, 'agent-1': -33.56598299195398} \n",
      "\n",
      "obs:  {'agent-0': array([0.7494537, 0.6620567, 3.54287  ], dtype=float32), 'agent-1': array([-0.7494537, -0.6620567,  3.54287  ], dtype=float32)}\n",
      "action:  {'agent-0': array([-1.        , -0.32756096,  0.31193525], dtype=float32), 'agent-1': array([-1.       , -1.       ,  0.7695925], dtype=float32)}\n",
      "reward:  {'agent-0': -34.35140838496935, 'agent-1': -34.35140838496935} \n",
      "\n",
      "obs:  {'agent-0': array([0.7456407, 0.6663482, 3.5653381], dtype=float32), 'agent-1': array([-0.7456407, -0.6663482,  3.5653381], dtype=float32)}\n",
      "action:  {'agent-0': array([0.33466065, 0.4397936 , 0.        ], dtype=float32), 'agent-1': array([ 0.9814818 , -0.82634723,  0.49487677], dtype=float32)}\n",
      "reward:  {'agent-0': -34.26750334062689, 'agent-1': -34.26750334062689} \n",
      "\n",
      "obs:  {'agent-0': array([0.7332923, 0.6799135, 3.562962 ], dtype=float32), 'agent-1': array([-0.7332923, -0.6799135,  3.562962 ], dtype=float32)}\n",
      "action:  {'agent-0': array([0.0790329 , 0.10199571, 0.        ], dtype=float32), 'agent-1': array([0.6037288, 0.7410886, 0.481219 ], dtype=float32)}\n",
      "reward:  {'agent-0': -33.81204878386762, 'agent-1': -33.81204878386762} \n",
      "\n",
      "obs:  {'agent-0': array([0.7345775, 0.6785248, 3.5499635], dtype=float32), 'agent-1': array([-0.7345775, -0.6785248,  3.5499635], dtype=float32)}\n",
      "action:  {'agent-0': array([ 0.5116539 , -1.        ,  0.19942701], dtype=float32), 'agent-1': array([ 0.522486, -1.      ,  0.      ], dtype=float32)}\n",
      "reward:  {'agent-0': -33.75237657517097, 'agent-1': -33.75237657517097} \n",
      "\n",
      "obs:  {'agent-0': array([0.7388993 , 0.67381585, 3.548248  ], dtype=float32), 'agent-1': array([-0.7388993 , -0.67381585,  3.548248  ], dtype=float32)}\n",
      "action:  {'agent-0': array([-0.75339556,  0.03779137,  0.6636809 ], dtype=float32), 'agent-1': array([0.10112965, 0.2147404 , 0.        ], dtype=float32)}\n",
      "reward:  {'agent-0': -33.40170798523741, 'agent-1': -33.40170798523741} \n",
      "\n",
      "obs:  {'agent-0': array([0.73168695, 0.6816408 , 3.5381062 ], dtype=float32), 'agent-1': array([-0.73168695, -0.6816408 ,  3.5381062 ], dtype=float32)}\n",
      "action:  {'agent-0': array([-1.        ,  0.08791828,  0.18580547], dtype=float32), 'agent-1': array([-1.        , -0.30379128,  0.05879945], dtype=float32)}\n",
      "reward:  {'agent-0': -33.33227720722612, 'agent-1': -33.33227720722612} \n",
      "\n",
      "obs:  {'agent-0': array([0.72940075, 0.6840867 , 3.5360858 ], dtype=float32), 'agent-1': array([-0.72940075, -0.6840867 ,  3.5360858 ], dtype=float32)}\n",
      "action:  {'agent-0': array([-0.15415198,  0.4354446 ,  0.52159804], dtype=float32), 'agent-1': array([1.        , 0.73719573, 0.46374002], dtype=float32)}\n",
      "reward:  {'agent-0': -32.85815198392622, 'agent-1': -32.85815198392622} \n",
      "\n",
      "obs:  {'agent-0': array([0.7233652, 0.6904657, 3.5221798], dtype=float32), 'agent-1': array([-0.7233652, -0.6904657,  3.5221798], dtype=float32)}\n",
      "action:  {'agent-0': array([-1.        , -0.01637256,  0.        ], dtype=float32), 'agent-1': array([-0.81031305,  0.00964105,  0.        ], dtype=float32)}\n",
      "reward:  {'agent-0': -32.85815198392622, 'agent-1': -32.85815198392622} \n",
      "\n",
      "obs:  {'agent-0': array([0.7233652, 0.6904657, 3.5221798], dtype=float32), 'agent-1': array([-0.7233652, -0.6904657,  3.5221798], dtype=float32)}\n",
      "action:  {'agent-0': array([-0.1824559 ,  0.48669267,  0.        ], dtype=float32), 'agent-1': array([ 0.39803374, -0.40635455,  0.12346804], dtype=float32)}\n",
      "reward:  {'agent-0': -32.857319554510745, 'agent-1': -32.857319554510745} \n",
      "\n",
      "obs:  {'agent-0': array([0.72188777, 0.6920101 , 3.5221553 ], dtype=float32), 'agent-1': array([-0.72188777, -0.6920101 ,  3.5221553 ], dtype=float32)}\n",
      "action:  {'agent-0': array([ 0.01809657, -0.89135295,  1.        ], dtype=float32), 'agent-1': array([ 1.       , -0.2067436,  0.       ], dtype=float32)}\n",
      "reward:  {'agent-0': -32.26022801862916, 'agent-1': -32.26022801862916} \n",
      "\n",
      "obs:  {'agent-0': array([0.73580986, 0.67718816, 3.5043623 ], dtype=float32), 'agent-1': array([-0.73580986, -0.67718816,  3.5043623 ], dtype=float32)}\n",
      "action:  {'agent-0': array([-1.       , -0.9763129,  0.3110417], dtype=float32), 'agent-1': array([-0.17222095,  0.27407682,  0.4895347 ], dtype=float32)}\n",
      "reward:  {'agent-0': -31.797339474724417, 'agent-1': -31.797339474724417} \n",
      "\n",
      "obs:  {'agent-0': array([0.73939085, 0.6732765 , 3.4903474 ], dtype=float32), 'agent-1': array([-0.73939085, -0.6732765 ,  3.4903474 ], dtype=float32)}\n",
      "action:  {'agent-0': array([-1.        , -0.70302314,  0.16022885], dtype=float32), 'agent-1': array([-0.6882088, -1.       ,  0.7875777], dtype=float32)}\n",
      "reward:  {'agent-0': -32.53494733591701, 'agent-1': -32.53494733591701} \n",
      "\n",
      "obs:  {'agent-0': array([0.73436266, 0.6787573 , 3.512588  ], dtype=float32), 'agent-1': array([-0.73436266, -0.6787573 ,  3.512588  ], dtype=float32)}\n",
      "action:  {'agent-0': array([-0.9264791 , -0.04653203,  0.50586635], dtype=float32), 'agent-1': array([ 0.92127883, -1.        ,  1.        ], dtype=float32)}\n",
      "reward:  {'agent-0': -32.219815140013836, 'agent-1': -32.219815140013836} \n",
      "\n",
      "obs:  {'agent-0': array([0.6984055, 0.7157023, 3.5031466], dtype=float32), 'agent-1': array([-0.6984055, -0.7157023,  3.5031466], dtype=float32)}\n",
      "action:  {'agent-0': array([-0.5854738 ,  1.        ,  0.22039244], dtype=float32), 'agent-1': array([0.3904283 , 0.20673597, 0.5190338 ], dtype=float32)}\n",
      "reward:  {'agent-0': -32.07066722435658, 'agent-1': -32.07066722435658} \n",
      "\n",
      "obs:  {'agent-0': array([0.69131136, 0.72255695, 3.4986467 ], dtype=float32), 'agent-1': array([-0.69131136, -0.72255695,  3.4986467 ], dtype=float32)}\n",
      "action:  {'agent-0': array([0.47980666, 0.29508972, 0.54747015], dtype=float32), 'agent-1': array([-0.11694896, -0.28016996,  1.        ], dtype=float32)}\n",
      "reward:  {'agent-0': -32.65229357595975, 'agent-1': -32.65229357595975} \n",
      "\n",
      "obs:  {'agent-0': array([0.69062364, 0.7232143 , 3.516081  ], dtype=float32), 'agent-1': array([-0.69062364, -0.7232143 ,  3.516081  ], dtype=float32)}\n",
      "action:  {'agent-0': array([-0.5934648 ,  0.42632926,  0.5932688 ], dtype=float32), 'agent-1': array([0.3862822, 1.       , 0.7764987], dtype=float32)}\n",
      "reward:  {'agent-0': -31.823521090769187, 'agent-1': -31.823521090769187} \n",
      "\n",
      "obs:  {'agent-0': array([0.6881204 , 0.72559655, 3.4911454 ], dtype=float32), 'agent-1': array([-0.6881204 , -0.72559655,  3.4911454 ], dtype=float32)}\n",
      "action:  {'agent-0': array([ 0.6770015 , -0.7499815 ,  0.87464476], dtype=float32), 'agent-1': array([-0.7884157 , -0.6773077 ,  0.54428196], dtype=float32)}\n",
      "reward:  {'agent-0': -32.331419931106865, 'agent-1': -32.331419931106865} \n",
      "\n",
      "obs:  {'agent-0': array([0.7088977, 0.7053113, 3.5065005], dtype=float32), 'agent-1': array([-0.7088977, -0.7053113,  3.5065005], dtype=float32)}\n",
      "action:  {'agent-0': array([ 0.9357064, -1.       ,  1.       ], dtype=float32), 'agent-1': array([1.        , 1.        , 0.08672166], dtype=float32)}\n",
      "reward:  {'agent-0': -32.19591233576236, 'agent-1': -32.19591233576236} \n",
      "\n",
      "obs:  {'agent-0': array([0.73825073, 0.6745264 , 3.5024269 ], dtype=float32), 'agent-1': array([-0.73825073, -0.6745264 ,  3.5024269 ], dtype=float32)}\n",
      "action:  {'agent-0': array([-0.72553545,  0.6152538 ,  0.70749205], dtype=float32), 'agent-1': array([-0.14778244,  0.21022546,  1.        ], dtype=float32)}\n",
      "reward:  {'agent-0': -32.080525847743075, 'agent-1': -32.080525847743075} \n",
      "\n",
      "obs:  {'agent-0': array([0.729512  , 0.68396807, 3.4989448 ], dtype=float32), 'agent-1': array([-0.729512  , -0.68396807,  3.4989448 ], dtype=float32)}\n",
      "action:  {'agent-0': array([0.16005588, 1.        , 0.20972669], dtype=float32), 'agent-1': array([-0.34316778, -0.50265574,  0.8130367 ], dtype=float32)}\n",
      "reward:  {'agent-0': -32.7323830090388, 'agent-1': -32.7323830090388} \n",
      "\n",
      "obs:  {'agent-0': array([0.7245333 , 0.68923974, 3.5184584 ], dtype=float32), 'agent-1': array([-0.7245333 , -0.68923974,  3.5184584 ], dtype=float32)}\n",
      "action:  {'agent-0': array([ 1.       , -0.7076346,  0.       ], dtype=float32), 'agent-1': array([0.2882036 , 1.        , 0.09972247], dtype=float32)}\n",
      "reward:  {'agent-0': -32.64286908176324, 'agent-1': -32.64286908176324} \n",
      "\n",
      "obs:  {'agent-0': array([0.7256397, 0.6880748, 3.5158012], dtype=float32), 'agent-1': array([-0.7256397, -0.6880748,  3.5158012], dtype=float32)}\n",
      "action:  {'agent-0': array([ 1.        , -0.32382727,  0.        ], dtype=float32), 'agent-1': array([0.46652353, 0.39602757, 0.5844023 ], dtype=float32)}\n",
      "reward:  {'agent-0': -32.28579086263802, 'agent-1': -32.28579086263802} \n",
      "\n",
      "obs:  {'agent-0': array([0.72522074, 0.68851644, 3.5051305 ], dtype=float32), 'agent-1': array([-0.72522074, -0.68851644,  3.5051305 ], dtype=float32)}\n",
      "action:  {'agent-0': array([-1.       ,  0.1589644,  0.       ], dtype=float32), 'agent-1': array([-0.7126781 , -0.04249012,  0.        ], dtype=float32)}\n",
      "reward:  {'agent-0': -32.28579086263802, 'agent-1': -32.28579086263802} \n",
      "\n",
      "obs:  {'agent-0': array([0.72522074, 0.68851644, 3.5051305 ], dtype=float32), 'agent-1': array([-0.72522074, -0.68851644,  3.5051305 ], dtype=float32)}\n",
      "action:  {'agent-0': array([ 1.        , -0.6452198 ,  0.49670273], dtype=float32), 'agent-1': array([ 1.      , -0.782892,  0.603364], dtype=float32)}\n",
      "reward:  {'agent-0': -32.31353571973042, 'agent-1': -32.31353571973042} \n",
      "\n",
      "obs:  {'agent-0': array([0.7212972 , 0.69262564, 3.5059638 ], dtype=float32), 'agent-1': array([-0.7212972 , -0.69262564,  3.5059638 ], dtype=float32)}\n",
      "action:  {'agent-0': array([0.5510855, 0.899076 , 0.       ], dtype=float32), 'agent-1': array([-0.56394094,  0.5508282 ,  0.35473707], dtype=float32)}\n",
      "reward:  {'agent-0': -32.32370172361012, 'agent-1': -32.32370172361012} \n",
      "\n",
      "obs:  {'agent-0': array([0.72725934, 0.68636274, 3.506269  ], dtype=float32), 'agent-1': array([-0.72725934, -0.68636274,  3.506269  ], dtype=float32)}\n",
      "action:  {'agent-0': array([0.16488862, 1.        , 0.2551645 ], dtype=float32), 'agent-1': array([0.41747332, 1.        , 0.6492106 ], dtype=float32)}\n",
      "reward:  {'agent-0': -31.886996646430376, 'agent-1': -31.886996646430376} \n",
      "\n",
      "obs:  {'agent-0': array([0.7300393 , 0.68340516, 3.4930773 ], dtype=float32), 'agent-1': array([-0.7300393 , -0.68340516,  3.4930773 ], dtype=float32)}\n",
      "action:  {'agent-0': array([ 0.4094094 , -0.92955786,  0.7530135 ], dtype=float32), 'agent-1': array([-0.06830829, -0.5627037 ,  0.6861394 ], dtype=float32)}\n",
      "reward:  {'agent-0': -31.935258682267914, 'agent-1': -31.935258682267914} \n",
      "\n"
     ]
    }
   ],
   "source": [
    "from ray.rllib.algorithms.ppo import PPOConfig\n",
    "from ray.tune.logger import pretty_print\n",
    "\n",
    "from gymnasium.wrappers.time_limit import TimeLimit\n",
    "\n",
    "trainin_steps = 5\n",
    "\n",
    "algo = (\n",
    "    PPOConfig()\n",
    "    .training(gamma = 0.99, \n",
    "              lr = 0.001,\n",
    "              train_batch_size = 4096, \n",
    "              sgd_minibatch_size = 256, \n",
    "              num_sgd_iter = 30,\n",
    "              #entropy_coeff=0.005,\n",
    "              )\n",
    "    .env_runners(num_env_runners=1)\n",
    "    .resources(num_gpus=0)\n",
    "    .environment(env=\"KeepTheDistance?dst=0&agent=2&spawn_area=100\")\n",
    "    .build()\n",
    ")\n",
    "clear_output()\n",
    "\n",
    "out = \"\"\n",
    "for i in range(trainin_steps):\n",
    "    result = algo.train()\n",
    "    clear_output()\n",
    "    out += ppo_result_format(result) + \"\\n\"\n",
    "    print(out)\n",
    "    simulate_episode(RenderableKeepTheDistance(env_config), algo, 300, sleep_between_frames=0.03, print_info=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c7a9c6b2587345d9b54caacb95487ee3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "CanvasWithBorders(height=300, width=300)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "env_config_2 = EnvironmentConfiguration(n_agents=2, target_distance=0, max_steps=500, speed=1, spawn_area=100)\n",
    "simulate_episode(RenderableKeepTheDistance(env_config_2), algo, 300, sleep_between_frames=0.03, print_info=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "An Algorithm checkpoint has been created inside directory: 'TrainingResult(checkpoint=Checkpoint(filesystem=local, path=/mnt/c/Users/nicol/Desktop/Universit/tesi/experiments/RL_experiments/algos/KeepTheDistance?dst=0&agent=2&spawn_area=100), metrics={'custom_metrics': {}, 'episode_media': {}, 'info': {'learner': {'default_policy': {'learner_stats': {'allreduce_latency': 0.0, 'grad_gnorm': 3.2092962741230924, 'cur_kl_coeff': 1.0124999999999997, 'cur_lr': 0.0010000000000000005, 'total_loss': 9.670777402321498, 'policy_loss': -0.010992191499099135, 'vf_loss': 9.667357384165127, 'vf_explained_var': -0.007525691576302051, 'kl': 0.014234306790058408, 'entropy': 4.4639156160255276, 'entropy_coeff': 0.0}, 'model': {}, 'custom_metrics': {}, 'num_agent_steps_trained': 256.0, 'num_grad_updates_lifetime': 28320.5, 'diff_num_grad_updates_vs_sampler_policy': 479.5}}, 'num_env_steps_sampled': 122880, 'num_env_steps_trained': 122880, 'num_agent_steps_sampled': 245760, 'num_agent_steps_trained': 245760}, 'sampler_results': {'episode_reward_max': 21760.91781020525, 'episode_reward_min': -487.0253461896814, 'episode_reward_mean': 6778.967388339689, 'episode_len_mean': 300.0, 'episode_media': {}, 'episodes_this_iter': 14, 'episodes_timesteps_total': 30000, 'policy_reward_min': {}, 'policy_reward_max': {}, 'policy_reward_mean': {}, 'custom_metrics': {}, 'hist_stats': {'episode_reward': [21.72163927863096, 130.50639328258856, -200.11705320080804, 109.77545729412371, 11658.560931555112, 13309.912728167541, 6440.095692923635, 21760.91781020525, -83.37554391204539, 16968.453856282442, -167.11310698158036, 15063.63259516731, 14815.708543542853, 17.599522435270757, -280.8698686962444, 11837.37769232794, 14104.691896549233, 13496.00170955889, 10926.183249686364, -175.59519568722766, 10733.527207260233, -45.12339255923229, 962.7555553308978, 1296.8141882699124, 18021.577551266506, -304.5797059657488, -456.56202335775805, 17266.959181585426, -264.0068021312194, 7638.247403370834, 10849.464498687008, 44.85814998787794, 19922.215257854772, -206.05804589550445, 16862.90542172877, 3205.5738332530373, 250.73691355829558, -128.21324623223475, 17730.60655628874, 9885.248767253383, 105.3865503592685, 16862.408233819453, -131.5912928481024, 21037.08779302675, 342.76319822838053, 14240.099938946592, 3414.9853054879964, 457.55369102676855, -81.436287585753, -73.40939408360782, -61.577553381160655, 17852.590328932238, 3114.0607130691437, 6040.1247649853085, 17253.694056119923, 7963.814920086761, 2419.525743953622, -425.0413723437486, 638.2319380111602, 13855.678765814926, 16402.10700649715, -472.1417941702525, -330.062933383699, 97.2253533949262, -8.635824980358034, 16189.6629661085, 16111.586693046824, 16039.87156686257, -321.4820955859268, 14885.72817261804, -337.0164681971936, -347.66649584159995, -241.46037742750946, 8360.097987251962, -418.95729994680454, 358.19774962221646, -273.244228576848, 17186.3621367048, 18532.710830609558, 19164.953304877272, -432.8855343327394, -293.250052438824, 13620.455309973811, 16720.623967603424, -429.45245834948656, 18845.69278251919, 15121.479424960842, -175.4119252924525, 8426.056528937752, -195.2068009853412, 216.8161293787013, -446.54087726252476, 5949.227225113308, 123.73431044808011, -487.0253461896814, 13076.930679953632, 19597.009122610503, 7426.526351414298, 12839.412544180599, -31.255058717074245], 'episode_lengths': [300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300]}, 'sampler_perf': {'mean_raw_obs_processing_ms': 0.4088502448788074, 'mean_inference_ms': 0.9774863697425679, 'mean_action_processing_ms': 0.2591886678959726, 'mean_env_wait_ms': 0.44242339859380553, 'mean_env_render_ms': 0.0}, 'num_faulty_episodes': 0, 'connector_metrics': {'ObsPreprocessorConnector_ms': 0.00664210319519043, 'StateBufferConnector_ms': 0.00599217414855957, 'ViewRequirementAgentConnector_ms': 0.1657242774963379}, 'num_episodes': 14, 'episode_return_max': 21760.91781020525, 'episode_return_min': -487.0253461896814, 'episode_return_mean': 6778.967388339689}, 'env_runner_results': {'episode_reward_max': 21760.91781020525, 'episode_reward_min': -487.0253461896814, 'episode_reward_mean': 6778.967388339689, 'episode_len_mean': 300.0, 'episode_media': {}, 'episodes_this_iter': 14, 'episodes_timesteps_total': 30000, 'policy_reward_min': {}, 'policy_reward_max': {}, 'policy_reward_mean': {}, 'custom_metrics': {}, 'hist_stats': {'episode_reward': [21.72163927863096, 130.50639328258856, -200.11705320080804, 109.77545729412371, 11658.560931555112, 13309.912728167541, 6440.095692923635, 21760.91781020525, -83.37554391204539, 16968.453856282442, -167.11310698158036, 15063.63259516731, 14815.708543542853, 17.599522435270757, -280.8698686962444, 11837.37769232794, 14104.691896549233, 13496.00170955889, 10926.183249686364, -175.59519568722766, 10733.527207260233, -45.12339255923229, 962.7555553308978, 1296.8141882699124, 18021.577551266506, -304.5797059657488, -456.56202335775805, 17266.959181585426, -264.0068021312194, 7638.247403370834, 10849.464498687008, 44.85814998787794, 19922.215257854772, -206.05804589550445, 16862.90542172877, 3205.5738332530373, 250.73691355829558, -128.21324623223475, 17730.60655628874, 9885.248767253383, 105.3865503592685, 16862.408233819453, -131.5912928481024, 21037.08779302675, 342.76319822838053, 14240.099938946592, 3414.9853054879964, 457.55369102676855, -81.436287585753, -73.40939408360782, -61.577553381160655, 17852.590328932238, 3114.0607130691437, 6040.1247649853085, 17253.694056119923, 7963.814920086761, 2419.525743953622, -425.0413723437486, 638.2319380111602, 13855.678765814926, 16402.10700649715, -472.1417941702525, -330.062933383699, 97.2253533949262, -8.635824980358034, 16189.6629661085, 16111.586693046824, 16039.87156686257, -321.4820955859268, 14885.72817261804, -337.0164681971936, -347.66649584159995, -241.46037742750946, 8360.097987251962, -418.95729994680454, 358.19774962221646, -273.244228576848, 17186.3621367048, 18532.710830609558, 19164.953304877272, -432.8855343327394, -293.250052438824, 13620.455309973811, 16720.623967603424, -429.45245834948656, 18845.69278251919, 15121.479424960842, -175.4119252924525, 8426.056528937752, -195.2068009853412, 216.8161293787013, -446.54087726252476, 5949.227225113308, 123.73431044808011, -487.0253461896814, 13076.930679953632, 19597.009122610503, 7426.526351414298, 12839.412544180599, -31.255058717074245], 'episode_lengths': [300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300]}, 'sampler_perf': {'mean_raw_obs_processing_ms': 0.4088502448788074, 'mean_inference_ms': 0.9774863697425679, 'mean_action_processing_ms': 0.2591886678959726, 'mean_env_wait_ms': 0.44242339859380553, 'mean_env_render_ms': 0.0}, 'num_faulty_episodes': 0, 'connector_metrics': {'ObsPreprocessorConnector_ms': 0.00664210319519043, 'StateBufferConnector_ms': 0.00599217414855957, 'ViewRequirementAgentConnector_ms': 0.1657242774963379}, 'num_episodes': 14, 'episode_return_max': 21760.91781020525, 'episode_return_min': -487.0253461896814, 'episode_return_mean': 6778.967388339689}, 'episode_reward_max': 21760.91781020525, 'episode_reward_min': -487.0253461896814, 'episode_reward_mean': 6778.967388339689, 'episode_len_mean': 300.0, 'episodes_this_iter': 14, 'episodes_timesteps_total': 30000, 'policy_reward_min': {}, 'policy_reward_max': {}, 'policy_reward_mean': {}, 'hist_stats': {'episode_reward': [21.72163927863096, 130.50639328258856, -200.11705320080804, 109.77545729412371, 11658.560931555112, 13309.912728167541, 6440.095692923635, 21760.91781020525, -83.37554391204539, 16968.453856282442, -167.11310698158036, 15063.63259516731, 14815.708543542853, 17.599522435270757, -280.8698686962444, 11837.37769232794, 14104.691896549233, 13496.00170955889, 10926.183249686364, -175.59519568722766, 10733.527207260233, -45.12339255923229, 962.7555553308978, 1296.8141882699124, 18021.577551266506, -304.5797059657488, -456.56202335775805, 17266.959181585426, -264.0068021312194, 7638.247403370834, 10849.464498687008, 44.85814998787794, 19922.215257854772, -206.05804589550445, 16862.90542172877, 3205.5738332530373, 250.73691355829558, -128.21324623223475, 17730.60655628874, 9885.248767253383, 105.3865503592685, 16862.408233819453, -131.5912928481024, 21037.08779302675, 342.76319822838053, 14240.099938946592, 3414.9853054879964, 457.55369102676855, -81.436287585753, -73.40939408360782, -61.577553381160655, 17852.590328932238, 3114.0607130691437, 6040.1247649853085, 17253.694056119923, 7963.814920086761, 2419.525743953622, -425.0413723437486, 638.2319380111602, 13855.678765814926, 16402.10700649715, -472.1417941702525, -330.062933383699, 97.2253533949262, -8.635824980358034, 16189.6629661085, 16111.586693046824, 16039.87156686257, -321.4820955859268, 14885.72817261804, -337.0164681971936, -347.66649584159995, -241.46037742750946, 8360.097987251962, -418.95729994680454, 358.19774962221646, -273.244228576848, 17186.3621367048, 18532.710830609558, 19164.953304877272, -432.8855343327394, -293.250052438824, 13620.455309973811, 16720.623967603424, -429.45245834948656, 18845.69278251919, 15121.479424960842, -175.4119252924525, 8426.056528937752, -195.2068009853412, 216.8161293787013, -446.54087726252476, 5949.227225113308, 123.73431044808011, -487.0253461896814, 13076.930679953632, 19597.009122610503, 7426.526351414298, 12839.412544180599, -31.255058717074245], 'episode_lengths': [300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300]}, 'sampler_perf': {'mean_raw_obs_processing_ms': 0.4088502448788074, 'mean_inference_ms': 0.9774863697425679, 'mean_action_processing_ms': 0.2591886678959726, 'mean_env_wait_ms': 0.44242339859380553, 'mean_env_render_ms': 0.0}, 'num_faulty_episodes': 0, 'connector_metrics': {'ObsPreprocessorConnector_ms': 0.00664210319519043, 'StateBufferConnector_ms': 0.00599217414855957, 'ViewRequirementAgentConnector_ms': 0.1657242774963379}, 'num_episodes': 14, 'episode_return_max': 21760.91781020525, 'episode_return_min': -487.0253461896814, 'episode_return_mean': 6778.967388339689, 'num_healthy_workers': 1, 'num_in_flight_async_reqs': 0, 'num_remote_worker_restarts': 0, 'num_agent_steps_sampled': 245760, 'num_agent_steps_trained': 245760, 'num_env_steps_sampled': 122880, 'num_env_steps_trained': 122880, 'num_env_steps_sampled_this_iter': 4096, 'num_env_steps_trained_this_iter': 4096, 'num_env_steps_sampled_throughput_per_sec': 234.12821034740094, 'num_env_steps_trained_throughput_per_sec': 234.12821034740094, 'timesteps_total': 122880, 'num_env_steps_sampled_lifetime': 122880, 'num_agent_steps_sampled_lifetime': 245760, 'num_steps_trained_this_iter': 4096, 'agent_timesteps_total': 245760, 'timers': {'training_iteration_time_ms': 17025.196, 'restore_workers_time_ms': 0.019, 'training_step_time_ms': 17025.144, 'sample_time_ms': 9407.782, 'load_time_ms': 0.646, 'load_throughput': 6342711.801, 'learn_time_ms': 7611.344, 'learn_throughput': 538.144, 'synch_weights_time_ms': 4.724}, 'counters': {'num_env_steps_sampled': 122880, 'num_env_steps_trained': 122880, 'num_agent_steps_sampled': 245760, 'num_agent_steps_trained': 245760}, 'done': False, 'episodes_total': 409, 'training_iteration': 30, 'trial_id': 'default', 'date': '2024-05-24_18-05-14', 'timestamp': 1716566714, 'time_this_iter_s': 17.500314474105835, 'time_total_s': 476.70808959007263, 'pid': 1113, 'hostname': 'LAPTOP-9AD2MD1C', 'node_ip': '172.23.89.64', 'config': {'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, '_fake_gpus': False, 'num_learner_workers': 0, 'num_gpus_per_learner_worker': 0, 'num_cpus_per_learner_worker': 1, 'local_gpu_idx': 0, 'custom_resources_per_worker': {}, 'placement_strategy': 'PACK', 'eager_tracing': True, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'torch_compile_learner': False, 'torch_compile_learner_what_to_compile': <TorchCompileWhatToCompile.FORWARD_TRAIN: 'forward_train'>, 'torch_compile_learner_dynamo_backend': 'inductor', 'torch_compile_learner_dynamo_mode': None, 'torch_compile_worker': False, 'torch_compile_worker_dynamo_backend': 'onnxrt', 'torch_compile_worker_dynamo_mode': None, 'enable_rl_module_and_learner': False, 'enable_env_runner_and_connector_v2': False, 'env': 'KeepTheDistance?dst=0&agent=2&spawn_area=100', 'env_config': {}, 'observation_space': None, 'action_space': None, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, '_is_atari': None, 'env_task_fn': None, 'render_env': False, 'action_mask_key': 'action_mask', 'env_runner_cls': None, 'num_envs_per_env_runner': 1, 'validate_env_runners_after_construction': True, 'sample_timeout_s': 60.0, '_env_to_module_connector': None, 'add_default_connectors_to_env_to_module_pipeline': True, '_module_to_env_connector': None, 'add_default_connectors_to_module_to_env_pipeline': True, 'episode_lookback_horizon': 1, 'rollout_fragment_length': 'auto', 'batch_mode': 'truncate_episodes', 'compress_observations': False, 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'enable_tf1_exec_eagerly': False, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'update_worker_filter_stats': True, 'use_worker_filter_stats': True, 'enable_connectors': True, 'sampler_perf_stats_ema_coef': None, 'gamma': 0.99, 'lr': 0.001, 'grad_clip': None, 'grad_clip_by': 'global_norm', 'train_batch_size': 4096, 'train_batch_size_per_learner': None, 'model': {'_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'fcnet_weights_initializer': None, 'fcnet_weights_initializer_config': None, 'fcnet_bias_initializer': None, 'fcnet_bias_initializer_config': None, 'conv_filters': None, 'conv_activation': 'relu', 'conv_kernel_initializer': None, 'conv_kernel_initializer_config': None, 'conv_bias_initializer': None, 'conv_bias_initializer_config': None, 'conv_transpose_kernel_initializer': None, 'conv_transpose_kernel_initializer_config': None, 'conv_transpose_bias_initializer': None, 'conv_transpose_bias_initializer_config': None, 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'post_fcnet_weights_initializer': None, 'post_fcnet_weights_initializer_config': None, 'post_fcnet_bias_initializer': None, 'post_fcnet_bias_initializer_config': None, 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, 'lstm_weights_initializer': None, 'lstm_weights_initializer_config': None, 'lstm_bias_initializer': None, 'lstm_bias_initializer_config': None, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'encoder_latent_dim': None, 'always_check_shapes': False, 'lstm_use_prev_action_reward': -1, '_use_default_native_models': -1}, '_learner_connector': None, 'add_default_connectors_to_learner_pipeline': True, 'optimizer': {}, 'max_requests_in_flight_per_sampler_worker': 2, '_learner_class': None, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'algorithm_config_overrides_per_module': {}, '_per_module_overrides': {}, 'count_steps_by': 'env_steps', 'policy_map_capacity': 100, 'policy_mapping_fn': <function AlgorithmConfig.DEFAULT_POLICY_MAPPING_FN at 0x7f282bcec2c0>, 'policies_to_train': None, 'policy_states_are_swappable': False, 'observation_fn': None, 'input_config': {}, 'actions_in_input_normalized': False, 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_config': {}, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'offline_sampling': False, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_sample_timeout_s': 120.0, 'evaluation_parallel_to_training': False, 'evaluation_force_reset_envs_before_iteration': True, 'evaluation_config': None, 'off_policy_estimation_methods': {}, 'ope_split_batch_by_episode': True, 'evaluation_num_env_runners': 0, 'always_attach_evaluation_results': True, 'in_evaluation': False, 'sync_filters_on_rollout_workers_timeout_s': 10.0, 'keep_per_episode_custom_metrics': False, 'metrics_episode_collection_timeout_s': 60.0, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_iteration': None, 'min_train_timesteps_per_iteration': 0, 'min_sample_timesteps_per_iteration': 0, 'export_native_model_files': False, 'checkpoint_trainable_policies_only': False, 'logger_creator': None, 'logger_config': None, 'log_level': 'WARN', 'log_sys_usage': True, 'fake_sampler': False, 'seed': None, '_run_training_always_in_thread': False, '_evaluation_parallel_to_training_wo_thread': False, 'ignore_env_runner_failures': False, 'recreate_failed_env_runners': False, 'max_num_env_runner_restarts': 1000, 'delay_between_env_runner_restarts_s': 60.0, 'restart_failed_sub_environments': False, 'num_consecutive_env_runner_failures_tolerance': 100, 'env_runner_health_probe_timeout_s': 30, 'env_runner_restore_timeout_s': 1800, '_model_config_dict': {}, '_rl_module_spec': None, '_AlgorithmConfig__prior_exploration_config': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_initialize_loss_from_dummy_batch': False, 'simple_optimizer': False, 'policy_map_cache': -1, 'worker_cls': -1, 'synchronize_filters': -1, 'enable_async_evaluation': -1, 'custom_async_evaluation_function': -1, '_enable_rl_module_api': -1, 'auto_wrap_old_gym_envs': -1, 'disable_env_checking': -1, 'replay_sequence_length': None, '_disable_execution_plan_api': -1, 'lr_schedule': None, 'use_critic': True, 'use_gae': True, 'use_kl_loss': True, 'kl_coeff': 0.2, 'kl_target': 0.01, 'sgd_minibatch_size': 256, 'mini_batch_size_per_learner': None, 'num_sgd_iter': 30, 'shuffle_sequences': True, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.0, 'entropy_coeff_schedule': None, 'clip_param': 0.3, 'vf_clip_param': 10.0, 'vf_share_layers': -1, 'lambda': 1.0, 'input': 'sampler', 'policies': {'default_policy': (None, None, None, None)}, 'callbacks': <class 'ray.rllib.algorithms.callbacks.DefaultCallbacks'>, 'create_env_on_driver': False, 'custom_eval_function': None, 'framework': 'torch', 'num_cpus_for_driver': 1, 'num_workers': 1}, 'time_since_restore': 476.70808959007263, 'iterations_since_restore': 30, 'perf': {'cpu_util_percent': 33.739999999999995, 'ram_util_percent': 76.03}})'.\n"
     ]
    }
   ],
   "source": [
    "save_algo(algo, \"KeepTheDistance?dst=0&agent=2&spawn_area=100\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KeepTheDistance?dst=0&agent=3&visible_nbrs=2&spawn_area=100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ray.tune.registry import register_env\n",
    "env_config = EnvironmentConfiguration(n_agents=3, visible_nbrs=2, target_distance=0, max_steps=300, speed=1, spawn_area=100)\n",
    "register_env(\"KeepTheDistance?dst=0&agent=3&visible_nbrs=2&spawn_area=100\", lambda _: KeepTheDistance(env_config))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-24 10:13:10,842\tWARNING deprecation.py:50 -- DeprecationWarning: `_enable_new_api_stack` has been deprecated. Use `AlgorithmConfig._enable_new_api_stack` instead. This will raise an error in the future!\n",
      "/home/nicolo/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/rllib/algorithms/algorithm.py:521: RayDeprecationWarning: This API is deprecated and may be removed in future Ray releases. You could suppress this warning by setting env variable PYTHONWARNINGS=\"ignore::DeprecationWarning\"\n",
      "`UnifiedLogger` will be removed in Ray 2.7.\n",
      "  return UnifiedLogger(config, logdir, loggers=None)\n",
      "/home/nicolo/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/tune/logger/unified.py:53: RayDeprecationWarning: This API is deprecated and may be removed in future Ray releases. You could suppress this warning by setting env variable PYTHONWARNINGS=\"ignore::DeprecationWarning\"\n",
      "The `JsonLogger interface is deprecated in favor of the `ray.tune.json.JsonLoggerCallback` interface and will be removed in Ray 2.7.\n",
      "  self._loggers.append(cls(self.config, self.logdir, self.trial))\n",
      "/home/nicolo/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/tune/logger/unified.py:53: RayDeprecationWarning: This API is deprecated and may be removed in future Ray releases. You could suppress this warning by setting env variable PYTHONWARNINGS=\"ignore::DeprecationWarning\"\n",
      "The `CSVLogger interface is deprecated in favor of the `ray.tune.csv.CSVLoggerCallback` interface and will be removed in Ray 2.7.\n",
      "  self._loggers.append(cls(self.config, self.logdir, self.trial))\n",
      "/home/nicolo/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/tune/logger/unified.py:53: RayDeprecationWarning: This API is deprecated and may be removed in future Ray releases. You could suppress this warning by setting env variable PYTHONWARNINGS=\"ignore::DeprecationWarning\"\n",
      "The `TBXLogger interface is deprecated in favor of the `ray.tune.tensorboardx.TBXLoggerCallback` interface and will be removed in Ray 2.7.\n",
      "  self._loggers.append(cls(self.config, self.logdir, self.trial))\n",
      "2024-05-24 10:13:10,913\tWARNING deprecation.py:50 -- DeprecationWarning: `WorkerSet(num_workers=... OR local_worker=...)` has been deprecated. Use `EnvRunnerGroup(num_env_runners=... AND local_env_runner=...)` instead. This will raise an error in the future!\n",
      "2024-05-24 10:13:10,914\tWARNING deprecation.py:50 -- DeprecationWarning: `max_num_worker_restarts` has been deprecated. Use `AlgorithmConfig.max_num_env_runner_restarts` instead. This will raise an error in the future!\n",
      "2024-05-24 10:13:14,302\tINFO worker.py:1749 -- Started a local Ray instance.\n",
      "2024-05-24 10:13:23,121\tINFO trainable.py:161 -- Trainable.setup took 12.209 seconds. If your trainable is slow to initialize, consider setting reuse_actors=True to reduce actor creation overheads.\n",
      "2024-05-24 10:13:23,124\tWARNING util.py:61 -- Install gputil for GPU system monitoring.\n"
     ]
    }
   ],
   "source": [
    "algo = load_algo(\"KeepTheDistance?dst=0&agent=3&visible_nbrs=2&spawn_area=100\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "816cbb37135841c39e70355852f091a1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "CanvasWithBorders(height=300, width=300)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration [1] => episode_reward_mean: -16085.280054435147, episode_len_mean: 1000.0, agent_steps_trained: 12288, env_steps_trained: 4096, entropy: 4.240592692130142, learning_rate: 0.0010000000000000002\n",
      "iteration [2] => episode_reward_mean: -16432.439549192502, episode_len_mean: 1000.0, agent_steps_trained: 24576, env_steps_trained: 8192, entropy: 4.213702567583985, learning_rate: 0.0010000000000000002\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[39], line 26\u001b[0m\n\u001b[1;32m     24\u001b[0m env_show \u001b[38;5;241m=\u001b[39m RenderableKeepTheDistance(env_config)\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(trainin_steps):\n\u001b[0;32m---> 26\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43malgo\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     27\u001b[0m     simulate_episode(env_show, algo, \u001b[38;5;241m150\u001b[39m, sleep_between_frames\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.03\u001b[39m, print_info\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m     28\u001b[0m     \u001b[38;5;28mprint\u001b[39m(ppo_result_format(result))\n",
      "File \u001b[0;32m~/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/tune/trainable/trainable.py:328\u001b[0m, in \u001b[0;36mTrainable.train\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    326\u001b[0m start \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m    327\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 328\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    329\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    330\u001b[0m     skipped \u001b[38;5;241m=\u001b[39m skip_exceptions(e)\n",
      "File \u001b[0;32m~/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/rllib/algorithms/algorithm.py:873\u001b[0m, in \u001b[0;36mAlgorithm.step\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    863\u001b[0m     (\n\u001b[1;32m    864\u001b[0m         train_results,\n\u001b[1;32m    865\u001b[0m         eval_results,\n\u001b[1;32m    866\u001b[0m         train_iter_ctx,\n\u001b[1;32m    867\u001b[0m     ) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_run_one_training_iteration_and_evaluation_in_parallel()\n\u001b[1;32m    869\u001b[0m \u001b[38;5;66;03m# - No evaluation necessary, just run the next training iteration.\u001b[39;00m\n\u001b[1;32m    870\u001b[0m \u001b[38;5;66;03m# - We have to evaluate in this training iteration, but no parallelism ->\u001b[39;00m\n\u001b[1;32m    871\u001b[0m \u001b[38;5;66;03m#   evaluate after the training iteration is entirely done.\u001b[39;00m\n\u001b[1;32m    872\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 873\u001b[0m     train_results, train_iter_ctx \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_one_training_iteration\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    875\u001b[0m \u001b[38;5;66;03m# Sequential: Train (already done above), then evaluate.\u001b[39;00m\n\u001b[1;32m    876\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m evaluate_this_iter \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mevaluation_parallel_to_training:\n",
      "File \u001b[0;32m~/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/rllib/algorithms/algorithm.py:3156\u001b[0m, in \u001b[0;36mAlgorithm._run_one_training_iteration\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   3154\u001b[0m             \u001b[38;5;66;03m# Try to train one step.\u001b[39;00m\n\u001b[1;32m   3155\u001b[0m             \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_timers[TRAINING_STEP_TIMER]:\n\u001b[0;32m-> 3156\u001b[0m                 results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3158\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m results, train_iter_ctx\n",
      "File \u001b[0;32m~/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/rllib/algorithms/ppo/ppo.py:428\u001b[0m, in \u001b[0;36mPPO.training_step\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    424\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_training_step_new_api_stack()\n\u001b[1;32m    425\u001b[0m \u001b[38;5;66;03m# Old and hybrid API stacks (Policy, RolloutWorker, Connector, maybe RLModule,\u001b[39;00m\n\u001b[1;32m    426\u001b[0m \u001b[38;5;66;03m# maybe Learner).\u001b[39;00m\n\u001b[1;32m    427\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 428\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_training_step_old_and_hybrid_api_stacks\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/rllib/algorithms/ppo/ppo.py:562\u001b[0m, in \u001b[0;36mPPO._training_step_old_and_hybrid_api_stacks\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    557\u001b[0m     train_batch \u001b[38;5;241m=\u001b[39m synchronous_parallel_sample(\n\u001b[1;32m    558\u001b[0m         worker_set\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mworkers,\n\u001b[1;32m    559\u001b[0m         max_agent_steps\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mtotal_train_batch_size,\n\u001b[1;32m    560\u001b[0m     )\n\u001b[1;32m    561\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 562\u001b[0m     train_batch \u001b[38;5;241m=\u001b[39m \u001b[43msynchronous_parallel_sample\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    563\u001b[0m \u001b[43m        \u001b[49m\u001b[43mworker_set\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mworkers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    564\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_env_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtotal_train_batch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    565\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    566\u001b[0m train_batch \u001b[38;5;241m=\u001b[39m train_batch\u001b[38;5;241m.\u001b[39mas_multi_agent()\n\u001b[1;32m    567\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_counters[NUM_AGENT_STEPS_SAMPLED] \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m train_batch\u001b[38;5;241m.\u001b[39magent_steps()\n",
      "File \u001b[0;32m~/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/rllib/execution/rollout_ops.py:97\u001b[0m, in \u001b[0;36msynchronous_parallel_sample\u001b[0;34m(worker_set, max_agent_steps, max_env_steps, concat, sample_timeout_s, _uses_new_env_runners, _return_metrics)\u001b[0m\n\u001b[1;32m     94\u001b[0m         stats_dicts \u001b[38;5;241m=\u001b[39m [worker_set\u001b[38;5;241m.\u001b[39mlocal_worker()\u001b[38;5;241m.\u001b[39mget_metrics()]\n\u001b[1;32m     95\u001b[0m \u001b[38;5;66;03m# Loop over remote workers' `sample()` method in parallel.\u001b[39;00m\n\u001b[1;32m     96\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 97\u001b[0m     sampled_data \u001b[38;5;241m=\u001b[39m \u001b[43mworker_set\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforeach_worker\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     98\u001b[0m \u001b[43m        \u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     99\u001b[0m \u001b[43m            \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mw\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mw\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msample\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    100\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m_return_metrics\u001b[49m\n\u001b[1;32m    101\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mw\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mw\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msample\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mw\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_metrics\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    102\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    103\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlocal_worker\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    104\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout_seconds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample_timeout_s\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    105\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    106\u001b[0m     \u001b[38;5;66;03m# Nothing was returned (maybe all workers are stalling) or no healthy\u001b[39;00m\n\u001b[1;32m    107\u001b[0m     \u001b[38;5;66;03m# remote workers left: Break.\u001b[39;00m\n\u001b[1;32m    108\u001b[0m     \u001b[38;5;66;03m# There is no point staying in this loop, since we will not be able to\u001b[39;00m\n\u001b[1;32m    109\u001b[0m     \u001b[38;5;66;03m# get any new samples if we don't have any healthy remote workers left.\u001b[39;00m\n\u001b[1;32m    110\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m sampled_data \u001b[38;5;129;01mor\u001b[39;00m worker_set\u001b[38;5;241m.\u001b[39mnum_healthy_remote_workers() \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m~/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/rllib/env/env_runner_group.py:840\u001b[0m, in \u001b[0;36mEnvRunnerGroup.foreach_worker\u001b[0;34m(self, func, local_worker, healthy_only, remote_worker_ids, timeout_seconds, return_obj_refs, mark_healthy)\u001b[0m\n\u001b[1;32m    837\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_worker_manager\u001b[38;5;241m.\u001b[39mactor_ids():\n\u001b[1;32m    838\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m local_result\n\u001b[0;32m--> 840\u001b[0m remote_results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_worker_manager\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforeach_actor\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    841\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    842\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhealthy_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhealthy_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    843\u001b[0m \u001b[43m    \u001b[49m\u001b[43mremote_actor_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mremote_worker_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    844\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout_seconds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout_seconds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    845\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_obj_refs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_obj_refs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    846\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmark_healthy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmark_healthy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    847\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    849\u001b[0m _handle_remote_call_result_errors(\n\u001b[1;32m    850\u001b[0m     remote_results, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ignore_env_runner_failures\n\u001b[1;32m    851\u001b[0m )\n\u001b[1;32m    853\u001b[0m \u001b[38;5;66;03m# With application errors handled, return good results.\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/rllib/utils/actor_manager.py:622\u001b[0m, in \u001b[0;36mFaultTolerantActorManager.foreach_actor\u001b[0;34m(self, func, healthy_only, remote_actor_ids, timeout_seconds, return_obj_refs, mark_healthy)\u001b[0m\n\u001b[1;32m    616\u001b[0m remote_calls \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_actors(\n\u001b[1;32m    617\u001b[0m     func\u001b[38;5;241m=\u001b[39mfunc,\n\u001b[1;32m    618\u001b[0m     remote_actor_ids\u001b[38;5;241m=\u001b[39mremote_actor_ids,\n\u001b[1;32m    619\u001b[0m )\n\u001b[1;32m    621\u001b[0m \u001b[38;5;66;03m# Collect remote request results (if available given timeout and/or errors).\u001b[39;00m\n\u001b[0;32m--> 622\u001b[0m _, remote_results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fetch_result\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    623\u001b[0m \u001b[43m    \u001b[49m\u001b[43mremote_actor_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mremote_actor_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    624\u001b[0m \u001b[43m    \u001b[49m\u001b[43mremote_calls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mremote_calls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    625\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtags\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mremote_calls\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    626\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout_seconds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout_seconds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    627\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_obj_refs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_obj_refs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    628\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmark_healthy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmark_healthy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    629\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    631\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m remote_results\n",
      "File \u001b[0;32m~/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/rllib/utils/actor_manager.py:476\u001b[0m, in \u001b[0;36mFaultTolerantActorManager._fetch_result\u001b[0;34m(self, remote_actor_ids, remote_calls, tags, timeout_seconds, return_obj_refs, mark_healthy)\u001b[0m\n\u001b[1;32m    473\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m remote_calls:\n\u001b[1;32m    474\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [], RemoteCallResults()\n\u001b[0;32m--> 476\u001b[0m ready, _ \u001b[38;5;241m=\u001b[39m \u001b[43mray\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    477\u001b[0m \u001b[43m    \u001b[49m\u001b[43mremote_calls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    478\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_returns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mremote_calls\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    479\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    480\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# Make sure remote results are fetched locally in parallel.\u001b[39;49;00m\n\u001b[1;32m    481\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfetch_local\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mreturn_obj_refs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    482\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    484\u001b[0m \u001b[38;5;66;03m# Remote data should already be fetched to local object store at this point.\u001b[39;00m\n\u001b[1;32m    485\u001b[0m remote_results \u001b[38;5;241m=\u001b[39m RemoteCallResults()\n",
      "File \u001b[0;32m~/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/_private/auto_init_hook.py:21\u001b[0m, in \u001b[0;36mwrap_auto_init.<locals>.auto_init_wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(fn)\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mauto_init_wrapper\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     20\u001b[0m     auto_init_ray()\n\u001b[0;32m---> 21\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/_private/client_mode_hook.py:103\u001b[0m, in \u001b[0;36mclient_mode_hook.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    101\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m func\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minit\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m is_client_mode_enabled_by_default:\n\u001b[1;32m    102\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(ray, func\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m)(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m--> 103\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/_private/worker.py:2854\u001b[0m, in \u001b[0;36mwait\u001b[0;34m(ray_waitables, num_returns, timeout, fetch_local)\u001b[0m\n\u001b[1;32m   2852\u001b[0m timeout \u001b[38;5;241m=\u001b[39m timeout \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;241m10\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m6\u001b[39m\n\u001b[1;32m   2853\u001b[0m timeout_milliseconds \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(timeout \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m1000\u001b[39m)\n\u001b[0;32m-> 2854\u001b[0m ready_ids, remaining_ids \u001b[38;5;241m=\u001b[39m \u001b[43mworker\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcore_worker\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2855\u001b[0m \u001b[43m    \u001b[49m\u001b[43mray_waitables\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2856\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_returns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2857\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout_milliseconds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2858\u001b[0m \u001b[43m    \u001b[49m\u001b[43mworker\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcurrent_task_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2859\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfetch_local\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2860\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2861\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m ready_ids, remaining_ids\n",
      "File \u001b[0;32mpython/ray/_raylet.pyx:3812\u001b[0m, in \u001b[0;36mray._raylet.CoreWorker.wait\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpython/ray/_raylet.pyx:571\u001b[0m, in \u001b[0;36mray._raylet.check_status\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from ray.rllib.algorithms.ppo import PPOConfig\n",
    "from ray.tune.logger import pretty_print\n",
    "\n",
    "from gymnasium.wrappers.time_limit import TimeLimit\n",
    "\n",
    "trainin_steps = 40\n",
    "\n",
    "algo = (\n",
    "    PPOConfig()\n",
    "    .training(gamma = 0.95, \n",
    "              lr = 0.001,\n",
    "              train_batch_size = 4092, \n",
    "              sgd_minibatch_size = 128, \n",
    "              num_sgd_iter = 30,\n",
    "              #entropy_coeff=0.005,\n",
    "              )\n",
    "    .env_runners(num_env_runners=1)\n",
    "    .resources(num_gpus=0)\n",
    "    .environment(env=\"KeepTheDistance?dst=0&agent=3&visible_nbrs=2&spawn_area=100\")\n",
    "    .build()\n",
    ")\n",
    "clear_output()\n",
    "\n",
    "env_show = RenderableKeepTheDistance(env_config)\n",
    "for i in range(trainin_steps):\n",
    "    result = algo.train()\n",
    "    simulate_episode(env_show, algo, 150, sleep_between_frames=0.03, print_info=False)\n",
    "    print(ppo_result_format(result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "79effdd57d3540f0b7889af0abfd794f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "CanvasWithBorders(height=300, width=300)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "env_config_2 = EnvironmentConfiguration(n_agents=3, visible_nbrs=2, target_distance=0, max_steps=500, speed=1, spawn_area=100)\n",
    "simulate_episode(RenderableKeepTheDistance(env_config_2), algo, 300, sleep_between_frames=0.01, print_info=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "An Algorithm checkpoint has been created inside directory: 'TrainingResult(checkpoint=Checkpoint(filesystem=local, path=/mnt/c/Users/nicol/Desktop/Universit/tesi/experiments/RL_experiments/algos/KeepTheDistance?dst=0&agent=3&visible_nbrs=2&spawn_area=100), metrics={'custom_metrics': {}, 'episode_media': {}, 'info': {'learner': {'default_policy': {'learner_stats': {'allreduce_latency': 0.0, 'grad_gnorm': 26.797518886129062, 'cur_kl_coeff': 5.473673629760742, 'cur_lr': 0.0010000000000000002, 'total_loss': 4.241659853690201, 'policy_loss': 0.0008611866208310757, 'vf_loss': 4.17278758486112, 'vf_explained_var': 0.026051732442445224, 'kl': 0.012425122178001438, 'entropy': 5.3370947420597075, 'entropy_coeff': 0.0}, 'model': {}, 'custom_metrics': {}, 'num_agent_steps_trained': 256.0, 'num_grad_updates_lifetime': 56880.5, 'diff_num_grad_updates_vs_sampler_policy': 719.5}}, 'num_env_steps_sampled': 163840, 'num_env_steps_trained': 163840, 'num_agent_steps_sampled': 491520, 'num_agent_steps_trained': 491520}, 'sampler_results': {'episode_reward_max': 26042.054873470093, 'episode_reward_min': -98.5686833518256, 'episode_reward_mean': 1930.20330570609, 'episode_len_mean': 300.0, 'episode_media': {}, 'episodes_this_iter': 14, 'episodes_timesteps_total': 30000, 'policy_reward_min': {}, 'policy_reward_max': {}, 'policy_reward_mean': {}, 'custom_metrics': {}, 'hist_stats': {'episode_reward': [-92.27266819856116, 25.73075983931291, 20.15284682220323, 4.749758335196418, 10.090455927671556, 70.70791792123086, 96.00401067477735, 63.07796837442251, 63.78997120042281, 33.44352543766661, -46.465104402316875, -0.5203327196826706, 18318.744591704028, 37.14122664199385, 21.13643332215554, 87.87719936101234, 71.45022590217889, -9.101330754919871, -98.5686833518256, 95.33530645806059, 55.4811982213292, 18.30271494299572, -51.26575824571279, 42.082533480452554, -22.445596925520377, 68.74256746797532, 33.05435525796253, -14.032245291742242, 79.26786278759919, 33.07432484402226, -38.11927735077796, 26.551249438526455, 39.52130742704978, 97.97914468484055, 50.10658463112787, 27.31634486537763, 11540.808485968026, 55.878779664342275, 39.91017477423976, 23326.82285928509, -8.006498538129037, 18704.947979701745, -8.362829241117481, 97.42257175376655, 51.19154584835426, 67.80124618096765, 41.902646135463215, -4.600723926245507, 35.53484550023718, 10580.234960158623, -18.71622448426686, 39.01829619121443, 84.04304932371977, -1.4464490134462125, 60.037719617980514, 23.785667828333658, 42.26677431276764, 34.40088252853897, 26042.054873470093, 41.77314813414468, 45.79029879641874, 32.55142799204754, -11.965768183545151, 42.00753025786594, 12293.034255574681, 43.79904868980046, -25.863541249120757, 60.73670702197117, 47.89978373191765, 61.30928534284479, 20899.3740854774, -14.818305615857966, -8.893512470663119, 14833.645636660005, 41.397579473451756, 22821.41310938688, 7.491018013864107, 59.11961027045365, 63.7071514643104, 5.028932524212735, 33.37970701764043, 14.878160110483378, 98.08106277634619, -11.823124640004565, -0.665076458756527, 38.628779430559916, 14.463635071025323, 85.03423308275264, 155.99019409589116, 1.79278515037403, 10570.213268957787, 9.93113203795204, 70.76003720368493, 26.41514838991324, -14.33627414254741, 13.77353032658921, 454.51181271704314, 95.00891723474588, 33.989863382171976, -53.28672419852414], 'episode_lengths': [300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300]}, 'sampler_perf': {'mean_raw_obs_processing_ms': 0.6170270567527532, 'mean_inference_ms': 1.07689741151318, 'mean_action_processing_ms': 0.3920014103326924, 'mean_env_wait_ms': 0.7321249918278235, 'mean_env_render_ms': 0.0}, 'num_faulty_episodes': 0, 'connector_metrics': {'ObsPreprocessorConnector_ms': 0.008563756942749023, 'StateBufferConnector_ms': 0.00696110725402832, 'ViewRequirementAgentConnector_ms': 0.23952293395996094}, 'num_episodes': 14, 'episode_return_max': 26042.054873470093, 'episode_return_min': -98.5686833518256, 'episode_return_mean': 1930.20330570609}, 'env_runner_results': {'episode_reward_max': 26042.054873470093, 'episode_reward_min': -98.5686833518256, 'episode_reward_mean': 1930.20330570609, 'episode_len_mean': 300.0, 'episode_media': {}, 'episodes_this_iter': 14, 'episodes_timesteps_total': 30000, 'policy_reward_min': {}, 'policy_reward_max': {}, 'policy_reward_mean': {}, 'custom_metrics': {}, 'hist_stats': {'episode_reward': [-92.27266819856116, 25.73075983931291, 20.15284682220323, 4.749758335196418, 10.090455927671556, 70.70791792123086, 96.00401067477735, 63.07796837442251, 63.78997120042281, 33.44352543766661, -46.465104402316875, -0.5203327196826706, 18318.744591704028, 37.14122664199385, 21.13643332215554, 87.87719936101234, 71.45022590217889, -9.101330754919871, -98.5686833518256, 95.33530645806059, 55.4811982213292, 18.30271494299572, -51.26575824571279, 42.082533480452554, -22.445596925520377, 68.74256746797532, 33.05435525796253, -14.032245291742242, 79.26786278759919, 33.07432484402226, -38.11927735077796, 26.551249438526455, 39.52130742704978, 97.97914468484055, 50.10658463112787, 27.31634486537763, 11540.808485968026, 55.878779664342275, 39.91017477423976, 23326.82285928509, -8.006498538129037, 18704.947979701745, -8.362829241117481, 97.42257175376655, 51.19154584835426, 67.80124618096765, 41.902646135463215, -4.600723926245507, 35.53484550023718, 10580.234960158623, -18.71622448426686, 39.01829619121443, 84.04304932371977, -1.4464490134462125, 60.037719617980514, 23.785667828333658, 42.26677431276764, 34.40088252853897, 26042.054873470093, 41.77314813414468, 45.79029879641874, 32.55142799204754, -11.965768183545151, 42.00753025786594, 12293.034255574681, 43.79904868980046, -25.863541249120757, 60.73670702197117, 47.89978373191765, 61.30928534284479, 20899.3740854774, -14.818305615857966, -8.893512470663119, 14833.645636660005, 41.397579473451756, 22821.41310938688, 7.491018013864107, 59.11961027045365, 63.7071514643104, 5.028932524212735, 33.37970701764043, 14.878160110483378, 98.08106277634619, -11.823124640004565, -0.665076458756527, 38.628779430559916, 14.463635071025323, 85.03423308275264, 155.99019409589116, 1.79278515037403, 10570.213268957787, 9.93113203795204, 70.76003720368493, 26.41514838991324, -14.33627414254741, 13.77353032658921, 454.51181271704314, 95.00891723474588, 33.989863382171976, -53.28672419852414], 'episode_lengths': [300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300]}, 'sampler_perf': {'mean_raw_obs_processing_ms': 0.6170270567527532, 'mean_inference_ms': 1.07689741151318, 'mean_action_processing_ms': 0.3920014103326924, 'mean_env_wait_ms': 0.7321249918278235, 'mean_env_render_ms': 0.0}, 'num_faulty_episodes': 0, 'connector_metrics': {'ObsPreprocessorConnector_ms': 0.008563756942749023, 'StateBufferConnector_ms': 0.00696110725402832, 'ViewRequirementAgentConnector_ms': 0.23952293395996094}, 'num_episodes': 14, 'episode_return_max': 26042.054873470093, 'episode_return_min': -98.5686833518256, 'episode_return_mean': 1930.20330570609}, 'episode_reward_max': 26042.054873470093, 'episode_reward_min': -98.5686833518256, 'episode_reward_mean': 1930.20330570609, 'episode_len_mean': 300.0, 'episodes_this_iter': 14, 'episodes_timesteps_total': 30000, 'policy_reward_min': {}, 'policy_reward_max': {}, 'policy_reward_mean': {}, 'hist_stats': {'episode_reward': [-92.27266819856116, 25.73075983931291, 20.15284682220323, 4.749758335196418, 10.090455927671556, 70.70791792123086, 96.00401067477735, 63.07796837442251, 63.78997120042281, 33.44352543766661, -46.465104402316875, -0.5203327196826706, 18318.744591704028, 37.14122664199385, 21.13643332215554, 87.87719936101234, 71.45022590217889, -9.101330754919871, -98.5686833518256, 95.33530645806059, 55.4811982213292, 18.30271494299572, -51.26575824571279, 42.082533480452554, -22.445596925520377, 68.74256746797532, 33.05435525796253, -14.032245291742242, 79.26786278759919, 33.07432484402226, -38.11927735077796, 26.551249438526455, 39.52130742704978, 97.97914468484055, 50.10658463112787, 27.31634486537763, 11540.808485968026, 55.878779664342275, 39.91017477423976, 23326.82285928509, -8.006498538129037, 18704.947979701745, -8.362829241117481, 97.42257175376655, 51.19154584835426, 67.80124618096765, 41.902646135463215, -4.600723926245507, 35.53484550023718, 10580.234960158623, -18.71622448426686, 39.01829619121443, 84.04304932371977, -1.4464490134462125, 60.037719617980514, 23.785667828333658, 42.26677431276764, 34.40088252853897, 26042.054873470093, 41.77314813414468, 45.79029879641874, 32.55142799204754, -11.965768183545151, 42.00753025786594, 12293.034255574681, 43.79904868980046, -25.863541249120757, 60.73670702197117, 47.89978373191765, 61.30928534284479, 20899.3740854774, -14.818305615857966, -8.893512470663119, 14833.645636660005, 41.397579473451756, 22821.41310938688, 7.491018013864107, 59.11961027045365, 63.7071514643104, 5.028932524212735, 33.37970701764043, 14.878160110483378, 98.08106277634619, -11.823124640004565, -0.665076458756527, 38.628779430559916, 14.463635071025323, 85.03423308275264, 155.99019409589116, 1.79278515037403, 10570.213268957787, 9.93113203795204, 70.76003720368493, 26.41514838991324, -14.33627414254741, 13.77353032658921, 454.51181271704314, 95.00891723474588, 33.989863382171976, -53.28672419852414], 'episode_lengths': [300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300]}, 'sampler_perf': {'mean_raw_obs_processing_ms': 0.6170270567527532, 'mean_inference_ms': 1.07689741151318, 'mean_action_processing_ms': 0.3920014103326924, 'mean_env_wait_ms': 0.7321249918278235, 'mean_env_render_ms': 0.0}, 'num_faulty_episodes': 0, 'connector_metrics': {'ObsPreprocessorConnector_ms': 0.008563756942749023, 'StateBufferConnector_ms': 0.00696110725402832, 'ViewRequirementAgentConnector_ms': 0.23952293395996094}, 'num_episodes': 14, 'episode_return_max': 26042.054873470093, 'episode_return_min': -98.5686833518256, 'episode_return_mean': 1930.20330570609, 'num_healthy_workers': 1, 'num_in_flight_async_reqs': 0, 'num_remote_worker_restarts': 0, 'num_agent_steps_sampled': 491520, 'num_agent_steps_trained': 491520, 'num_env_steps_sampled': 163840, 'num_env_steps_trained': 163840, 'num_env_steps_sampled_this_iter': 4096, 'num_env_steps_trained_this_iter': 4096, 'num_env_steps_sampled_throughput_per_sec': 174.64544401360163, 'num_env_steps_trained_throughput_per_sec': 174.64544401360163, 'timesteps_total': 163840, 'num_env_steps_sampled_lifetime': 163840, 'num_agent_steps_sampled_lifetime': 491520, 'num_steps_trained_this_iter': 4096, 'agent_timesteps_total': 491520, 'timers': {'training_iteration_time_ms': 23517.072, 'restore_workers_time_ms': 0.02, 'training_step_time_ms': 23517.014, 'sample_time_ms': 11617.955, 'load_time_ms': 0.833, 'load_throughput': 4919920.153, 'learn_time_ms': 11892.561, 'learn_throughput': 344.417, 'synch_weights_time_ms': 4.667}, 'counters': {'num_env_steps_sampled': 163840, 'num_env_steps_trained': 163840, 'num_agent_steps_sampled': 491520, 'num_agent_steps_trained': 491520}, 'done': False, 'episodes_total': 546, 'training_iteration': 40, 'trial_id': 'default', 'date': '2024-05-23_18-19-04', 'timestamp': 1716481144, 'time_this_iter_s': 23.458407163619995, 'time_total_s': 933.9117562770844, 'pid': 13426, 'hostname': 'LAPTOP-9AD2MD1C', 'node_ip': '172.23.84.145', 'config': {'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, '_fake_gpus': False, 'num_learner_workers': 0, 'num_gpus_per_learner_worker': 0, 'num_cpus_per_learner_worker': 1, 'local_gpu_idx': 0, 'custom_resources_per_worker': {}, 'placement_strategy': 'PACK', 'eager_tracing': True, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'torch_compile_learner': False, 'torch_compile_learner_what_to_compile': <TorchCompileWhatToCompile.FORWARD_TRAIN: 'forward_train'>, 'torch_compile_learner_dynamo_backend': 'inductor', 'torch_compile_learner_dynamo_mode': None, 'torch_compile_worker': False, 'torch_compile_worker_dynamo_backend': 'onnxrt', 'torch_compile_worker_dynamo_mode': None, 'enable_rl_module_and_learner': False, 'enable_env_runner_and_connector_v2': False, 'env': 'KeepTheDistance?dst=0&agent=3&visible_nbrs=2&spawn_area=100', 'env_config': {}, 'observation_space': None, 'action_space': None, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, '_is_atari': None, 'env_task_fn': None, 'render_env': False, 'action_mask_key': 'action_mask', 'env_runner_cls': None, 'num_envs_per_env_runner': 1, 'validate_env_runners_after_construction': True, 'sample_timeout_s': 60.0, '_env_to_module_connector': None, 'add_default_connectors_to_env_to_module_pipeline': True, '_module_to_env_connector': None, 'add_default_connectors_to_module_to_env_pipeline': True, 'episode_lookback_horizon': 1, 'rollout_fragment_length': 'auto', 'batch_mode': 'truncate_episodes', 'compress_observations': False, 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'enable_tf1_exec_eagerly': False, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'update_worker_filter_stats': True, 'use_worker_filter_stats': True, 'enable_connectors': True, 'sampler_perf_stats_ema_coef': None, 'gamma': 0.95, 'lr': 0.001, 'grad_clip': None, 'grad_clip_by': 'global_norm', 'train_batch_size': 4096, 'train_batch_size_per_learner': None, 'model': {'_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'fcnet_weights_initializer': None, 'fcnet_weights_initializer_config': None, 'fcnet_bias_initializer': None, 'fcnet_bias_initializer_config': None, 'conv_filters': None, 'conv_activation': 'relu', 'conv_kernel_initializer': None, 'conv_kernel_initializer_config': None, 'conv_bias_initializer': None, 'conv_bias_initializer_config': None, 'conv_transpose_kernel_initializer': None, 'conv_transpose_kernel_initializer_config': None, 'conv_transpose_bias_initializer': None, 'conv_transpose_bias_initializer_config': None, 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'post_fcnet_weights_initializer': None, 'post_fcnet_weights_initializer_config': None, 'post_fcnet_bias_initializer': None, 'post_fcnet_bias_initializer_config': None, 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, 'lstm_weights_initializer': None, 'lstm_weights_initializer_config': None, 'lstm_bias_initializer': None, 'lstm_bias_initializer_config': None, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'encoder_latent_dim': None, 'always_check_shapes': False, 'lstm_use_prev_action_reward': -1, '_use_default_native_models': -1}, '_learner_connector': None, 'add_default_connectors_to_learner_pipeline': True, 'optimizer': {}, 'max_requests_in_flight_per_sampler_worker': 2, '_learner_class': None, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'algorithm_config_overrides_per_module': {}, '_per_module_overrides': {}, 'count_steps_by': 'env_steps', 'policy_map_capacity': 100, 'policy_mapping_fn': <function AlgorithmConfig.DEFAULT_POLICY_MAPPING_FN at 0x7f326e0fc220>, 'policies_to_train': None, 'policy_states_are_swappable': False, 'observation_fn': None, 'input_config': {}, 'actions_in_input_normalized': False, 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_config': {}, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'offline_sampling': False, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_sample_timeout_s': 120.0, 'evaluation_parallel_to_training': False, 'evaluation_force_reset_envs_before_iteration': True, 'evaluation_config': None, 'off_policy_estimation_methods': {}, 'ope_split_batch_by_episode': True, 'evaluation_num_env_runners': 0, 'always_attach_evaluation_results': True, 'in_evaluation': False, 'sync_filters_on_rollout_workers_timeout_s': 10.0, 'keep_per_episode_custom_metrics': False, 'metrics_episode_collection_timeout_s': 60.0, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_iteration': None, 'min_train_timesteps_per_iteration': 0, 'min_sample_timesteps_per_iteration': 0, 'export_native_model_files': False, 'checkpoint_trainable_policies_only': False, 'logger_creator': None, 'logger_config': None, 'log_level': 'WARN', 'log_sys_usage': True, 'fake_sampler': False, 'seed': None, '_run_training_always_in_thread': False, '_evaluation_parallel_to_training_wo_thread': False, 'ignore_env_runner_failures': False, 'recreate_failed_env_runners': False, 'max_num_env_runner_restarts': 1000, 'delay_between_env_runner_restarts_s': 60.0, 'restart_failed_sub_environments': False, 'num_consecutive_env_runner_failures_tolerance': 100, 'env_runner_health_probe_timeout_s': 30, 'env_runner_restore_timeout_s': 1800, '_model_config_dict': {}, '_rl_module_spec': None, '_AlgorithmConfig__prior_exploration_config': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_initialize_loss_from_dummy_batch': False, 'simple_optimizer': False, 'policy_map_cache': -1, 'worker_cls': -1, 'synchronize_filters': -1, 'enable_async_evaluation': -1, 'custom_async_evaluation_function': -1, '_enable_rl_module_api': -1, 'auto_wrap_old_gym_envs': -1, 'disable_env_checking': -1, 'replay_sequence_length': None, '_disable_execution_plan_api': -1, 'lr_schedule': None, 'use_critic': True, 'use_gae': True, 'use_kl_loss': True, 'kl_coeff': 0.2, 'kl_target': 0.01, 'sgd_minibatch_size': 256, 'mini_batch_size_per_learner': None, 'num_sgd_iter': 30, 'shuffle_sequences': True, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.0, 'entropy_coeff_schedule': None, 'clip_param': 0.3, 'vf_clip_param': 10.0, 'vf_share_layers': -1, 'lambda': 1.0, 'input': 'sampler', 'policies': {'default_policy': (None, None, None, None)}, 'callbacks': <class 'ray.rllib.algorithms.callbacks.DefaultCallbacks'>, 'create_env_on_driver': False, 'custom_eval_function': None, 'framework': 'torch', 'num_cpus_for_driver': 1, 'num_workers': 1}, 'time_since_restore': 933.9117562770844, 'iterations_since_restore': 40, 'perf': {'cpu_util_percent': 35.35609756097562, 'ram_util_percent': 79.00731707317074}})'.\n"
     ]
    }
   ],
   "source": [
    "save_algo(algo, \"KeepTheDistance?dst=0&agent=3&visible_nbrs=2&spawn_area=100\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KeepTheDistance?dst=0&agent=4&visible_nbrs=3&spawn_area=100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ray.tune.registry import register_env\n",
    "\n",
    "train_batch_size = 4096\n",
    "reset_per_batch = train_batch_size/300\n",
    "\n",
    "spawn_area_schedule = [[0,10],[4,30],[9,50],[18,100]]\n",
    "#spawn_area_schedule = [[0,10],[4,30],[10,50],[18,100]]\n",
    "\n",
    "env_config = EnvironmentConfiguration(n_agents=4, visible_nbrs=3, target_distance=0, max_steps=1000, speed=1, spawn_area=20, \n",
    "                                      spawn_area_schedule=[[schedule[0]*train_batch_size, schedule[1]] for schedule in spawn_area_schedule])\n",
    "env_config_show = EnvironmentConfiguration(n_agents=4, visible_nbrs=3, target_distance=0, max_steps=300, speed=1, spawn_area=20, \n",
    "                                      spawn_area_schedule=spawn_area_schedule)\n",
    "register_env(\"KeepTheDistance?dst=0&agent=4&visible_nbrs=3&spawn_area=100\", lambda _: KeepTheDistance(env_config))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "The specified subfolder '/mnt/c/Users/nicol/Desktop/Universit/tesi/experiments/RL_experiments/algos/KeepTheDistance?dst=0&agent=4&visible_nbrs=3&spawn_area=100' does not exist.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[101], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m algo \u001b[38;5;241m=\u001b[39m \u001b[43mload_algo\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mKeepTheDistance?dst=0&agent=4&visible_nbrs=3&spawn_area=100\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[1], line 33\u001b[0m, in \u001b[0;36mload_algo\u001b[0;34m(name)\u001b[0m\n\u001b[1;32m     31\u001b[0m subfolder_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(base_dir, name)\n\u001b[1;32m     32\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexists(subfolder_path):\n\u001b[0;32m---> 33\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe specified subfolder \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00msubfolder_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m does not exist.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     35\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m Algorithm\u001b[38;5;241m.\u001b[39mfrom_checkpoint(subfolder_path)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: The specified subfolder '/mnt/c/Users/nicol/Desktop/Universit/tesi/experiments/RL_experiments/algos/KeepTheDistance?dst=0&agent=4&visible_nbrs=3&spawn_area=100' does not exist."
     ]
    }
   ],
   "source": [
    "algo = load_algo(\"KeepTheDistance?dst=0&agent=4&visible_nbrs=3&spawn_area=100\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c9e5141cfd324caa90526592c7cad488",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "CanvasWithBorders(height=300, width=300)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration [1] => episode_reward_mean: 241.53260623370483, episode_len_mean: 1000.0, agent_steps_trained: 16384, env_steps_trained: 4096, entropy: 4.262171379725138, learning_rate: 0.0010000000000000005\n",
      "iteration [2] => episode_reward_mean: 8972.364221824693, episode_len_mean: 1000.0, agent_steps_trained: 32768, env_steps_trained: 8192, entropy: 4.166322618474563, learning_rate: 0.0010000000000000005\n",
      "iteration [3] => episode_reward_mean: 20690.61975089599, episode_len_mean: 1000.0, agent_steps_trained: 49152, env_steps_trained: 12288, entropy: 4.057953937600057, learning_rate: 0.0010000000000000005\n",
      "iteration [4] => episode_reward_mean: 29882.30053861677, episode_len_mean: 1000.0, agent_steps_trained: 65536, env_steps_trained: 16384, entropy: 4.133171391238769, learning_rate: 0.0010000000000000005\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[105], line 26\u001b[0m\n\u001b[1;32m     24\u001b[0m env_to_show \u001b[38;5;241m=\u001b[39m RenderableKeepTheDistance(env_config_show)\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(trainin_steps):\n\u001b[0;32m---> 26\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43malgo\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     27\u001b[0m     \u001b[38;5;66;03m#clear_output()\u001b[39;00m\n\u001b[1;32m     28\u001b[0m     \u001b[38;5;66;03m#print(out)\u001b[39;00m\n\u001b[1;32m     29\u001b[0m     simulate_episode(env_to_show, algo, \u001b[38;5;241m150\u001b[39m, sleep_between_frames\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.03\u001b[39m, print_info\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "File \u001b[0;32m~/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/tune/trainable/trainable.py:328\u001b[0m, in \u001b[0;36mTrainable.train\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    326\u001b[0m start \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m    327\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 328\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    329\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    330\u001b[0m     skipped \u001b[38;5;241m=\u001b[39m skip_exceptions(e)\n",
      "File \u001b[0;32m~/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/rllib/algorithms/algorithm.py:873\u001b[0m, in \u001b[0;36mAlgorithm.step\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    863\u001b[0m     (\n\u001b[1;32m    864\u001b[0m         train_results,\n\u001b[1;32m    865\u001b[0m         eval_results,\n\u001b[1;32m    866\u001b[0m         train_iter_ctx,\n\u001b[1;32m    867\u001b[0m     ) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_run_one_training_iteration_and_evaluation_in_parallel()\n\u001b[1;32m    869\u001b[0m \u001b[38;5;66;03m# - No evaluation necessary, just run the next training iteration.\u001b[39;00m\n\u001b[1;32m    870\u001b[0m \u001b[38;5;66;03m# - We have to evaluate in this training iteration, but no parallelism ->\u001b[39;00m\n\u001b[1;32m    871\u001b[0m \u001b[38;5;66;03m#   evaluate after the training iteration is entirely done.\u001b[39;00m\n\u001b[1;32m    872\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 873\u001b[0m     train_results, train_iter_ctx \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_one_training_iteration\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    875\u001b[0m \u001b[38;5;66;03m# Sequential: Train (already done above), then evaluate.\u001b[39;00m\n\u001b[1;32m    876\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m evaluate_this_iter \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mevaluation_parallel_to_training:\n",
      "File \u001b[0;32m~/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/rllib/algorithms/algorithm.py:3156\u001b[0m, in \u001b[0;36mAlgorithm._run_one_training_iteration\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   3154\u001b[0m             \u001b[38;5;66;03m# Try to train one step.\u001b[39;00m\n\u001b[1;32m   3155\u001b[0m             \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_timers[TRAINING_STEP_TIMER]:\n\u001b[0;32m-> 3156\u001b[0m                 results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3158\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m results, train_iter_ctx\n",
      "File \u001b[0;32m~/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/rllib/algorithms/ppo/ppo.py:428\u001b[0m, in \u001b[0;36mPPO.training_step\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    424\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_training_step_new_api_stack()\n\u001b[1;32m    425\u001b[0m \u001b[38;5;66;03m# Old and hybrid API stacks (Policy, RolloutWorker, Connector, maybe RLModule,\u001b[39;00m\n\u001b[1;32m    426\u001b[0m \u001b[38;5;66;03m# maybe Learner).\u001b[39;00m\n\u001b[1;32m    427\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 428\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_training_step_old_and_hybrid_api_stacks\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/rllib/algorithms/ppo/ppo.py:587\u001b[0m, in \u001b[0;36mPPO._training_step_old_and_hybrid_api_stacks\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    585\u001b[0m     train_results \u001b[38;5;241m=\u001b[39m train_one_step(\u001b[38;5;28mself\u001b[39m, train_batch)\n\u001b[1;32m    586\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 587\u001b[0m     train_results \u001b[38;5;241m=\u001b[39m \u001b[43mmulti_gpu_train_one_step\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    589\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39menable_rl_module_and_learner:\n\u001b[1;32m    590\u001b[0m     \u001b[38;5;66;03m# The train results's loss keys are pids to their loss values. But we also\u001b[39;00m\n\u001b[1;32m    591\u001b[0m     \u001b[38;5;66;03m# return a total_loss key at the same level as the pid keys. So we need to\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    594\u001b[0m     \u001b[38;5;66;03m#  passing medium to infer which policies to update. We could use\u001b[39;00m\n\u001b[1;32m    595\u001b[0m     \u001b[38;5;66;03m#  policies_to_train variable that is given by the user to infer this.\u001b[39;00m\n\u001b[1;32m    596\u001b[0m     policies_to_update \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m(train_results\u001b[38;5;241m.\u001b[39mkeys()) \u001b[38;5;241m-\u001b[39m {ALL_MODULES}\n",
      "File \u001b[0;32m~/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/rllib/execution/train_ops.py:176\u001b[0m, in \u001b[0;36mmulti_gpu_train_one_step\u001b[0;34m(algorithm, train_batch)\u001b[0m\n\u001b[1;32m    171\u001b[0m         permutation \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mpermutation(num_batches)\n\u001b[1;32m    172\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m batch_index \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_batches):\n\u001b[1;32m    173\u001b[0m             \u001b[38;5;66;03m# Learn on the pre-loaded data in the buffer.\u001b[39;00m\n\u001b[1;32m    174\u001b[0m             \u001b[38;5;66;03m# Note: For minibatch SGD, the data is an offset into\u001b[39;00m\n\u001b[1;32m    175\u001b[0m             \u001b[38;5;66;03m# the pre-loaded entire train batch.\u001b[39;00m\n\u001b[0;32m--> 176\u001b[0m             results \u001b[38;5;241m=\u001b[39m \u001b[43mpolicy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearn_on_loaded_batch\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    177\u001b[0m \u001b[43m                \u001b[49m\u001b[43mpermutation\u001b[49m\u001b[43m[\u001b[49m\u001b[43mbatch_index\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mper_device_batch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\n\u001b[1;32m    178\u001b[0m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    180\u001b[0m             learner_info_builder\u001b[38;5;241m.\u001b[39madd_learn_on_batch_results(results, policy_id)\n\u001b[1;32m    182\u001b[0m \u001b[38;5;66;03m# Tower reduce and finalize results.\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/rllib/policy/torch_policy_v2.py:838\u001b[0m, in \u001b[0;36mTorchPolicyV2.learn_on_loaded_batch\u001b[0;34m(self, offset, buffer_index)\u001b[0m\n\u001b[1;32m    835\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    836\u001b[0m         batch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_loaded_batches[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m][offset : offset \u001b[38;5;241m+\u001b[39m device_batch_size]\n\u001b[0;32m--> 838\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearn_on_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    840\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevices) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m    841\u001b[0m     \u001b[38;5;66;03m# Copy weights of main model (tower-0) to all other towers.\u001b[39;00m\n\u001b[1;32m    842\u001b[0m     state_dict \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mstate_dict()\n",
      "File \u001b[0;32m~/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/rllib/utils/threading.py:24\u001b[0m, in \u001b[0;36mwith_lock.<locals>.wrapper\u001b[0;34m(self, *a, **k)\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     23\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[0;32m---> 24\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mk\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     26\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas no attribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_lock\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m e\u001b[38;5;241m.\u001b[39margs[\u001b[38;5;241m0\u001b[39m]:\n",
      "File \u001b[0;32m~/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/rllib/policy/torch_policy_v2.py:715\u001b[0m, in \u001b[0;36mTorchPolicyV2.learn_on_batch\u001b[0;34m(self, postprocessed_batch)\u001b[0m\n\u001b[1;32m    709\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallbacks\u001b[38;5;241m.\u001b[39mon_learn_on_batch(\n\u001b[1;32m    710\u001b[0m     policy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m, train_batch\u001b[38;5;241m=\u001b[39mpostprocessed_batch, result\u001b[38;5;241m=\u001b[39mlearn_stats\n\u001b[1;32m    711\u001b[0m )\n\u001b[1;32m    713\u001b[0m \u001b[38;5;66;03m# Compute gradients (will calculate all losses and `backward()`\u001b[39;00m\n\u001b[1;32m    714\u001b[0m \u001b[38;5;66;03m# them to get the grads).\u001b[39;00m\n\u001b[0;32m--> 715\u001b[0m grads, fetches \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_gradients\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpostprocessed_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    717\u001b[0m \u001b[38;5;66;03m# Step the optimizers.\u001b[39;00m\n\u001b[1;32m    718\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply_gradients(_directStepOptimizerSingleton)\n",
      "File \u001b[0;32m~/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/rllib/utils/threading.py:24\u001b[0m, in \u001b[0;36mwith_lock.<locals>.wrapper\u001b[0;34m(self, *a, **k)\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     23\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[0;32m---> 24\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mk\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     26\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas no attribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_lock\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m e\u001b[38;5;241m.\u001b[39margs[\u001b[38;5;241m0\u001b[39m]:\n",
      "File \u001b[0;32m~/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/rllib/policy/torch_policy_v2.py:933\u001b[0m, in \u001b[0;36mTorchPolicyV2.compute_gradients\u001b[0;34m(self, postprocessed_batch)\u001b[0m\n\u001b[1;32m    930\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lazy_tensor_dict(postprocessed_batch, device\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevices[\u001b[38;5;241m0\u001b[39m])\n\u001b[1;32m    932\u001b[0m \u001b[38;5;66;03m# Do the (maybe parallelized) gradient calculation step.\u001b[39;00m\n\u001b[0;32m--> 933\u001b[0m tower_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_multi_gpu_parallel_grad_calc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mpostprocessed_batch\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    935\u001b[0m all_grads, grad_info \u001b[38;5;241m=\u001b[39m tower_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    937\u001b[0m grad_info[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mallreduce_latency\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_optimizers)\n",
      "File \u001b[0;32m~/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/rllib/policy/torch_policy_v2.py:1429\u001b[0m, in \u001b[0;36mTorchPolicyV2._multi_gpu_parallel_grad_calc\u001b[0;34m(self, sample_batches)\u001b[0m\n\u001b[1;32m   1425\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevices) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_fake_gpus\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[1;32m   1426\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m shard_idx, (model, sample_batch, device) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\n\u001b[1;32m   1427\u001b[0m         \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_gpu_towers, sample_batches, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevices)\n\u001b[1;32m   1428\u001b[0m     ):\n\u001b[0;32m-> 1429\u001b[0m         \u001b[43m_worker\u001b[49m\u001b[43m(\u001b[49m\u001b[43mshard_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_batch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1430\u001b[0m         \u001b[38;5;66;03m# Raise errors right away for better debugging.\u001b[39;00m\n\u001b[1;32m   1431\u001b[0m         last_result \u001b[38;5;241m=\u001b[39m results[\u001b[38;5;28mlen\u001b[39m(results) \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m]\n",
      "File \u001b[0;32m~/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/rllib/policy/torch_policy_v2.py:1348\u001b[0m, in \u001b[0;36mTorchPolicyV2._multi_gpu_parallel_grad_calc.<locals>._worker\u001b[0;34m(shard_idx, model, sample_batch, device)\u001b[0m\n\u001b[1;32m   1343\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1344\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m NullContextManager() \u001b[38;5;28;01mif\u001b[39;00m device\u001b[38;5;241m.\u001b[39mtype \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mdevice(  \u001b[38;5;66;03m# noqa: E501\u001b[39;00m\n\u001b[1;32m   1345\u001b[0m         device\n\u001b[1;32m   1346\u001b[0m     ):\n\u001b[1;32m   1347\u001b[0m         loss_out \u001b[38;5;241m=\u001b[39m force_list(\n\u001b[0;32m-> 1348\u001b[0m             \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdist_class\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1349\u001b[0m         )\n\u001b[1;32m   1351\u001b[0m         \u001b[38;5;66;03m# Call Model's custom-loss with Policy loss outputs and\u001b[39;00m\n\u001b[1;32m   1352\u001b[0m         \u001b[38;5;66;03m# train_batch.\u001b[39;00m\n\u001b[1;32m   1353\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(model, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcustom_loss\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m~/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/rllib/algorithms/ppo/ppo_torch_policy.py:84\u001b[0m, in \u001b[0;36mPPOTorchPolicy.loss\u001b[0;34m(self, model, dist_class, train_batch)\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;129m@override\u001b[39m(TorchPolicyV2)\n\u001b[1;32m     67\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mloss\u001b[39m(\n\u001b[1;32m     68\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     71\u001b[0m     train_batch: SampleBatch,\n\u001b[1;32m     72\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Union[TensorType, List[TensorType]]:\n\u001b[1;32m     73\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Compute loss for Proximal Policy Objective.\u001b[39;00m\n\u001b[1;32m     74\u001b[0m \n\u001b[1;32m     75\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     81\u001b[0m \u001b[38;5;124;03m        The PPO loss tensor given the input batch.\u001b[39;00m\n\u001b[1;32m     82\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 84\u001b[0m     logits, state \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     85\u001b[0m     curr_action_dist \u001b[38;5;241m=\u001b[39m dist_class(logits, model)\n\u001b[1;32m     87\u001b[0m     \u001b[38;5;66;03m# RNN case: Mask away 0-padded chunks at end of time axis.\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/rllib/models/modelv2.py:244\u001b[0m, in \u001b[0;36mModelV2.__call__\u001b[0;34m(self, input_dict, state, seq_lens)\u001b[0m\n\u001b[1;32m    238\u001b[0m     restored[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mobs_flat\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m input_dict[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mobs\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m    239\u001b[0m \u001b[38;5;66;03m# Input to this Model went through a Preprocessor.\u001b[39;00m\n\u001b[1;32m    240\u001b[0m \u001b[38;5;66;03m# Generate extra keys: \"obs_flat\" (vs \"obs\", which will hold the\u001b[39;00m\n\u001b[1;32m    241\u001b[0m \u001b[38;5;66;03m# original obs).\u001b[39;00m\n\u001b[1;32m    242\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    243\u001b[0m     restored[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mobs\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m restore_original_dimensions(\n\u001b[0;32m--> 244\u001b[0m         \u001b[43minput_dict\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mobs\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobs_space, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mframework\n\u001b[1;32m    245\u001b[0m     )\n\u001b[1;32m    246\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    247\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(input_dict[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mobs\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mshape) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m2\u001b[39m:\n",
      "File \u001b[0;32m~/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/rllib/policy/sample_batch.py:953\u001b[0m, in \u001b[0;36mSampleBatch.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    951\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_interceptor \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    952\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m key \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mintercepted_values:\n\u001b[0;32m--> 953\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mintercepted_values[key] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_interceptor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    954\u001b[0m     value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mintercepted_values[key]\n\u001b[1;32m    955\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m value\n",
      "File \u001b[0;32m~/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/rllib/utils/torch_utils.py:276\u001b[0m, in \u001b[0;36mconvert_to_torch_tensor\u001b[0;34m(x, device)\u001b[0m\n\u001b[1;32m    272\u001b[0m         tensor \u001b[38;5;241m=\u001b[39m tensor\u001b[38;5;241m.\u001b[39mfloat()\n\u001b[1;32m    274\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m tensor \u001b[38;5;28;01mif\u001b[39;00m device \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m tensor\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m--> 276\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtree\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap_structure\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmapping\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/tianEnv/lib/python3.11/site-packages/tree/__init__.py:435\u001b[0m, in \u001b[0;36mmap_structure\u001b[0;34m(func, *structures, **kwargs)\u001b[0m\n\u001b[1;32m    432\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m other \u001b[38;5;129;01min\u001b[39;00m structures[\u001b[38;5;241m1\u001b[39m:]:\n\u001b[1;32m    433\u001b[0m   assert_same_structure(structures[\u001b[38;5;241m0\u001b[39m], other, check_types\u001b[38;5;241m=\u001b[39mcheck_types)\n\u001b[1;32m    434\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m unflatten_as(structures[\u001b[38;5;241m0\u001b[39m],\n\u001b[0;32m--> 435\u001b[0m                     \u001b[43m[\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mzip\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mmap\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mflatten\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstructures\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m)\n",
      "File \u001b[0;32m~/anaconda3/envs/tianEnv/lib/python3.11/site-packages/tree/__init__.py:435\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    432\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m other \u001b[38;5;129;01min\u001b[39;00m structures[\u001b[38;5;241m1\u001b[39m:]:\n\u001b[1;32m    433\u001b[0m   assert_same_structure(structures[\u001b[38;5;241m0\u001b[39m], other, check_types\u001b[38;5;241m=\u001b[39mcheck_types)\n\u001b[1;32m    434\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m unflatten_as(structures[\u001b[38;5;241m0\u001b[39m],\n\u001b[0;32m--> 435\u001b[0m                     [\u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m args \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mmap\u001b[39m(flatten, structures))])\n",
      "File \u001b[0;32m~/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/rllib/utils/torch_utils.py:265\u001b[0m, in \u001b[0;36mconvert_to_torch_tensor.<locals>.mapping\u001b[0;34m(item)\u001b[0m\n\u001b[1;32m    262\u001b[0m             tensor \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mfrom_numpy(item)\n\u001b[1;32m    263\u001b[0m     \u001b[38;5;66;03m# Already numpy: Wrap as torch tensor.\u001b[39;00m\n\u001b[1;32m    264\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 265\u001b[0m         tensor \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_numpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mitem\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    266\u001b[0m \u001b[38;5;66;03m# Everything else: Convert to numpy, then wrap as torch tensor.\u001b[39;00m\n\u001b[1;32m    267\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    268\u001b[0m     tensor \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mfrom_numpy(np\u001b[38;5;241m.\u001b[39masarray(item))\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from ray.rllib.algorithms.ppo import PPOConfig\n",
    "from ray.tune.logger import pretty_print\n",
    "\n",
    "from gymnasium.wrappers.time_limit import TimeLimit\n",
    "\n",
    "trainin_steps = 40\n",
    "\n",
    "algo = (\n",
    "    PPOConfig()\n",
    "    .training(gamma = 0.95, \n",
    "              lr = 0.001,\n",
    "              train_batch_size = train_batch_size, \n",
    "              sgd_minibatch_size = 256, \n",
    "              num_sgd_iter = 30,\n",
    "              #entropy_coeff=0.005,\n",
    "              )\n",
    "    .env_runners(num_env_runners=1)\n",
    "    .resources(num_gpus=0)\n",
    "    .environment(env=\"KeepTheDistance?dst=0&agent=4&visible_nbrs=3&spawn_area=100\")\n",
    "    .build()\n",
    ")\n",
    "clear_output()\n",
    "\n",
    "env_to_show = RenderableKeepTheDistance(env_config_show)\n",
    "for i in range(trainin_steps):\n",
    "    result = algo.train()\n",
    "    #clear_output()\n",
    "    #print(out)\n",
    "    simulate_episode(env_to_show, algo, 150, sleep_between_frames=0.03, print_info=False)\n",
    "    print(ppo_result_format(result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5066be2dfd4447a39660264441ee19cb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "CanvasWithBorders(height=300, width=300)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "env_config_2 = EnvironmentConfiguration(n_agents=4, visible_nbrs=3, target_distance=0, max_steps=500, speed=1, spawn_area=100)\n",
    "simulate_episode(RenderableKeepTheDistance(env_config_2), algo, 300, sleep_between_frames=0.03, print_info=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_algo(algo, \"KeepTheDistance?dst=0&agent=3&visible_nbrs=2&spawn_area=100\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tianEnv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
