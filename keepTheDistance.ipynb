{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Keep average distance\n",
    "\n",
    "the agents goal is to position close to each others at a distance previously defined\n",
    "\n",
    "challenges:\n",
    "- deal with continuous space environment\n",
    "- limited vision of an agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ipycanvas import Canvas, hold_canvas\n",
    "\n",
    "class CanvasWithBorders(Canvas):\n",
    "    def clear(self):\n",
    "        super().clear()\n",
    "        border_color = 'black'  # You can customize the border color here\n",
    "        border_width = 1  # You can customize the border width here\n",
    "        \n",
    "        self.fill_style = border_color\n",
    "        # Draw top border\n",
    "        self.fill_rect(0, 0, self.width, border_width)\n",
    "        # Draw bottom border\n",
    "        self.fill_rect(0, self.height - border_width, self.width, border_width)\n",
    "        # Draw left border\n",
    "        self.fill_rect(0, 0, border_width, self.height)\n",
    "        # Draw right border\n",
    "        self.fill_rect(self.width - border_width, 0, border_width, self.height)\n",
    "\n",
    "import os\n",
    "from ray.rllib.algorithms.algorithm import Algorithm\n",
    "\n",
    "def save_algo(algo, name):\n",
    "    base_dir = os.path.join(os.getcwd(), \"algos\")\n",
    "    subfolder_path = os.path.join(base_dir, name)\n",
    "    os.makedirs(subfolder_path, exist_ok=True)\n",
    "    path_to_checkpoint  = algo.save(subfolder_path)\n",
    "    print(f\"An Algorithm checkpoint has been created inside directory: '{path_to_checkpoint}'.\")\n",
    "\n",
    "def load_algo(name):\n",
    "    base_dir = os.path.join(os.getcwd(), \"algos\")\n",
    "    subfolder_path = os.path.join(base_dir, name)\n",
    "    if not os.path.exists(subfolder_path):\n",
    "        raise FileNotFoundError(f\"The specified subfolder '{subfolder_path}' does not exist.\")\n",
    "    \n",
    "    return Algorithm.from_checkpoint(subfolder_path)\n",
    "\n",
    "#save_algo(algo, \"KeepTheDistance_dst=0_agent=2_100x100train\")\n",
    "#algo2 = load_algo(\"KeepTheDistance_dst=0_agent=2_100x100train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import random as rnd\n",
    "\n",
    "def simulate_episode(env, policy, steps, sleep_between_frames=0.3, print_info=True):\n",
    "    obs, _ = env.reset()\n",
    "    env.render()\n",
    "\n",
    "    for i in range(steps):\n",
    "        if print_info:\n",
    "            print(f\"obs: \", obs)\n",
    "        actions = policy.compute_actions(obs)\n",
    "        #actions = {agent: np.array([rnd.random()*2-1, rnd.random()*2-1, 1.0], np.float32) for agent in obs.keys()}\n",
    "        #actions = {agent: env.action_space.sample() for agent in obs.keys()}\n",
    "        obs, reward, _, _, _ = env.step(actions)\n",
    "        env.render()\n",
    "        if print_info:\n",
    "            print(f\"action: \", actions)\n",
    "            print(f\"reward: \", reward, \"\\n\")\n",
    "        time.sleep(sleep_between_frames)\n",
    "\n",
    "def simulate_random_episode(env, steps, sleep_between_frames=0.3, print_info=True):\n",
    "    obs, _ = env.reset()\n",
    "    env.render()\n",
    "\n",
    "    for i in range(steps):\n",
    "        if print_info:\n",
    "            print(f\"obs: \", obs)\n",
    "        actions = {agent: np.array([rnd.random()*2-1, rnd.random()*2-1, rnd.random()], np.float32) for agent in obs.keys()}\n",
    "        obs, reward, _, _, _ = env.step(actions)\n",
    "        env.render()\n",
    "        if print_info:\n",
    "            print(f\"action: \", actions)\n",
    "            print(f\"reward: \", reward, \"\\n\")\n",
    "        time.sleep(sleep_between_frames)\n",
    "\n",
    "def ppo_result_format(result):\n",
    "    return (f\"iteration [{result['training_iteration']}] => \" +\n",
    "          f\"episode_reward_mean: {result['sampler_results']['episode_reward_mean']}, \" +\n",
    "          f\"episode_len_mean: {result['sampler_results']['episode_len_mean']}, \" +\n",
    "          f\"agent_steps_trained: {result['info']['num_agent_steps_trained']}, \" +\n",
    "          f\"env_steps_trained: {result['info']['num_env_steps_trained']}, \" + \n",
    "          f\"entropy: {result['info']['learner']['default_policy']['learner_stats']['entropy']}, \" +\n",
    "          f\"learning_rate: {result['info']['learner']['default_policy']['learner_stats']['cur_lr']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### environment definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Set\n",
    "from ray.rllib.env.multi_agent_env import MultiAgentEnv\n",
    "import random as rnd\n",
    "from gymnasium.spaces import Discrete, Box, Dict, Tuple\n",
    "from gymnasium.spaces.utils import flatten, flatten_space\n",
    "import numpy as np\n",
    "from IPython.display import clear_output\n",
    "import math\n",
    "from ipycanvas import Canvas, hold_canvas\n",
    "\n",
    "class EnvironmentConfiguration: \n",
    "    def __init__(self, n_agents, target_distance, speed, spawn_area=100, visible_nbrs=1, max_steps=None, spawn_area_schedule=None):\n",
    "        self.n_agents = n_agents\n",
    "        self.visible_nbrs = visible_nbrs\n",
    "        self.target_distance = target_distance\n",
    "        self.max_steps = max_steps\n",
    "        self.speed = speed\n",
    "        self.spawn_area = spawn_area\n",
    "        self.spawn_area_schedule = spawn_area_schedule\n",
    "\n",
    "class KeepTheDistance(MultiAgentEnv):\n",
    "\n",
    "    canvas = None\n",
    "    CANVAS_WIDTH, CANVAS_HEIGHT = 300.0, 300.0\n",
    "\n",
    "    def __init__(self, config: EnvironmentConfiguration):\n",
    "        assert config.n_agents > config.visible_nbrs # just base case implemented \n",
    "             \n",
    "        self.n_agents = config.n_agents\n",
    "        self.visible_nbrs = config.visible_nbrs\n",
    "        self.target_distance = config.target_distance\n",
    "        self.max_steps = config.max_steps\n",
    "        self.speed = config.speed\n",
    "        self.spawn_area = config.spawn_area\n",
    "        self.spawn_area_schedule = config.spawn_area_schedule\n",
    "        if self.spawn_area_schedule != None:\n",
    "            self.spawn_area_schedule_index = 0\n",
    "            self.n_reset = 0\n",
    "            self.spawn_area = self.spawn_area_schedule[0][1]\n",
    "        \n",
    "        self.agents_ids = ['agent-' + str(i) for i in range(self.n_agents)]\n",
    "        self.agent_colors = {agent: self.rgb_to_hex(rnd.randint(0, 255), rnd.randint(0, 255), rnd.randint(0, 255)) for agent in self.agents_ids}\n",
    "        self.observation_space = self.observation_space('agent-0')\n",
    "        self.action_space = self.action_space(\"\")\n",
    "\n",
    "    def unflatten_observation_space(self, agent):\n",
    "        #distance_vector = Box(low=-np.inf, high=np.inf, shape=(2,1), dtype=np.float32)\n",
    "        #obs_space = Dict({\"nbr-1\": distance_vector})\n",
    "        direction = Box(low=-1, high=1, shape=(2,1), dtype=np.float32)\n",
    "        distance = Box(low=-np.inf, high=np.inf, shape=(1,1), dtype=np.float32)\n",
    "        return Dict({f\"nbr-{i}\": Dict({'direction': direction, 'distance': distance}) for i in range(self.visible_nbrs)})\n",
    "\n",
    "    def observation_space(self, agent):\n",
    "        return flatten_space(self.unflatten_observation_space(agent))\n",
    "\n",
    "    def action_space(self, agent):\n",
    "        direction = Box(low=-1.0, high=1.0, shape=(2,1), dtype=np.float32)\n",
    "        speed = Box(0.0, 1.0, dtype=np.float32)\n",
    "        return flatten_space(Tuple([direction, speed]))\n",
    "    \n",
    "    def __get_random_point(self, max_x, max_y, min_x=0, min_y=0):\n",
    "        return (rnd.randint(min_x, max_x-1), rnd.randint(min_y, max_y-1))\n",
    "    \n",
    "    def __get_observation(self, agent):\n",
    "        distance_vectors = [self.__distance_vector_between(agent, nbr) \n",
    "                            for nbr in self.__get_n_closest_neighbours(agent, self.visible_nbrs)]\n",
    "\n",
    "        obs = {\n",
    "            f\"nbr-{i}\": {\n",
    "                \"direction\": self.__compute_unit_vector(distance_vectors[i]),\n",
    "                \"distance\": self.__compute_norm(distance_vectors[i])\n",
    "            }\n",
    "            for i in range(len(distance_vectors))\n",
    "            }\n",
    "        return flatten(self.unflatten_observation_space(agent), obs)\n",
    "\n",
    "    def rgb_to_hex(self, r, g, b):\n",
    "        return f'#{r:02x}{g:02x}{b:02x}'\n",
    "\n",
    "    def __distance_vector_between(self, agent1, agent2):\n",
    "        agent1_pos = self.agents_pos[agent1]\n",
    "        agent2_pos = self.agents_pos[agent2]\n",
    "        return (agent1_pos[0]-agent2_pos[0], agent1_pos[1]-agent2_pos[1])\n",
    "    \n",
    "    def __distance_between(self, agent1, agent2):\n",
    "        return self.__compute_norm(self.__distance_vector_between(agent1, agent2))\n",
    "\n",
    "    def __compute_norm(self, vector):\n",
    "        return math.sqrt(math.pow(vector[0], 2) + math.pow(vector[1], 2))\n",
    "    \n",
    "    def __compute_unit_vector(self, vector):\n",
    "        norm = self.__compute_norm(vector)\n",
    "        if norm == 0:\n",
    "            return [0,0]\n",
    "        return [vector[0]/norm, vector[1]/norm]\n",
    "\n",
    "    def __total_distance_from_closest_neighbours(self, agent):\n",
    "        return sum([abs(self.__distance_between(agent, nbr) - self.target_distance) for nbr in self.__get_n_closest_neighbours(agent, self.visible_nbrs)])\n",
    "\n",
    "    def __get_local_reward(self, agent, action):\n",
    "        newDistance = self.__total_distance_from_closest_neighbours(agent)\n",
    "        reward_1 = self.last_step_distances[agent] - newDistance\n",
    "        self.last_step_distances[agent] = newDistance\n",
    "\n",
    "        closest_nbrs = self.__get_n_closest_neighbours(agent, self.visible_nbrs)\n",
    "        reward_2 = sum([100 if abs(self.__distance_between(agent, nbr) - self.target_distance) < 0.5 else 0 for nbr in closest_nbrs])\n",
    "\n",
    "        return reward_1 + reward_2 # working for two agents using value for reward_2 equals to one\n",
    "\n",
    "    def __get_global_reward(self):\n",
    "        return 0\n",
    "    \n",
    "    def __get_other_agents(self, agent):\n",
    "        return [other for other in self.agents_ids if other != agent]\n",
    "\n",
    "    def __get_n_closest_neighbours(self, agent, n=1):\n",
    "        distances = {other: self.__distance_between(agent, other) for other in self.__get_other_agents(agent)}\n",
    "        return [neighbour[0] for neighbour in sorted(list(distances.items()), key=lambda d: d[1])[:n]]\n",
    "        # return {neighbour[0]: neighbour[1] for neighbour in sorted(list(dst.items()), key=lambda d: d[0])[:n]}\n",
    "\n",
    "    def __update_agent_position(self, agent, action):\n",
    "        unit_movement = self.__compute_unit_vector([action[0], action[1]])\n",
    "        self.agents_pos[agent] = (self.agents_pos[agent][0] + unit_movement[0]*action[2]*self.speed, \n",
    "                                 self.agents_pos[agent][1] + unit_movement[1]*action[2]*self.speed)\n",
    "\n",
    "    def reset(self, seed=None, options=None):\n",
    "        if self.spawn_area_schedule != None:\n",
    "            self.n_reset += 1\n",
    "            if (self.spawn_area_schedule_index < len(self.spawn_area_schedule)-1 and \n",
    "                self.n_reset >= self.spawn_area_schedule[self.spawn_area_schedule_index+1][0]):\n",
    "                self.spawn_area_schedule_index += 1\n",
    "                self.spawn_area = self.spawn_area_schedule[self.spawn_area_schedule_index][1]\n",
    "\n",
    "        self.steps = 0\n",
    "        self.agents_pos = {agent: self.__get_random_point(max_x=self.spawn_area, max_y=self.spawn_area) for agent in self.agents_ids}\n",
    "        self.last_step_distances = {agent: self.__total_distance_from_closest_neighbours(agent) for agent in self.agents_ids}\n",
    "        return {agent: self.__get_observation(agent) for agent in self.agents_ids}, {}\n",
    "     \n",
    "    def step(self, actions):\n",
    "        self.steps += 1\n",
    "        observations, rewards, terminated, truncated, infos = {}, {}, {}, {}, {}\n",
    "\n",
    "        for agent, action in actions.items():\n",
    "            self.__update_agent_position(agent, action)\n",
    "\n",
    "        for agent, action in actions.items():\n",
    "            observations[agent] = self.__get_observation(agent)\n",
    "            rewards[agent] = self.__get_local_reward(agent, action) + self.__get_global_reward()\n",
    "            terminated[agent] = False\n",
    "            truncated[agent] = False\n",
    "            infos[agent] = {}\n",
    "\n",
    "        truncated['__all__'] = False\n",
    "        if self.max_steps != None and self.steps == self.max_steps:\n",
    "            terminated['__all__'] = True\n",
    "        else:\n",
    "            terminated['__all__'] = False\n",
    "\n",
    "        return observations, rewards, terminated, truncated, infos\n",
    "     \n",
    "    def rgb_to_hex(self, r, g, b):\n",
    "        return f'#{r:02x}{g:02x}{b:02x}'\n",
    "\n",
    "    def render(self):\n",
    "        pass\n",
    "\n",
    "    def get_agent_ids(self):\n",
    "       return self.agents\n",
    "\n",
    "\n",
    "class RenderableKeepTheDistance(KeepTheDistance):\n",
    "    def render(self):\n",
    "        if self.canvas is None:\n",
    "            self.canvas = CanvasWithBorders(width=self.CANVAS_WIDTH, height=self.CANVAS_HEIGHT)\n",
    "            display(self.canvas)\n",
    "        \n",
    "        with hold_canvas():\n",
    "            agent_size = max(self.CANVAS_WIDTH/float(self.spawn_area),1)\n",
    "            top_left = (0.0,0.0)\n",
    "            bottom_right = (self.spawn_area, self.spawn_area)\n",
    "            self.canvas.clear()\n",
    "\n",
    "            for agent in self.agents_ids:\n",
    "                raw_pos = self.agents_pos[agent]\n",
    "                color = self.agent_colors[agent]\n",
    "                \n",
    "                agent_pos_in_frame = [((raw_pos[0]-top_left[0])/(bottom_right[0]-top_left[0]))*self.CANVAS_WIDTH,\n",
    "                            ((raw_pos[1]-top_left[1])/(bottom_right[1]-top_left[1]))*self.CANVAS_HEIGHT,]\n",
    "\n",
    "                self.canvas.fill_style = color\n",
    "                self.canvas.fill_circle(\n",
    "                    agent_pos_in_frame[0],\n",
    "                    agent_pos_in_frame[1],\n",
    "                    agent_size/2.0\n",
    "                )\n",
    "                \n",
    "                self.canvas.stroke_style = \"black\"\n",
    "                self.canvas.stroke_circle(\n",
    "                    agent_pos_in_frame[0],\n",
    "                    agent_pos_in_frame[1],\n",
    "                    agent_size/2.0\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "67bd7f3eea1b478592d4cf4dca156d03",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "CanvasWithBorders(height=300, width=300)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "obs:  {'agent-0': array([ 0.        , -1.        ,  1.        ,  0.        ,  1.        ,\n",
      "        2.        , -0.70710677,  0.70710677,  4.2426405 ], dtype=float32), 'agent-1': array([ 0. ,  1. ,  1. ,  0. ,  1. ,  3. , -0.6,  0.8,  5. ],\n",
      "      dtype=float32), 'agent-2': array([ 0.9486833 , -0.31622776,  3.1622777 ,  0.70710677, -0.70710677,\n",
      "        4.2426405 ,  0.6       , -0.8       ,  5.        ], dtype=float32), 'agent-3': array([ 0.        , -1.        ,  2.        ,  0.        , -1.        ,\n",
      "        3.        , -0.9486833 ,  0.31622776,  3.1622777 ], dtype=float32)}\n",
      "action:  {'agent-0': array([0.6097982 , 0.07546367, 0.44884953], dtype=float32), 'agent-1': array([-0.46585688,  0.34105814,  0.41662952], dtype=float32), 'agent-2': array([ 0.01434647, -0.14281936,  0.43751654], dtype=float32), 'agent-3': array([-0.9751907 , -0.89755845,  0.21460941], dtype=float32)}\n",
      "reward:  {'agent-0': -0.8149595920210837, 'agent-1': -1.5947346764162305, 'agent-2': -1.1722251508729116, 'agent-3': -0.9672797051154536} \n",
      "\n",
      "obs:  {'agent-0': array([ 0.54867357, -0.8360367 ,  1.4245625 ,  0.26443577,  0.9644033 ,\n",
      "        2.281682  , -0.59711915,  0.8021526 ,  4.3513556 ], dtype=float32), 'agent-1': array([-0.54867357,  0.8360367 ,  1.4245625 , -0.05248958,  0.99862146,\n",
      "        3.3961298 , -0.5853607 ,  0.810773  ,  5.7740426 ], dtype=float32), 'agent-2': array([ 0.9275412 , -0.3737209 ,  3.4517455 ,  0.59711915, -0.8021526 ,\n",
      "        4.3513556 ,  0.5853607 , -0.810773  ,  5.7740426 ], dtype=float32), 'agent-3': array([-0.26443577, -0.9644033 ,  2.281682  ,  0.05248958, -0.99862146,\n",
      "        3.3961298 , -0.9275412 ,  0.3737209 ,  3.4517455 ], dtype=float32)}\n",
      "action:  {'agent-0': array([0.7870553 , 0.940989  , 0.33371508], dtype=float32), 'agent-1': array([-0.5104738 , -0.26349083,  0.03331013], dtype=float32), 'agent-2': array([-0.6789299 ,  0.40367338,  0.81116486], dtype=float32), 'agent-3': array([-0.8867922 , -0.17193972,  0.41132894], dtype=float32)}\n",
      "reward:  {'agent-0': 0.13201811099498073, 'agent-1': 0.714266156963614, 'agent-2': 1.784004688672825, 'agent-3': -0.15954910026288793} \n",
      "\n",
      "obs:  {'agent-0': array([ 0.74439853, -0.66773564,  1.3773851 ,  0.4340589 ,  0.9008845 ,\n",
      "        2.813607  , -0.45170736,  0.8921662 ,  3.73459   ], dtype=float32), 'agent-1': array([-0.74439853,  0.66773564,  1.3773851 ,  0.05663201,  0.99839514,\n",
      "        3.460017  , -0.53782064,  0.8430593 ,  5.0430665 ], dtype=float32), 'agent-2': array([ 0.96442705, -0.26434904,  3.0154824 ,  0.45170736, -0.8921662 ,\n",
      "        3.73459   ,  0.53782064, -0.8430593 ,  5.0430665 ], dtype=float32), 'agent-3': array([-0.4340589 , -0.9008845 ,  2.813607  , -0.96442705,  0.26434904,\n",
      "        3.0154824 , -0.05663201, -0.99839514,  3.460017  ], dtype=float32)}\n",
      "action:  {'agent-0': array([-0.24569097,  0.11857177,  0.78684694], dtype=float32), 'agent-1': array([ 0.43118402, -0.78147584,  0.46395096], dtype=float32), 'agent-2': array([-0.9794071 ,  0.4878066 ,  0.24335444], dtype=float32), 'agent-3': array([-0.09813718, -0.9554341 ,  0.3097529 ], dtype=float32)}\n",
      "reward:  {'agent-0': 100.32179936724756, 'agent-1': 101.9264525227038, 'agent-2': 0.49430029374281936, 'agent-3': -0.07706394107128922} \n",
      "\n",
      "obs:  {'agent-0': array([ 0.4748768 , -0.88005227,  0.19489369,  0.1684545 ,  0.9857094 ,\n",
      "        3.2310328 , -0.52125996,  0.85339797,  4.1778564 ], dtype=float32), 'agent-1': array([-0.4748768 ,  0.88005227,  0.19489369,  0.13338639,  0.99106413,\n",
      "        3.3866389 , -0.5192243 ,  0.854638  ,  4.3724837 ], dtype=float32), 'agent-2': array([ 0.9903702 , -0.13844444,  2.7484987 ,  0.52125996, -0.85339797,\n",
      "        4.1778564 ,  0.5192243 , -0.854638  ,  4.3724837 ], dtype=float32), 'agent-3': array([-0.9903702 ,  0.13844444,  2.7484987 , -0.1684545 , -0.9857094 ,\n",
      "        3.2310328 , -0.13338639, -0.99106413,  3.3866389 ], dtype=float32)}\n",
      "action:  {'agent-0': array([0.0604791, 0.754768 , 0.7182599], dtype=float32), 'agent-1': array([-0.6507463,  0.9828005,  0.5202364], dtype=float32), 'agent-2': array([0.7354497 , 0.8218264 , 0.08220741], dtype=float32), 'agent-3': array([0.7232874 , 0.25657296, 0.36080578], dtype=float32)}\n",
      "reward:  {'agent-0': 98.61592334254865, 'agent-1': 98.95993334416566, 'agent-2': -0.7944837323734415, 'agent-3': -0.5729774298892956} \n",
      "\n",
      "obs:  {'agent-0': array([ 0.9694091 ,  0.24545062,  0.45092744,  0.06903942,  0.9976139 ,\n",
      "        3.7892413 , -0.45815966,  0.88886994,  4.7476907 ], dtype=float32), 'agent-1': array([-0.9694091 , -0.24545062,  0.45092744, -0.04777892,  0.9988579 ,\n",
      "        3.673715  , -0.5364751 ,  0.8439161 ,  4.86944   ], dtype=float32), 'agent-2': array([ 0.9840949 , -0.1776436 ,  2.4761915 ,  0.45815966, -0.88886994,\n",
      "        4.7476907 ,  0.5364751 , -0.8439161 ,  4.86944   ], dtype=float32), 'agent-3': array([-0.9840949 ,  0.1776436 ,  2.4761915 ,  0.04777892, -0.9988579 ,\n",
      "        3.673715  , -0.06903942, -0.9976139 ,  3.7892413 ], dtype=float32)}\n",
      "action:  {'agent-0': array([-0.917891  ,  0.49370039,  0.3864733 ], dtype=float32), 'agent-1': array([-7.2196269e-01,  2.5201048e-04,  6.2030512e-01], dtype=float32), 'agent-2': array([-0.3574879 , -0.97159135,  0.61959964], dtype=float32), 'agent-3': array([-0.30371907,  0.77999926,  0.9789661 ], dtype=float32)}\n",
      "reward:  {'agent-0': -0.34084474612540205, 'agent-1': -0.15164642820723095, 'agent-2': -2.1980915090924604, 'agent-3': 0.8605342536527374} \n",
      "\n",
      "obs:  {'agent-0': array([ 0.92546356,  0.3788366 ,  0.77482766,  0.09024204,  0.9959199 ,\n",
      "        3.0635202 , -0.41920975,  0.9078894 ,  5.4903564 ], dtype=float32), 'agent-1': array([-0.92546356, -0.3788366 ,  0.77482766, -0.15778744,  0.9874731 ,\n",
      "        2.7924685 , -0.5411351 ,  0.8409357 ,  5.578433  ], dtype=float32), 'agent-2': array([ 0.79999053, -0.60001266,  3.2226248 ,  0.41920975, -0.9078894 ,\n",
      "        5.4903564 ,  0.5411351 , -0.8409357 ,  5.578433  ], dtype=float32), 'agent-3': array([ 0.15778744, -0.9874731 ,  2.7924685 , -0.09024204, -0.9959199 ,\n",
      "        3.0635202 , -0.79999053,  0.60001266,  3.2226248 ], dtype=float32)}\n",
      "action:  {'agent-0': array([-0.01795474, -0.58889484,  0.13317482], dtype=float32), 'agent-1': array([ 0.47033545, -0.07294189,  0.791493  ], dtype=float32), 'agent-2': array([ 0.02725152, -0.837873  ,  0.4655959 ], dtype=float32), 'agent-3': array([ 0.10155346, -0.29729342,  0.47201148], dtype=float32)}\n",
      "reward:  {'agent-0': 99.87060105923322, 'agent-1': 100.25131940847632, 'agent-2': -0.15098973540006, 'agent-3': -0.5021384137449445} \n",
      "\n",
      "obs:  {'agent-0': array([-0.23830599,  0.9711901 ,  0.2900757 ,  0.03558971,  0.99936646,\n",
      "        3.366711  , -0.40004793,  0.91649425,  5.8013167 ], dtype=float32), 'agent-1': array([ 0.23830599, -0.9711901 ,  0.2900757 ,  0.06117475,  0.99812704,\n",
      "        3.088644  , -0.4082314 ,  0.91287845,  5.51569   ], dtype=float32), 'agent-2': array([ 0.7809008 , -0.6246551 ,  3.125397  ,  0.4082314 , -0.91287845,\n",
      "        5.51569   ,  0.40004793, -0.91649425,  5.8013167 ], dtype=float32), 'agent-3': array([-0.06117475, -0.99812704,  3.088644  , -0.7809008 ,  0.6246551 ,\n",
      "        3.125397  , -0.03558971, -0.99936646,  3.366711  ], dtype=float32)}\n",
      "action:  {'agent-0': array([-0.83912396, -0.8055375 ,  0.64344865], dtype=float32), 'agent-1': array([-0.6787503,  0.5967583,  0.0677982], dtype=float32), 'agent-2': array([0.5449346 , 0.21356766, 0.1648424 ], dtype=float32), 'agent-3': array([ 0.04706398, -0.18377328,  0.5588724 ], dtype=float32)}\n",
      "reward:  {'agent-0': -0.1989059297166147, 'agent-1': -0.8886140254616581, 'agent-2': 0.4140299693726117, 'agent-3': -0.38465124970819886} \n",
      "\n",
      "obs:  {'agent-0': array([-0.91782326, -0.39698923,  0.52558184, -0.1382437 ,  0.9903982 ,\n",
      "        3.4939234 , -0.52123475,  0.85341334,  5.6375036 ], dtype=float32), 'agent-1': array([ 9.1782326e-01,  3.9698923e-01,  5.2558184e-01, -1.6944107e-04,\n",
      "        1.0000000e+00,  3.6690259e+00, -4.3949336e-01,  8.9824587e-01,\n",
      "        5.5884161e+00], dtype=float32), 'agent-2': array([ 0.87617856, -0.48198667,  2.8024538 ,  0.43949336, -0.8982459 ,\n",
      "        5.588416  ,  0.52123475, -0.85341334,  5.6375036 ], dtype=float32), 'agent-3': array([-8.7617856e-01,  4.8198667e-01,  2.8024538e+00,  1.3824371e-01,\n",
      "       -9.9039823e-01,  3.4939234e+00,  1.6944107e-04, -1.0000000e+00,\n",
      "        3.6690259e+00], dtype=float32)}\n",
      "action:  {'agent-0': array([ 0.93230397, -0.5532964 ,  0.84417796], dtype=float32), 'agent-1': array([ 0.03013292, -0.7161439 ,  0.8225245 ], dtype=float32), 'agent-2': array([0.57992166, 0.7157657 , 0.5801984 ], dtype=float32), 'agent-3': array([-0.31705922, -0.8774808 ,  0.07624751], dtype=float32)}\n",
      "reward:  {'agent-0': 101.56726122968736, 'agent-1': 101.91632016688588, 'agent-2': 1.6943734036734934, 'agent-3': 0.9683206739054384} \n",
      "\n",
      "obs:  {'agent-0': array([ 0.7535676 ,  0.65737045,  0.27733317,  0.08636916,  0.9962632 ,\n",
      "        3.1128814 , -0.5485121 ,  0.8361426 ,  4.6995335 ], dtype=float32), 'agent-1': array([-0.7535676 , -0.65737045,  0.27733317,  0.02050576,  0.9997897 ,\n",
      "        2.9195523 , -0.5967556 ,  0.802423  ,  4.669818  ], dtype=float32), 'agent-2': array([ 0.9601838, -0.279369 ,  2.9646487,  0.5967556, -0.802423 ,\n",
      "        4.669818 ,  0.5485121, -0.8361426,  4.6995335], dtype=float32), 'agent-3': array([-0.02050576, -0.9997897 ,  2.9195523 , -0.9601838 ,  0.279369  ,\n",
      "        2.9646487 , -0.08636916, -0.9962632 ,  3.1128814 ], dtype=float32)}\n",
      "action:  {'agent-0': array([-0.55768675, -0.4703992 ,  0.7930986 ], dtype=float32), 'agent-1': array([0.8648851 , 0.74781936, 0.2934441 ], dtype=float32), 'agent-2': array([-0.38955808, -0.38886175,  0.12289175], dtype=float32), 'agent-3': array([0.94245416, 0.6607608 , 0.00288807], dtype=float32)}\n",
      "reward:  {'agent-0': -0.007116972115561282, 'agent-1': -0.7915788182931056, 'agent-2': 0.023830468922511727, 'agent-3': 0.3592311191041606} \n",
      "\n",
      "obs:  {'agent-0': array([-0.765204  , -0.64378786,  0.8092266 , -0.13014887,  0.9914945 ,\n",
      "        2.6104429 , -0.66215193,  0.7493696 ,  4.6771955 ], dtype=float32), 'agent-1': array([ 0.765204  ,  0.64378786,  0.8092266 ,  0.08952596,  0.9959845 ,\n",
      "        3.121745  , -0.5241438 ,  0.8516298 ,  4.7273107 ], dtype=float32), 'agent-2': array([ 0.9489288 , -0.31549034,  2.9056635 ,  0.66215193, -0.7493696 ,\n",
      "        4.6771955 ,  0.5241438 , -0.8516298 ,  4.7273107 ], dtype=float32), 'agent-3': array([ 0.13014887, -0.9914945 ,  2.6104429 , -0.9489288 ,  0.31549034,\n",
      "        2.9056635 , -0.08952596, -0.9959845 ,  3.121745  ], dtype=float32)}\n",
      "action:  {'agent-0': array([ 0.0371947, -0.892063 ,  0.7311852], dtype=float32), 'agent-1': array([0.6941441 , 0.13283731, 0.7450627 ], dtype=float32), 'agent-2': array([ 0.11983839, -0.2182808 ,  0.05589744], dtype=float32), 'agent-3': array([-0.09821981,  0.90512705,  0.5748691 ], dtype=float32)}\n",
      "reward:  {'agent-0': 0.6805287650013003, 'agent-1': -0.7175899836506083, 'agent-2': 0.3145412897261277, 'agent-3': 1.20851757341371} \n",
      "\n",
      "obs:  {'agent-0': array([-0.18879335,  0.98201686,  1.3097281 , -0.6883558 , -0.7253732 ,\n",
      "        1.918407  , -0.7386117 ,  0.6741311 ,  4.188201  ], dtype=float32), 'agent-1': array([ 0.6883558 ,  0.7253732 ,  1.918407  ,  0.37204328,  0.9282154 ,\n",
      "        2.884822  , -0.38772053,  0.92177695,  4.5726433 ], dtype=float32), 'agent-2': array([ 0.8798691 , -0.47521612,  3.234784  ,  0.7386117 , -0.6741311 ,\n",
      "        4.188201  ,  0.38772053, -0.92177695,  4.5726433 ], dtype=float32), 'agent-3': array([ 0.18879335, -0.98201686,  1.3097281 , -0.37204328, -0.9282154 ,\n",
      "        2.884822  , -0.8798691 ,  0.47521612,  3.234784  ], dtype=float32)}\n",
      "action:  {'agent-0': array([ 0.80619794, -0.1689759 ,  0.596264  ], dtype=float32), 'agent-1': array([ 0.3239012 , -0.01714045,  0.3963949 ], dtype=float32), 'agent-2': array([ 0.0718261 , -0.17079522,  0.30607924], dtype=float32), 'agent-3': array([-0.3844785,  0.620085 ,  0.2775181], dtype=float32)}\n",
      "reward:  {'agent-0': 0.5202893992286066, 'agent-1': -0.12800417759430616, 'agent-2': -0.4284829998862403, 'agent-3': -0.2556527320739743} \n",
      "\n",
      "obs:  {'agent-0': array([ 0.46135122,  0.8872176 ,  1.0459659 , -0.6044666 , -0.7966305 ,\n",
      "        1.8740562 , -0.6610936 ,  0.75030345,  3.9760246 ], dtype=float32), 'agent-1': array([ 0.6044666 ,  0.7966305 ,  1.8740562 ,  0.5550353 ,  0.8318268 ,\n",
      "        2.910377  , -0.3169272 ,  0.94844985,  4.7194433 ], dtype=float32), 'agent-2': array([ 0.83437365, -0.55119926,  3.7286437 ,  0.6610936 , -0.75030345,\n",
      "        3.9760246 ,  0.3169272 , -0.94844985,  4.7194433 ], dtype=float32), 'agent-3': array([-0.46135122, -0.8872176 ,  1.0459659 , -0.5550353 , -0.8318268 ,\n",
      "        2.910377  , -0.83437365,  0.55119926,  3.7286437 ], dtype=float32)}\n",
      "action:  {'agent-0': array([-0.7001712,  0.5631317,  0.5024572], dtype=float32), 'agent-1': array([-0.474173  ,  0.4333175 ,  0.34487015], dtype=float32), 'agent-2': array([0.6062581 , 0.54948324, 0.28725287], dtype=float32), 'agent-3': array([ 0.39712536, -0.5768699 ,  0.3582608 ], dtype=float32)}\n",
      "reward:  {'agent-0': -1.026637996244724, 'agent-1': -0.48600986048060335, 'agent-2': -0.4748635334855429, 'agent-3': -0.5170576232898494} \n",
      "\n",
      "obs:  {'agent-0': array([-0.07271056,  0.9973531 ,  1.5420797 , -0.6690091 , -0.74325424,\n",
      "        1.8979689 , -0.7212049 ,  0.69272184,  4.482636  ], dtype=float32), 'agent-1': array([ 0.6690091 ,  0.74325424,  1.8979689 ,  0.36544082,  0.9308346 ,\n",
      "        3.1677716 , -0.39867637,  0.91709167,  4.9241457 ], dtype=float32), 'agent-2': array([ 0.8936429 , -0.44877872,  3.492193  ,  0.7212049 , -0.69272184,\n",
      "        4.482636  ,  0.39867637, -0.91709167,  4.9241457 ], dtype=float32), 'agent-3': array([ 0.07271056, -0.9973531 ,  1.5420797 , -0.36544082, -0.9308346 ,\n",
      "        3.1677716 , -0.8936429 ,  0.44877872,  3.492193  ], dtype=float32)}\n",
      "action:  {'agent-0': array([-0.01495471,  0.07258625,  0.59528863], dtype=float32), 'agent-1': array([-0.51084536, -0.766001  ,  0.5884365 ], dtype=float32), 'agent-2': array([ 0.30032367, -0.5123844 ,  0.56270456], dtype=float32), 'agent-3': array([ 0.4859109 , -0.77295524,  0.65961343], dtype=float32)}\n",
      "reward:  {'agent-0': -1.4718007438594007, 'agent-1': 0.6240565641096918, 'agent-2': -1.2321076481654352, 'agent-3': -0.9958801180613222} \n",
      "\n",
      "obs:  {'agent-0': array([-0.95299804, -0.30297646,  1.115843  , -0.21271104,  0.97711515,\n",
      "        2.742232  , -0.65702575,  0.7538681 ,  5.5364103 ], dtype=float32), 'agent-1': array([ 0.95299804,  0.30297646,  1.115843  ,  0.15712412,  0.98757887,\n",
      "        3.0555034 , -0.49555802,  0.86857486,  5.1944833 ], dtype=float32), 'agent-2': array([ 0.8982622 , -0.43945995,  3.400189  ,  0.49555802, -0.86857486,\n",
      "        5.1944833 ,  0.65702575, -0.7538681 ,  5.5364103 ], dtype=float32), 'agent-3': array([ 0.21271104, -0.97711515,  2.742232  , -0.15712412, -0.98757887,\n",
      "        3.0555034 , -0.8982622 ,  0.43945995,  3.400189  ], dtype=float32)}\n",
      "action:  {'agent-0': array([0.7505958 , 0.15915711, 0.16982482], dtype=float32), 'agent-1': array([0.5368833 , 0.36614364, 0.68088675], dtype=float32), 'agent-2': array([0.6234422 , 0.07504769, 0.7366163 ], dtype=float32), 'agent-3': array([-0.6827482 ,  0.6081639 ,  0.66671455], dtype=float32)}\n",
      "reward:  {'agent-0': -0.37735291348680633, 'agent-1': -1.1170391917280362, 'agent-2': -1.955566446428179, 'agent-3': -1.0753329313617925} \n",
      "\n",
      "obs:  {'agent-0': array([-0.90493375, -0.42555252,  1.6131458 ,  0.03549751,  0.99936974,\n",
      "        2.2726746 , -0.714026  ,  0.7001192 ,  5.886018  ], dtype=float32), 'agent-1': array([ 0.90493375,  0.42555252,  1.6131458 ,  0.46193087,  0.8869159 ,\n",
      "        3.3348374 , -0.49558017,  0.8685622 ,  5.534886  ], dtype=float32), 'agent-2': array([ 0.9180621 , -0.39643657,  4.6657453 ,  0.49558017, -0.8685622 ,\n",
      "        5.534886  ,  0.714026  , -0.7001192 ,  5.886018  ], dtype=float32), 'agent-3': array([-0.03549751, -0.99936974,  2.2726746 , -0.46193087, -0.8869159 ,\n",
      "        3.3348374 , -0.9180621 ,  0.39643657,  4.6657453 ], dtype=float32)}\n",
      "action:  {'agent-0': array([0.9544645 , 0.7583748 , 0.68646884], dtype=float32), 'agent-1': array([-0.8892622 , -0.60756546,  0.26985016], dtype=float32), 'agent-2': array([0.442238  , 0.43251082, 0.5070063 ], dtype=float32), 'agent-3': array([-0.61364865, -0.19864692,  0.66423625], dtype=float32)}\n",
      "reward:  {'agent-0': 0.08909767212730735, 'agent-1': 0.7507987137819985, 'agent-2': -0.5993043508227593, 'agent-3': -1.9069847460423048} \n",
      "\n",
      "obs:  {'agent-0': array([-0.98845994, -0.15148264,  0.70767844,  0.39552453,  0.9184554 ,\n",
      "        3.1605895 , -0.6927154 ,  0.7212111 ,  5.8144727 ], dtype=float32), 'agent-1': array([ 0.98845994,  0.15148264,  0.70767844,  0.54362774,  0.83932644,\n",
      "        3.586282  , -0.61202574,  0.7908379 ,  5.43811   ], dtype=float32), 'agent-2': array([ 0.97137964, -0.23753242,  5.4333706 ,  0.61202574, -0.7908379 ,\n",
      "        5.43811   ,  0.6927154 , -0.7212111 ,  5.8144727 ], dtype=float32), 'agent-3': array([-0.39552453, -0.9184554 ,  3.1605895 , -0.54362774, -0.83932644,\n",
      "        3.586282  , -0.97137964,  0.23753242,  5.4333706 ], dtype=float32)}\n",
      "action:  {'agent-0': array([-0.743371  ,  0.77902377,  0.39698568], dtype=float32), 'agent-1': array([-0.13388173, -0.13626571,  0.29713842], dtype=float32), 'agent-2': array([-0.58941954,  0.45818278,  0.8116291 ], dtype=float32), 'agent-3': array([0.00855816, 0.2532599 , 0.15711094], dtype=float32)}\n",
      "reward:  {'agent-0': 0.22866846739308144, 'agent-1': 1.0988875534432623, 'agent-2': 1.935500772960559, 'agent-3': 1.1069879197251904} \n",
      "\n",
      "obs:  {'agent-0': array([-0.8900609 ,  0.4558417 ,  0.8598598 ,  0.30481783,  0.9524107 ,\n",
      "        3.1845984 , -0.6767658 ,  0.73619837,  5.409614  ], dtype=float32), 'agent-1': array([ 0.8900609 , -0.4558417 ,  0.8598598 ,  0.5492838 ,  0.83563584,\n",
      "        3.16057   , -0.6277627 ,  0.77840483,  4.612753  ], dtype=float32), 'agent-2': array([ 0.6277627 , -0.77840483,  4.612753  ,  0.9796277 , -0.20082197,\n",
      "        4.7280855 ,  0.6767658 , -0.73619837,  5.409614  ], dtype=float32), 'agent-3': array([-0.5492838 , -0.83563584,  3.16057   , -0.30481783, -0.9524107 ,\n",
      "        3.1845984 , -0.9796277 ,  0.20082197,  4.7280855 ], dtype=float32)}\n",
      "action:  {'agent-0': array([ 0.93327165, -0.5942683 ,  0.5560957 ], dtype=float32), 'agent-1': array([ 0.31664026, -0.6654663 ,  0.79870206], dtype=float32), 'agent-2': array([-0.23231998,  0.04187506,  0.8710454 ], dtype=float32), 'agent-3': array([-0.5878873 ,  0.58533967,  0.28700542], dtype=float32)}\n",
      "reward:  {'agent-0': 1.168533275333834, 'agent-1': 1.5404707654775969, 'agent-2': 3.216969454527689, 'agent-3': 1.0973433916973097} \n",
      "\n",
      "obs:  {'agent-0': array([-0.6175013 ,  0.78656983,  1.0355027 ,  0.5443993 ,  0.8388262 ,\n",
      "        3.0183337 , -0.5517253 ,  0.83402586,  4.231703  ], dtype=float32), 'agent-1': array([ 0.6175013 , -0.78656983,  1.0355027 ,  0.79909   ,  0.6012114 ,\n",
      "        2.856503  , -0.5296685 ,  0.84820473,  3.2007065 ], dtype=float32), 'agent-2': array([ 0.5296685 , -0.84820473,  3.2007065 ,  0.9699694 , -0.24322703,\n",
      "        4.101074  ,  0.5517253 , -0.83402586,  4.231703  ], dtype=float32), 'agent-3': array([-0.79909   , -0.6012114 ,  2.856503  , -0.5443993 , -0.8388262 ,\n",
      "        3.0183337 , -0.9699694 ,  0.24322703,  4.101074  ], dtype=float32)}\n",
      "action:  {'agent-0': array([-0.04927406,  0.4839076 ,  0.7151095 ], dtype=float32), 'agent-1': array([0.18114531, 0.4385733 , 0.2300607 ], dtype=float32), 'agent-2': array([ 0.534563  , -0.6990196 ,  0.18024077], dtype=float32), 'agent-3': array([-0.4777912, -0.8143769,  0.2205373], dtype=float32)}\n",
      "reward:  {'agent-0': -2.1305411436408477, 'agent-1': -1.2273093536856017, 'agent-2': -1.3448068343995132, 'agent-3': -1.4171450869131803} \n",
      "\n",
      "obs:  {'agent-0': array([-0.52008873,  0.85411227,  1.537607  ,  0.43999815,  0.8979987 ,\n",
      "        3.8235075 , -0.49786076,  0.867257  ,  5.0549655 ], dtype=float32), 'agent-1': array([ 0.52008873, -0.85411227,  1.537607  ,  0.760351  ,  0.6495124 ,\n",
      "        3.2643192 , -0.4880416 ,  0.8728204 ,  3.5180953 ], dtype=float32), 'agent-2': array([ 0.4880416 , -0.8728204 ,  3.5180953 ,  0.9753267 , -0.22076628,\n",
      "        4.305229  ,  0.49786076, -0.867257  ,  5.0549655 ], dtype=float32), 'agent-3': array([-0.760351  , -0.6495124 ,  3.2643192 , -0.43999815, -0.8979987 ,\n",
      "        3.8235075 , -0.9753267 ,  0.22076628,  4.305229  ], dtype=float32)}\n",
      "action:  {'agent-0': array([0.21770544, 0.23231907, 0.9655551 ], dtype=float32), 'agent-1': array([0.7688381 , 0.4728072 , 0.62155455], dtype=float32), 'agent-2': array([0.8077867 , 0.86293906, 0.04275465], dtype=float32), 'agent-3': array([0.01696806, 0.95452416, 0.00870401], dtype=float32)}\n",
      "reward:  {'agent-0': -1.5484377499952107, 'agent-1': -0.9518594225290276, 'agent-2': -0.4260727382091787, 'agent-3': -1.5570709736390036} \n",
      "\n",
      "obs:  {'agent-0': array([-0.36760408,  0.92998236,  1.8196535 ,  0.49340266,  0.86980104,\n",
      "        4.7474675 , -0.3493639 ,  0.9369871 ,  5.3973966 ], dtype=float32), 'agent-1': array([ 0.36760408, -0.92998236,  1.8196535 , -0.34003708,  0.94041204,\n",
      "        3.578267  ,  0.7773248 ,  0.6290995 ,  3.8739603 ], dtype=float32), 'agent-2': array([ 0.34003708, -0.94041204,  3.578267  ,  0.97675276, -0.21436901,\n",
      "        4.328699  ,  0.3493639 , -0.9369871 ,  5.3973966 ], dtype=float32), 'agent-3': array([-0.7773248 , -0.6290995 ,  3.8739603 , -0.97675276,  0.21436901,\n",
      "        4.328699  , -0.49340266, -0.86980104,  4.7474675 ], dtype=float32)}\n",
      "action:  {'agent-0': array([-0.85244477, -0.33998135,  0.33244348], dtype=float32), 'agent-1': array([0.35805345, 0.76823336, 0.54166603], dtype=float32), 'agent-2': array([-0.27425635, -0.38480002,  0.08906144], dtype=float32), 'agent-3': array([ 0.3587478 , -0.75384885,  0.3650647 ], dtype=float32)}\n",
      "reward:  {'agent-0': 0.17596792579524845, 'agent-1': -0.8701631843907016, 'agent-2': -0.2552068871991544, 'agent-3': -0.33500387782027374} \n",
      "\n",
      "obs:  {'agent-0': array([-0.7456689,  0.6663166,  1.6180453,  0.3972309,  0.9177187,\n",
      "        4.724583 , -0.3934605,  0.9193415,  5.4459214], dtype=float32), 'agent-1': array([ 0.7456689 , -0.6663166 ,  1.6180453 , -0.23182306,  0.972758  ,\n",
      "        4.0385494 ,  0.6873953 ,  0.7262835 ,  4.4854493 ], dtype=float32), 'agent-2': array([ 0.23182306, -0.972758  ,  4.0385494 ,  0.98635787, -0.1646153 ,\n",
      "        4.0750985 ,  0.3934605 , -0.9193415 ,  5.4459214 ], dtype=float32), 'agent-3': array([-0.98635787,  0.1646153 ,  4.0750985 , -0.6873953 , -0.7262835 ,\n",
      "        4.4854493 , -0.3972309 , -0.9177187 ,  4.724583  ], dtype=float32)}\n",
      "action:  {'agent-0': array([-0.29339308,  0.8085568 ,  0.60589457], dtype=float32), 'agent-1': array([ 0.10023022, -0.4246875 ,  0.11787361], dtype=float32), 'agent-2': array([ 0.81582654, -0.80179757,  0.5677567 ], dtype=float32), 'agent-3': array([0.6791069 , 0.63760805, 0.5033341 ], dtype=float32)}\n",
      "reward:  {'agent-0': -1.8094574317725893, 'agent-1': -0.46492347278345925, 'agent-2': -1.7275919632515766, 'agent-3': 0.32626660565278875} \n",
      "\n",
      "obs:  {'agent-0': array([-0.63279015,  0.77432334,  2.2760646 ,  0.2747266 ,  0.9615224 ,\n",
      "        4.743387  , -0.41868705,  0.9081306 ,  6.578555  ], dtype=float32), 'agent-1': array([ 0.63279015, -0.77432334,  2.2760646 ,  0.7000473 ,  0.7140965 ,\n",
      "        3.9188867 , -0.2978422 ,  0.9546151 ,  4.412016  ], dtype=float32), 'agent-2': array([ 0.9443513 , -0.32893854,  4.29659   ,  0.2978422 , -0.9546151 ,\n",
      "        4.412016  ,  0.41868705, -0.9081306 ,  6.578555  ], dtype=float32), 'agent-3': array([-0.7000473 , -0.7140965 ,  3.9188867 , -0.9443513 ,  0.32893854,\n",
      "        4.29659   , -0.2747266 , -0.9615224 ,  4.743387  ], dtype=float32)}\n",
      "action:  {'agent-0': array([-0.17748894,  0.30372933,  0.42266083], dtype=float32), 'agent-1': array([-0.13658263,  0.920717  ,  0.35654765], dtype=float32), 'agent-2': array([-0.30694014,  0.9037967 ,  0.15600754], dtype=float32), 'agent-3': array([-0.83075523,  0.47842312,  0.9982196 ], dtype=float32)}\n",
      "reward:  {'agent-0': -0.4765457948392964, 'agent-1': -0.8286633894500728, 'agent-2': -1.3476187214637179, 'agent-3': -1.4996029142131135} \n",
      "\n",
      "obs:  {'agent-0': array([-0.66989356,  0.74245715,  2.39023   ,  0.40390825,  0.9147995 ,\n",
      "        4.8400035 , -0.42625648,  0.90460235,  6.8443193 ], dtype=float32), 'agent-1': array([ 0.66989356, -0.74245715,  2.39023   ,  0.8015227 ,  0.5979644 ,\n",
      "        4.436702  , -0.28559813,  0.95834947,  4.608699  ], dtype=float32), 'agent-2': array([ 0.28559813, -0.95834947,  4.608699  ,  0.9402889 , -0.34037742,\n",
      "        5.1817617 ,  0.42625648, -0.90460235,  6.8443193 ], dtype=float32), 'agent-3': array([-0.8015227 , -0.5979644 ,  4.436702  , -0.40390825, -0.9147995 ,\n",
      "        4.8400035 , -0.9402889 ,  0.34037742,  5.1817617 ], dtype=float32)}\n",
      "action:  {'agent-0': array([ 0.08131941, -0.5500892 ,  0.45932534], dtype=float32), 'agent-1': array([-0.8831933 , -0.04843763,  0.15978935], dtype=float32), 'agent-2': array([-0.76616997, -0.07654748,  0.61752546], dtype=float32), 'agent-3': array([ 0.86382973, -0.562755  ,  0.99161196], dtype=float32)}\n",
      "reward:  {'agent-0': 1.2790826913103377, 'agent-1': 0.8818074453530098, 'agent-2': 2.209018774562214, 'agent-3': 2.0395226888404068} \n",
      "\n",
      "obs:  {'agent-0': array([-0.7188985 ,  0.6951151 ,  1.9119227 ,  0.25513503,  0.9669054 ,\n",
      "        4.669038  , -0.3597705 ,  0.9330408 ,  6.21451   ], dtype=float32), 'agent-1': array([ 0.7188985 , -0.6951151 ,  1.9119227 ,  0.62727123,  0.7788009 ,\n",
      "        4.0902777 , -0.18923339,  0.98193216,  4.551623  ], dtype=float32), 'agent-2': array([ 0.9364427 , -0.3508206 ,  3.6596286 ,  0.18923339, -0.98193216,\n",
      "        4.551623  ,  0.3597705 , -0.9330408 ,  6.21451   ], dtype=float32), 'agent-3': array([-0.9364427 ,  0.3508206 ,  3.6596286 , -0.62727123, -0.7788009 ,\n",
      "        4.0902777 , -0.25513503, -0.9669054 ,  4.669038  ], dtype=float32)}\n",
      "action:  {'agent-0': array([-0.1073355 ,  0.11791869,  0.9788803 ], dtype=float32), 'agent-1': array([ 0.3669655 , -0.2554327 ,  0.25155172], dtype=float32), 'agent-2': array([ 0.24180831, -0.0997453 ,  0.7970263 ], dtype=float32), 'agent-3': array([-0.6984007 ,  0.6902936 ,  0.56515294], dtype=float32)}\n",
      "reward:  {'agent-0': -3.0042823955042532, 'agent-1': -1.549043510063358, 'agent-2': -3.119225245607133, 'agent-3': -1.6215675118899266} \n",
      "\n",
      "obs:  {'agent-0': array([-0.7139666 ,  0.70017976,  3.1372106 ,  0.1894875 ,  0.9818831 ,\n",
      "        4.930454  , -0.4696694 ,  0.88284236,  7.7320886 ], dtype=float32), 'agent-1': array([ 0.7139666 , -0.70017976,  3.1372106 ,  0.7682909 ,  0.6401009 ,\n",
      "        4.131408  , -0.2878755 ,  0.9576678 ,  4.834248  ], dtype=float32), 'agent-2': array([ 0.2878755 , -0.9576678 ,  4.834248  ,  0.91707283, -0.39871973,\n",
      "        4.97865   ,  0.4696694 , -0.88284236,  7.7320886 ], dtype=float32), 'agent-3': array([-0.7682909 , -0.6401009 ,  4.131408  , -0.1894875 , -0.9818831 ,\n",
      "        4.930454  , -0.91707283,  0.39871973,  4.97865   ], dtype=float32)}\n",
      "action:  {'agent-0': array([-0.9036059, -0.8476659,  0.6188246], dtype=float32), 'agent-1': array([-0.9439077 , -0.5019438 ,  0.06810644], dtype=float32), 'agent-2': array([-0.1730638,  0.4905288,  0.2030788], dtype=float32), 'agent-3': array([0.67759496, 0.05855709, 0.3762878 ], dtype=float32)}\n",
      "reward:  {'agent-0': 0.8261217359525936, 'agent-1': 0.5306770529409484, 'agent-2': 1.0208528753534303, 'agent-3': 1.381332338268571} \n",
      "\n",
      "obs:  {'agent-0': array([-0.8245745 ,  0.5657534 ,  3.1908007 ,  0.02463063,  0.9996966 ,\n",
      "        4.3866806 , -0.542888  ,  0.8398051 ,  7.3961496 ], dtype=float32), 'agent-1': array([ 0.8245745 , -0.5657534 ,  3.1908007 ,  0.7279125 ,  0.6856701 ,\n",
      "        3.7629519 , -0.29971784,  0.9540279 ,  4.6184373 ], dtype=float32), 'agent-2': array([ 0.9143552 , -0.40491307,  4.5095468 ,  0.29971784, -0.9540279 ,\n",
      "        4.6184373 ,  0.542888  , -0.8398051 ,  7.3961496 ], dtype=float32), 'agent-3': array([-0.7279125 , -0.6856701 ,  3.7629519 , -0.02463063, -0.9996966 ,\n",
      "        4.3866806 , -0.9143552 ,  0.40491307,  4.5095468 ], dtype=float32)}\n",
      "action:  {'agent-0': array([0.14019702, 0.9826815 , 0.19003883], dtype=float32), 'agent-1': array([0.19622543, 0.98404515, 0.11478657], dtype=float32), 'agent-2': array([ 0.8799345 , -0.49067748,  0.20257635], dtype=float32), 'agent-3': array([ 0.13638599, -0.5301541 ,  0.78221416], dtype=float32)}\n",
      "reward:  {'agent-0': -1.306854414296165, 'agent-1': -0.8256115351396094, 'agent-2': -0.3294268244460916, 'agent-3': -1.240468526563916} \n",
      "\n",
      "obs:  {'agent-0': array([-0.8130617 ,  0.58217746,  3.2305782 , -0.01125365,  0.9999367 ,\n",
      "        5.3313694 , -0.53965765,  0.84188455,  7.718538  ], dtype=float32), 'agent-1': array([ 0.8130617 , -0.58217746,  3.2305782 ,  0.59686494,  0.80234176,\n",
      "        4.3002396 , -0.31615236,  0.9487084 ,  4.8669834 ], dtype=float32), 'agent-2': array([ 0.9618868 , -0.27344793,  4.268039  ,  0.31615236, -0.9487084 ,\n",
      "        4.8669834 ,  0.53965765, -0.84188455,  7.718538  ], dtype=float32), 'agent-3': array([-0.9618868 ,  0.27344793,  4.268039  , -0.59686494, -0.80234176,\n",
      "        4.3002396 ,  0.01125365, -0.9999367 ,  5.3313694 ], dtype=float32)}\n",
      "action:  {'agent-0': array([ 0.07489913, -0.7408997 ,  0.57620925], dtype=float32), 'agent-1': array([ 0.95610976, -0.9181263 ,  0.71646357], dtype=float32), 'agent-2': array([ 0.8031516 , -0.64283466,  0.61664116], dtype=float32), 'agent-3': array([-0.04443406, -0.07425025,  0.26815256], dtype=float32)}\n",
      "reward:  {'agent-0': -0.08550044108513788, 'agent-1': -0.45606921146265833, 'agent-2': -0.6053453885993072, 'agent-3': -0.5253630394961561} \n",
      "\n",
      "obs:  {'agent-0': array([-0.86330783,  0.5046777 ,  3.574023  ,  0.02718732,  0.99963033,\n",
      "        4.989686  , -0.5881408 ,  0.80875856,  7.8022766 ], dtype=float32), 'agent-1': array([ 0.86330783, -0.5046777 ,  3.574023  ,  0.7111824 ,  0.7030075 ,\n",
      "        4.5292716 , -0.31645748,  0.94860667,  4.750576  ], dtype=float32), 'agent-2': array([ 0.31645748, -0.94860667,  4.750576  ,  0.96299267, -0.2695275 ,\n",
      "        4.9060535 ,  0.5881408 , -0.80875856,  7.8022766 ], dtype=float32), 'agent-3': array([-0.7111824 , -0.7030075 ,  4.5292716 , -0.96299267,  0.2695275 ,\n",
      "        4.9060535 , -0.02718732, -0.99963033,  4.989686  ], dtype=float32)}\n",
      "action:  {'agent-0': array([-0.75798166,  0.774263  ,  0.73897105], dtype=float32), 'agent-1': array([0.9018653, 0.5464903, 0.6196267], dtype=float32), 'agent-2': array([0.6015992 , 0.81461114, 0.47307795], dtype=float32), 'agent-3': array([-0.59448034,  0.9174098 ,  0.4061465 ], dtype=float32)}\n",
      "reward:  {'agent-0': -1.8165934985391239, 'agent-1': -1.4405768554982643, 'agent-2': -0.9525946157082643, 'agent-3': -1.212351451833623} \n",
      "\n",
      "obs:  {'agent-0': array([-0.8992065 ,  0.4375244 ,  4.595563  , -0.03098544,  0.9995198 ,\n",
      "        5.1775393 , -0.64056635,  0.76790285,  8.409477  ], dtype=float32), 'agent-1': array([ 0.8992065 , -0.4375244 ,  4.595563  , -0.271498  ,  0.962439  ,\n",
      "        4.6205425 ,  0.78213155,  0.62311333,  5.078342  ], dtype=float32), 'agent-2': array([ 0.271498  , -0.962439  ,  4.6205425 ,  0.9711824 , -0.23833735,\n",
      "        5.3814807 ,  0.64056635, -0.76790285,  8.409477  ], dtype=float32), 'agent-3': array([-0.78213155, -0.62311333,  5.078342  ,  0.03098544, -0.9995198 ,\n",
      "        5.1775393 , -0.9711824 ,  0.23833735,  5.3814807 ], dtype=float32)}\n",
      "action:  {'agent-0': array([0.46803808, 0.74833024, 0.6054228 ], dtype=float32), 'agent-1': array([-0.27755442, -0.07771834,  0.00730258], dtype=float32), 'agent-2': array([ 0.8063689 , -0.9687671 ,  0.30174237], dtype=float32), 'agent-3': array([0.9336802 , 0.50822186, 0.73875564], dtype=float32)}\n",
      "reward:  {'agent-0': -0.6597143545949997, 'agent-1': 0.484023980488109, 'agent-2': -0.5278905506505076, 'agent-3': 0.8107291109510832} \n",
      "\n",
      "obs:  {'agent-0': array([-0.8330857 ,  0.5531439 ,  4.566506  , -0.09113479,  0.9958386 ,\n",
      "        5.3574557 , -0.5896651 ,  0.8076478 ,  8.918332  ], dtype=float32), 'agent-1': array([ 0.76300657,  0.64639074,  4.346018  ,  0.8330857 , -0.5531439 ,\n",
      "        4.566506  , -0.2969719 ,  0.9548862 ,  4.897899  ], dtype=float32), 'agent-2': array([ 0.2969719, -0.9548862,  4.897899 ,  0.931179 , -0.3645623,\n",
      "        5.12316  ,  0.5896651, -0.8076478,  8.918332 ], dtype=float32), 'agent-3': array([-0.76300657, -0.64639074,  4.346018  , -0.931179  ,  0.3645623 ,\n",
      "        5.12316   ,  0.09113479, -0.9958386 ,  5.3574557 ], dtype=float32)}\n",
      "action:  {'agent-0': array([ 0.42307028, -0.5413776 ,  0.88359386], dtype=float32), 'agent-1': array([-0.29034376, -0.68756247,  0.7334085 ], dtype=float32), 'agent-2': array([-0.7633186 ,  0.6343593 ,  0.08483102], dtype=float32), 'agent-3': array([0.72674745, 0.7161082 , 0.44441828], dtype=float32)}\n",
      "reward:  {'agent-0': 2.6649964717551633, 'agent-1': 2.364649656548659, 'agent-2': 1.8223152294377272, 'agent-3': 2.346602009814621} \n",
      "\n"
     ]
    }
   ],
   "source": [
    "env_config = EnvironmentConfiguration(n_agents=4, visible_nbrs=3, target_distance=0, max_steps=100, speed=1, spawn_area=100, \n",
    "                         spawn_area_schedule=[[0,5],[3,10],[4,100],[5,1000]])\n",
    "env = RenderableKeepTheDistance(env_config)\n",
    "for i in range(1):\n",
    "    obs, _ = env.reset()\n",
    "    env.render()\n",
    "    simulate_random_episode(env, 30, sleep_between_frames=0.03)\n",
    "    #time.sleep(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "08e0666d15e44541975d10cb21dc2c4c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "CanvasWithBorders(height=300, width=300)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'agent-0': array([ 0.8       , -0.6       , 10.        ,  0.6950221 ,  0.71898836,\n",
      "       41.725292  ], dtype=float32), 'agent-1': array([-0.503871  , -0.8637789 , 41.677334  , -0.6950221 , -0.71898836,\n",
      "       41.725292  ], dtype=float32), 'agent-2': array([-0.8      ,  0.6      , 10.       ,  0.503871 ,  0.8637789,\n",
      "       41.677334 ], dtype=float32)}\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "env_config = EnvironmentConfiguration(n_agents=3, visible_nbrs=2, target_distance=0, max_steps=100, speed=1, spawn_area=50)\n",
    "env = RenderableKeepTheDistance(env_config)\n",
    "obs, _ = env.reset()\n",
    "env.render()\n",
    "print(obs)\n",
    "\n",
    "for i in range(0):\n",
    "    print(obs)\n",
    "    #actions = {agent: np.array([rnd.random()*2-1, rnd.random()*2-1, 1.0], np.float32) for agent in obs.keys()}\n",
    "    actions = {'agent-0': np.array([-1.0, -1.0, 1.0], np.float32),\n",
    "               'agent-1': np.array([0.0, 0.0, 1], np.float32)}\n",
    "    #actions = {agent: env.action_space.sample() for agent in obs.keys()}\n",
    "    obs, reward, _, _, _ = env.step(actions)\n",
    "    print(actions)\n",
    "    print(reward, \"\\n\")\n",
    "    env.render()\n",
    "    time.sleep(0.3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## policy training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ray\n",
    "ray.shutdown()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KeepTheDistance?dst=0&agent=2&spawn_area=100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ray.tune.registry import register_env\n",
    "\n",
    "env_config = EnvironmentConfiguration(n_agents=2, target_distance=0, max_steps=300, speed=1, spawn_area=100)\n",
    "register_env(\"KeepTheDistance?dst=0&agent=2&spawn_area=100\", lambda _: KeepTheDistance(env_config))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-23 17:56:02,620\tWARNING deprecation.py:50 -- DeprecationWarning: `_enable_new_api_stack` has been deprecated. Use `AlgorithmConfig._enable_new_api_stack` instead. This will raise an error in the future!\n",
      "/home/nicolo/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/rllib/algorithms/algorithm.py:521: RayDeprecationWarning: This API is deprecated and may be removed in future Ray releases. You could suppress this warning by setting env variable PYTHONWARNINGS=\"ignore::DeprecationWarning\"\n",
      "`UnifiedLogger` will be removed in Ray 2.7.\n",
      "  return UnifiedLogger(config, logdir, loggers=None)\n",
      "/home/nicolo/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/tune/logger/unified.py:53: RayDeprecationWarning: This API is deprecated and may be removed in future Ray releases. You could suppress this warning by setting env variable PYTHONWARNINGS=\"ignore::DeprecationWarning\"\n",
      "The `JsonLogger interface is deprecated in favor of the `ray.tune.json.JsonLoggerCallback` interface and will be removed in Ray 2.7.\n",
      "  self._loggers.append(cls(self.config, self.logdir, self.trial))\n",
      "/home/nicolo/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/tune/logger/unified.py:53: RayDeprecationWarning: This API is deprecated and may be removed in future Ray releases. You could suppress this warning by setting env variable PYTHONWARNINGS=\"ignore::DeprecationWarning\"\n",
      "The `CSVLogger interface is deprecated in favor of the `ray.tune.csv.CSVLoggerCallback` interface and will be removed in Ray 2.7.\n",
      "  self._loggers.append(cls(self.config, self.logdir, self.trial))\n",
      "/home/nicolo/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/tune/logger/unified.py:53: RayDeprecationWarning: This API is deprecated and may be removed in future Ray releases. You could suppress this warning by setting env variable PYTHONWARNINGS=\"ignore::DeprecationWarning\"\n",
      "The `TBXLogger interface is deprecated in favor of the `ray.tune.tensorboardx.TBXLoggerCallback` interface and will be removed in Ray 2.7.\n",
      "  self._loggers.append(cls(self.config, self.logdir, self.trial))\n",
      "2024-05-23 17:56:02,683\tWARNING deprecation.py:50 -- DeprecationWarning: `WorkerSet(num_workers=... OR local_worker=...)` has been deprecated. Use `EnvRunnerGroup(num_env_runners=... AND local_env_runner=...)` instead. This will raise an error in the future!\n",
      "2024-05-23 17:56:02,684\tWARNING deprecation.py:50 -- DeprecationWarning: `max_num_worker_restarts` has been deprecated. Use `AlgorithmConfig.max_num_env_runner_restarts` instead. This will raise an error in the future!\n",
      "2024-05-23 17:56:05,845\tINFO worker.py:1749 -- Started a local Ray instance.\n",
      "2024-05-23 17:56:12,313\tWARNING util.py:61 -- Install gputil for GPU system monitoring.\n"
     ]
    }
   ],
   "source": [
    "algo = load_algo(\"KeepTheDistance?dst=0&agent=2&spawn_area=100\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration [1] => episode_reward_mean: -4.95664815891495, episode_len_mean: 300.0, agent_steps_trained: 8192, env_steps_trained: 4096, entropy: 4.245799648761749, learning_rate: 0.0010000000000000005\n",
      "iteration [2] => episode_reward_mean: 25.736145597755076, episode_len_mean: 300.0, agent_steps_trained: 16384, env_steps_trained: 8192, entropy: 4.2675507590174675, learning_rate: 0.0010000000000000005\n",
      "iteration [3] => episode_reward_mean: 46.87691646133618, episode_len_mean: 300.0, agent_steps_trained: 24576, env_steps_trained: 12288, entropy: 4.28767983019352, learning_rate: 0.0010000000000000005\n",
      "iteration [4] => episode_reward_mean: 63.17620430531192, episode_len_mean: 300.0, agent_steps_trained: 32768, env_steps_trained: 16384, entropy: 4.272764860590299, learning_rate: 0.0010000000000000005\n",
      "iteration [5] => episode_reward_mean: 83.51466914032467, episode_len_mean: 300.0, agent_steps_trained: 40960, env_steps_trained: 20480, entropy: 4.080347933868567, learning_rate: 0.0010000000000000005\n",
      "iteration [6] => episode_reward_mean: 97.2725900401565, episode_len_mean: 300.0, agent_steps_trained: 49152, env_steps_trained: 24576, entropy: 4.035366955151161, learning_rate: 0.0010000000000000005\n",
      "iteration [7] => episode_reward_mean: 111.82767845339714, episode_len_mean: 300.0, agent_steps_trained: 57344, env_steps_trained: 28672, entropy: 3.9716993694504104, learning_rate: 0.0010000000000000005\n",
      "iteration [8] => episode_reward_mean: 136.62884647100984, episode_len_mean: 300.0, agent_steps_trained: 65536, env_steps_trained: 32768, entropy: 3.955365744729837, learning_rate: 0.0010000000000000005\n",
      "iteration [9] => episode_reward_mean: 164.88432376674643, episode_len_mean: 300.0, agent_steps_trained: 73728, env_steps_trained: 36864, entropy: 3.8659255050122736, learning_rate: 0.0010000000000000005\n",
      "iteration [10] => episode_reward_mean: 195.05470119939545, episode_len_mean: 300.0, agent_steps_trained: 81920, env_steps_trained: 40960, entropy: 3.8061600769559543, learning_rate: 0.0010000000000000005\n",
      "iteration [11] => episode_reward_mean: 226.75336202358946, episode_len_mean: 300.0, agent_steps_trained: 90112, env_steps_trained: 45056, entropy: 3.7295295434693494, learning_rate: 0.0010000000000000005\n",
      "iteration [12] => episode_reward_mean: 254.77739146322128, episode_len_mean: 300.0, agent_steps_trained: 98304, env_steps_trained: 49152, entropy: 3.7193174014488855, learning_rate: 0.0010000000000000005\n",
      "iteration [13] => episode_reward_mean: 289.128688300256, episode_len_mean: 300.0, agent_steps_trained: 106496, env_steps_trained: 53248, entropy: 3.6365751929581167, learning_rate: 0.0010000000000000005\n",
      "iteration [14] => episode_reward_mean: 325.6940806998776, episode_len_mean: 300.0, agent_steps_trained: 114688, env_steps_trained: 57344, entropy: 3.6089514570931596, learning_rate: 0.0010000000000000005\n",
      "iteration [15] => episode_reward_mean: 358.4698142168033, episode_len_mean: 300.0, agent_steps_trained: 122880, env_steps_trained: 61440, entropy: 3.6058398122588793, learning_rate: 0.0010000000000000005\n",
      "iteration [16] => episode_reward_mean: 397.62178977112677, episode_len_mean: 300.0, agent_steps_trained: 131072, env_steps_trained: 65536, entropy: 3.5509075559675694, learning_rate: 0.0010000000000000005\n",
      "iteration [17] => episode_reward_mean: 430.0716298706977, episode_len_mean: 300.0, agent_steps_trained: 139264, env_steps_trained: 69632, entropy: 3.5135758027434347, learning_rate: 0.0010000000000000005\n",
      "iteration [18] => episode_reward_mean: 459.53089803529224, episode_len_mean: 300.0, agent_steps_trained: 147456, env_steps_trained: 73728, entropy: 3.491550320883592, learning_rate: 0.0010000000000000005\n",
      "iteration [19] => episode_reward_mean: 488.62102982390604, episode_len_mean: 300.0, agent_steps_trained: 155648, env_steps_trained: 77824, entropy: 3.4550479906300704, learning_rate: 0.0010000000000000005\n",
      "iteration [20] => episode_reward_mean: 510.65966583025437, episode_len_mean: 300.0, agent_steps_trained: 163840, env_steps_trained: 81920, entropy: 3.4639352411031723, learning_rate: 0.0010000000000000005\n",
      "iteration [21] => episode_reward_mean: 529.9404729947166, episode_len_mean: 300.0, agent_steps_trained: 172032, env_steps_trained: 86016, entropy: 3.4784888106087846, learning_rate: 0.0010000000000000005\n",
      "iteration [22] => episode_reward_mean: 546.4029696187386, episode_len_mean: 300.0, agent_steps_trained: 180224, env_steps_trained: 90112, entropy: 3.4598235140244165, learning_rate: 0.0010000000000000005\n",
      "iteration [23] => episode_reward_mean: 559.27084482982, episode_len_mean: 300.0, agent_steps_trained: 188416, env_steps_trained: 94208, entropy: 3.4430141928295295, learning_rate: 0.0010000000000000005\n",
      "iteration [24] => episode_reward_mean: 568.6788158436156, episode_len_mean: 300.0, agent_steps_trained: 196608, env_steps_trained: 98304, entropy: 3.4166217731932798, learning_rate: 0.0010000000000000005\n",
      "iteration [25] => episode_reward_mean: 576.044729303482, episode_len_mean: 300.0, agent_steps_trained: 204800, env_steps_trained: 102400, entropy: 3.3975195427735647, learning_rate: 0.0010000000000000005\n",
      "iteration [26] => episode_reward_mean: 584.9244146978964, episode_len_mean: 300.0, agent_steps_trained: 212992, env_steps_trained: 106496, entropy: 3.3621628026167554, learning_rate: 0.0010000000000000005\n",
      "iteration [27] => episode_reward_mean: 590.5687960574658, episode_len_mean: 300.0, agent_steps_trained: 221184, env_steps_trained: 110592, entropy: 3.3647831340630847, learning_rate: 0.0010000000000000005\n",
      "iteration [28] => episode_reward_mean: 596.0001495913068, episode_len_mean: 300.0, agent_steps_trained: 229376, env_steps_trained: 114688, entropy: 3.328010957191388, learning_rate: 0.0010000000000000005\n",
      "iteration [29] => episode_reward_mean: 599.8579169317861, episode_len_mean: 300.0, agent_steps_trained: 237568, env_steps_trained: 118784, entropy: 3.349336307992538, learning_rate: 0.0010000000000000005\n",
      "iteration [30] => episode_reward_mean: 603.4171706591706, episode_len_mean: 300.0, agent_steps_trained: 245760, env_steps_trained: 122880, entropy: 3.2973313165207703, learning_rate: 0.0010000000000000005\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "abcec2f0359a44ce9ab36e6a70d2cd1a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "CanvasWithBorders(height=300, width=300)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "obs:  {'agent-0': array([-0.4472136,  0.8944272, 35.77709  ], dtype=float32), 'agent-1': array([ 0.4472136, -0.8944272, 35.77709  ], dtype=float32)}\n",
      "action:  {'agent-0': array([ 0.93228984, -0.17839646,  0.80335176], dtype=float32), 'agent-1': array([-1.        ,  0.72599816,  1.        ], dtype=float32)}\n",
      "reward:  {'agent-0': 1.35772393719202, 'agent-1': 1.35772393719202} \n",
      "\n",
      "obs:  {'agent-0': array([-0.41841963,  0.90825385, 34.419365  ], dtype=float32), 'agent-1': array([ 0.41841963, -0.90825385, 34.419365  ], dtype=float32)}\n",
      "action:  {'agent-0': array([ 0.47593868, -1.        ,  0.8037338 ], dtype=float32), 'agent-1': array([0.15418804, 1.        , 1.        ], dtype=float32)}\n",
      "reward:  {'agent-0': 1.63307693167814, 'agent-1': 1.63307693167814} \n",
      "\n",
      "obs:  {'agent-0': array([-0.43337393,  0.9012142 , 32.786285  ], dtype=float32), 'agent-1': array([ 0.43337393, -0.9012142 , 32.786285  ], dtype=float32)}\n",
      "action:  {'agent-0': array([ 0.24133325, -1.        ,  1.        ], dtype=float32), 'agent-1': array([-0.15044987,  1.        ,  1.        ], dtype=float32)}\n",
      "reward:  {'agent-0': 1.9292706061529614, 'agent-1': 1.9292706061529614} \n",
      "\n",
      "obs:  {'agent-0': array([-0.44804552,  0.8940107 , 30.857016  ], dtype=float32), 'agent-1': array([ 0.44804552, -0.8940107 , 30.857016  ], dtype=float32)}\n",
      "action:  {'agent-0': array([ 1., -1.,  1.], dtype=float32), 'agent-1': array([-0.8417731 ,  1.        ,  0.87403256], dtype=float32)}\n",
      "reward:  {'agent-0': 1.7943279554823377, 'agent-1': 1.7943279554823377} \n",
      "\n",
      "obs:  {'agent-0': array([-0.4320101 ,  0.90186876, 29.062689  ], dtype=float32), 'agent-1': array([ 0.4320101 , -0.90186876, 29.062689  ], dtype=float32)}\n",
      "action:  {'agent-0': array([ 0.4099177 , -1.        ,  0.78837353], dtype=float32), 'agent-1': array([-0.6240711,  1.       ,  1.       ], dtype=float32)}\n",
      "reward:  {'agent-0': 1.7808056048701033, 'agent-1': 1.7808056048701033} \n",
      "\n",
      "obs:  {'agent-0': array([-0.42984286,  0.90290374, 27.281883  ], dtype=float32), 'agent-1': array([ 0.42984286, -0.90290374, 27.281883  ], dtype=float32)}\n",
      "action:  {'agent-0': array([ 0.1998682, -1.       ,  1.       ], dtype=float32), 'agent-1': array([-0.04411101,  1.        ,  1.        ], dtype=float32)}\n",
      "reward:  {'agent-0': 1.8826885044724726, 'agent-1': 1.8826885044724726} \n",
      "\n",
      "obs:  {'agent-0': array([-0.45225304,  0.8918897 , 25.399195  ], dtype=float32), 'agent-1': array([ 0.45225304, -0.8918897 , 25.399195  ], dtype=float32)}\n",
      "action:  {'agent-0': array([ 1., -1.,  1.], dtype=float32), 'agent-1': array([-0.51298016,  1.        ,  0.9756477 ], dtype=float32)}\n",
      "reward:  {'agent-0': 1.9239699401411663, 'agent-1': 1.9239699401411663} \n",
      "\n",
      "obs:  {'agent-0': array([-0.4402276,  0.8978862, 23.475224 ], dtype=float32), 'agent-1': array([ 0.4402276, -0.8978862, 23.475224 ], dtype=float32)}\n",
      "action:  {'agent-0': array([0.58056355, 0.45263517, 0.26482058], dtype=float32), 'agent-1': array([-0.71045536,  1.        ,  1.        ], dtype=float32)}\n",
      "reward:  {'agent-0': 0.9287508892932763, 'agent-1': 0.9287508892932763} \n",
      "\n",
      "obs:  {'agent-0': array([-0.42341104,  0.9059377 , 22.546473  ], dtype=float32), 'agent-1': array([ 0.42341104, -0.9059377 , 22.546473  ], dtype=float32)}\n",
      "action:  {'agent-0': array([ 0.9055842 , -0.96766686,  0.49681768], dtype=float32), 'agent-1': array([-0.91058475,  1.        ,  0.9557551 ], dtype=float32)}\n",
      "reward:  {'agent-0': 1.3805011743677476, 'agent-1': 1.3805011743677476} \n",
      "\n",
      "obs:  {'agent-0': array([-0.40458637,  0.9144998 , 21.165972  ], dtype=float32), 'agent-1': array([ 0.40458637, -0.9144998 , 21.165972  ], dtype=float32)}\n",
      "action:  {'agent-0': array([ 0.6303904, -1.       ,  0.7119421], dtype=float32), 'agent-1': array([-0.08203042,  0.354841  ,  0.9280351 ], dtype=float32)}\n",
      "reward:  {'agent-0': 1.6156939521001128, 'agent-1': 1.6156939521001128} \n",
      "\n",
      "obs:  {'agent-0': array([-0.4079112,  0.9130216, 19.550278 ], dtype=float32), 'agent-1': array([ 0.4079112, -0.9130216, 19.550278 ], dtype=float32)}\n",
      "action:  {'agent-0': array([ 1., -1.,  1.], dtype=float32), 'agent-1': array([-0.36872065,  1.        ,  0.9706974 ], dtype=float32)}\n",
      "reward:  {'agent-0': 1.9001458684887567, 'agent-1': 1.9001458684887567} \n",
      "\n",
      "obs:  {'agent-0': array([-0.39273673,  0.919651  , 17.650133  ], dtype=float32), 'agent-1': array([ 0.39273673, -0.919651  , 17.650133  ], dtype=float32)}\n",
      "action:  {'agent-0': array([ 0.80490613, -1.        ,  0.8519956 ], dtype=float32), 'agent-1': array([-0.32113492,  1.        ,  0.2170971 ], dtype=float32)}\n",
      "reward:  {'agent-0': 1.0350133360674647, 'agent-1': 1.0350133360674647} \n",
      "\n",
      "obs:  {'agent-0': array([-0.3810539,  0.9245528, 16.615118 ], dtype=float32), 'agent-1': array([ 0.3810539, -0.9245528, 16.615118 ], dtype=float32)}\n",
      "action:  {'agent-0': array([ 0.40411544, -0.45990586,  1.        ], dtype=float32), 'agent-1': array([-0.59355986,  0.75297976,  1.        ], dtype=float32)}\n",
      "reward:  {'agent-0': 1.8959157323069942, 'agent-1': 1.8959157323069942} \n",
      "\n",
      "obs:  {'agent-0': array([-0.3432329 ,  0.93925035, 14.719203  ], dtype=float32), 'agent-1': array([ 0.3432329 , -0.93925035, 14.719203  ], dtype=float32)}\n",
      "action:  {'agent-0': array([-0.42342377, -1.        ,  0.8688137 ], dtype=float32), 'agent-1': array([-0.11630499,  1.        ,  1.        ], dtype=float32)}\n",
      "reward:  {'agent-0': 1.581843984905607, 'agent-1': 1.581843984905607} \n",
      "\n",
      "obs:  {'agent-0': array([-0.40155318,  0.9158357 , 13.13736   ], dtype=float32), 'agent-1': array([ 0.40155318, -0.9158357 , 13.13736   ], dtype=float32)}\n",
      "action:  {'agent-0': array([ 1., -1.,  1.], dtype=float32), 'agent-1': array([-0.70778656,  1.        ,  1.        ], dtype=float32)}\n",
      "reward:  {'agent-0': 1.896848954333766, 'agent-1': 1.896848954333766} \n",
      "\n",
      "obs:  {'agent-0': array([-0.35501248,  0.93486154, 11.24051   ], dtype=float32), 'agent-1': array([ 0.35501248, -0.93486154, 11.24051   ], dtype=float32)}\n",
      "action:  {'agent-0': array([-0.2564398, -1.       ,  1.       ], dtype=float32), 'agent-1': array([-0.32356697,  1.        ,  1.        ], dtype=float32)}\n",
      "reward:  {'agent-0': 1.7953520892789747, 'agent-1': 1.7953520892789747} \n",
      "\n",
      "obs:  {'agent-0': array([-0.4161996 ,  0.90927327,  9.445158  ], dtype=float32), 'agent-1': array([ 0.4161996 , -0.90927327,  9.445158  ], dtype=float32)}\n",
      "action:  {'agent-0': array([ 0.15766299, -1.        ,  1.        ], dtype=float32), 'agent-1': array([-0.3565457 ,  1.        ,  0.69993985], dtype=float32)}\n",
      "reward:  {'agent-0': 1.6533055175741893, 'agent-1': 1.6533055175741893} \n",
      "\n",
      "obs:  {'agent-0': array([-0.45435482,  0.8908208 ,  7.7918525 ], dtype=float32), 'agent-1': array([ 0.45435482, -0.8908208 ,  7.7918525 ], dtype=float32)}\n",
      "action:  {'agent-0': array([ 0.5184549, -1.       ,  1.       ], dtype=float32), 'agent-1': array([-0.55000377,  1.        ,  1.        ], dtype=float32)}\n",
      "reward:  {'agent-0': 1.999367584637631, 'agent-1': 1.999367584637631} \n",
      "\n",
      "obs:  {'agent-0': array([-0.44852456,  0.8937705 ,  5.792485  ], dtype=float32), 'agent-1': array([ 0.44852456, -0.8937705 ,  5.792485  ], dtype=float32)}\n",
      "action:  {'agent-0': array([ 0.24656296, -1.        ,  0.96329516], dtype=float32), 'agent-1': array([-0.8465425 ,  1.        ,  0.68376493], dtype=float32)}\n",
      "reward:  {'agent-0': 1.6036247212782122, 'agent-1': 1.6036247212782122} \n",
      "\n",
      "obs:  {'agent-0': array([-0.45971322,  0.8880674 ,  4.1888604 ], dtype=float32), 'agent-1': array([ 0.45971322, -0.8880674 ,  4.1888604 ], dtype=float32)}\n",
      "action:  {'agent-0': array([ 0.36702335, -0.75211805,  1.        ], dtype=float32), 'agent-1': array([-1.        ,  0.48883414,  1.        ], dtype=float32)}\n",
      "reward:  {'agent-0': 1.7350751842971932, 'agent-1': 1.7350751842971932} \n",
      "\n",
      "obs:  {'agent-0': array([-0.23992129,  0.97079235,  2.4537852 ], dtype=float32), 'agent-1': array([ 0.23992129, -0.97079235,  2.4537852 ], dtype=float32)}\n",
      "action:  {'agent-0': array([ 1.       , -1.       ,  0.9247196], dtype=float32), 'agent-1': array([-0.47703385,  1.        ,  1.        ], dtype=float32)}\n",
      "reward:  {'agent-0': 1.4907313554752741, 'agent-1': 1.4907313554752741} \n",
      "\n",
      "obs:  {'agent-0': array([0.5147315, 0.8573514, 0.9630538], dtype=float32), 'agent-1': array([-0.5147315, -0.8573514,  0.9630538], dtype=float32)}\n",
      "action:  {'agent-0': array([-1.      , -0.958294,  1.      ], dtype=float32), 'agent-1': array([1.        , 0.79119456, 0.23362559], dtype=float32)}\n",
      "reward:  {'agent-0': 1.5533979069332475, 'agent-1': 1.5533979069332475} \n",
      "\n",
      "obs:  {'agent-0': array([-0.99962795, -0.02727634,  0.4096559 ], dtype=float32), 'agent-1': array([0.99962795, 0.02727634, 0.4096559 ], dtype=float32)}\n",
      "action:  {'agent-0': array([1.       , 0.7540169, 0.1373634], dtype=float32), 'agent-1': array([-1.        ,  1.        ,  0.10824865], dtype=float32)}\n",
      "reward:  {'agent-0': 1.1863182953900577, 'agent-1': 1.1863182953900577} \n",
      "\n",
      "obs:  {'agent-0': array([-0.99974763, -0.02246614,  0.22333762], dtype=float32), 'agent-1': array([0.99974763, 0.02246614, 0.22333762], dtype=float32)}\n",
      "action:  {'agent-0': array([0.5470309, 0.9152627, 0.       ], dtype=float32), 'agent-1': array([ 0.22920477, -1.        ,  0.17412215], dtype=float32)}\n",
      "reward:  {'agent-0': 0.9137140896318069, 'agent-1': 0.9137140896318069} \n",
      "\n",
      "obs:  {'agent-0': array([-0.84677714,  0.5319478 ,  0.3096235 ], dtype=float32), 'agent-1': array([ 0.84677714, -0.5319478 ,  0.3096235 ], dtype=float32)}\n",
      "action:  {'agent-0': array([ 0.40859485, -0.20052803,  0.31935647], dtype=float32), 'agent-1': array([-0.84670174, -0.02143335,  0.45737344], dtype=float32)}\n",
      "reward:  {'agent-0': 0.826575562920339, 'agent-1': 0.826575562920339} \n",
      "\n",
      "obs:  {'agent-0': array([0.99728405, 0.07365101, 0.48304796], dtype=float32), 'agent-1': array([-0.99728405, -0.07365101,  0.48304796], dtype=float32)}\n",
      "action:  {'agent-0': array([ 0.09389639, -1.        ,  0.3488434 ], dtype=float32), 'agent-1': array([0.65122104, 0.03060663, 0.13314426], dtype=float32)}\n",
      "reward:  {'agent-0': 0.9865146533211222, 'agent-1': 0.9865146533211222} \n",
      "\n",
      "obs:  {'agent-0': array([ 0.7680256, -0.6404192,  0.4965333], dtype=float32), 'agent-1': array([-0.7680256,  0.6404192,  0.4965333], dtype=float32)}\n",
      "action:  {'agent-0': array([-0.00822723,  1.        ,  0.30663303], dtype=float32), 'agent-1': array([ 1.        , -0.5083235 ,  0.35619044], dtype=float32)}\n",
      "reward:  {'agent-0': 1.3344545786672932, 'agent-1': 1.3344545786672932} \n",
      "\n",
      "obs:  {'agent-0': array([0.37824443, 0.92570573, 0.16207872], dtype=float32), 'agent-1': array([-0.37824443, -0.92570573,  0.16207872], dtype=float32)}\n",
      "action:  {'agent-0': array([ 1., -1.,  0.], dtype=float32), 'agent-1': array([0.8896339 , 1.        , 0.10598114], dtype=float32)}\n",
      "reward:  {'agent-0': 1.0906367396954288, 'agent-1': 1.0906367396954288} \n",
      "\n",
      "obs:  {'agent-0': array([-0.12790248,  0.9917868 ,  0.07144199], dtype=float32), 'agent-1': array([ 0.12790248, -0.9917868 ,  0.07144199], dtype=float32)}\n",
      "action:  {'agent-0': array([-0.15478194, -1.        ,  0.        ], dtype=float32), 'agent-1': array([-0.61905867, -0.7208432 ,  0.        ], dtype=float32)}\n",
      "reward:  {'agent-0': 1.0, 'agent-1': 1.0} \n",
      "\n",
      "obs:  {'agent-0': array([-0.12790248,  0.9917868 ,  0.07144199], dtype=float32), 'agent-1': array([ 0.12790248, -0.9917868 ,  0.07144199], dtype=float32)}\n",
      "action:  {'agent-0': array([ 1.        , -0.16922808,  0.        ], dtype=float32), 'agent-1': array([0.08091855, 0.04517484, 0.        ], dtype=float32)}\n",
      "reward:  {'agent-0': 1.0, 'agent-1': 1.0} \n",
      "\n",
      "obs:  {'agent-0': array([-0.12790248,  0.9917868 ,  0.07144199], dtype=float32), 'agent-1': array([ 0.12790248, -0.9917868 ,  0.07144199], dtype=float32)}\n",
      "action:  {'agent-0': array([-1.        , -0.44836354,  0.        ], dtype=float32), 'agent-1': array([0.7723088 , 0.31505895, 0.        ], dtype=float32)}\n",
      "reward:  {'agent-0': 1.0, 'agent-1': 1.0} \n",
      "\n",
      "obs:  {'agent-0': array([-0.12790248,  0.9917868 ,  0.07144199], dtype=float32), 'agent-1': array([ 0.12790248, -0.9917868 ,  0.07144199], dtype=float32)}\n",
      "action:  {'agent-0': array([-0.00817412, -1.        ,  0.48356867], dtype=float32), 'agent-1': array([-0.5574116 ,  0.6079099 ,  0.28671002], dtype=float32)}\n",
      "reward:  {'agent-0': -0.5782070392673244, 'agent-1': -0.5782070392673244} \n",
      "\n",
      "obs:  {'agent-0': array([ 0.27811506, -0.96054775,  0.649649  ], dtype=float32), 'agent-1': array([-0.27811506,  0.96054775,  0.649649  ], dtype=float32)}\n",
      "action:  {'agent-0': array([0.22241807, 1.        , 0.3326051 ], dtype=float32), 'agent-1': array([ 1.        , -1.        ,  0.18771994], dtype=float32)}\n",
      "reward:  {'agent-0': 1.44423426071512, 'agent-1': 1.44423426071512} \n",
      "\n",
      "obs:  {'agent-0': array([ 0.5849234 , -0.8110885 ,  0.20541477], dtype=float32), 'agent-1': array([-0.5849234 ,  0.8110885 ,  0.20541477], dtype=float32)}\n",
      "action:  {'agent-0': array([-1.        , -0.06030774,  0.        ], dtype=float32), 'agent-1': array([ 1., -1.,  0.], dtype=float32)}\n",
      "reward:  {'agent-0': 1.0, 'agent-1': 1.0} \n",
      "\n",
      "obs:  {'agent-0': array([ 0.5849234 , -0.8110885 ,  0.20541477], dtype=float32), 'agent-1': array([-0.5849234 ,  0.8110885 ,  0.20541477], dtype=float32)}\n",
      "action:  {'agent-0': array([ 0.7301152 , -0.14625478,  0.        ], dtype=float32), 'agent-1': array([ 0.7052932 , -0.93488014,  0.24875894], dtype=float32)}\n",
      "reward:  {'agent-0': 1.1617979992636864, 'agent-1': 1.1617979992636864} \n",
      "\n",
      "obs:  {'agent-0': array([-0.68012244,  0.7330985 ,  0.04361677], dtype=float32), 'agent-1': array([ 0.68012244, -0.7330985 ,  0.04361677], dtype=float32)}\n",
      "action:  {'agent-0': array([1.       , 0.9839598, 0.8176191], dtype=float32), 'agent-1': array([0.15816367, 0.42118979, 0.        ], dtype=float32)}\n",
      "reward:  {'agent-0': -0.7764435036964044, 'agent-1': -0.7764435036964044} \n",
      "\n",
      "obs:  {'agent-0': array([0.67450464, 0.7382706 , 0.82006025], dtype=float32), 'agent-1': array([-0.67450464, -0.7382706 ,  0.82006025], dtype=float32)}\n",
      "action:  {'agent-0': array([-0.7986876 ,  0.5347936 ,  0.06736454], dtype=float32), 'agent-1': array([0.01599765, 0.56049466, 1.        ], dtype=float32)}\n",
      "reward:  {'agent-0': 0.23113055698059237, 'agent-1': 0.23113055698059237} \n",
      "\n",
      "obs:  {'agent-0': array([ 0.79573   , -0.60565156,  0.5889297 ], dtype=float32), 'agent-1': array([-0.79573   ,  0.60565156,  0.5889297 ], dtype=float32)}\n",
      "action:  {'agent-0': array([-1.        ,  0.04184282,  0.        ], dtype=float32), 'agent-1': array([ 1.       , -0.7001514,  0.       ], dtype=float32)}\n",
      "reward:  {'agent-0': 0.0, 'agent-1': 0.0} \n",
      "\n",
      "obs:  {'agent-0': array([ 0.79573   , -0.60565156,  0.5889297 ], dtype=float32), 'agent-1': array([-0.79573   ,  0.60565156,  0.5889297 ], dtype=float32)}\n",
      "action:  {'agent-0': array([-1.        ,  0.07244563,  0.        ], dtype=float32), 'agent-1': array([ 1.        , -1.        ,  0.02269915], dtype=float32)}\n",
      "reward:  {'agent-0': 0.022484967844318637, 'agent-1': 0.022484967844318637} \n",
      "\n",
      "obs:  {'agent-0': array([ 0.79898053, -0.6013569 ,  0.56644475], dtype=float32), 'agent-1': array([-0.79898053,  0.6013569 ,  0.56644475], dtype=float32)}\n",
      "action:  {'agent-0': array([-1.        ,  1.        ,  0.15107897], dtype=float32), 'agent-1': array([ 1.        , -0.5256555 ,  0.33330703], dtype=float32)}\n",
      "reward:  {'agent-0': 1.472797987668293, 'agent-1': 1.472797987668293} \n",
      "\n",
      "obs:  {'agent-0': array([ 0.54160595, -0.8406325 ,  0.09364676], dtype=float32), 'agent-1': array([-0.54160595,  0.8406325 ,  0.09364676], dtype=float32)}\n",
      "action:  {'agent-0': array([-0.97458315,  0.4275186 ,  0.        ], dtype=float32), 'agent-1': array([-0.8102484 ,  0.17263508,  0.        ], dtype=float32)}\n",
      "reward:  {'agent-0': 1.0, 'agent-1': 1.0} \n",
      "\n",
      "obs:  {'agent-0': array([ 0.54160595, -0.8406325 ,  0.09364676], dtype=float32), 'agent-1': array([-0.54160595,  0.8406325 ,  0.09364676], dtype=float32)}\n",
      "action:  {'agent-0': array([0.5435258 , 0.84047127, 0.5836774 ], dtype=float32), 'agent-1': array([-0.9905745 , -0.79861546,  0.        ], dtype=float32)}\n",
      "reward:  {'agent-0': -0.4581085388656593, 'agent-1': -0.4581085388656593} \n",
      "\n",
      "obs:  {'agent-0': array([0.6663757 , 0.74561614, 0.5517553 ], dtype=float32), 'agent-1': array([-0.6663757 , -0.74561614,  0.5517553 ], dtype=float32)}\n",
      "action:  {'agent-0': array([-1.       , -1.       ,  0.2054568], dtype=float32), 'agent-1': array([ 1.        , -0.01508552,  0.03017402], dtype=float32)}\n",
      "reward:  {'agent-0': 1.2231036740897099, 'agent-1': 1.2231036740897099} \n",
      "\n",
      "obs:  {'agent-0': array([0.5848924 , 0.8111109 , 0.32865164], dtype=float32), 'agent-1': array([-0.5848924 , -0.8111109 ,  0.32865164], dtype=float32)}\n",
      "action:  {'agent-0': array([-1.       , -0.5558251,  0.427484 ], dtype=float32), 'agent-1': array([1.        , 1.        , 0.20390978], dtype=float32)}\n",
      "reward:  {'agent-0': 0.9920595701612989, 'agent-1': 0.9920595701612989} \n",
      "\n",
      "obs:  {'agent-0': array([-0.9673599 , -0.25340652,  0.33659205], dtype=float32), 'agent-1': array([0.9673599 , 0.25340652, 0.33659205], dtype=float32)}\n",
      "action:  {'agent-0': array([ 1.       , -0.9205156,  0.       ], dtype=float32), 'agent-1': array([ 0.5983063 , -0.32849813,  0.18264729], dtype=float32)}\n",
      "reward:  {'agent-0': 0.8508765340585381, 'agent-1': 0.8508765340585381} \n",
      "\n",
      "obs:  {'agent-0': array([-0.9999856 ,  0.00537215,  0.4857155 ], dtype=float32), 'agent-1': array([ 0.9999856 , -0.00537215,  0.4857155 ], dtype=float32)}\n",
      "action:  {'agent-0': array([1.        , 0.08839989, 0.3886686 ], dtype=float32), 'agent-1': array([-0.638591  , -1.        ,  0.07449734], dtype=float32)}\n",
      "reward:  {'agent-0': 1.3702109241546583, 'agent-1': 1.3702109241546583} \n",
      "\n",
      "obs:  {'agent-0': array([-0.5060787 ,  0.8624873 ,  0.11550459], dtype=float32), 'agent-1': array([ 0.5060787 , -0.8624873 ,  0.11550459], dtype=float32)}\n",
      "action:  {'agent-0': array([ 0.50268126, -0.08223605,  0.        ], dtype=float32), 'agent-1': array([-0.9466722 ,  0.4555931 ,  0.18127996], dtype=float32)}\n",
      "reward:  {'agent-0': 1.0085279492255528, 'agent-1': 1.0085279492255528} \n",
      "\n",
      "obs:  {'agent-0': array([0.9805264 , 0.19638744, 0.10697664], dtype=float32), 'agent-1': array([-0.9805264 , -0.19638744,  0.10697664], dtype=float32)}\n",
      "action:  {'agent-0': array([-0.39089406, -1.        ,  0.        ], dtype=float32), 'agent-1': array([1.       , 0.6985618, 0.       ], dtype=float32)}\n",
      "reward:  {'agent-0': 1.0, 'agent-1': 1.0} \n",
      "\n",
      "obs:  {'agent-0': array([0.9805264 , 0.19638744, 0.10697664], dtype=float32), 'agent-1': array([-0.9805264 , -0.19638744,  0.10697664], dtype=float32)}\n",
      "action:  {'agent-0': array([-1.       ,  0.3490293,  0.       ], dtype=float32), 'agent-1': array([0.344759  , 0.44704628, 0.        ], dtype=float32)}\n",
      "reward:  {'agent-0': 1.0, 'agent-1': 1.0} \n",
      "\n",
      "obs:  {'agent-0': array([0.9805264 , 0.19638744, 0.10697664], dtype=float32), 'agent-1': array([-0.9805264 , -0.19638744,  0.10697664], dtype=float32)}\n",
      "action:  {'agent-0': array([ 0.7875941, -1.       ,  0.       ], dtype=float32), 'agent-1': array([ 0.85621333, -0.10429955,  0.        ], dtype=float32)}\n",
      "reward:  {'agent-0': 1.0, 'agent-1': 1.0} \n",
      "\n"
     ]
    }
   ],
   "source": [
    "from ray.rllib.algorithms.ppo import PPOConfig\n",
    "from ray.tune.logger import pretty_print\n",
    "\n",
    "from gymnasium.wrappers.time_limit import TimeLimit\n",
    "\n",
    "trainin_steps = 30\n",
    "\n",
    "algo = (\n",
    "    PPOConfig()\n",
    "    .training(gamma = 0.95, \n",
    "              lr = 0.001,\n",
    "              train_batch_size = 4096, \n",
    "              sgd_minibatch_size = 256, \n",
    "              num_sgd_iter = 30,\n",
    "              #entropy_coeff=0.005,\n",
    "              )\n",
    "    .env_runners(num_env_runners=1)\n",
    "    .resources(num_gpus=0)\n",
    "    .environment(env=\"KeepTheDistance?dst=0&agent=2&spawn_area=100\")\n",
    "    .build()\n",
    ")\n",
    "clear_output()\n",
    "\n",
    "out = \"\"\n",
    "for i in range(trainin_steps):\n",
    "    result = algo.train()\n",
    "    clear_output()\n",
    "    out += ppo_result_format(result) + \"\\n\"\n",
    "    print(out)\n",
    "    simulate_episode(RenderableKeepTheDistance(env_config), algo, 50, sleep_between_frames=0.08, print_info=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3bbf86ca5ff74da487ebf89baa653890",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "CanvasWithBorders(height=300, width=300)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "env_config_2 = EnvironmentConfiguration(n_agents=2, target_distance=0, max_steps=500, speed=1, spawn_area=500)\n",
    "simulate_episode(RenderableKeepTheDistance(env_config_2), algo, 200, sleep_between_frames=0.03, print_info=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "An Algorithm checkpoint has been created inside directory: 'TrainingResult(checkpoint=Checkpoint(filesystem=local, path=/mnt/c/Users/nicol/Desktop/Università/tesi/experiments/RL_experiments/algos/KeepTheDistance?dst=0&agent=2&spawn_area=100), metrics={'custom_metrics': {}, 'episode_media': {}, 'info': {'learner': {'default_policy': {'learner_stats': {'allreduce_latency': 0.0, 'grad_gnorm': 5.732406087343891, 'cur_kl_coeff': 0.6750000000000002, 'cur_lr': 0.0010000000000000005, 'total_loss': 2.0669429610628867, 'policy_loss': -0.009641030859590198, 'vf_loss': 2.0618812701043985, 'vf_explained_var': 0.41663303778817257, 'kl': 0.021781805181126438, 'entropy': 3.2973313165207703, 'entropy_coeff': 0.0}, 'model': {}, 'custom_metrics': {}, 'num_agent_steps_trained': 256.0, 'num_grad_updates_lifetime': 28320.5, 'diff_num_grad_updates_vs_sampler_policy': 479.5}}, 'num_env_steps_sampled': 122880, 'num_env_steps_trained': 122880, 'num_agent_steps_sampled': 245760, 'num_agent_steps_trained': 245760}, 'sampler_results': {'episode_reward_max': 656.3426232576753, 'episode_reward_min': 554.0001613174567, 'episode_reward_mean': 603.4171706591706, 'episode_len_mean': 300.0, 'episode_media': {}, 'episodes_this_iter': 14, 'episodes_timesteps_total': 30000, 'policy_reward_min': {}, 'policy_reward_max': {}, 'policy_reward_mean': {}, 'custom_metrics': {}, 'hist_stats': {'episode_reward': [557.9595628590914, 598.6723485242048, 576.7905364410914, 623.9128949566787, 593.9106186498275, 569.062897996119, 590.2013885065847, 589.5060260096283, 597.8507385469798, 603.3792850540186, 569.6136375997933, 644.0588800895705, 593.328203558257, 570.5709290327773, 571.9257168448096, 588.9998994945554, 598.7589479373542, 596.5288003010861, 570.1350830746087, 568.0480647727866, 620.0024518546635, 576.4197241751825, 601.7157758808075, 584.2411923236373, 604.9850591253866, 554.0001613174567, 611.4221957176887, 584.6438986226336, 576.1361015052117, 589.8023402153758, 626.6456881142497, 572.008304005685, 591.9557121248316, 612.8474983675018, 609.5166032129912, 603.8943007676796, 636.1130024943114, 611.9740266708382, 602.5425770692818, 628.1434802365283, 610.6536786191289, 609.0682140526151, 578.0150787062627, 625.5863911032764, 588.5307544495234, 622.7886128417579, 621.1413480448987, 632.7839459952713, 610.1124621743139, 592.1165754452259, 611.1495018386091, 583.7511119350513, 617.5665974881094, 616.9876188074414, 568.6591477732395, 619.8716461880304, 594.7810736546629, 611.3439781196436, 593.3561105262047, 591.1017537228189, 625.1183979513283, 632.7680447687559, 614.8452452281645, 627.5385971308539, 596.5194669359113, 592.5112024770766, 573.9262997589445, 630.0462283258863, 587.5262928094561, 587.3002309029821, 649.5510743605195, 647.6793445327738, 587.8095600275365, 601.3975749074691, 584.5722420902252, 595.9262462625873, 606.7030501305279, 625.70331948732, 623.5837370558404, 607.4836750975663, 589.7268695213564, 617.6180812142297, 603.5783583390864, 599.7037170633893, 603.8087675304085, 649.9313244777413, 588.2463279765618, 638.4685068391265, 611.2580967862943, 637.421943401637, 621.1299963854806, 591.8926883816252, 583.5864293679192, 606.1657572522845, 656.3426232576753, 625.6637389284845, 598.4456045648764, 630.5142560957363, 610.6707109474588, 607.4432818060959], 'episode_lengths': [300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300]}, 'sampler_perf': {'mean_raw_obs_processing_ms': 0.34054384452797193, 'mean_inference_ms': 0.8364510221606176, 'mean_action_processing_ms': 0.21899213523915542, 'mean_env_wait_ms': 0.3332731499742183, 'mean_env_render_ms': 0.0}, 'num_faulty_episodes': 0, 'connector_metrics': {'ObsPreprocessorConnector_ms': 0.0053632259368896484, 'StateBufferConnector_ms': 0.0042836666107177734, 'ViewRequirementAgentConnector_ms': 0.12398958206176758}, 'num_episodes': 14, 'episode_return_max': 656.3426232576753, 'episode_return_min': 554.0001613174567, 'episode_return_mean': 603.4171706591706}, 'env_runner_results': {'episode_reward_max': 656.3426232576753, 'episode_reward_min': 554.0001613174567, 'episode_reward_mean': 603.4171706591706, 'episode_len_mean': 300.0, 'episode_media': {}, 'episodes_this_iter': 14, 'episodes_timesteps_total': 30000, 'policy_reward_min': {}, 'policy_reward_max': {}, 'policy_reward_mean': {}, 'custom_metrics': {}, 'hist_stats': {'episode_reward': [557.9595628590914, 598.6723485242048, 576.7905364410914, 623.9128949566787, 593.9106186498275, 569.062897996119, 590.2013885065847, 589.5060260096283, 597.8507385469798, 603.3792850540186, 569.6136375997933, 644.0588800895705, 593.328203558257, 570.5709290327773, 571.9257168448096, 588.9998994945554, 598.7589479373542, 596.5288003010861, 570.1350830746087, 568.0480647727866, 620.0024518546635, 576.4197241751825, 601.7157758808075, 584.2411923236373, 604.9850591253866, 554.0001613174567, 611.4221957176887, 584.6438986226336, 576.1361015052117, 589.8023402153758, 626.6456881142497, 572.008304005685, 591.9557121248316, 612.8474983675018, 609.5166032129912, 603.8943007676796, 636.1130024943114, 611.9740266708382, 602.5425770692818, 628.1434802365283, 610.6536786191289, 609.0682140526151, 578.0150787062627, 625.5863911032764, 588.5307544495234, 622.7886128417579, 621.1413480448987, 632.7839459952713, 610.1124621743139, 592.1165754452259, 611.1495018386091, 583.7511119350513, 617.5665974881094, 616.9876188074414, 568.6591477732395, 619.8716461880304, 594.7810736546629, 611.3439781196436, 593.3561105262047, 591.1017537228189, 625.1183979513283, 632.7680447687559, 614.8452452281645, 627.5385971308539, 596.5194669359113, 592.5112024770766, 573.9262997589445, 630.0462283258863, 587.5262928094561, 587.3002309029821, 649.5510743605195, 647.6793445327738, 587.8095600275365, 601.3975749074691, 584.5722420902252, 595.9262462625873, 606.7030501305279, 625.70331948732, 623.5837370558404, 607.4836750975663, 589.7268695213564, 617.6180812142297, 603.5783583390864, 599.7037170633893, 603.8087675304085, 649.9313244777413, 588.2463279765618, 638.4685068391265, 611.2580967862943, 637.421943401637, 621.1299963854806, 591.8926883816252, 583.5864293679192, 606.1657572522845, 656.3426232576753, 625.6637389284845, 598.4456045648764, 630.5142560957363, 610.6707109474588, 607.4432818060959], 'episode_lengths': [300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300]}, 'sampler_perf': {'mean_raw_obs_processing_ms': 0.34054384452797193, 'mean_inference_ms': 0.8364510221606176, 'mean_action_processing_ms': 0.21899213523915542, 'mean_env_wait_ms': 0.3332731499742183, 'mean_env_render_ms': 0.0}, 'num_faulty_episodes': 0, 'connector_metrics': {'ObsPreprocessorConnector_ms': 0.0053632259368896484, 'StateBufferConnector_ms': 0.0042836666107177734, 'ViewRequirementAgentConnector_ms': 0.12398958206176758}, 'num_episodes': 14, 'episode_return_max': 656.3426232576753, 'episode_return_min': 554.0001613174567, 'episode_return_mean': 603.4171706591706}, 'episode_reward_max': 656.3426232576753, 'episode_reward_min': 554.0001613174567, 'episode_reward_mean': 603.4171706591706, 'episode_len_mean': 300.0, 'episodes_this_iter': 14, 'episodes_timesteps_total': 30000, 'policy_reward_min': {}, 'policy_reward_max': {}, 'policy_reward_mean': {}, 'hist_stats': {'episode_reward': [557.9595628590914, 598.6723485242048, 576.7905364410914, 623.9128949566787, 593.9106186498275, 569.062897996119, 590.2013885065847, 589.5060260096283, 597.8507385469798, 603.3792850540186, 569.6136375997933, 644.0588800895705, 593.328203558257, 570.5709290327773, 571.9257168448096, 588.9998994945554, 598.7589479373542, 596.5288003010861, 570.1350830746087, 568.0480647727866, 620.0024518546635, 576.4197241751825, 601.7157758808075, 584.2411923236373, 604.9850591253866, 554.0001613174567, 611.4221957176887, 584.6438986226336, 576.1361015052117, 589.8023402153758, 626.6456881142497, 572.008304005685, 591.9557121248316, 612.8474983675018, 609.5166032129912, 603.8943007676796, 636.1130024943114, 611.9740266708382, 602.5425770692818, 628.1434802365283, 610.6536786191289, 609.0682140526151, 578.0150787062627, 625.5863911032764, 588.5307544495234, 622.7886128417579, 621.1413480448987, 632.7839459952713, 610.1124621743139, 592.1165754452259, 611.1495018386091, 583.7511119350513, 617.5665974881094, 616.9876188074414, 568.6591477732395, 619.8716461880304, 594.7810736546629, 611.3439781196436, 593.3561105262047, 591.1017537228189, 625.1183979513283, 632.7680447687559, 614.8452452281645, 627.5385971308539, 596.5194669359113, 592.5112024770766, 573.9262997589445, 630.0462283258863, 587.5262928094561, 587.3002309029821, 649.5510743605195, 647.6793445327738, 587.8095600275365, 601.3975749074691, 584.5722420902252, 595.9262462625873, 606.7030501305279, 625.70331948732, 623.5837370558404, 607.4836750975663, 589.7268695213564, 617.6180812142297, 603.5783583390864, 599.7037170633893, 603.8087675304085, 649.9313244777413, 588.2463279765618, 638.4685068391265, 611.2580967862943, 637.421943401637, 621.1299963854806, 591.8926883816252, 583.5864293679192, 606.1657572522845, 656.3426232576753, 625.6637389284845, 598.4456045648764, 630.5142560957363, 610.6707109474588, 607.4432818060959], 'episode_lengths': [300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300]}, 'sampler_perf': {'mean_raw_obs_processing_ms': 0.34054384452797193, 'mean_inference_ms': 0.8364510221606176, 'mean_action_processing_ms': 0.21899213523915542, 'mean_env_wait_ms': 0.3332731499742183, 'mean_env_render_ms': 0.0}, 'num_faulty_episodes': 0, 'connector_metrics': {'ObsPreprocessorConnector_ms': 0.0053632259368896484, 'StateBufferConnector_ms': 0.0042836666107177734, 'ViewRequirementAgentConnector_ms': 0.12398958206176758}, 'num_episodes': 14, 'episode_return_max': 656.3426232576753, 'episode_return_min': 554.0001613174567, 'episode_return_mean': 603.4171706591706, 'num_healthy_workers': 1, 'num_in_flight_async_reqs': 0, 'num_remote_worker_restarts': 0, 'num_agent_steps_sampled': 245760, 'num_agent_steps_trained': 245760, 'num_env_steps_sampled': 122880, 'num_env_steps_trained': 122880, 'num_env_steps_sampled_this_iter': 4096, 'num_env_steps_trained_this_iter': 4096, 'num_env_steps_sampled_throughput_per_sec': 312.59178580771174, 'num_env_steps_trained_throughput_per_sec': 312.59178580771174, 'timesteps_total': 122880, 'num_env_steps_sampled_lifetime': 122880, 'num_agent_steps_sampled_lifetime': 245760, 'num_steps_trained_this_iter': 4096, 'agent_timesteps_total': 245760, 'timers': {'training_iteration_time_ms': 13107.549, 'restore_workers_time_ms': 0.021, 'training_step_time_ms': 13107.482, 'sample_time_ms': 6865.865, 'load_time_ms': 0.603, 'load_throughput': 6788315.625, 'learn_time_ms': 6236.751, 'learn_throughput': 656.752, 'synch_weights_time_ms': 3.669}, 'counters': {'num_env_steps_sampled': 122880, 'num_env_steps_trained': 122880, 'num_agent_steps_sampled': 245760, 'num_agent_steps_trained': 245760}, 'done': False, 'episodes_total': 409, 'training_iteration': 30, 'trial_id': 'default', 'date': '2024-05-23_10-17-00', 'timestamp': 1716452220, 'time_this_iter_s': 13.10737919807434, 'time_total_s': 406.53645038604736, 'pid': 3351, 'hostname': 'LAPTOP-9AD2MD1C', 'node_ip': '172.23.87.14', 'config': {'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, '_fake_gpus': False, 'num_learner_workers': 0, 'num_gpus_per_learner_worker': 0, 'num_cpus_per_learner_worker': 1, 'local_gpu_idx': 0, 'custom_resources_per_worker': {}, 'placement_strategy': 'PACK', 'eager_tracing': True, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'torch_compile_learner': False, 'torch_compile_learner_what_to_compile': <TorchCompileWhatToCompile.FORWARD_TRAIN: 'forward_train'>, 'torch_compile_learner_dynamo_backend': 'inductor', 'torch_compile_learner_dynamo_mode': None, 'torch_compile_worker': False, 'torch_compile_worker_dynamo_backend': 'onnxrt', 'torch_compile_worker_dynamo_mode': None, 'enable_rl_module_and_learner': False, 'enable_env_runner_and_connector_v2': False, 'env': 'KeepTheDistance?dst=0&agent=2&spawn_area=100', 'env_config': {}, 'observation_space': None, 'action_space': None, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, '_is_atari': None, 'env_task_fn': None, 'render_env': False, 'action_mask_key': 'action_mask', 'env_runner_cls': None, 'num_envs_per_env_runner': 1, 'validate_env_runners_after_construction': True, 'sample_timeout_s': 60.0, '_env_to_module_connector': None, 'add_default_connectors_to_env_to_module_pipeline': True, '_module_to_env_connector': None, 'add_default_connectors_to_module_to_env_pipeline': True, 'episode_lookback_horizon': 1, 'rollout_fragment_length': 'auto', 'batch_mode': 'truncate_episodes', 'compress_observations': False, 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'enable_tf1_exec_eagerly': False, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'update_worker_filter_stats': True, 'use_worker_filter_stats': True, 'enable_connectors': True, 'sampler_perf_stats_ema_coef': None, 'gamma': 0.95, 'lr': 0.001, 'grad_clip': None, 'grad_clip_by': 'global_norm', 'train_batch_size': 4096, 'train_batch_size_per_learner': None, 'model': {'_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'fcnet_weights_initializer': None, 'fcnet_weights_initializer_config': None, 'fcnet_bias_initializer': None, 'fcnet_bias_initializer_config': None, 'conv_filters': None, 'conv_activation': 'relu', 'conv_kernel_initializer': None, 'conv_kernel_initializer_config': None, 'conv_bias_initializer': None, 'conv_bias_initializer_config': None, 'conv_transpose_kernel_initializer': None, 'conv_transpose_kernel_initializer_config': None, 'conv_transpose_bias_initializer': None, 'conv_transpose_bias_initializer_config': None, 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'post_fcnet_weights_initializer': None, 'post_fcnet_weights_initializer_config': None, 'post_fcnet_bias_initializer': None, 'post_fcnet_bias_initializer_config': None, 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, 'lstm_weights_initializer': None, 'lstm_weights_initializer_config': None, 'lstm_bias_initializer': None, 'lstm_bias_initializer_config': None, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'encoder_latent_dim': None, 'always_check_shapes': False, 'lstm_use_prev_action_reward': -1, '_use_default_native_models': -1}, '_learner_connector': None, 'add_default_connectors_to_learner_pipeline': True, 'optimizer': {}, 'max_requests_in_flight_per_sampler_worker': 2, '_learner_class': None, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'algorithm_config_overrides_per_module': {}, '_per_module_overrides': {}, 'count_steps_by': 'env_steps', 'policy_map_capacity': 100, 'policy_mapping_fn': <function AlgorithmConfig.DEFAULT_POLICY_MAPPING_FN at 0x7fb8255f62a0>, 'policies_to_train': None, 'policy_states_are_swappable': False, 'observation_fn': None, 'input_config': {}, 'actions_in_input_normalized': False, 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_config': {}, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'offline_sampling': False, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_sample_timeout_s': 120.0, 'evaluation_parallel_to_training': False, 'evaluation_force_reset_envs_before_iteration': True, 'evaluation_config': None, 'off_policy_estimation_methods': {}, 'ope_split_batch_by_episode': True, 'evaluation_num_env_runners': 0, 'always_attach_evaluation_results': True, 'in_evaluation': False, 'sync_filters_on_rollout_workers_timeout_s': 10.0, 'keep_per_episode_custom_metrics': False, 'metrics_episode_collection_timeout_s': 60.0, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_iteration': None, 'min_train_timesteps_per_iteration': 0, 'min_sample_timesteps_per_iteration': 0, 'export_native_model_files': False, 'checkpoint_trainable_policies_only': False, 'logger_creator': None, 'logger_config': None, 'log_level': 'WARN', 'log_sys_usage': True, 'fake_sampler': False, 'seed': None, '_run_training_always_in_thread': False, '_evaluation_parallel_to_training_wo_thread': False, 'ignore_env_runner_failures': False, 'recreate_failed_env_runners': False, 'max_num_env_runner_restarts': 1000, 'delay_between_env_runner_restarts_s': 60.0, 'restart_failed_sub_environments': False, 'num_consecutive_env_runner_failures_tolerance': 100, 'env_runner_health_probe_timeout_s': 30, 'env_runner_restore_timeout_s': 1800, '_model_config_dict': {}, '_rl_module_spec': None, '_AlgorithmConfig__prior_exploration_config': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_initialize_loss_from_dummy_batch': False, 'simple_optimizer': False, 'policy_map_cache': -1, 'worker_cls': -1, 'synchronize_filters': -1, 'enable_async_evaluation': -1, 'custom_async_evaluation_function': -1, '_enable_rl_module_api': -1, 'auto_wrap_old_gym_envs': -1, 'disable_env_checking': -1, 'replay_sequence_length': None, '_disable_execution_plan_api': -1, 'lr_schedule': None, 'use_critic': True, 'use_gae': True, 'use_kl_loss': True, 'kl_coeff': 0.2, 'kl_target': 0.01, 'sgd_minibatch_size': 256, 'mini_batch_size_per_learner': None, 'num_sgd_iter': 30, 'shuffle_sequences': True, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.0, 'entropy_coeff_schedule': None, 'clip_param': 0.3, 'vf_clip_param': 10.0, 'vf_share_layers': -1, 'lambda': 1.0, 'input': 'sampler', 'policies': {'default_policy': (None, None, None, None)}, 'callbacks': <class 'ray.rllib.algorithms.callbacks.DefaultCallbacks'>, 'create_env_on_driver': False, 'custom_eval_function': None, 'framework': 'torch', 'num_cpus_for_driver': 1, 'num_workers': 1}, 'time_since_restore': 406.53645038604736, 'iterations_since_restore': 30, 'perf': {'cpu_util_percent': 29.76, 'ram_util_percent': 87.492}})'.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainingResult(checkpoint=Checkpoint(filesystem=local, path=/mnt/c/Users/nicol/Desktop/Università/tesi/experiments/RL_experiments/algos/KeepTheDistance?dst=0&agent=2&spawn_area=100), metrics={'custom_metrics': {}, 'episode_media': {}, 'info': {'learner': {'default_policy': {'learner_stats': {'allreduce_latency': 0.0, 'grad_gnorm': 5.732406087343891, 'cur_kl_coeff': 0.6750000000000002, 'cur_lr': 0.0010000000000000005, 'total_loss': 2.0669429610628867, 'policy_loss': -0.009641030859590198, 'vf_loss': 2.0618812701043985, 'vf_explained_var': 0.41663303778817257, 'kl': 0.021781805181126438, 'entropy': 3.2973313165207703, 'entropy_coeff': 0.0}, 'model': {}, 'custom_metrics': {}, 'num_agent_steps_trained': 256.0, 'num_grad_updates_lifetime': 28320.5, 'diff_num_grad_updates_vs_sampler_policy': 479.5}}, 'num_env_steps_sampled': 122880, 'num_env_steps_trained': 122880, 'num_agent_steps_sampled': 245760, 'num_agent_steps_trained': 245760}, 'sampler_results': {'episode_reward_max': 656.3426232576753, 'episode_reward_min': 554.0001613174567, 'episode_reward_mean': 603.4171706591706, 'episode_len_mean': 300.0, 'episode_media': {}, 'episodes_this_iter': 14, 'episodes_timesteps_total': 30000, 'policy_reward_min': {}, 'policy_reward_max': {}, 'policy_reward_mean': {}, 'custom_metrics': {}, 'hist_stats': {'episode_reward': [557.9595628590914, 598.6723485242048, 576.7905364410914, 623.9128949566787, 593.9106186498275, 569.062897996119, 590.2013885065847, 589.5060260096283, 597.8507385469798, 603.3792850540186, 569.6136375997933, 644.0588800895705, 593.328203558257, 570.5709290327773, 571.9257168448096, 588.9998994945554, 598.7589479373542, 596.5288003010861, 570.1350830746087, 568.0480647727866, 620.0024518546635, 576.4197241751825, 601.7157758808075, 584.2411923236373, 604.9850591253866, 554.0001613174567, 611.4221957176887, 584.6438986226336, 576.1361015052117, 589.8023402153758, 626.6456881142497, 572.008304005685, 591.9557121248316, 612.8474983675018, 609.5166032129912, 603.8943007676796, 636.1130024943114, 611.9740266708382, 602.5425770692818, 628.1434802365283, 610.6536786191289, 609.0682140526151, 578.0150787062627, 625.5863911032764, 588.5307544495234, 622.7886128417579, 621.1413480448987, 632.7839459952713, 610.1124621743139, 592.1165754452259, 611.1495018386091, 583.7511119350513, 617.5665974881094, 616.9876188074414, 568.6591477732395, 619.8716461880304, 594.7810736546629, 611.3439781196436, 593.3561105262047, 591.1017537228189, 625.1183979513283, 632.7680447687559, 614.8452452281645, 627.5385971308539, 596.5194669359113, 592.5112024770766, 573.9262997589445, 630.0462283258863, 587.5262928094561, 587.3002309029821, 649.5510743605195, 647.6793445327738, 587.8095600275365, 601.3975749074691, 584.5722420902252, 595.9262462625873, 606.7030501305279, 625.70331948732, 623.5837370558404, 607.4836750975663, 589.7268695213564, 617.6180812142297, 603.5783583390864, 599.7037170633893, 603.8087675304085, 649.9313244777413, 588.2463279765618, 638.4685068391265, 611.2580967862943, 637.421943401637, 621.1299963854806, 591.8926883816252, 583.5864293679192, 606.1657572522845, 656.3426232576753, 625.6637389284845, 598.4456045648764, 630.5142560957363, 610.6707109474588, 607.4432818060959], 'episode_lengths': [300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300]}, 'sampler_perf': {'mean_raw_obs_processing_ms': 0.34054384452797193, 'mean_inference_ms': 0.8364510221606176, 'mean_action_processing_ms': 0.21899213523915542, 'mean_env_wait_ms': 0.3332731499742183, 'mean_env_render_ms': 0.0}, 'num_faulty_episodes': 0, 'connector_metrics': {'ObsPreprocessorConnector_ms': 0.0053632259368896484, 'StateBufferConnector_ms': 0.0042836666107177734, 'ViewRequirementAgentConnector_ms': 0.12398958206176758}, 'num_episodes': 14, 'episode_return_max': 656.3426232576753, 'episode_return_min': 554.0001613174567, 'episode_return_mean': 603.4171706591706}, 'env_runner_results': {'episode_reward_max': 656.3426232576753, 'episode_reward_min': 554.0001613174567, 'episode_reward_mean': 603.4171706591706, 'episode_len_mean': 300.0, 'episode_media': {}, 'episodes_this_iter': 14, 'episodes_timesteps_total': 30000, 'policy_reward_min': {}, 'policy_reward_max': {}, 'policy_reward_mean': {}, 'custom_metrics': {}, 'hist_stats': {'episode_reward': [557.9595628590914, 598.6723485242048, 576.7905364410914, 623.9128949566787, 593.9106186498275, 569.062897996119, 590.2013885065847, 589.5060260096283, 597.8507385469798, 603.3792850540186, 569.6136375997933, 644.0588800895705, 593.328203558257, 570.5709290327773, 571.9257168448096, 588.9998994945554, 598.7589479373542, 596.5288003010861, 570.1350830746087, 568.0480647727866, 620.0024518546635, 576.4197241751825, 601.7157758808075, 584.2411923236373, 604.9850591253866, 554.0001613174567, 611.4221957176887, 584.6438986226336, 576.1361015052117, 589.8023402153758, 626.6456881142497, 572.008304005685, 591.9557121248316, 612.8474983675018, 609.5166032129912, 603.8943007676796, 636.1130024943114, 611.9740266708382, 602.5425770692818, 628.1434802365283, 610.6536786191289, 609.0682140526151, 578.0150787062627, 625.5863911032764, 588.5307544495234, 622.7886128417579, 621.1413480448987, 632.7839459952713, 610.1124621743139, 592.1165754452259, 611.1495018386091, 583.7511119350513, 617.5665974881094, 616.9876188074414, 568.6591477732395, 619.8716461880304, 594.7810736546629, 611.3439781196436, 593.3561105262047, 591.1017537228189, 625.1183979513283, 632.7680447687559, 614.8452452281645, 627.5385971308539, 596.5194669359113, 592.5112024770766, 573.9262997589445, 630.0462283258863, 587.5262928094561, 587.3002309029821, 649.5510743605195, 647.6793445327738, 587.8095600275365, 601.3975749074691, 584.5722420902252, 595.9262462625873, 606.7030501305279, 625.70331948732, 623.5837370558404, 607.4836750975663, 589.7268695213564, 617.6180812142297, 603.5783583390864, 599.7037170633893, 603.8087675304085, 649.9313244777413, 588.2463279765618, 638.4685068391265, 611.2580967862943, 637.421943401637, 621.1299963854806, 591.8926883816252, 583.5864293679192, 606.1657572522845, 656.3426232576753, 625.6637389284845, 598.4456045648764, 630.5142560957363, 610.6707109474588, 607.4432818060959], 'episode_lengths': [300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300]}, 'sampler_perf': {'mean_raw_obs_processing_ms': 0.34054384452797193, 'mean_inference_ms': 0.8364510221606176, 'mean_action_processing_ms': 0.21899213523915542, 'mean_env_wait_ms': 0.3332731499742183, 'mean_env_render_ms': 0.0}, 'num_faulty_episodes': 0, 'connector_metrics': {'ObsPreprocessorConnector_ms': 0.0053632259368896484, 'StateBufferConnector_ms': 0.0042836666107177734, 'ViewRequirementAgentConnector_ms': 0.12398958206176758}, 'num_episodes': 14, 'episode_return_max': 656.3426232576753, 'episode_return_min': 554.0001613174567, 'episode_return_mean': 603.4171706591706}, 'episode_reward_max': 656.3426232576753, 'episode_reward_min': 554.0001613174567, 'episode_reward_mean': 603.4171706591706, 'episode_len_mean': 300.0, 'episodes_this_iter': 14, 'episodes_timesteps_total': 30000, 'policy_reward_min': {}, 'policy_reward_max': {}, 'policy_reward_mean': {}, 'hist_stats': {'episode_reward': [557.9595628590914, 598.6723485242048, 576.7905364410914, 623.9128949566787, 593.9106186498275, 569.062897996119, 590.2013885065847, 589.5060260096283, 597.8507385469798, 603.3792850540186, 569.6136375997933, 644.0588800895705, 593.328203558257, 570.5709290327773, 571.9257168448096, 588.9998994945554, 598.7589479373542, 596.5288003010861, 570.1350830746087, 568.0480647727866, 620.0024518546635, 576.4197241751825, 601.7157758808075, 584.2411923236373, 604.9850591253866, 554.0001613174567, 611.4221957176887, 584.6438986226336, 576.1361015052117, 589.8023402153758, 626.6456881142497, 572.008304005685, 591.9557121248316, 612.8474983675018, 609.5166032129912, 603.8943007676796, 636.1130024943114, 611.9740266708382, 602.5425770692818, 628.1434802365283, 610.6536786191289, 609.0682140526151, 578.0150787062627, 625.5863911032764, 588.5307544495234, 622.7886128417579, 621.1413480448987, 632.7839459952713, 610.1124621743139, 592.1165754452259, 611.1495018386091, 583.7511119350513, 617.5665974881094, 616.9876188074414, 568.6591477732395, 619.8716461880304, 594.7810736546629, 611.3439781196436, 593.3561105262047, 591.1017537228189, 625.1183979513283, 632.7680447687559, 614.8452452281645, 627.5385971308539, 596.5194669359113, 592.5112024770766, 573.9262997589445, 630.0462283258863, 587.5262928094561, 587.3002309029821, 649.5510743605195, 647.6793445327738, 587.8095600275365, 601.3975749074691, 584.5722420902252, 595.9262462625873, 606.7030501305279, 625.70331948732, 623.5837370558404, 607.4836750975663, 589.7268695213564, 617.6180812142297, 603.5783583390864, 599.7037170633893, 603.8087675304085, 649.9313244777413, 588.2463279765618, 638.4685068391265, 611.2580967862943, 637.421943401637, 621.1299963854806, 591.8926883816252, 583.5864293679192, 606.1657572522845, 656.3426232576753, 625.6637389284845, 598.4456045648764, 630.5142560957363, 610.6707109474588, 607.4432818060959], 'episode_lengths': [300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300]}, 'sampler_perf': {'mean_raw_obs_processing_ms': 0.34054384452797193, 'mean_inference_ms': 0.8364510221606176, 'mean_action_processing_ms': 0.21899213523915542, 'mean_env_wait_ms': 0.3332731499742183, 'mean_env_render_ms': 0.0}, 'num_faulty_episodes': 0, 'connector_metrics': {'ObsPreprocessorConnector_ms': 0.0053632259368896484, 'StateBufferConnector_ms': 0.0042836666107177734, 'ViewRequirementAgentConnector_ms': 0.12398958206176758}, 'num_episodes': 14, 'episode_return_max': 656.3426232576753, 'episode_return_min': 554.0001613174567, 'episode_return_mean': 603.4171706591706, 'num_healthy_workers': 1, 'num_in_flight_async_reqs': 0, 'num_remote_worker_restarts': 0, 'num_agent_steps_sampled': 245760, 'num_agent_steps_trained': 245760, 'num_env_steps_sampled': 122880, 'num_env_steps_trained': 122880, 'num_env_steps_sampled_this_iter': 4096, 'num_env_steps_trained_this_iter': 4096, 'num_env_steps_sampled_throughput_per_sec': 312.59178580771174, 'num_env_steps_trained_throughput_per_sec': 312.59178580771174, 'timesteps_total': 122880, 'num_env_steps_sampled_lifetime': 122880, 'num_agent_steps_sampled_lifetime': 245760, 'num_steps_trained_this_iter': 4096, 'agent_timesteps_total': 245760, 'timers': {'training_iteration_time_ms': 13107.549, 'restore_workers_time_ms': 0.021, 'training_step_time_ms': 13107.482, 'sample_time_ms': 6865.865, 'load_time_ms': 0.603, 'load_throughput': 6788315.625, 'learn_time_ms': 6236.751, 'learn_throughput': 656.752, 'synch_weights_time_ms': 3.669}, 'counters': {'num_env_steps_sampled': 122880, 'num_env_steps_trained': 122880, 'num_agent_steps_sampled': 245760, 'num_agent_steps_trained': 245760}, 'done': False, 'episodes_total': 409, 'training_iteration': 30, 'trial_id': 'default', 'date': '2024-05-23_10-17-00', 'timestamp': 1716452220, 'time_this_iter_s': 13.10737919807434, 'time_total_s': 406.53645038604736, 'pid': 3351, 'hostname': 'LAPTOP-9AD2MD1C', 'node_ip': '172.23.87.14', 'config': {'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, '_fake_gpus': False, 'num_learner_workers': 0, 'num_gpus_per_learner_worker': 0, 'num_cpus_per_learner_worker': 1, 'local_gpu_idx': 0, 'custom_resources_per_worker': {}, 'placement_strategy': 'PACK', 'eager_tracing': True, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'torch_compile_learner': False, 'torch_compile_learner_what_to_compile': <TorchCompileWhatToCompile.FORWARD_TRAIN: 'forward_train'>, 'torch_compile_learner_dynamo_backend': 'inductor', 'torch_compile_learner_dynamo_mode': None, 'torch_compile_worker': False, 'torch_compile_worker_dynamo_backend': 'onnxrt', 'torch_compile_worker_dynamo_mode': None, 'enable_rl_module_and_learner': False, 'enable_env_runner_and_connector_v2': False, 'env': 'KeepTheDistance?dst=0&agent=2&spawn_area=100', 'env_config': {}, 'observation_space': None, 'action_space': None, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, '_is_atari': None, 'env_task_fn': None, 'render_env': False, 'action_mask_key': 'action_mask', 'env_runner_cls': None, 'num_envs_per_env_runner': 1, 'validate_env_runners_after_construction': True, 'sample_timeout_s': 60.0, '_env_to_module_connector': None, 'add_default_connectors_to_env_to_module_pipeline': True, '_module_to_env_connector': None, 'add_default_connectors_to_module_to_env_pipeline': True, 'episode_lookback_horizon': 1, 'rollout_fragment_length': 'auto', 'batch_mode': 'truncate_episodes', 'compress_observations': False, 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'enable_tf1_exec_eagerly': False, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'update_worker_filter_stats': True, 'use_worker_filter_stats': True, 'enable_connectors': True, 'sampler_perf_stats_ema_coef': None, 'gamma': 0.95, 'lr': 0.001, 'grad_clip': None, 'grad_clip_by': 'global_norm', 'train_batch_size': 4096, 'train_batch_size_per_learner': None, 'model': {'_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'fcnet_weights_initializer': None, 'fcnet_weights_initializer_config': None, 'fcnet_bias_initializer': None, 'fcnet_bias_initializer_config': None, 'conv_filters': None, 'conv_activation': 'relu', 'conv_kernel_initializer': None, 'conv_kernel_initializer_config': None, 'conv_bias_initializer': None, 'conv_bias_initializer_config': None, 'conv_transpose_kernel_initializer': None, 'conv_transpose_kernel_initializer_config': None, 'conv_transpose_bias_initializer': None, 'conv_transpose_bias_initializer_config': None, 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'post_fcnet_weights_initializer': None, 'post_fcnet_weights_initializer_config': None, 'post_fcnet_bias_initializer': None, 'post_fcnet_bias_initializer_config': None, 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, 'lstm_weights_initializer': None, 'lstm_weights_initializer_config': None, 'lstm_bias_initializer': None, 'lstm_bias_initializer_config': None, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'encoder_latent_dim': None, 'always_check_shapes': False, 'lstm_use_prev_action_reward': -1, '_use_default_native_models': -1}, '_learner_connector': None, 'add_default_connectors_to_learner_pipeline': True, 'optimizer': {}, 'max_requests_in_flight_per_sampler_worker': 2, '_learner_class': None, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'algorithm_config_overrides_per_module': {}, '_per_module_overrides': {}, 'count_steps_by': 'env_steps', 'policy_map_capacity': 100, 'policy_mapping_fn': <function AlgorithmConfig.DEFAULT_POLICY_MAPPING_FN at 0x7fb8255f62a0>, 'policies_to_train': None, 'policy_states_are_swappable': False, 'observation_fn': None, 'input_config': {}, 'actions_in_input_normalized': False, 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_config': {}, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'offline_sampling': False, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_sample_timeout_s': 120.0, 'evaluation_parallel_to_training': False, 'evaluation_force_reset_envs_before_iteration': True, 'evaluation_config': None, 'off_policy_estimation_methods': {}, 'ope_split_batch_by_episode': True, 'evaluation_num_env_runners': 0, 'always_attach_evaluation_results': True, 'in_evaluation': False, 'sync_filters_on_rollout_workers_timeout_s': 10.0, 'keep_per_episode_custom_metrics': False, 'metrics_episode_collection_timeout_s': 60.0, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_iteration': None, 'min_train_timesteps_per_iteration': 0, 'min_sample_timesteps_per_iteration': 0, 'export_native_model_files': False, 'checkpoint_trainable_policies_only': False, 'logger_creator': None, 'logger_config': None, 'log_level': 'WARN', 'log_sys_usage': True, 'fake_sampler': False, 'seed': None, '_run_training_always_in_thread': False, '_evaluation_parallel_to_training_wo_thread': False, 'ignore_env_runner_failures': False, 'recreate_failed_env_runners': False, 'max_num_env_runner_restarts': 1000, 'delay_between_env_runner_restarts_s': 60.0, 'restart_failed_sub_environments': False, 'num_consecutive_env_runner_failures_tolerance': 100, 'env_runner_health_probe_timeout_s': 30, 'env_runner_restore_timeout_s': 1800, '_model_config_dict': {}, '_rl_module_spec': None, '_AlgorithmConfig__prior_exploration_config': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_initialize_loss_from_dummy_batch': False, 'simple_optimizer': False, 'policy_map_cache': -1, 'worker_cls': -1, 'synchronize_filters': -1, 'enable_async_evaluation': -1, 'custom_async_evaluation_function': -1, '_enable_rl_module_api': -1, 'auto_wrap_old_gym_envs': -1, 'disable_env_checking': -1, 'replay_sequence_length': None, '_disable_execution_plan_api': -1, 'lr_schedule': None, 'use_critic': True, 'use_gae': True, 'use_kl_loss': True, 'kl_coeff': 0.2, 'kl_target': 0.01, 'sgd_minibatch_size': 256, 'mini_batch_size_per_learner': None, 'num_sgd_iter': 30, 'shuffle_sequences': True, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.0, 'entropy_coeff_schedule': None, 'clip_param': 0.3, 'vf_clip_param': 10.0, 'vf_share_layers': -1, 'lambda': 1.0, 'input': 'sampler', 'policies': {'default_policy': (None, None, None, None)}, 'callbacks': <class 'ray.rllib.algorithms.callbacks.DefaultCallbacks'>, 'create_env_on_driver': False, 'custom_eval_function': None, 'framework': 'torch', 'num_cpus_for_driver': 1, 'num_workers': 1}, 'time_since_restore': 406.53645038604736, 'iterations_since_restore': 30, 'perf': {'cpu_util_percent': 29.76, 'ram_util_percent': 87.492}})"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "save_algo(algo, \"KeepTheDistance?dst=0&agent=2&spawn_area=100\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KeepTheDistance?dst=0&agent=3&visible_nbrs=2&spawn_area=100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ray.tune.registry import register_env\n",
    "\n",
    "env_config = EnvironmentConfiguration(n_agents=3, visible_nbrs=2, target_distance=0, max_steps=300, speed=1, spawn_area=100)\n",
    "register_env(\"KeepTheDistance?dst=0&agent=3&visible_nbrs=2&spawn_area=100\", lambda _: KeepTheDistance(env_config))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-24 10:13:10,842\tWARNING deprecation.py:50 -- DeprecationWarning: `_enable_new_api_stack` has been deprecated. Use `AlgorithmConfig._enable_new_api_stack` instead. This will raise an error in the future!\n",
      "/home/nicolo/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/rllib/algorithms/algorithm.py:521: RayDeprecationWarning: This API is deprecated and may be removed in future Ray releases. You could suppress this warning by setting env variable PYTHONWARNINGS=\"ignore::DeprecationWarning\"\n",
      "`UnifiedLogger` will be removed in Ray 2.7.\n",
      "  return UnifiedLogger(config, logdir, loggers=None)\n",
      "/home/nicolo/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/tune/logger/unified.py:53: RayDeprecationWarning: This API is deprecated and may be removed in future Ray releases. You could suppress this warning by setting env variable PYTHONWARNINGS=\"ignore::DeprecationWarning\"\n",
      "The `JsonLogger interface is deprecated in favor of the `ray.tune.json.JsonLoggerCallback` interface and will be removed in Ray 2.7.\n",
      "  self._loggers.append(cls(self.config, self.logdir, self.trial))\n",
      "/home/nicolo/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/tune/logger/unified.py:53: RayDeprecationWarning: This API is deprecated and may be removed in future Ray releases. You could suppress this warning by setting env variable PYTHONWARNINGS=\"ignore::DeprecationWarning\"\n",
      "The `CSVLogger interface is deprecated in favor of the `ray.tune.csv.CSVLoggerCallback` interface and will be removed in Ray 2.7.\n",
      "  self._loggers.append(cls(self.config, self.logdir, self.trial))\n",
      "/home/nicolo/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/tune/logger/unified.py:53: RayDeprecationWarning: This API is deprecated and may be removed in future Ray releases. You could suppress this warning by setting env variable PYTHONWARNINGS=\"ignore::DeprecationWarning\"\n",
      "The `TBXLogger interface is deprecated in favor of the `ray.tune.tensorboardx.TBXLoggerCallback` interface and will be removed in Ray 2.7.\n",
      "  self._loggers.append(cls(self.config, self.logdir, self.trial))\n",
      "2024-05-24 10:13:10,913\tWARNING deprecation.py:50 -- DeprecationWarning: `WorkerSet(num_workers=... OR local_worker=...)` has been deprecated. Use `EnvRunnerGroup(num_env_runners=... AND local_env_runner=...)` instead. This will raise an error in the future!\n",
      "2024-05-24 10:13:10,914\tWARNING deprecation.py:50 -- DeprecationWarning: `max_num_worker_restarts` has been deprecated. Use `AlgorithmConfig.max_num_env_runner_restarts` instead. This will raise an error in the future!\n",
      "2024-05-24 10:13:14,302\tINFO worker.py:1749 -- Started a local Ray instance.\n",
      "2024-05-24 10:13:23,121\tINFO trainable.py:161 -- Trainable.setup took 12.209 seconds. If your trainable is slow to initialize, consider setting reuse_actors=True to reduce actor creation overheads.\n",
      "2024-05-24 10:13:23,124\tWARNING util.py:61 -- Install gputil for GPU system monitoring.\n"
     ]
    }
   ],
   "source": [
    "algo = load_algo(\"KeepTheDistance?dst=0&agent=3&visible_nbrs=2&spawn_area=100\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration [1] => episode_reward_mean: 107.85348831992805, episode_len_mean: 300.0, agent_steps_trained: 12288, env_steps_trained: 4096, entropy: 4.3298547991447975, learning_rate: 0.0010000000000000002\n",
      "iteration [2] => episode_reward_mean: 204.20732318592715, episode_len_mean: 300.0, agent_steps_trained: 24576, env_steps_trained: 8192, entropy: 4.3800533443689345, learning_rate: 0.0010000000000000002\n",
      "iteration [3] => episode_reward_mean: 141.2888108605651, episode_len_mean: 300.0, agent_steps_trained: 36864, env_steps_trained: 12288, entropy: 4.342063478463226, learning_rate: 0.0010000000000000002\n",
      "iteration [4] => episode_reward_mean: 113.73474792159456, episode_len_mean: 300.0, agent_steps_trained: 49152, env_steps_trained: 16384, entropy: 4.385226429502169, learning_rate: 0.0010000000000000002\n",
      "iteration [5] => episode_reward_mean: 188.20725056371347, episode_len_mean: 300.0, agent_steps_trained: 61440, env_steps_trained: 20480, entropy: 4.309993063244555, learning_rate: 0.0010000000000000002\n",
      "iteration [6] => episode_reward_mean: 386.772386959698, episode_len_mean: 300.0, agent_steps_trained: 73728, env_steps_trained: 24576, entropy: 4.319519369138612, learning_rate: 0.0010000000000000002\n",
      "iteration [7] => episode_reward_mean: 550.5829238901728, episode_len_mean: 300.0, agent_steps_trained: 86016, env_steps_trained: 28672, entropy: 4.258530805342727, learning_rate: 0.0010000000000000002\n",
      "iteration [8] => episode_reward_mean: 556.183602729987, episode_len_mean: 300.0, agent_steps_trained: 98304, env_steps_trained: 32768, entropy: 4.429368437329928, learning_rate: 0.0010000000000000002\n",
      "iteration [9] => episode_reward_mean: 1010.9412720579935, episode_len_mean: 300.0, agent_steps_trained: 110592, env_steps_trained: 36864, entropy: 4.131807916528649, learning_rate: 0.0010000000000000002\n",
      "iteration [10] => episode_reward_mean: 1259.9163492308533, episode_len_mean: 300.0, agent_steps_trained: 122880, env_steps_trained: 40960, entropy: 4.401074873242114, learning_rate: 0.0010000000000000002\n",
      "iteration [11] => episode_reward_mean: 1745.5241082740204, episode_len_mean: 300.0, agent_steps_trained: 135168, env_steps_trained: 45056, entropy: 4.257308100660642, learning_rate: 0.0010000000000000002\n",
      "iteration [12] => episode_reward_mean: 2188.8864729776233, episode_len_mean: 300.0, agent_steps_trained: 147456, env_steps_trained: 49152, entropy: 4.358779812190268, learning_rate: 0.0010000000000000002\n",
      "iteration [13] => episode_reward_mean: 2575.6298270967704, episode_len_mean: 300.0, agent_steps_trained: 159744, env_steps_trained: 53248, entropy: 4.284995860358079, learning_rate: 0.0010000000000000002\n",
      "iteration [14] => episode_reward_mean: 2640.077735365031, episode_len_mean: 300.0, agent_steps_trained: 172032, env_steps_trained: 57344, entropy: 4.561032598879602, learning_rate: 0.0010000000000000002\n",
      "iteration [15] => episode_reward_mean: 3034.0434596291643, episode_len_mean: 300.0, agent_steps_trained: 184320, env_steps_trained: 61440, entropy: 4.309982602132691, learning_rate: 0.0010000000000000002\n",
      "iteration [16] => episode_reward_mean: 2821.797395575365, episode_len_mean: 300.0, agent_steps_trained: 196608, env_steps_trained: 65536, entropy: 4.879472306370735, learning_rate: 0.0010000000000000002\n",
      "iteration [17] => episode_reward_mean: 2747.999898879699, episode_len_mean: 300.0, agent_steps_trained: 208896, env_steps_trained: 69632, entropy: 4.867018652624554, learning_rate: 0.0010000000000000002\n",
      "iteration [18] => episode_reward_mean: 2636.660816018819, episode_len_mean: 300.0, agent_steps_trained: 221184, env_steps_trained: 73728, entropy: 4.998342904117372, learning_rate: 0.0010000000000000002\n",
      "iteration [19] => episode_reward_mean: 2644.7092521564364, episode_len_mean: 300.0, agent_steps_trained: 233472, env_steps_trained: 77824, entropy: 4.904564993083477, learning_rate: 0.0010000000000000002\n",
      "iteration [20] => episode_reward_mean: 2200.3685635646475, episode_len_mean: 300.0, agent_steps_trained: 245760, env_steps_trained: 81920, entropy: 5.1946640955077275, learning_rate: 0.0010000000000000002\n",
      "iteration [21] => episode_reward_mean: 2125.270338668334, episode_len_mean: 300.0, agent_steps_trained: 258048, env_steps_trained: 86016, entropy: 5.077576796213786, learning_rate: 0.0010000000000000002\n",
      "iteration [22] => episode_reward_mean: 2033.0011841574815, episode_len_mean: 300.0, agent_steps_trained: 270336, env_steps_trained: 90112, entropy: 5.0641085351506865, learning_rate: 0.0010000000000000002\n",
      "iteration [23] => episode_reward_mean: 2063.000331281143, episode_len_mean: 300.0, agent_steps_trained: 282624, env_steps_trained: 94208, entropy: 4.973038358324104, learning_rate: 0.0010000000000000002\n",
      "iteration [24] => episode_reward_mean: 1954.6371716566878, episode_len_mean: 300.0, agent_steps_trained: 294912, env_steps_trained: 98304, entropy: 5.100368512835767, learning_rate: 0.0010000000000000002\n",
      "iteration [25] => episode_reward_mean: 2010.2649908157757, episode_len_mean: 300.0, agent_steps_trained: 307200, env_steps_trained: 102400, entropy: 4.9595140177342625, learning_rate: 0.0010000000000000002\n",
      "iteration [26] => episode_reward_mean: 1749.423782860198, episode_len_mean: 300.0, agent_steps_trained: 319488, env_steps_trained: 106496, entropy: 5.078162951933013, learning_rate: 0.0010000000000000002\n",
      "iteration [27] => episode_reward_mean: 1593.5061272959251, episode_len_mean: 300.0, agent_steps_trained: 331776, env_steps_trained: 110592, entropy: 5.102656249205271, learning_rate: 0.0010000000000000002\n",
      "iteration [28] => episode_reward_mean: 1748.944217535815, episode_len_mean: 300.0, agent_steps_trained: 344064, env_steps_trained: 114688, entropy: 4.970724732511574, learning_rate: 0.0010000000000000002\n",
      "iteration [29] => episode_reward_mean: 1942.0877300600355, episode_len_mean: 300.0, agent_steps_trained: 356352, env_steps_trained: 118784, entropy: 4.891528057886495, learning_rate: 0.0010000000000000002\n",
      "iteration [30] => episode_reward_mean: 1847.7740530195213, episode_len_mean: 300.0, agent_steps_trained: 368640, env_steps_trained: 122880, entropy: 5.070382753180133, learning_rate: 0.0010000000000000002\n",
      "iteration [31] => episode_reward_mean: 1390.2719431394307, episode_len_mean: 300.0, agent_steps_trained: 380928, env_steps_trained: 126976, entropy: 5.106146540244421, learning_rate: 0.0010000000000000002\n",
      "iteration [32] => episode_reward_mean: 1341.2522136604957, episode_len_mean: 300.0, agent_steps_trained: 393216, env_steps_trained: 131072, entropy: 4.950418012175295, learning_rate: 0.0010000000000000002\n",
      "iteration [33] => episode_reward_mean: 1414.4396505697819, episode_len_mean: 300.0, agent_steps_trained: 405504, env_steps_trained: 135168, entropy: 5.117236028280523, learning_rate: 0.0010000000000000002\n",
      "iteration [34] => episode_reward_mean: 1598.140196173292, episode_len_mean: 300.0, agent_steps_trained: 417792, env_steps_trained: 139264, entropy: 5.023951746357811, learning_rate: 0.0010000000000000002\n",
      "iteration [35] => episode_reward_mean: 1406.3703667331697, episode_len_mean: 300.0, agent_steps_trained: 430080, env_steps_trained: 143360, entropy: 5.087983704606692, learning_rate: 0.0010000000000000002\n",
      "iteration [36] => episode_reward_mean: 1376.9718211822997, episode_len_mean: 300.0, agent_steps_trained: 442368, env_steps_trained: 147456, entropy: 4.9322064712643625, learning_rate: 0.0010000000000000002\n",
      "iteration [37] => episode_reward_mean: 1549.1601611387973, episode_len_mean: 300.0, agent_steps_trained: 454656, env_steps_trained: 151552, entropy: 5.0958556832538715, learning_rate: 0.0010000000000000002\n",
      "iteration [38] => episode_reward_mean: 1881.8542922499892, episode_len_mean: 300.0, agent_steps_trained: 466944, env_steps_trained: 155648, entropy: 5.030408658749527, learning_rate: 0.0010000000000000002\n",
      "iteration [39] => episode_reward_mean: 2015.4371162250256, episode_len_mean: 300.0, agent_steps_trained: 479232, env_steps_trained: 159744, entropy: 5.208640211986171, learning_rate: 0.0010000000000000002\n",
      "iteration [40] => episode_reward_mean: 1930.20330570609, episode_len_mean: 300.0, agent_steps_trained: 491520, env_steps_trained: 163840, entropy: 5.3370947420597075, learning_rate: 0.0010000000000000002\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c8d11ab4c665428586d8f2cfb255cd25",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "CanvasWithBorders(height=300, width=300)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from ray.rllib.algorithms.ppo import PPOConfig\n",
    "from ray.tune.logger import pretty_print\n",
    "\n",
    "from gymnasium.wrappers.time_limit import TimeLimit\n",
    "\n",
    "trainin_steps = 40\n",
    "\n",
    "algo = (\n",
    "    PPOConfig()\n",
    "    .training(gamma = 0.95, \n",
    "              lr = 0.001,\n",
    "              train_batch_size = 4096, \n",
    "              sgd_minibatch_size = 256, \n",
    "              num_sgd_iter = 30,\n",
    "              #entropy_coeff=0.005,\n",
    "              )\n",
    "    .env_runners(num_env_runners=1)\n",
    "    .resources(num_gpus=0)\n",
    "    .environment(env=\"KeepTheDistance?dst=0&agent=3&visible_nbrs=2&spawn_area=100\")\n",
    "    .build()\n",
    ")\n",
    "clear_output()\n",
    "\n",
    "out = \"\"\n",
    "for i in range(trainin_steps):\n",
    "    result = algo.train()\n",
    "    clear_output()\n",
    "    out += ppo_result_format(result) + \"\\n\"\n",
    "    print(out)\n",
    "    simulate_episode(RenderableKeepTheDistance(env_config), algo, 150, sleep_between_frames=0.03, print_info=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "79effdd57d3540f0b7889af0abfd794f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "CanvasWithBorders(height=300, width=300)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "env_config_2 = EnvironmentConfiguration(n_agents=3, visible_nbrs=2, target_distance=0, max_steps=500, speed=1, spawn_area=100)\n",
    "simulate_episode(RenderableKeepTheDistance(env_config_2), algo, 300, sleep_between_frames=0.01, print_info=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "An Algorithm checkpoint has been created inside directory: 'TrainingResult(checkpoint=Checkpoint(filesystem=local, path=/mnt/c/Users/nicol/Desktop/Università/tesi/experiments/RL_experiments/algos/KeepTheDistance?dst=0&agent=3&visible_nbrs=2&spawn_area=100), metrics={'custom_metrics': {}, 'episode_media': {}, 'info': {'learner': {'default_policy': {'learner_stats': {'allreduce_latency': 0.0, 'grad_gnorm': 26.797518886129062, 'cur_kl_coeff': 5.473673629760742, 'cur_lr': 0.0010000000000000002, 'total_loss': 4.241659853690201, 'policy_loss': 0.0008611866208310757, 'vf_loss': 4.17278758486112, 'vf_explained_var': 0.026051732442445224, 'kl': 0.012425122178001438, 'entropy': 5.3370947420597075, 'entropy_coeff': 0.0}, 'model': {}, 'custom_metrics': {}, 'num_agent_steps_trained': 256.0, 'num_grad_updates_lifetime': 56880.5, 'diff_num_grad_updates_vs_sampler_policy': 719.5}}, 'num_env_steps_sampled': 163840, 'num_env_steps_trained': 163840, 'num_agent_steps_sampled': 491520, 'num_agent_steps_trained': 491520}, 'sampler_results': {'episode_reward_max': 26042.054873470093, 'episode_reward_min': -98.5686833518256, 'episode_reward_mean': 1930.20330570609, 'episode_len_mean': 300.0, 'episode_media': {}, 'episodes_this_iter': 14, 'episodes_timesteps_total': 30000, 'policy_reward_min': {}, 'policy_reward_max': {}, 'policy_reward_mean': {}, 'custom_metrics': {}, 'hist_stats': {'episode_reward': [-92.27266819856116, 25.73075983931291, 20.15284682220323, 4.749758335196418, 10.090455927671556, 70.70791792123086, 96.00401067477735, 63.07796837442251, 63.78997120042281, 33.44352543766661, -46.465104402316875, -0.5203327196826706, 18318.744591704028, 37.14122664199385, 21.13643332215554, 87.87719936101234, 71.45022590217889, -9.101330754919871, -98.5686833518256, 95.33530645806059, 55.4811982213292, 18.30271494299572, -51.26575824571279, 42.082533480452554, -22.445596925520377, 68.74256746797532, 33.05435525796253, -14.032245291742242, 79.26786278759919, 33.07432484402226, -38.11927735077796, 26.551249438526455, 39.52130742704978, 97.97914468484055, 50.10658463112787, 27.31634486537763, 11540.808485968026, 55.878779664342275, 39.91017477423976, 23326.82285928509, -8.006498538129037, 18704.947979701745, -8.362829241117481, 97.42257175376655, 51.19154584835426, 67.80124618096765, 41.902646135463215, -4.600723926245507, 35.53484550023718, 10580.234960158623, -18.71622448426686, 39.01829619121443, 84.04304932371977, -1.4464490134462125, 60.037719617980514, 23.785667828333658, 42.26677431276764, 34.40088252853897, 26042.054873470093, 41.77314813414468, 45.79029879641874, 32.55142799204754, -11.965768183545151, 42.00753025786594, 12293.034255574681, 43.79904868980046, -25.863541249120757, 60.73670702197117, 47.89978373191765, 61.30928534284479, 20899.3740854774, -14.818305615857966, -8.893512470663119, 14833.645636660005, 41.397579473451756, 22821.41310938688, 7.491018013864107, 59.11961027045365, 63.7071514643104, 5.028932524212735, 33.37970701764043, 14.878160110483378, 98.08106277634619, -11.823124640004565, -0.665076458756527, 38.628779430559916, 14.463635071025323, 85.03423308275264, 155.99019409589116, 1.79278515037403, 10570.213268957787, 9.93113203795204, 70.76003720368493, 26.41514838991324, -14.33627414254741, 13.77353032658921, 454.51181271704314, 95.00891723474588, 33.989863382171976, -53.28672419852414], 'episode_lengths': [300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300]}, 'sampler_perf': {'mean_raw_obs_processing_ms': 0.6170270567527532, 'mean_inference_ms': 1.07689741151318, 'mean_action_processing_ms': 0.3920014103326924, 'mean_env_wait_ms': 0.7321249918278235, 'mean_env_render_ms': 0.0}, 'num_faulty_episodes': 0, 'connector_metrics': {'ObsPreprocessorConnector_ms': 0.008563756942749023, 'StateBufferConnector_ms': 0.00696110725402832, 'ViewRequirementAgentConnector_ms': 0.23952293395996094}, 'num_episodes': 14, 'episode_return_max': 26042.054873470093, 'episode_return_min': -98.5686833518256, 'episode_return_mean': 1930.20330570609}, 'env_runner_results': {'episode_reward_max': 26042.054873470093, 'episode_reward_min': -98.5686833518256, 'episode_reward_mean': 1930.20330570609, 'episode_len_mean': 300.0, 'episode_media': {}, 'episodes_this_iter': 14, 'episodes_timesteps_total': 30000, 'policy_reward_min': {}, 'policy_reward_max': {}, 'policy_reward_mean': {}, 'custom_metrics': {}, 'hist_stats': {'episode_reward': [-92.27266819856116, 25.73075983931291, 20.15284682220323, 4.749758335196418, 10.090455927671556, 70.70791792123086, 96.00401067477735, 63.07796837442251, 63.78997120042281, 33.44352543766661, -46.465104402316875, -0.5203327196826706, 18318.744591704028, 37.14122664199385, 21.13643332215554, 87.87719936101234, 71.45022590217889, -9.101330754919871, -98.5686833518256, 95.33530645806059, 55.4811982213292, 18.30271494299572, -51.26575824571279, 42.082533480452554, -22.445596925520377, 68.74256746797532, 33.05435525796253, -14.032245291742242, 79.26786278759919, 33.07432484402226, -38.11927735077796, 26.551249438526455, 39.52130742704978, 97.97914468484055, 50.10658463112787, 27.31634486537763, 11540.808485968026, 55.878779664342275, 39.91017477423976, 23326.82285928509, -8.006498538129037, 18704.947979701745, -8.362829241117481, 97.42257175376655, 51.19154584835426, 67.80124618096765, 41.902646135463215, -4.600723926245507, 35.53484550023718, 10580.234960158623, -18.71622448426686, 39.01829619121443, 84.04304932371977, -1.4464490134462125, 60.037719617980514, 23.785667828333658, 42.26677431276764, 34.40088252853897, 26042.054873470093, 41.77314813414468, 45.79029879641874, 32.55142799204754, -11.965768183545151, 42.00753025786594, 12293.034255574681, 43.79904868980046, -25.863541249120757, 60.73670702197117, 47.89978373191765, 61.30928534284479, 20899.3740854774, -14.818305615857966, -8.893512470663119, 14833.645636660005, 41.397579473451756, 22821.41310938688, 7.491018013864107, 59.11961027045365, 63.7071514643104, 5.028932524212735, 33.37970701764043, 14.878160110483378, 98.08106277634619, -11.823124640004565, -0.665076458756527, 38.628779430559916, 14.463635071025323, 85.03423308275264, 155.99019409589116, 1.79278515037403, 10570.213268957787, 9.93113203795204, 70.76003720368493, 26.41514838991324, -14.33627414254741, 13.77353032658921, 454.51181271704314, 95.00891723474588, 33.989863382171976, -53.28672419852414], 'episode_lengths': [300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300]}, 'sampler_perf': {'mean_raw_obs_processing_ms': 0.6170270567527532, 'mean_inference_ms': 1.07689741151318, 'mean_action_processing_ms': 0.3920014103326924, 'mean_env_wait_ms': 0.7321249918278235, 'mean_env_render_ms': 0.0}, 'num_faulty_episodes': 0, 'connector_metrics': {'ObsPreprocessorConnector_ms': 0.008563756942749023, 'StateBufferConnector_ms': 0.00696110725402832, 'ViewRequirementAgentConnector_ms': 0.23952293395996094}, 'num_episodes': 14, 'episode_return_max': 26042.054873470093, 'episode_return_min': -98.5686833518256, 'episode_return_mean': 1930.20330570609}, 'episode_reward_max': 26042.054873470093, 'episode_reward_min': -98.5686833518256, 'episode_reward_mean': 1930.20330570609, 'episode_len_mean': 300.0, 'episodes_this_iter': 14, 'episodes_timesteps_total': 30000, 'policy_reward_min': {}, 'policy_reward_max': {}, 'policy_reward_mean': {}, 'hist_stats': {'episode_reward': [-92.27266819856116, 25.73075983931291, 20.15284682220323, 4.749758335196418, 10.090455927671556, 70.70791792123086, 96.00401067477735, 63.07796837442251, 63.78997120042281, 33.44352543766661, -46.465104402316875, -0.5203327196826706, 18318.744591704028, 37.14122664199385, 21.13643332215554, 87.87719936101234, 71.45022590217889, -9.101330754919871, -98.5686833518256, 95.33530645806059, 55.4811982213292, 18.30271494299572, -51.26575824571279, 42.082533480452554, -22.445596925520377, 68.74256746797532, 33.05435525796253, -14.032245291742242, 79.26786278759919, 33.07432484402226, -38.11927735077796, 26.551249438526455, 39.52130742704978, 97.97914468484055, 50.10658463112787, 27.31634486537763, 11540.808485968026, 55.878779664342275, 39.91017477423976, 23326.82285928509, -8.006498538129037, 18704.947979701745, -8.362829241117481, 97.42257175376655, 51.19154584835426, 67.80124618096765, 41.902646135463215, -4.600723926245507, 35.53484550023718, 10580.234960158623, -18.71622448426686, 39.01829619121443, 84.04304932371977, -1.4464490134462125, 60.037719617980514, 23.785667828333658, 42.26677431276764, 34.40088252853897, 26042.054873470093, 41.77314813414468, 45.79029879641874, 32.55142799204754, -11.965768183545151, 42.00753025786594, 12293.034255574681, 43.79904868980046, -25.863541249120757, 60.73670702197117, 47.89978373191765, 61.30928534284479, 20899.3740854774, -14.818305615857966, -8.893512470663119, 14833.645636660005, 41.397579473451756, 22821.41310938688, 7.491018013864107, 59.11961027045365, 63.7071514643104, 5.028932524212735, 33.37970701764043, 14.878160110483378, 98.08106277634619, -11.823124640004565, -0.665076458756527, 38.628779430559916, 14.463635071025323, 85.03423308275264, 155.99019409589116, 1.79278515037403, 10570.213268957787, 9.93113203795204, 70.76003720368493, 26.41514838991324, -14.33627414254741, 13.77353032658921, 454.51181271704314, 95.00891723474588, 33.989863382171976, -53.28672419852414], 'episode_lengths': [300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300]}, 'sampler_perf': {'mean_raw_obs_processing_ms': 0.6170270567527532, 'mean_inference_ms': 1.07689741151318, 'mean_action_processing_ms': 0.3920014103326924, 'mean_env_wait_ms': 0.7321249918278235, 'mean_env_render_ms': 0.0}, 'num_faulty_episodes': 0, 'connector_metrics': {'ObsPreprocessorConnector_ms': 0.008563756942749023, 'StateBufferConnector_ms': 0.00696110725402832, 'ViewRequirementAgentConnector_ms': 0.23952293395996094}, 'num_episodes': 14, 'episode_return_max': 26042.054873470093, 'episode_return_min': -98.5686833518256, 'episode_return_mean': 1930.20330570609, 'num_healthy_workers': 1, 'num_in_flight_async_reqs': 0, 'num_remote_worker_restarts': 0, 'num_agent_steps_sampled': 491520, 'num_agent_steps_trained': 491520, 'num_env_steps_sampled': 163840, 'num_env_steps_trained': 163840, 'num_env_steps_sampled_this_iter': 4096, 'num_env_steps_trained_this_iter': 4096, 'num_env_steps_sampled_throughput_per_sec': 174.64544401360163, 'num_env_steps_trained_throughput_per_sec': 174.64544401360163, 'timesteps_total': 163840, 'num_env_steps_sampled_lifetime': 163840, 'num_agent_steps_sampled_lifetime': 491520, 'num_steps_trained_this_iter': 4096, 'agent_timesteps_total': 491520, 'timers': {'training_iteration_time_ms': 23517.072, 'restore_workers_time_ms': 0.02, 'training_step_time_ms': 23517.014, 'sample_time_ms': 11617.955, 'load_time_ms': 0.833, 'load_throughput': 4919920.153, 'learn_time_ms': 11892.561, 'learn_throughput': 344.417, 'synch_weights_time_ms': 4.667}, 'counters': {'num_env_steps_sampled': 163840, 'num_env_steps_trained': 163840, 'num_agent_steps_sampled': 491520, 'num_agent_steps_trained': 491520}, 'done': False, 'episodes_total': 546, 'training_iteration': 40, 'trial_id': 'default', 'date': '2024-05-23_18-19-04', 'timestamp': 1716481144, 'time_this_iter_s': 23.458407163619995, 'time_total_s': 933.9117562770844, 'pid': 13426, 'hostname': 'LAPTOP-9AD2MD1C', 'node_ip': '172.23.84.145', 'config': {'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, '_fake_gpus': False, 'num_learner_workers': 0, 'num_gpus_per_learner_worker': 0, 'num_cpus_per_learner_worker': 1, 'local_gpu_idx': 0, 'custom_resources_per_worker': {}, 'placement_strategy': 'PACK', 'eager_tracing': True, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'torch_compile_learner': False, 'torch_compile_learner_what_to_compile': <TorchCompileWhatToCompile.FORWARD_TRAIN: 'forward_train'>, 'torch_compile_learner_dynamo_backend': 'inductor', 'torch_compile_learner_dynamo_mode': None, 'torch_compile_worker': False, 'torch_compile_worker_dynamo_backend': 'onnxrt', 'torch_compile_worker_dynamo_mode': None, 'enable_rl_module_and_learner': False, 'enable_env_runner_and_connector_v2': False, 'env': 'KeepTheDistance?dst=0&agent=3&visible_nbrs=2&spawn_area=100', 'env_config': {}, 'observation_space': None, 'action_space': None, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, '_is_atari': None, 'env_task_fn': None, 'render_env': False, 'action_mask_key': 'action_mask', 'env_runner_cls': None, 'num_envs_per_env_runner': 1, 'validate_env_runners_after_construction': True, 'sample_timeout_s': 60.0, '_env_to_module_connector': None, 'add_default_connectors_to_env_to_module_pipeline': True, '_module_to_env_connector': None, 'add_default_connectors_to_module_to_env_pipeline': True, 'episode_lookback_horizon': 1, 'rollout_fragment_length': 'auto', 'batch_mode': 'truncate_episodes', 'compress_observations': False, 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'enable_tf1_exec_eagerly': False, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'update_worker_filter_stats': True, 'use_worker_filter_stats': True, 'enable_connectors': True, 'sampler_perf_stats_ema_coef': None, 'gamma': 0.95, 'lr': 0.001, 'grad_clip': None, 'grad_clip_by': 'global_norm', 'train_batch_size': 4096, 'train_batch_size_per_learner': None, 'model': {'_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'fcnet_weights_initializer': None, 'fcnet_weights_initializer_config': None, 'fcnet_bias_initializer': None, 'fcnet_bias_initializer_config': None, 'conv_filters': None, 'conv_activation': 'relu', 'conv_kernel_initializer': None, 'conv_kernel_initializer_config': None, 'conv_bias_initializer': None, 'conv_bias_initializer_config': None, 'conv_transpose_kernel_initializer': None, 'conv_transpose_kernel_initializer_config': None, 'conv_transpose_bias_initializer': None, 'conv_transpose_bias_initializer_config': None, 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'post_fcnet_weights_initializer': None, 'post_fcnet_weights_initializer_config': None, 'post_fcnet_bias_initializer': None, 'post_fcnet_bias_initializer_config': None, 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, 'lstm_weights_initializer': None, 'lstm_weights_initializer_config': None, 'lstm_bias_initializer': None, 'lstm_bias_initializer_config': None, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'encoder_latent_dim': None, 'always_check_shapes': False, 'lstm_use_prev_action_reward': -1, '_use_default_native_models': -1}, '_learner_connector': None, 'add_default_connectors_to_learner_pipeline': True, 'optimizer': {}, 'max_requests_in_flight_per_sampler_worker': 2, '_learner_class': None, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'algorithm_config_overrides_per_module': {}, '_per_module_overrides': {}, 'count_steps_by': 'env_steps', 'policy_map_capacity': 100, 'policy_mapping_fn': <function AlgorithmConfig.DEFAULT_POLICY_MAPPING_FN at 0x7f326e0fc220>, 'policies_to_train': None, 'policy_states_are_swappable': False, 'observation_fn': None, 'input_config': {}, 'actions_in_input_normalized': False, 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_config': {}, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'offline_sampling': False, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_sample_timeout_s': 120.0, 'evaluation_parallel_to_training': False, 'evaluation_force_reset_envs_before_iteration': True, 'evaluation_config': None, 'off_policy_estimation_methods': {}, 'ope_split_batch_by_episode': True, 'evaluation_num_env_runners': 0, 'always_attach_evaluation_results': True, 'in_evaluation': False, 'sync_filters_on_rollout_workers_timeout_s': 10.0, 'keep_per_episode_custom_metrics': False, 'metrics_episode_collection_timeout_s': 60.0, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_iteration': None, 'min_train_timesteps_per_iteration': 0, 'min_sample_timesteps_per_iteration': 0, 'export_native_model_files': False, 'checkpoint_trainable_policies_only': False, 'logger_creator': None, 'logger_config': None, 'log_level': 'WARN', 'log_sys_usage': True, 'fake_sampler': False, 'seed': None, '_run_training_always_in_thread': False, '_evaluation_parallel_to_training_wo_thread': False, 'ignore_env_runner_failures': False, 'recreate_failed_env_runners': False, 'max_num_env_runner_restarts': 1000, 'delay_between_env_runner_restarts_s': 60.0, 'restart_failed_sub_environments': False, 'num_consecutive_env_runner_failures_tolerance': 100, 'env_runner_health_probe_timeout_s': 30, 'env_runner_restore_timeout_s': 1800, '_model_config_dict': {}, '_rl_module_spec': None, '_AlgorithmConfig__prior_exploration_config': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_initialize_loss_from_dummy_batch': False, 'simple_optimizer': False, 'policy_map_cache': -1, 'worker_cls': -1, 'synchronize_filters': -1, 'enable_async_evaluation': -1, 'custom_async_evaluation_function': -1, '_enable_rl_module_api': -1, 'auto_wrap_old_gym_envs': -1, 'disable_env_checking': -1, 'replay_sequence_length': None, '_disable_execution_plan_api': -1, 'lr_schedule': None, 'use_critic': True, 'use_gae': True, 'use_kl_loss': True, 'kl_coeff': 0.2, 'kl_target': 0.01, 'sgd_minibatch_size': 256, 'mini_batch_size_per_learner': None, 'num_sgd_iter': 30, 'shuffle_sequences': True, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.0, 'entropy_coeff_schedule': None, 'clip_param': 0.3, 'vf_clip_param': 10.0, 'vf_share_layers': -1, 'lambda': 1.0, 'input': 'sampler', 'policies': {'default_policy': (None, None, None, None)}, 'callbacks': <class 'ray.rllib.algorithms.callbacks.DefaultCallbacks'>, 'create_env_on_driver': False, 'custom_eval_function': None, 'framework': 'torch', 'num_cpus_for_driver': 1, 'num_workers': 1}, 'time_since_restore': 933.9117562770844, 'iterations_since_restore': 40, 'perf': {'cpu_util_percent': 35.35609756097562, 'ram_util_percent': 79.00731707317074}})'.\n"
     ]
    }
   ],
   "source": [
    "save_algo(algo, \"KeepTheDistance?dst=0&agent=3&visible_nbrs=2&spawn_area=100\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KeepTheDistance?dst=0&agent=4&visible_nbrs=3&spawn_area=100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ray.tune.registry import register_env\n",
    "\n",
    "train_batch_size = 4096\n",
    "reset_per_batch = train_batch_size/300\n",
    "\n",
    "spawn_area_schedule = [[0,10],[4,30],[9,50],[18,100]]\n",
    "#spawn_area_schedule = [[0,10],[4,30],[10,50],[18,100]]\n",
    "\n",
    "env_config = EnvironmentConfiguration(n_agents=4, visible_nbrs=3, target_distance=0, max_steps=1000, speed=1, spawn_area=20, \n",
    "                                      spawn_area_schedule=[[schedule[0]*train_batch_size, schedule[1]] for schedule in spawn_area_schedule])\n",
    "env_config_show = EnvironmentConfiguration(n_agents=4, visible_nbrs=3, target_distance=0, max_steps=300, speed=1, spawn_area=20, \n",
    "                                      spawn_area_schedule=spawn_area_schedule)\n",
    "register_env(\"KeepTheDistance?dst=0&agent=4&visible_nbrs=3&spawn_area=100\", lambda _: KeepTheDistance(env_config))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "The specified subfolder '/mnt/c/Users/nicol/Desktop/Università/tesi/experiments/RL_experiments/algos/KeepTheDistance?dst=0&agent=4&visible_nbrs=3&spawn_area=100' does not exist.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[101], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m algo \u001b[38;5;241m=\u001b[39m \u001b[43mload_algo\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mKeepTheDistance?dst=0&agent=4&visible_nbrs=3&spawn_area=100\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[1], line 33\u001b[0m, in \u001b[0;36mload_algo\u001b[0;34m(name)\u001b[0m\n\u001b[1;32m     31\u001b[0m subfolder_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(base_dir, name)\n\u001b[1;32m     32\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexists(subfolder_path):\n\u001b[0;32m---> 33\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe specified subfolder \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00msubfolder_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m does not exist.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     35\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m Algorithm\u001b[38;5;241m.\u001b[39mfrom_checkpoint(subfolder_path)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: The specified subfolder '/mnt/c/Users/nicol/Desktop/Università/tesi/experiments/RL_experiments/algos/KeepTheDistance?dst=0&agent=4&visible_nbrs=3&spawn_area=100' does not exist."
     ]
    }
   ],
   "source": [
    "algo = load_algo(\"KeepTheDistance?dst=0&agent=4&visible_nbrs=3&spawn_area=100\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c9e5141cfd324caa90526592c7cad488",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "CanvasWithBorders(height=300, width=300)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration [1] => episode_reward_mean: 241.53260623370483, episode_len_mean: 1000.0, agent_steps_trained: 16384, env_steps_trained: 4096, entropy: 4.262171379725138, learning_rate: 0.0010000000000000005\n",
      "iteration [2] => episode_reward_mean: 8972.364221824693, episode_len_mean: 1000.0, agent_steps_trained: 32768, env_steps_trained: 8192, entropy: 4.166322618474563, learning_rate: 0.0010000000000000005\n",
      "iteration [3] => episode_reward_mean: 20690.61975089599, episode_len_mean: 1000.0, agent_steps_trained: 49152, env_steps_trained: 12288, entropy: 4.057953937600057, learning_rate: 0.0010000000000000005\n",
      "iteration [4] => episode_reward_mean: 29882.30053861677, episode_len_mean: 1000.0, agent_steps_trained: 65536, env_steps_trained: 16384, entropy: 4.133171391238769, learning_rate: 0.0010000000000000005\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[105], line 26\u001b[0m\n\u001b[1;32m     24\u001b[0m env_to_show \u001b[38;5;241m=\u001b[39m RenderableKeepTheDistance(env_config_show)\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(trainin_steps):\n\u001b[0;32m---> 26\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43malgo\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     27\u001b[0m     \u001b[38;5;66;03m#clear_output()\u001b[39;00m\n\u001b[1;32m     28\u001b[0m     \u001b[38;5;66;03m#print(out)\u001b[39;00m\n\u001b[1;32m     29\u001b[0m     simulate_episode(env_to_show, algo, \u001b[38;5;241m150\u001b[39m, sleep_between_frames\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.03\u001b[39m, print_info\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "File \u001b[0;32m~/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/tune/trainable/trainable.py:328\u001b[0m, in \u001b[0;36mTrainable.train\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    326\u001b[0m start \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m    327\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 328\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    329\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    330\u001b[0m     skipped \u001b[38;5;241m=\u001b[39m skip_exceptions(e)\n",
      "File \u001b[0;32m~/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/rllib/algorithms/algorithm.py:873\u001b[0m, in \u001b[0;36mAlgorithm.step\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    863\u001b[0m     (\n\u001b[1;32m    864\u001b[0m         train_results,\n\u001b[1;32m    865\u001b[0m         eval_results,\n\u001b[1;32m    866\u001b[0m         train_iter_ctx,\n\u001b[1;32m    867\u001b[0m     ) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_run_one_training_iteration_and_evaluation_in_parallel()\n\u001b[1;32m    869\u001b[0m \u001b[38;5;66;03m# - No evaluation necessary, just run the next training iteration.\u001b[39;00m\n\u001b[1;32m    870\u001b[0m \u001b[38;5;66;03m# - We have to evaluate in this training iteration, but no parallelism ->\u001b[39;00m\n\u001b[1;32m    871\u001b[0m \u001b[38;5;66;03m#   evaluate after the training iteration is entirely done.\u001b[39;00m\n\u001b[1;32m    872\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 873\u001b[0m     train_results, train_iter_ctx \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_one_training_iteration\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    875\u001b[0m \u001b[38;5;66;03m# Sequential: Train (already done above), then evaluate.\u001b[39;00m\n\u001b[1;32m    876\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m evaluate_this_iter \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mevaluation_parallel_to_training:\n",
      "File \u001b[0;32m~/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/rllib/algorithms/algorithm.py:3156\u001b[0m, in \u001b[0;36mAlgorithm._run_one_training_iteration\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   3154\u001b[0m             \u001b[38;5;66;03m# Try to train one step.\u001b[39;00m\n\u001b[1;32m   3155\u001b[0m             \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_timers[TRAINING_STEP_TIMER]:\n\u001b[0;32m-> 3156\u001b[0m                 results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3158\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m results, train_iter_ctx\n",
      "File \u001b[0;32m~/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/rllib/algorithms/ppo/ppo.py:428\u001b[0m, in \u001b[0;36mPPO.training_step\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    424\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_training_step_new_api_stack()\n\u001b[1;32m    425\u001b[0m \u001b[38;5;66;03m# Old and hybrid API stacks (Policy, RolloutWorker, Connector, maybe RLModule,\u001b[39;00m\n\u001b[1;32m    426\u001b[0m \u001b[38;5;66;03m# maybe Learner).\u001b[39;00m\n\u001b[1;32m    427\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 428\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_training_step_old_and_hybrid_api_stacks\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/rllib/algorithms/ppo/ppo.py:587\u001b[0m, in \u001b[0;36mPPO._training_step_old_and_hybrid_api_stacks\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    585\u001b[0m     train_results \u001b[38;5;241m=\u001b[39m train_one_step(\u001b[38;5;28mself\u001b[39m, train_batch)\n\u001b[1;32m    586\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 587\u001b[0m     train_results \u001b[38;5;241m=\u001b[39m \u001b[43mmulti_gpu_train_one_step\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    589\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39menable_rl_module_and_learner:\n\u001b[1;32m    590\u001b[0m     \u001b[38;5;66;03m# The train results's loss keys are pids to their loss values. But we also\u001b[39;00m\n\u001b[1;32m    591\u001b[0m     \u001b[38;5;66;03m# return a total_loss key at the same level as the pid keys. So we need to\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    594\u001b[0m     \u001b[38;5;66;03m#  passing medium to infer which policies to update. We could use\u001b[39;00m\n\u001b[1;32m    595\u001b[0m     \u001b[38;5;66;03m#  policies_to_train variable that is given by the user to infer this.\u001b[39;00m\n\u001b[1;32m    596\u001b[0m     policies_to_update \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m(train_results\u001b[38;5;241m.\u001b[39mkeys()) \u001b[38;5;241m-\u001b[39m {ALL_MODULES}\n",
      "File \u001b[0;32m~/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/rllib/execution/train_ops.py:176\u001b[0m, in \u001b[0;36mmulti_gpu_train_one_step\u001b[0;34m(algorithm, train_batch)\u001b[0m\n\u001b[1;32m    171\u001b[0m         permutation \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mpermutation(num_batches)\n\u001b[1;32m    172\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m batch_index \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_batches):\n\u001b[1;32m    173\u001b[0m             \u001b[38;5;66;03m# Learn on the pre-loaded data in the buffer.\u001b[39;00m\n\u001b[1;32m    174\u001b[0m             \u001b[38;5;66;03m# Note: For minibatch SGD, the data is an offset into\u001b[39;00m\n\u001b[1;32m    175\u001b[0m             \u001b[38;5;66;03m# the pre-loaded entire train batch.\u001b[39;00m\n\u001b[0;32m--> 176\u001b[0m             results \u001b[38;5;241m=\u001b[39m \u001b[43mpolicy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearn_on_loaded_batch\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    177\u001b[0m \u001b[43m                \u001b[49m\u001b[43mpermutation\u001b[49m\u001b[43m[\u001b[49m\u001b[43mbatch_index\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mper_device_batch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\n\u001b[1;32m    178\u001b[0m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    180\u001b[0m             learner_info_builder\u001b[38;5;241m.\u001b[39madd_learn_on_batch_results(results, policy_id)\n\u001b[1;32m    182\u001b[0m \u001b[38;5;66;03m# Tower reduce and finalize results.\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/rllib/policy/torch_policy_v2.py:838\u001b[0m, in \u001b[0;36mTorchPolicyV2.learn_on_loaded_batch\u001b[0;34m(self, offset, buffer_index)\u001b[0m\n\u001b[1;32m    835\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    836\u001b[0m         batch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_loaded_batches[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m][offset : offset \u001b[38;5;241m+\u001b[39m device_batch_size]\n\u001b[0;32m--> 838\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearn_on_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    840\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevices) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m    841\u001b[0m     \u001b[38;5;66;03m# Copy weights of main model (tower-0) to all other towers.\u001b[39;00m\n\u001b[1;32m    842\u001b[0m     state_dict \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mstate_dict()\n",
      "File \u001b[0;32m~/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/rllib/utils/threading.py:24\u001b[0m, in \u001b[0;36mwith_lock.<locals>.wrapper\u001b[0;34m(self, *a, **k)\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     23\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[0;32m---> 24\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mk\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     26\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas no attribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_lock\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m e\u001b[38;5;241m.\u001b[39margs[\u001b[38;5;241m0\u001b[39m]:\n",
      "File \u001b[0;32m~/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/rllib/policy/torch_policy_v2.py:715\u001b[0m, in \u001b[0;36mTorchPolicyV2.learn_on_batch\u001b[0;34m(self, postprocessed_batch)\u001b[0m\n\u001b[1;32m    709\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallbacks\u001b[38;5;241m.\u001b[39mon_learn_on_batch(\n\u001b[1;32m    710\u001b[0m     policy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m, train_batch\u001b[38;5;241m=\u001b[39mpostprocessed_batch, result\u001b[38;5;241m=\u001b[39mlearn_stats\n\u001b[1;32m    711\u001b[0m )\n\u001b[1;32m    713\u001b[0m \u001b[38;5;66;03m# Compute gradients (will calculate all losses and `backward()`\u001b[39;00m\n\u001b[1;32m    714\u001b[0m \u001b[38;5;66;03m# them to get the grads).\u001b[39;00m\n\u001b[0;32m--> 715\u001b[0m grads, fetches \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_gradients\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpostprocessed_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    717\u001b[0m \u001b[38;5;66;03m# Step the optimizers.\u001b[39;00m\n\u001b[1;32m    718\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply_gradients(_directStepOptimizerSingleton)\n",
      "File \u001b[0;32m~/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/rllib/utils/threading.py:24\u001b[0m, in \u001b[0;36mwith_lock.<locals>.wrapper\u001b[0;34m(self, *a, **k)\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     23\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[0;32m---> 24\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mk\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     26\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas no attribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_lock\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m e\u001b[38;5;241m.\u001b[39margs[\u001b[38;5;241m0\u001b[39m]:\n",
      "File \u001b[0;32m~/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/rllib/policy/torch_policy_v2.py:933\u001b[0m, in \u001b[0;36mTorchPolicyV2.compute_gradients\u001b[0;34m(self, postprocessed_batch)\u001b[0m\n\u001b[1;32m    930\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lazy_tensor_dict(postprocessed_batch, device\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevices[\u001b[38;5;241m0\u001b[39m])\n\u001b[1;32m    932\u001b[0m \u001b[38;5;66;03m# Do the (maybe parallelized) gradient calculation step.\u001b[39;00m\n\u001b[0;32m--> 933\u001b[0m tower_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_multi_gpu_parallel_grad_calc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mpostprocessed_batch\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    935\u001b[0m all_grads, grad_info \u001b[38;5;241m=\u001b[39m tower_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    937\u001b[0m grad_info[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mallreduce_latency\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_optimizers)\n",
      "File \u001b[0;32m~/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/rllib/policy/torch_policy_v2.py:1429\u001b[0m, in \u001b[0;36mTorchPolicyV2._multi_gpu_parallel_grad_calc\u001b[0;34m(self, sample_batches)\u001b[0m\n\u001b[1;32m   1425\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevices) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_fake_gpus\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[1;32m   1426\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m shard_idx, (model, sample_batch, device) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\n\u001b[1;32m   1427\u001b[0m         \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_gpu_towers, sample_batches, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevices)\n\u001b[1;32m   1428\u001b[0m     ):\n\u001b[0;32m-> 1429\u001b[0m         \u001b[43m_worker\u001b[49m\u001b[43m(\u001b[49m\u001b[43mshard_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_batch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1430\u001b[0m         \u001b[38;5;66;03m# Raise errors right away for better debugging.\u001b[39;00m\n\u001b[1;32m   1431\u001b[0m         last_result \u001b[38;5;241m=\u001b[39m results[\u001b[38;5;28mlen\u001b[39m(results) \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m]\n",
      "File \u001b[0;32m~/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/rllib/policy/torch_policy_v2.py:1348\u001b[0m, in \u001b[0;36mTorchPolicyV2._multi_gpu_parallel_grad_calc.<locals>._worker\u001b[0;34m(shard_idx, model, sample_batch, device)\u001b[0m\n\u001b[1;32m   1343\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1344\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m NullContextManager() \u001b[38;5;28;01mif\u001b[39;00m device\u001b[38;5;241m.\u001b[39mtype \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mdevice(  \u001b[38;5;66;03m# noqa: E501\u001b[39;00m\n\u001b[1;32m   1345\u001b[0m         device\n\u001b[1;32m   1346\u001b[0m     ):\n\u001b[1;32m   1347\u001b[0m         loss_out \u001b[38;5;241m=\u001b[39m force_list(\n\u001b[0;32m-> 1348\u001b[0m             \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdist_class\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1349\u001b[0m         )\n\u001b[1;32m   1351\u001b[0m         \u001b[38;5;66;03m# Call Model's custom-loss with Policy loss outputs and\u001b[39;00m\n\u001b[1;32m   1352\u001b[0m         \u001b[38;5;66;03m# train_batch.\u001b[39;00m\n\u001b[1;32m   1353\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(model, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcustom_loss\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m~/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/rllib/algorithms/ppo/ppo_torch_policy.py:84\u001b[0m, in \u001b[0;36mPPOTorchPolicy.loss\u001b[0;34m(self, model, dist_class, train_batch)\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;129m@override\u001b[39m(TorchPolicyV2)\n\u001b[1;32m     67\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mloss\u001b[39m(\n\u001b[1;32m     68\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     71\u001b[0m     train_batch: SampleBatch,\n\u001b[1;32m     72\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Union[TensorType, List[TensorType]]:\n\u001b[1;32m     73\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Compute loss for Proximal Policy Objective.\u001b[39;00m\n\u001b[1;32m     74\u001b[0m \n\u001b[1;32m     75\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     81\u001b[0m \u001b[38;5;124;03m        The PPO loss tensor given the input batch.\u001b[39;00m\n\u001b[1;32m     82\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 84\u001b[0m     logits, state \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     85\u001b[0m     curr_action_dist \u001b[38;5;241m=\u001b[39m dist_class(logits, model)\n\u001b[1;32m     87\u001b[0m     \u001b[38;5;66;03m# RNN case: Mask away 0-padded chunks at end of time axis.\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/rllib/models/modelv2.py:244\u001b[0m, in \u001b[0;36mModelV2.__call__\u001b[0;34m(self, input_dict, state, seq_lens)\u001b[0m\n\u001b[1;32m    238\u001b[0m     restored[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mobs_flat\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m input_dict[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mobs\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m    239\u001b[0m \u001b[38;5;66;03m# Input to this Model went through a Preprocessor.\u001b[39;00m\n\u001b[1;32m    240\u001b[0m \u001b[38;5;66;03m# Generate extra keys: \"obs_flat\" (vs \"obs\", which will hold the\u001b[39;00m\n\u001b[1;32m    241\u001b[0m \u001b[38;5;66;03m# original obs).\u001b[39;00m\n\u001b[1;32m    242\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    243\u001b[0m     restored[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mobs\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m restore_original_dimensions(\n\u001b[0;32m--> 244\u001b[0m         \u001b[43minput_dict\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mobs\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobs_space, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mframework\n\u001b[1;32m    245\u001b[0m     )\n\u001b[1;32m    246\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    247\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(input_dict[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mobs\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mshape) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m2\u001b[39m:\n",
      "File \u001b[0;32m~/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/rllib/policy/sample_batch.py:953\u001b[0m, in \u001b[0;36mSampleBatch.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    951\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_interceptor \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    952\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m key \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mintercepted_values:\n\u001b[0;32m--> 953\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mintercepted_values[key] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_interceptor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    954\u001b[0m     value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mintercepted_values[key]\n\u001b[1;32m    955\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m value\n",
      "File \u001b[0;32m~/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/rllib/utils/torch_utils.py:276\u001b[0m, in \u001b[0;36mconvert_to_torch_tensor\u001b[0;34m(x, device)\u001b[0m\n\u001b[1;32m    272\u001b[0m         tensor \u001b[38;5;241m=\u001b[39m tensor\u001b[38;5;241m.\u001b[39mfloat()\n\u001b[1;32m    274\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m tensor \u001b[38;5;28;01mif\u001b[39;00m device \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m tensor\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m--> 276\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtree\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap_structure\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmapping\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/tianEnv/lib/python3.11/site-packages/tree/__init__.py:435\u001b[0m, in \u001b[0;36mmap_structure\u001b[0;34m(func, *structures, **kwargs)\u001b[0m\n\u001b[1;32m    432\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m other \u001b[38;5;129;01min\u001b[39;00m structures[\u001b[38;5;241m1\u001b[39m:]:\n\u001b[1;32m    433\u001b[0m   assert_same_structure(structures[\u001b[38;5;241m0\u001b[39m], other, check_types\u001b[38;5;241m=\u001b[39mcheck_types)\n\u001b[1;32m    434\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m unflatten_as(structures[\u001b[38;5;241m0\u001b[39m],\n\u001b[0;32m--> 435\u001b[0m                     \u001b[43m[\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mzip\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mmap\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mflatten\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstructures\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m)\n",
      "File \u001b[0;32m~/anaconda3/envs/tianEnv/lib/python3.11/site-packages/tree/__init__.py:435\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    432\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m other \u001b[38;5;129;01min\u001b[39;00m structures[\u001b[38;5;241m1\u001b[39m:]:\n\u001b[1;32m    433\u001b[0m   assert_same_structure(structures[\u001b[38;5;241m0\u001b[39m], other, check_types\u001b[38;5;241m=\u001b[39mcheck_types)\n\u001b[1;32m    434\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m unflatten_as(structures[\u001b[38;5;241m0\u001b[39m],\n\u001b[0;32m--> 435\u001b[0m                     [\u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m args \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mmap\u001b[39m(flatten, structures))])\n",
      "File \u001b[0;32m~/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/rllib/utils/torch_utils.py:265\u001b[0m, in \u001b[0;36mconvert_to_torch_tensor.<locals>.mapping\u001b[0;34m(item)\u001b[0m\n\u001b[1;32m    262\u001b[0m             tensor \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mfrom_numpy(item)\n\u001b[1;32m    263\u001b[0m     \u001b[38;5;66;03m# Already numpy: Wrap as torch tensor.\u001b[39;00m\n\u001b[1;32m    264\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 265\u001b[0m         tensor \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_numpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mitem\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    266\u001b[0m \u001b[38;5;66;03m# Everything else: Convert to numpy, then wrap as torch tensor.\u001b[39;00m\n\u001b[1;32m    267\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    268\u001b[0m     tensor \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mfrom_numpy(np\u001b[38;5;241m.\u001b[39masarray(item))\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from ray.rllib.algorithms.ppo import PPOConfig\n",
    "from ray.tune.logger import pretty_print\n",
    "\n",
    "from gymnasium.wrappers.time_limit import TimeLimit\n",
    "\n",
    "trainin_steps = 40\n",
    "\n",
    "algo = (\n",
    "    PPOConfig()\n",
    "    .training(gamma = 0.95, \n",
    "              lr = 0.001,\n",
    "              train_batch_size = train_batch_size, \n",
    "              sgd_minibatch_size = 256, \n",
    "              num_sgd_iter = 30,\n",
    "              #entropy_coeff=0.005,\n",
    "              )\n",
    "    .env_runners(num_env_runners=1)\n",
    "    .resources(num_gpus=0)\n",
    "    .environment(env=\"KeepTheDistance?dst=0&agent=4&visible_nbrs=3&spawn_area=100\")\n",
    "    .build()\n",
    ")\n",
    "clear_output()\n",
    "\n",
    "env_to_show = RenderableKeepTheDistance(env_config_show)\n",
    "for i in range(trainin_steps):\n",
    "    result = algo.train()\n",
    "    #clear_output()\n",
    "    #print(out)\n",
    "    simulate_episode(env_to_show, algo, 150, sleep_between_frames=0.03, print_info=False)\n",
    "    print(ppo_result_format(result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5066be2dfd4447a39660264441ee19cb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "CanvasWithBorders(height=300, width=300)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "env_config_2 = EnvironmentConfiguration(n_agents=4, visible_nbrs=3, target_distance=0, max_steps=500, speed=1, spawn_area=100)\n",
    "simulate_episode(RenderableKeepTheDistance(env_config_2), algo, 300, sleep_between_frames=0.03, print_info=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_algo(algo, \"KeepTheDistance?dst=0&agent=3&visible_nbrs=2&spawn_area=100\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tianEnv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
