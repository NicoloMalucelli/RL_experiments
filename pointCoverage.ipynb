{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Set\n",
    "from ray.rllib.env.multi_agent_env import MultiAgentEnv\n",
    "import random as rnd\n",
    "import math\n",
    "from gymnasium.spaces import Discrete, Box, Sequence, Dict\n",
    "from gymnasium.spaces.utils import flatten, flatten_space\n",
    "import numpy as np\n",
    "from IPython.display import clear_output\n",
    "\n",
    "from ray.rllib.utils.typing import AgentID\n",
    "\n",
    "class PointCoverageEnv(MultiAgentEnv):\n",
    "\n",
    "    actions_dict = [(0,-1),(0,1),(1,0),(-1,0),(0,0)]\n",
    "\n",
    "    def __init__(self, config):\n",
    "        self.width = config[\"width\"]\n",
    "        self.height = config[\"height\"]\n",
    "        self.n_agents = config[\"n_agents\"]\n",
    "        self.n_targets = config[\"n_targets\"]\n",
    "        self.max_steps = config[\"max_steps\"] if \"max_steps\" in config.keys() else None\n",
    "        self.agents = ['agent-' + str(i) for i in range(self.n_agents)]\n",
    "        self.observation_space = self.observation_space('agent-0')\n",
    "        self.action_space = Discrete(5)\n",
    "\n",
    "    def unflatten_observation_space(self, agent):\n",
    "       coordinates_space = Box(low=np.array([0, 0]), high=np.array([self.width-1, self.height-1]), dtype=np.int32)\n",
    "       if self.n_agents > 1:\n",
    "            return Dict({\n",
    "                \"position\": coordinates_space,\n",
    "                \"other_agents\": Dict({f\"other_agent-{i}\": coordinates_space for i in range(self.n_agents-1)}),\n",
    "                \"targets\": Dict({f\"target-{i}\": coordinates_space for i in range(self.n_targets)})\n",
    "            })\n",
    "       else:\n",
    "           return Dict({\n",
    "                \"position\": coordinates_space,\n",
    "                \"targets\": Dict({f\"target-{i}\": coordinates_space for i in range(self.n_targets)})\n",
    "            })\n",
    "\n",
    "    def observation_space(self, agent):\n",
    "       return flatten_space(self.unflatten_observation_space(agent))\n",
    "       #return self.unflatten_observation_space(agent)\n",
    "\n",
    "    def action_space(self, agent):\n",
    "        return Discrete(5)\n",
    "    \n",
    "    def __get_other_agents(self, agent):\n",
    "        return [other for other in self.agents if other != agent]\n",
    "\n",
    "    def __get_random_point(self):\n",
    "        return (rnd.randint(0, self.width-1), rnd.randint(0, self.height-1))\n",
    "    \n",
    "    def __get_observation(self, agent):\n",
    "        if self.n_agents > 1:\n",
    "            return flatten(self.unflatten_observation_space(agent), \n",
    "                {\n",
    "                    \"position\": self.agent_pos[agent],\n",
    "                    \"other_agents\": {f\"other_agent-{i}\": self.agent_pos[other] for i, other in enumerate(self.__get_other_agents(agent))},\n",
    "                    \"targets\": {f\"target-{i}\": pos for i, pos in enumerate(self.targets)}\n",
    "                }\n",
    "            )\n",
    "        else:\n",
    "            return flatten(self.unflatten_observation_space(agent), \n",
    "                {\n",
    "                    \"position\": self.agent_pos[agent],\n",
    "                    \"targets\": {f\"target-{i}\": pos for i, pos in enumerate(self.targets)}\n",
    "                }\n",
    "            )\n",
    "\n",
    "    def __get_not_covered_targets(self):\n",
    "        return set(self.targets) - set(self.agent_pos.values())\n",
    "\n",
    "    def __is_target_contended(self, target):\n",
    "        return len([t for t in self.agent_pos.values() if target == t]) > 1\n",
    "\n",
    "    def __get_reward(self, agent):\n",
    "        return -1 + self.__get_global_reward()\n",
    "        if self.agent_pos[agent] in self.targets:\n",
    "            if self.agent_pos[agent] in [pos[1] for pos in self.old_agent_pos if pos[0] != agent]:\n",
    "                return -1 # someone was already covering the target -> no +10 reward\n",
    "            if self.__is_target_contended(self.agent_pos[agent]):\n",
    "                return -2 # someone arrived at the target at the same time of me -> someone has to leave\n",
    "            return 10\n",
    "        else:\n",
    "            return -1\n",
    "    \n",
    "    def __get_global_reward(self):\n",
    "        return (len(self.not_covered_target) - len(set(self.not_covered_target) - set(self.agent_pos.values())))*10\n",
    "    \n",
    "    def __update_agent_position(self, agent, x, y):\n",
    "        self.agent_pos[agent] = (max(min(self.agent_pos[agent][0] + x, self.width-1), 0),\n",
    "                                 max(min(self.agent_pos[agent][1] + y, self.height-1), 0))\n",
    "\n",
    "    def reset(self, seed=None, options=None):\n",
    "        self.agent_pos = {agent: self.__get_random_point() for agent in self.agents}\n",
    "        self.targets = [self.__get_random_point() for _ in range(self.n_targets)]\n",
    "        self.not_covered_target = self.targets.copy()\n",
    "        self.steps = 0;\n",
    "        return {agent: self.__get_observation(agent) for agent in self.agents}, {}\n",
    "     \n",
    "    def step(self, actions):\n",
    "        self.steps += 1\n",
    "        observations, rewards, terminated, truncated, infos = {}, {}, {}, {}, {}\n",
    "\n",
    "        self.old_agent_pos = self.agent_pos.copy()\n",
    "        for agent, action in actions.items():\n",
    "            self.__update_agent_position(agent, self.actions_dict[action][0], self.actions_dict[action][1])\n",
    "\n",
    "        for agent in actions.keys():\n",
    "            if not (self.agent_pos[agent] in self.targets and not self.__is_target_contended(self.agent_pos[agent])):\n",
    "                observations[agent] = self.__get_observation(agent)\n",
    "                rewards[agent] = self.__get_reward(agent)\n",
    "                terminated[agent] = False\n",
    "                truncated[agent] = False\n",
    "                infos[agent] = {}\n",
    "        \n",
    "        if self.max_steps != None and self.steps > self.max_steps:\n",
    "            truncated['__all__'] = True\n",
    "        else:\n",
    "            truncated['__all__'] = False\n",
    "\n",
    "        self.not_covered_target = list(set(self.not_covered_target) - set(self.agent_pos.values())) \n",
    "\n",
    "        terminated['__all__'] = len(self.__get_not_covered_targets()) == 0\n",
    "        return observations, rewards, terminated, truncated, infos\n",
    "     \n",
    "    def render(self, mode='text'):\n",
    "        str = '_' * (self.width+2) + '\\n'\n",
    "        for i in range(self.height):\n",
    "            str = str + \"|\"\n",
    "            for j in range(self.width):\n",
    "                if (j,i) in self.agent_pos.values() and (j,i) in self.targets:\n",
    "                    str = str + '*'\n",
    "                elif (j,i) in self.agent_pos.values():\n",
    "                    str = str + 'o'\n",
    "                elif (j,i) in self.targets:\n",
    "                    str = str + 'x'\n",
    "                else:\n",
    "                    str = str + ' '\n",
    "            str = str + '|\\n'\n",
    "        str = str + '‾' * (self.width+2)\n",
    "        print(str)\n",
    "\n",
    "    def get_agent_ids(self):\n",
    "       return self.agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[7 2 4 7 1 2 6 4]\n",
      "[4 7 7 2 1 2 6 4]\n",
      "____________\n",
      "|          |\n",
      "|          |\n",
      "| x     o  |\n",
      "|          |\n",
      "|      x   |\n",
      "|          |\n",
      "|          |\n",
      "|    o     |\n",
      "|          |\n",
      "|          |\n",
      "‾‾‾‾‾‾‾‾‾‾‾‾\n",
      "[7 1 4 8 1 2 6 4]\n",
      "____________\n",
      "|          |\n",
      "|       o  |\n",
      "| x        |\n",
      "|          |\n",
      "|      x   |\n",
      "|          |\n",
      "|          |\n",
      "|          |\n",
      "|    o     |\n",
      "|          |\n",
      "‾‾‾‾‾‾‾‾‾‾‾‾\n"
     ]
    }
   ],
   "source": [
    "env = PointCoverageEnv({\"height\": 10, \"width\": 10, \"n_agents\": 2, \"n_targets\": 2})\n",
    "obs, _ = env.reset() \n",
    "print(obs['agent-0'])\n",
    "print(obs['agent-1'])\n",
    "env.render()\n",
    "\n",
    "actions = {\"agent-0\": 1, \"agent-1\": 0}\n",
    "obs, _, _, _, _ = env.step(actions)\n",
    "print(obs['agent-0'])\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-16 12:02:42,628\tINFO worker.py:1740 -- Started a local Ray instance. View the dashboard at \u001b[1m\u001b[32mhttp://127.0.0.1:8265 \u001b[39m\u001b[22m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "afa8c62683df4b75b19afe4c886414b8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/html": [
       "<div class=\"lm-Widget p-Widget lm-Panel p-Panel jp-Cell-outputWrapper\">\n",
       "    <div style=\"margin-left: 50px;display: flex;flex-direction: row;align-items: center\">\n",
       "        <div class=\"jp-RenderedHTMLCommon\" style=\"display: flex; flex-direction: row;\">\n",
       "  <svg viewBox=\"0 0 567 224\" fill=\"none\" xmlns=\"http://www.w3.org/2000/svg\" style=\"height: 3em;\">\n",
       "    <g clip-path=\"url(#clip0_4338_178347)\">\n",
       "        <path d=\"M341.29 165.561H355.29L330.13 129.051C345.63 123.991 354.21 112.051 354.21 94.2307C354.21 71.3707 338.72 58.1807 311.88 58.1807H271V165.561H283.27V131.661H311.8C314.25 131.661 316.71 131.501 319.01 131.351L341.25 165.561H341.29ZM283.29 119.851V70.0007H311.82C331.3 70.0007 342.34 78.2907 342.34 94.5507C342.34 111.271 331.34 119.861 311.82 119.861L283.29 119.851ZM451.4 138.411L463.4 165.561H476.74L428.74 58.1807H416L367.83 165.561H380.83L392.83 138.411H451.4ZM446.19 126.601H398L422 72.1407L446.24 126.601H446.19ZM526.11 128.741L566.91 58.1807H554.35L519.99 114.181L485.17 58.1807H472.44L514.01 129.181V165.541H526.13V128.741H526.11Z\" fill=\"var(--jp-ui-font-color0)\"/>\n",
       "        <path d=\"M82.35 104.44C84.0187 97.8827 87.8248 92.0678 93.1671 87.9146C98.5094 83.7614 105.083 81.5067 111.85 81.5067C118.617 81.5067 125.191 83.7614 130.533 87.9146C135.875 92.0678 139.681 97.8827 141.35 104.44H163.75C164.476 101.562 165.622 98.8057 167.15 96.2605L127.45 56.5605C121.071 60.3522 113.526 61.6823 106.235 60.3005C98.9443 58.9187 92.4094 54.9203 87.8602 49.0574C83.3109 43.1946 81.0609 35.8714 81.5332 28.4656C82.0056 21.0599 85.1679 14.0819 90.4252 8.8446C95.6824 3.60726 102.672 0.471508 110.08 0.0272655C117.487 -0.416977 124.802 1.86091 130.647 6.4324C136.493 11.0039 140.467 17.5539 141.821 24.8501C143.175 32.1463 141.816 39.6859 138 46.0505L177.69 85.7505C182.31 82.9877 187.58 81.4995 192.962 81.4375C198.345 81.3755 203.648 82.742 208.33 85.3976C213.012 88.0532 216.907 91.9029 219.616 96.5544C222.326 101.206 223.753 106.492 223.753 111.875C223.753 117.258 222.326 122.545 219.616 127.197C216.907 131.848 213.012 135.698 208.33 138.353C203.648 141.009 198.345 142.375 192.962 142.313C187.58 142.251 182.31 140.763 177.69 138L138 177.7C141.808 184.071 143.155 191.614 141.79 198.91C140.424 206.205 136.44 212.75 130.585 217.313C124.731 221.875 117.412 224.141 110.004 223.683C102.596 223.226 95.6103 220.077 90.3621 214.828C85.1139 209.58 81.9647 202.595 81.5072 195.187C81.0497 187.779 83.3154 180.459 87.878 174.605C92.4405 168.751 98.9853 164.766 106.281 163.401C113.576 162.035 121.119 163.383 127.49 167.19L167.19 127.49C165.664 124.941 164.518 122.182 163.79 119.3H141.39C139.721 125.858 135.915 131.673 130.573 135.826C125.231 139.98 118.657 142.234 111.89 142.234C105.123 142.234 98.5494 139.98 93.2071 135.826C87.8648 131.673 84.0587 125.858 82.39 119.3H60C58.1878 126.495 53.8086 132.78 47.6863 136.971C41.5641 141.163 34.1211 142.972 26.7579 142.059C19.3947 141.146 12.6191 137.574 7.70605 132.014C2.79302 126.454 0.0813599 119.29 0.0813599 111.87C0.0813599 104.451 2.79302 97.2871 7.70605 91.7272C12.6191 86.1673 19.3947 82.5947 26.7579 81.6817C34.1211 80.7686 41.5641 82.5781 47.6863 86.7696C53.8086 90.9611 58.1878 97.2456 60 104.44H82.35ZM100.86 204.32C103.407 206.868 106.759 208.453 110.345 208.806C113.93 209.159 117.527 208.258 120.522 206.256C123.517 204.254 125.725 201.276 126.771 197.828C127.816 194.38 127.633 190.677 126.253 187.349C124.874 184.021 122.383 181.274 119.205 179.577C116.027 177.88 112.359 177.337 108.826 178.042C105.293 178.746 102.113 180.654 99.8291 183.44C97.5451 186.226 96.2979 189.718 96.3 193.32C96.2985 195.364 96.7006 197.388 97.4831 199.275C98.2656 201.163 99.4132 202.877 100.86 204.32ZM204.32 122.88C206.868 120.333 208.453 116.981 208.806 113.396C209.159 109.811 208.258 106.214 206.256 103.219C204.254 100.223 201.275 98.0151 197.827 96.97C194.38 95.9249 190.676 96.1077 187.348 97.4873C184.02 98.8669 181.274 101.358 179.577 104.536C177.879 107.714 177.337 111.382 178.041 114.915C178.746 118.448 180.653 121.627 183.439 123.911C186.226 126.195 189.717 127.443 193.32 127.44C195.364 127.443 197.388 127.042 199.275 126.259C201.163 125.476 202.878 124.328 204.32 122.88ZM122.88 19.4205C120.333 16.8729 116.981 15.2876 113.395 14.9347C109.81 14.5817 106.213 15.483 103.218 17.4849C100.223 19.4868 98.0146 22.4654 96.9696 25.9131C95.9245 29.3608 96.1073 33.0642 97.4869 36.3922C98.8665 39.7202 101.358 42.4668 104.535 44.1639C107.713 45.861 111.381 46.4036 114.914 45.6992C118.447 44.9949 121.627 43.0871 123.911 40.301C126.195 37.515 127.442 34.0231 127.44 30.4205C127.44 28.3772 127.038 26.3539 126.255 24.4664C125.473 22.5788 124.326 20.8642 122.88 19.4205ZM19.42 100.86C16.8725 103.408 15.2872 106.76 14.9342 110.345C14.5813 113.93 15.4826 117.527 17.4844 120.522C19.4863 123.518 22.4649 125.726 25.9127 126.771C29.3604 127.816 33.0638 127.633 36.3918 126.254C39.7198 124.874 42.4664 122.383 44.1635 119.205C45.8606 116.027 46.4032 112.359 45.6988 108.826C44.9944 105.293 43.0866 102.114 40.3006 99.8296C37.5145 97.5455 34.0227 96.2983 30.42 96.3005C26.2938 96.3018 22.337 97.9421 19.42 100.86ZM100.86 100.86C98.3125 103.408 96.7272 106.76 96.3742 110.345C96.0213 113.93 96.9226 117.527 98.9244 120.522C100.926 123.518 103.905 125.726 107.353 126.771C110.8 127.816 114.504 127.633 117.832 126.254C121.16 124.874 123.906 122.383 125.604 119.205C127.301 116.027 127.843 112.359 127.139 108.826C126.434 105.293 124.527 102.114 121.741 99.8296C118.955 97.5455 115.463 96.2983 111.86 96.3005C109.817 96.299 107.793 96.701 105.905 97.4835C104.018 98.2661 102.303 99.4136 100.86 100.86Z\" fill=\"#00AEEF\"/>\n",
       "    </g>\n",
       "    <defs>\n",
       "        <clipPath id=\"clip0_4338_178347\">\n",
       "            <rect width=\"566.93\" height=\"223.75\" fill=\"white\"/>\n",
       "        </clipPath>\n",
       "    </defs>\n",
       "  </svg>\n",
       "</div>\n",
       "\n",
       "        <table class=\"jp-RenderedHTMLCommon\" style=\"border-collapse: collapse;color: var(--jp-ui-font-color1);font-size: var(--jp-ui-font-size1);\">\n",
       "    <tr>\n",
       "        <td style=\"text-align: left\"><b>Python version:</b></td>\n",
       "        <td style=\"text-align: left\"><b>3.11.9</b></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td style=\"text-align: left\"><b>Ray version:</b></td>\n",
       "        <td style=\"text-align: left\"><b>2.21.0</b></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "    <td style=\"text-align: left\"><b>Dashboard:</b></td>\n",
       "    <td style=\"text-align: left\"><b><a href=\"http://127.0.0.1:8265\" target=\"_blank\">http://127.0.0.1:8265</a></b></td>\n",
       "</tr>\n",
       "\n",
       "</table>\n",
       "\n",
       "    </div>\n",
       "</div>\n"
      ],
      "text/plain": [
       "RayContext(dashboard_url='127.0.0.1:8265', python_version='3.11.9', ray_version='2.21.0', ray_commit='a912be84d6944bc32cfe84a9a3fe0b0227574536')"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import ray\n",
    "ray.shutdown()\n",
    "ray.init()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Single agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nicolo/anaconda3/envs/rayEnv/lib/python3.11/site-packages/ray/rllib/algorithms/algorithm.py:521: RayDeprecationWarning: This API is deprecated and may be removed in future Ray releases. You could suppress this warning by setting env variable PYTHONWARNINGS=\"ignore::DeprecationWarning\"\n",
      "`UnifiedLogger` will be removed in Ray 2.7.\n",
      "  return UnifiedLogger(config, logdir, loggers=None)\n",
      "/home/nicolo/anaconda3/envs/rayEnv/lib/python3.11/site-packages/ray/tune/logger/unified.py:53: RayDeprecationWarning: This API is deprecated and may be removed in future Ray releases. You could suppress this warning by setting env variable PYTHONWARNINGS=\"ignore::DeprecationWarning\"\n",
      "The `JsonLogger interface is deprecated in favor of the `ray.tune.json.JsonLoggerCallback` interface and will be removed in Ray 2.7.\n",
      "  self._loggers.append(cls(self.config, self.logdir, self.trial))\n",
      "/home/nicolo/anaconda3/envs/rayEnv/lib/python3.11/site-packages/ray/tune/logger/unified.py:53: RayDeprecationWarning: This API is deprecated and may be removed in future Ray releases. You could suppress this warning by setting env variable PYTHONWARNINGS=\"ignore::DeprecationWarning\"\n",
      "The `CSVLogger interface is deprecated in favor of the `ray.tune.csv.CSVLoggerCallback` interface and will be removed in Ray 2.7.\n",
      "  self._loggers.append(cls(self.config, self.logdir, self.trial))\n",
      "/home/nicolo/anaconda3/envs/rayEnv/lib/python3.11/site-packages/ray/tune/logger/unified.py:53: RayDeprecationWarning: This API is deprecated and may be removed in future Ray releases. You could suppress this warning by setting env variable PYTHONWARNINGS=\"ignore::DeprecationWarning\"\n",
      "The `TBXLogger interface is deprecated in favor of the `ray.tune.tensorboardx.TBXLoggerCallback` interface and will be removed in Ray 2.7.\n",
      "  self._loggers.append(cls(self.config, self.logdir, self.trial))\n",
      "2024-05-16 10:25:46,629\tWARNING deprecation.py:50 -- DeprecationWarning: `WorkerSet(num_workers=... OR local_worker=...)` has been deprecated. Use `EnvRunnerGroup(num_env_runners=... AND local_env_runner=...)` instead. This will raise an error in the future!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-16 10:25:55,664\tWARNING util.py:61 -- Install gputil for GPU system monitoring.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'episode_reward_max': -4.0, 'episode_reward_min': -31.0, 'episode_reward_mean': -29.1, 'episode_len_mean': 29.228571428571428, 'episode_media': {}, 'episodes_this_iter': 70, 'episodes_timesteps_total': 2046, 'policy_reward_min': {}, 'policy_reward_max': {}, 'policy_reward_mean': {}, 'custom_metrics': {}, 'hist_stats': {'episode_reward': [-31.0, -31.0, -31.0, -21.0, -31.0, -31.0, -31.0, -31.0, -31.0, -31.0, -31.0, -15.0, -31.0, -31.0, -4.0, -31.0, -31.0, -31.0, -31.0, -10.0, -31.0, -15.0, -31.0, -31.0, -31.0, -31.0, -31.0, -31.0, -31.0, -31.0, -31.0, -31.0, -31.0, -31.0, -31.0, -31.0, -31.0, -16.0, -31.0, -31.0, -31.0, -29.0, -31.0, -31.0, -31.0, -31.0, -31.0, -31.0, -31.0, -31.0, -31.0, -31.0, -31.0, -31.0, -31.0, -31.0, -31.0, -31.0, -31.0, -21.0, -31.0, -31.0, -31.0, -31.0, -31.0, -31.0, -15.0, -31.0, -31.0, -31.0], 'episode_lengths': [31, 31, 31, 22, 31, 31, 31, 31, 31, 31, 31, 16, 31, 31, 5, 31, 31, 31, 31, 11, 31, 16, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 17, 31, 31, 31, 30, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 22, 31, 31, 31, 31, 31, 31, 16, 31, 31, 31]}, 'sampler_perf': {'mean_raw_obs_processing_ms': 0.23702739680203183, 'mean_inference_ms': 0.7797894447591027, 'mean_action_processing_ms': 0.0791913884741437, 'mean_env_wait_ms': 0.13818596560178933, 'mean_env_render_ms': 0.0}, 'num_faulty_episodes': 0, 'connector_metrics': {'ObsPreprocessorConnector_ms': 0.00444378171648298, 'StateBufferConnector_ms': 0.002921308789934431, 'ViewRequirementAgentConnector_ms': 0.07343871252877372}, 'num_episodes': 70, 'episode_return_max': -4.0, 'episode_return_min': -31.0, 'episode_return_mean': -29.1}\n",
      "[0]\n",
      "Checkpoint saved in directory /tmp/tmppxw67u_r\n",
      "{'episode_reward_max': -1.0, 'episode_reward_min': -31.0, 'episode_reward_mean': -28.45, 'episode_len_mean': 28.62, 'episode_media': {}, 'episodes_this_iter': 73, 'episodes_timesteps_total': 2862, 'policy_reward_min': {}, 'policy_reward_max': {}, 'policy_reward_mean': {}, 'custom_metrics': {}, 'hist_stats': {'episode_reward': [-31.0, -31.0, -31.0, -31.0, -31.0, -31.0, -31.0, -31.0, -31.0, -31.0, -31.0, -31.0, -31.0, -31.0, -31.0, -31.0, -21.0, -31.0, -31.0, -31.0, -31.0, -31.0, -31.0, -15.0, -31.0, -31.0, -31.0, -31.0, -31.0, -31.0, -31.0, -31.0, -31.0, -31.0, -10.0, -31.0, -13.0, -31.0, -31.0, -31.0, -31.0, -31.0, -24.0, -31.0, -31.0, -31.0, -25.0, -31.0, -31.0, -31.0, -31.0, -31.0, -31.0, -31.0, -31.0, -31.0, -31.0, -31.0, -31.0, -31.0, -31.0, -31.0, -4.0, -31.0, -30.0, -31.0, -31.0, -31.0, -26.0, -31.0, -2.0, -31.0, -31.0, -31.0, -31.0, -5.0, -3.0, -31.0, -31.0, -21.0, -31.0, -17.0, -31.0, -31.0, -31.0, -31.0, -31.0, -31.0, -31.0, -25.0, -31.0, -31.0, -31.0, -31.0, -31.0, -31.0, -1.0, -30.0, -31.0, -31.0], 'episode_lengths': [31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 22, 31, 31, 31, 31, 31, 31, 16, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 11, 31, 14, 31, 31, 31, 31, 31, 25, 31, 31, 31, 26, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 5, 31, 31, 31, 31, 31, 27, 31, 3, 31, 31, 31, 31, 6, 4, 31, 31, 22, 31, 18, 31, 31, 31, 31, 31, 31, 31, 26, 31, 31, 31, 31, 31, 31, 2, 31, 31, 31]}, 'sampler_perf': {'mean_raw_obs_processing_ms': 0.2357026269304482, 'mean_inference_ms': 0.7812524841635073, 'mean_action_processing_ms': 0.0794858747879581, 'mean_env_wait_ms': 0.13861067560052345, 'mean_env_render_ms': 0.0}, 'num_faulty_episodes': 0, 'connector_metrics': {'ObsPreprocessorConnector_ms': 0.00433802604675293, 'StateBufferConnector_ms': 0.0028913021087646484, 'ViewRequirementAgentConnector_ms': 0.07172751426696777}, 'num_episodes': 73, 'episode_return_max': -1.0, 'episode_return_min': -31.0, 'episode_return_mean': -28.45}\n",
      "[1]\n",
      "{'episode_reward_max': -1.0, 'episode_reward_min': -31.0, 'episode_reward_mean': -27.41, 'episode_len_mean': 27.63, 'episode_media': {}, 'episodes_this_iter': 73, 'episodes_timesteps_total': 2763, 'policy_reward_min': {}, 'policy_reward_max': {}, 'policy_reward_mean': {}, 'custom_metrics': {}, 'hist_stats': {'episode_reward': [-31.0, -31.0, -5.0, -3.0, -31.0, -31.0, -21.0, -31.0, -17.0, -31.0, -31.0, -31.0, -31.0, -31.0, -31.0, -31.0, -25.0, -31.0, -31.0, -31.0, -31.0, -31.0, -31.0, -1.0, -30.0, -31.0, -31.0, -31.0, -31.0, -31.0, -31.0, -31.0, -31.0, -31.0, -31.0, -31.0, -31.0, -31.0, -31.0, -31.0, -8.0, -22.0, -18.0, -31.0, -31.0, -10.0, -31.0, -2.0, -31.0, -31.0, -31.0, -19.0, -31.0, -3.0, -31.0, -31.0, -31.0, -31.0, -17.0, -31.0, -31.0, -31.0, -6.0, -31.0, -29.0, -31.0, -31.0, -31.0, -19.0, -31.0, -31.0, -31.0, -31.0, -31.0, -27.0, -31.0, -31.0, -31.0, -31.0, -31.0, -31.0, -21.0, -17.0, -31.0, -31.0, -31.0, -31.0, -31.0, -31.0, -31.0, -31.0, -31.0, -31.0, -31.0, -31.0, -31.0, -31.0, -3.0, -31.0, -31.0], 'episode_lengths': [31, 31, 6, 4, 31, 31, 22, 31, 18, 31, 31, 31, 31, 31, 31, 31, 26, 31, 31, 31, 31, 31, 31, 2, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 9, 23, 19, 31, 31, 11, 31, 3, 31, 31, 31, 20, 31, 4, 31, 31, 31, 31, 18, 31, 31, 31, 7, 31, 30, 31, 31, 31, 20, 31, 31, 31, 31, 31, 28, 31, 31, 31, 31, 31, 31, 22, 18, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 4, 31, 31]}, 'sampler_perf': {'mean_raw_obs_processing_ms': 0.23530403593093696, 'mean_inference_ms': 0.7820446089360494, 'mean_action_processing_ms': 0.07946266483763148, 'mean_env_wait_ms': 0.13823588076355084, 'mean_env_render_ms': 0.0}, 'num_faulty_episodes': 0, 'connector_metrics': {'ObsPreprocessorConnector_ms': 0.0043375492095947266, 'StateBufferConnector_ms': 0.0031015872955322266, 'ViewRequirementAgentConnector_ms': 0.07324695587158203}, 'num_episodes': 73, 'episode_return_max': -1.0, 'episode_return_min': -31.0, 'episode_return_mean': -27.41}\n",
      "[2]\n",
      "{'episode_reward_max': 0.0, 'episode_reward_min': -31.0, 'episode_reward_mean': -25.08, 'episode_len_mean': 25.41, 'episode_media': {}, 'episodes_this_iter': 83, 'episodes_timesteps_total': 2541, 'policy_reward_min': {}, 'policy_reward_max': {}, 'policy_reward_mean': {}, 'custom_metrics': {}, 'hist_stats': {'episode_reward': [-31.0, -31.0, -31.0, -31.0, -31.0, -31.0, -31.0, -31.0, -31.0, -31.0, -31.0, -31.0, -31.0, -31.0, -3.0, -31.0, -31.0, -31.0, -31.0, -11.0, -31.0, -31.0, -31.0, -31.0, 0.0, -31.0, -31.0, -31.0, -17.0, -31.0, -31.0, -31.0, -31.0, -31.0, -31.0, -31.0, -31.0, -7.0, -31.0, -13.0, -31.0, -31.0, -31.0, -31.0, -31.0, -11.0, -30.0, -10.0, -31.0, -2.0, -31.0, -24.0, -31.0, -31.0, -31.0, -31.0, -5.0, -1.0, -2.0, -4.0, -31.0, -31.0, -31.0, -31.0, -31.0, -31.0, -31.0, -31.0, -17.0, -30.0, -31.0, -31.0, -27.0, -31.0, -31.0, -31.0, -21.0, -31.0, -29.0, -12.0, -13.0, -11.0, -1.0, -31.0, -31.0, -15.0, 0.0, -22.0, -31.0, -31.0, -31.0, -28.0, -31.0, -10.0, -31.0, -7.0, -15.0, -31.0, -15.0, -18.0], 'episode_lengths': [31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 4, 31, 31, 31, 31, 12, 31, 31, 31, 31, 1, 31, 31, 31, 18, 31, 31, 31, 31, 31, 31, 31, 31, 8, 31, 14, 31, 31, 31, 31, 31, 12, 31, 11, 31, 3, 31, 25, 31, 31, 31, 31, 6, 2, 3, 5, 31, 31, 31, 31, 31, 31, 31, 31, 18, 31, 31, 31, 28, 31, 31, 31, 22, 31, 30, 13, 14, 12, 2, 31, 31, 16, 1, 23, 31, 31, 31, 29, 31, 11, 31, 8, 16, 31, 16, 19]}, 'sampler_perf': {'mean_raw_obs_processing_ms': 0.23437294985305826, 'mean_inference_ms': 0.7930890391351436, 'mean_action_processing_ms': 0.0791578358559796, 'mean_env_wait_ms': 0.13730907032497516, 'mean_env_render_ms': 0.0}, 'num_faulty_episodes': 0, 'connector_metrics': {'ObsPreprocessorConnector_ms': 0.004206657409667969, 'StateBufferConnector_ms': 0.003324747085571289, 'ViewRequirementAgentConnector_ms': 0.07354545593261719}, 'num_episodes': 83, 'episode_return_max': 0.0, 'episode_return_min': -31.0, 'episode_return_mean': -25.08}\n",
      "[3]\n",
      "{'episode_reward_max': 0.0, 'episode_reward_min': -31.0, 'episode_reward_mean': -22.67, 'episode_len_mean': 23.15, 'episode_media': {}, 'episodes_this_iter': 89, 'episodes_timesteps_total': 2315, 'policy_reward_min': {}, 'policy_reward_max': {}, 'policy_reward_mean': {}, 'custom_metrics': {}, 'hist_stats': {'episode_reward': [-31.0, -31.0, -28.0, -31.0, -10.0, -31.0, -7.0, -15.0, -31.0, -15.0, -18.0, -31.0, -31.0, -20.0, -9.0, -10.0, -26.0, -31.0, -31.0, -6.0, -23.0, -31.0, -3.0, -22.0, -18.0, -31.0, -31.0, -31.0, -31.0, 0.0, -31.0, -31.0, -4.0, -11.0, -31.0, -5.0, -30.0, -31.0, -31.0, -31.0, -3.0, -31.0, -31.0, -31.0, -31.0, -31.0, -8.0, -31.0, -31.0, -31.0, -20.0, -29.0, -11.0, -24.0, -31.0, -31.0, -31.0, -16.0, -4.0, -21.0, -31.0, -14.0, -4.0, -21.0, -10.0, -2.0, -15.0, -5.0, -31.0, -31.0, -31.0, -31.0, -2.0, -31.0, -14.0, -11.0, -31.0, -31.0, -6.0, -31.0, -31.0, -20.0, -31.0, -31.0, -9.0, -31.0, -31.0, -20.0, -31.0, -13.0, -21.0, -31.0, -31.0, -20.0, -31.0, -11.0, -21.0, -31.0, -31.0, -31.0], 'episode_lengths': [31, 31, 29, 31, 11, 31, 8, 16, 31, 16, 19, 31, 31, 21, 10, 11, 27, 31, 31, 7, 24, 31, 4, 23, 19, 31, 31, 31, 31, 1, 31, 31, 5, 12, 31, 6, 31, 31, 31, 31, 4, 31, 31, 31, 31, 31, 9, 31, 31, 31, 21, 30, 12, 25, 31, 31, 31, 17, 5, 22, 31, 15, 5, 22, 11, 3, 16, 6, 31, 31, 31, 31, 3, 31, 15, 12, 31, 31, 7, 31, 31, 21, 31, 31, 10, 31, 31, 21, 31, 14, 22, 31, 31, 21, 31, 12, 22, 31, 31, 31]}, 'sampler_perf': {'mean_raw_obs_processing_ms': 0.23411325143614256, 'mean_inference_ms': 0.7896635018972299, 'mean_action_processing_ms': 0.07897971729449911, 'mean_env_wait_ms': 0.13656849011997874, 'mean_env_render_ms': 0.0}, 'num_faulty_episodes': 0, 'connector_metrics': {'ObsPreprocessorConnector_ms': 0.004447221755981445, 'StateBufferConnector_ms': 0.0033311843872070312, 'ViewRequirementAgentConnector_ms': 0.07443594932556152}, 'num_episodes': 89, 'episode_return_max': 0.0, 'episode_return_min': -31.0, 'episode_return_mean': -22.67}\n",
      "[4]\n",
      "{'episode_reward_max': -3.0, 'episode_reward_min': -31.0, 'episode_reward_mean': -21.16, 'episode_len_mean': 21.82, 'episode_media': {}, 'episodes_this_iter': 95, 'episodes_timesteps_total': 2182, 'policy_reward_min': {}, 'policy_reward_max': {}, 'policy_reward_mean': {}, 'custom_metrics': {}, 'hist_stats': {'episode_reward': [-11.0, -21.0, -31.0, -31.0, -31.0, -31.0, -8.0, -6.0, -31.0, -16.0, -31.0, -31.0, -27.0, -10.0, -23.0, -24.0, -31.0, -31.0, -29.0, -12.0, -31.0, -15.0, -19.0, -16.0, -7.0, -15.0, -8.0, -7.0, -31.0, -31.0, -18.0, -31.0, -28.0, -30.0, -26.0, -24.0, -31.0, -31.0, -17.0, -4.0, -19.0, -31.0, -28.0, -6.0, -31.0, -16.0, -31.0, -18.0, -9.0, -16.0, -12.0, -26.0, -12.0, -30.0, -6.0, -27.0, -3.0, -13.0, -31.0, -31.0, -31.0, -20.0, -6.0, -15.0, -31.0, -31.0, -31.0, -18.0, -16.0, -31.0, -18.0, -23.0, -8.0, -9.0, -16.0, -17.0, -3.0, -31.0, -31.0, -11.0, -4.0, -19.0, -8.0, -31.0, -7.0, -8.0, -26.0, -31.0, -16.0, -16.0, -31.0, -31.0, -17.0, -31.0, -31.0, -16.0, -29.0, -26.0, -31.0, -28.0], 'episode_lengths': [12, 22, 31, 31, 31, 31, 9, 7, 31, 17, 31, 31, 28, 11, 24, 25, 31, 31, 30, 13, 31, 16, 20, 17, 8, 16, 9, 8, 31, 31, 19, 31, 29, 31, 27, 25, 31, 31, 18, 5, 20, 31, 29, 7, 31, 17, 31, 19, 10, 17, 13, 27, 13, 31, 7, 28, 4, 14, 31, 31, 31, 21, 7, 16, 31, 31, 31, 19, 17, 31, 19, 24, 9, 10, 17, 18, 4, 31, 31, 12, 5, 20, 9, 31, 8, 9, 27, 31, 17, 17, 31, 31, 18, 31, 31, 17, 30, 27, 31, 29]}, 'sampler_perf': {'mean_raw_obs_processing_ms': 0.23433083117624784, 'mean_inference_ms': 0.7853475000076289, 'mean_action_processing_ms': 0.07894521380805901, 'mean_env_wait_ms': 0.13591337450306515, 'mean_env_render_ms': 0.0}, 'num_faulty_episodes': 0, 'connector_metrics': {'ObsPreprocessorConnector_ms': 0.004339456558227539, 'StateBufferConnector_ms': 0.0033469200134277344, 'ViewRequirementAgentConnector_ms': 0.07412505149841309}, 'num_episodes': 95, 'episode_return_max': -3.0, 'episode_return_min': -31.0, 'episode_return_mean': -21.16}\n",
      "[5]\n",
      "Checkpoint saved in directory /tmp/tmpnwkohnwg\n",
      "{'episode_reward_max': -3.0, 'episode_reward_min': -31.0, 'episode_reward_mean': -19.405940594059405, 'episode_len_mean': 20.15841584158416, 'episode_media': {}, 'episodes_this_iter': 101, 'episodes_timesteps_total': 2036, 'policy_reward_min': {}, 'policy_reward_max': {}, 'policy_reward_mean': {}, 'custom_metrics': {}, 'hist_stats': {'episode_reward': [-13.0, -31.0, -12.0, -12.0, -18.0, -31.0, -17.0, -7.0, -29.0, -16.0, -12.0, -14.0, -31.0, -11.0, -25.0, -22.0, -14.0, -16.0, -8.0, -11.0, -15.0, -31.0, -13.0, -19.0, -6.0, -12.0, -9.0, -31.0, -3.0, -31.0, -16.0, -31.0, -19.0, -13.0, -31.0, -31.0, -7.0, -15.0, -27.0, -13.0, -15.0, -30.0, -12.0, -28.0, -31.0, -27.0, -31.0, -7.0, -23.0, -31.0, -6.0, -22.0, -31.0, -20.0, -10.0, -4.0, -14.0, -18.0, -15.0, -5.0, -8.0, -13.0, -9.0, -8.0, -21.0, -31.0, -22.0, -11.0, -31.0, -11.0, -26.0, -27.0, -12.0, -23.0, -12.0, -31.0, -27.0, -15.0, -31.0, -11.0, -31.0, -31.0, -31.0, -31.0, -20.0, -17.0, -19.0, -31.0, -17.0, -18.0, -8.0, -3.0, -31.0, -31.0, -21.0, -24.0, -16.0, -31.0, -17.0, -21.0, -28.0], 'episode_lengths': [14, 31, 13, 13, 19, 31, 18, 8, 30, 17, 13, 15, 31, 12, 26, 23, 15, 17, 9, 12, 16, 31, 14, 20, 7, 13, 10, 31, 4, 31, 17, 31, 20, 14, 31, 31, 8, 16, 28, 14, 16, 31, 13, 29, 31, 28, 31, 8, 24, 31, 7, 23, 31, 21, 11, 5, 15, 19, 16, 6, 9, 14, 10, 9, 22, 31, 23, 12, 31, 12, 27, 28, 13, 24, 13, 31, 28, 16, 31, 12, 31, 31, 31, 31, 21, 18, 20, 31, 18, 19, 9, 4, 31, 31, 22, 25, 17, 31, 18, 22, 29]}, 'sampler_perf': {'mean_raw_obs_processing_ms': 0.23464698575984275, 'mean_inference_ms': 0.7853004617605231, 'mean_action_processing_ms': 0.07906072831697655, 'mean_env_wait_ms': 0.1358982408662849, 'mean_env_render_ms': 0.0}, 'num_faulty_episodes': 0, 'connector_metrics': {'ObsPreprocessorConnector_ms': 0.004287285379844137, 'StateBufferConnector_ms': 0.003167190174065014, 'ViewRequirementAgentConnector_ms': 0.07221840395785795}, 'num_episodes': 101, 'episode_return_max': -3.0, 'episode_return_min': -31.0, 'episode_return_mean': -19.405940594059405}\n",
      "[6]\n",
      "{'episode_reward_max': 0.0, 'episode_reward_min': -31.0, 'episode_reward_mean': -15.632, 'episode_len_mean': 16.488, 'episode_media': {}, 'episodes_this_iter': 125, 'episodes_timesteps_total': 2061, 'policy_reward_min': {}, 'policy_reward_max': {}, 'policy_reward_mean': {}, 'custom_metrics': {}, 'hist_stats': {'episode_reward': [-22.0, -27.0, -6.0, -6.0, -31.0, -12.0, -18.0, -17.0, -2.0, -5.0, 0.0, -13.0, -10.0, -22.0, 0.0, -14.0, -12.0, -31.0, -31.0, -31.0, -5.0, -22.0, -17.0, -25.0, -5.0, -17.0, -3.0, -11.0, -16.0, -26.0, -1.0, -4.0, -21.0, -16.0, -31.0, -9.0, -13.0, -26.0, -18.0, -8.0, -24.0, -9.0, -31.0, -23.0, -2.0, -24.0, -9.0, -9.0, -31.0, -14.0, -18.0, -16.0, -22.0, -5.0, -14.0, -5.0, -30.0, -31.0, -31.0, -3.0, -2.0, -9.0, -13.0, -30.0, -14.0, -31.0, -11.0, -8.0, -12.0, -5.0, -31.0, -27.0, -19.0, -3.0, -5.0, -31.0, -8.0, -16.0, -18.0, -5.0, -9.0, -31.0, -22.0, -14.0, -8.0, -6.0, -10.0, -31.0, -16.0, -17.0, -19.0, -18.0, -5.0, -2.0, -15.0, -18.0, -19.0, -5.0, -30.0, -9.0, -15.0, -26.0, -4.0, -16.0, -12.0, -31.0, -20.0, -3.0, -16.0, -6.0, -14.0, -31.0, -7.0, -31.0, -8.0, -23.0, -31.0, -6.0, -3.0, -19.0, -25.0, -8.0, -13.0, -7.0, -22.0], 'episode_lengths': [23, 28, 7, 7, 31, 13, 19, 18, 3, 6, 1, 14, 11, 23, 1, 15, 13, 31, 31, 31, 6, 23, 18, 26, 6, 18, 4, 12, 17, 27, 2, 5, 22, 17, 31, 10, 14, 27, 19, 9, 25, 10, 31, 24, 3, 25, 10, 10, 31, 15, 19, 17, 23, 6, 15, 6, 31, 31, 31, 4, 3, 10, 14, 31, 15, 31, 12, 9, 13, 6, 31, 28, 20, 4, 6, 31, 9, 17, 19, 6, 10, 31, 23, 15, 9, 7, 11, 31, 17, 18, 20, 19, 6, 3, 16, 19, 20, 6, 31, 10, 16, 27, 5, 17, 13, 31, 21, 4, 17, 7, 15, 31, 8, 31, 9, 24, 31, 7, 4, 20, 26, 9, 14, 8, 23]}, 'sampler_perf': {'mean_raw_obs_processing_ms': 0.23628529667090387, 'mean_inference_ms': 0.7835229722908585, 'mean_action_processing_ms': 0.07908129641149046, 'mean_env_wait_ms': 0.1353126929815419, 'mean_env_render_ms': 0.0}, 'num_faulty_episodes': 0, 'connector_metrics': {'ObsPreprocessorConnector_ms': 0.004375267028808594, 'StateBufferConnector_ms': 0.004002571105957031, 'ViewRequirementAgentConnector_ms': 0.07514495849609375}, 'num_episodes': 125, 'episode_return_max': 0.0, 'episode_return_min': -31.0, 'episode_return_mean': -15.632}\n",
      "[7]\n",
      "{'episode_reward_max': 0.0, 'episode_reward_min': -31.0, 'episode_reward_mean': -15.53225806451613, 'episode_len_mean': 16.411290322580644, 'episode_media': {}, 'episodes_this_iter': 124, 'episodes_timesteps_total': 2035, 'policy_reward_min': {}, 'policy_reward_max': {}, 'policy_reward_mean': {}, 'custom_metrics': {}, 'hist_stats': {'episode_reward': [-9.0, -31.0, 0.0, -21.0, -17.0, -6.0, -5.0, 0.0, -1.0, -13.0, -4.0, -22.0, -31.0, -17.0, -13.0, -7.0, -30.0, -17.0, -13.0, -6.0, -6.0, -14.0, -26.0, -10.0, -27.0, -14.0, -15.0, -11.0, -17.0, -31.0, -9.0, -31.0, -13.0, -6.0, -23.0, -31.0, -31.0, -14.0, -15.0, -31.0, -20.0, -25.0, -16.0, -12.0, -16.0, -16.0, -8.0, -9.0, -5.0, -12.0, -31.0, -20.0, -10.0, -8.0, -31.0, -8.0, -19.0, -23.0, -25.0, -25.0, -3.0, -31.0, -7.0, -9.0, -1.0, -7.0, -6.0, -15.0, -31.0, -15.0, -31.0, -9.0, -9.0, -16.0, -8.0, -9.0, -22.0, -30.0, -22.0, -19.0, -7.0, -4.0, -2.0, -9.0, -14.0, -21.0, -18.0, -10.0, -2.0, -6.0, -31.0, -26.0, -10.0, -24.0, -5.0, -1.0, -22.0, -11.0, -13.0, -15.0, -25.0, -7.0, -16.0, -26.0, -10.0, -25.0, -10.0, -15.0, -3.0, -22.0, -1.0, -2.0, -16.0, -19.0, -20.0, -26.0, -18.0, -31.0, -31.0, -20.0, -30.0, -14.0, -4.0, -7.0], 'episode_lengths': [10, 31, 1, 22, 18, 7, 6, 1, 2, 14, 5, 23, 31, 18, 14, 8, 31, 18, 14, 7, 7, 15, 27, 11, 28, 15, 16, 12, 18, 31, 10, 31, 14, 7, 24, 31, 31, 15, 16, 31, 21, 26, 17, 13, 17, 17, 9, 10, 6, 13, 31, 21, 11, 9, 31, 9, 20, 24, 26, 26, 4, 31, 8, 10, 2, 8, 7, 16, 31, 16, 31, 10, 10, 17, 9, 10, 23, 31, 23, 20, 8, 5, 3, 10, 15, 22, 19, 11, 3, 7, 31, 27, 11, 25, 6, 2, 23, 12, 14, 16, 26, 8, 17, 27, 11, 26, 11, 16, 4, 23, 2, 3, 17, 20, 21, 27, 19, 31, 31, 21, 31, 15, 5, 8]}, 'sampler_perf': {'mean_raw_obs_processing_ms': 0.2388432003533025, 'mean_inference_ms': 0.7900637475212766, 'mean_action_processing_ms': 0.07969730488742964, 'mean_env_wait_ms': 0.13606250690300234, 'mean_env_render_ms': 0.0}, 'num_faulty_episodes': 0, 'connector_metrics': {'ObsPreprocessorConnector_ms': 0.004791059801655431, 'StateBufferConnector_ms': 0.0039060269632647114, 'ViewRequirementAgentConnector_ms': 0.07940088548967915}, 'num_episodes': 124, 'episode_return_max': 0.0, 'episode_return_min': -31.0, 'episode_return_mean': -15.53225806451613}\n",
      "[8]\n",
      "{'episode_reward_max': 0.0, 'episode_reward_min': -31.0, 'episode_reward_mean': -12.238709677419354, 'episode_len_mean': 13.2, 'episode_media': {}, 'episodes_this_iter': 155, 'episodes_timesteps_total': 2046, 'policy_reward_min': {}, 'policy_reward_max': {}, 'policy_reward_mean': {}, 'custom_metrics': {}, 'hist_stats': {'episode_reward': [-17.0, -17.0, -19.0, -12.0, 0.0, -4.0, -3.0, -21.0, -22.0, -25.0, -11.0, -26.0, -8.0, -9.0, -10.0, -16.0, -3.0, -7.0, -18.0, -19.0, -2.0, -31.0, -28.0, -6.0, -17.0, -4.0, -13.0, -3.0, -31.0, -23.0, -14.0, -11.0, -10.0, -31.0, -6.0, -20.0, 0.0, -15.0, -15.0, -4.0, -8.0, -1.0, -10.0, -17.0, -19.0, -25.0, -7.0, 0.0, -7.0, -10.0, -7.0, -17.0, -8.0, -11.0, -15.0, -4.0, -31.0, -10.0, -13.0, -9.0, -27.0, -21.0, -11.0, 0.0, -11.0, -16.0, -7.0, -10.0, -12.0, -14.0, -28.0, -12.0, -18.0, -22.0, -9.0, -4.0, -11.0, -9.0, -3.0, -9.0, -8.0, -15.0, -5.0, -6.0, -14.0, -8.0, -13.0, -22.0, -4.0, -11.0, -11.0, -13.0, -8.0, -15.0, -14.0, -11.0, -13.0, -10.0, -10.0, -4.0, -11.0, -16.0, -15.0, -31.0, -10.0, -12.0, -4.0, -6.0, -22.0, -20.0, -13.0, -25.0, -13.0, -5.0, -14.0, -22.0, -1.0, -24.0, -10.0, -25.0, -25.0, -3.0, -5.0, -31.0, -25.0, -7.0, -2.0, -6.0, -13.0, -10.0, -23.0, -1.0, -15.0, -1.0, -9.0, 0.0, -1.0, -12.0, -7.0, -3.0, -2.0, -11.0, -9.0, -9.0, -14.0, -9.0, -5.0, -8.0, -7.0, -6.0, -15.0, -25.0, -1.0, -20.0, -9.0], 'episode_lengths': [18, 18, 20, 13, 1, 5, 4, 22, 23, 26, 12, 27, 9, 10, 11, 17, 4, 8, 19, 20, 3, 31, 29, 7, 18, 5, 14, 4, 31, 24, 15, 12, 11, 31, 7, 21, 1, 16, 16, 5, 9, 2, 11, 18, 20, 26, 8, 1, 8, 11, 8, 18, 9, 12, 16, 5, 31, 11, 14, 10, 28, 22, 12, 1, 12, 17, 8, 11, 13, 15, 29, 13, 19, 23, 10, 5, 12, 10, 4, 10, 9, 16, 6, 7, 15, 9, 14, 23, 5, 12, 12, 14, 9, 16, 15, 12, 14, 11, 11, 5, 12, 17, 16, 31, 11, 13, 5, 7, 23, 21, 14, 26, 14, 6, 15, 23, 2, 25, 11, 26, 26, 4, 6, 31, 26, 8, 3, 7, 14, 11, 24, 2, 16, 2, 10, 1, 2, 13, 8, 4, 3, 12, 10, 10, 15, 10, 6, 9, 8, 7, 16, 26, 2, 21, 10]}, 'sampler_perf': {'mean_raw_obs_processing_ms': 0.24065451721888748, 'mean_inference_ms': 0.7893155837721392, 'mean_action_processing_ms': 0.07968115146536946, 'mean_env_wait_ms': 0.13543131016094784, 'mean_env_render_ms': 0.0}, 'num_faulty_episodes': 0, 'connector_metrics': {'ObsPreprocessorConnector_ms': 0.004406282978673135, 'StateBufferConnector_ms': 0.004136331619754914, 'ViewRequirementAgentConnector_ms': 0.07663111532888105}, 'num_episodes': 155, 'episode_return_max': 0.0, 'episode_return_min': -31.0, 'episode_return_mean': -12.238709677419354}\n",
      "[9]\n",
      "{'episode_reward_max': 0.0, 'episode_reward_min': -31.0, 'episode_reward_mean': -11.962264150943396, 'episode_len_mean': 12.89308176100629, 'episode_media': {}, 'episodes_this_iter': 159, 'episodes_timesteps_total': 2050, 'policy_reward_min': {}, 'policy_reward_max': {}, 'policy_reward_mean': {}, 'custom_metrics': {}, 'hist_stats': {'episode_reward': [-25.0, -5.0, -3.0, -31.0, -10.0, -3.0, -10.0, -10.0, -19.0, -6.0, -3.0, -9.0, -2.0, -4.0, -16.0, -7.0, -9.0, -9.0, -14.0, -14.0, -21.0, -31.0, -1.0, -5.0, -18.0, -25.0, -9.0, -13.0, -4.0, -5.0, -7.0, -7.0, -12.0, -11.0, -6.0, -16.0, -2.0, -12.0, -9.0, -16.0, -8.0, -31.0, -11.0, -5.0, -6.0, -8.0, -11.0, -20.0, -12.0, -12.0, -4.0, -8.0, -19.0, -4.0, -9.0, -10.0, -4.0, -19.0, -13.0, -11.0, -11.0, -10.0, -11.0, -13.0, 0.0, -11.0, -11.0, -16.0, -22.0, -15.0, -12.0, -16.0, -8.0, -31.0, -18.0, -11.0, -6.0, -11.0, -10.0, 0.0, -5.0, -16.0, -11.0, -19.0, -31.0, -3.0, -10.0, -10.0, -4.0, 0.0, -1.0, -2.0, -26.0, -4.0, -6.0, -9.0, -8.0, -31.0, -1.0, -5.0, -30.0, -19.0, -14.0, -31.0, -13.0, -13.0, -9.0, -20.0, -19.0, -31.0, -7.0, -12.0, -13.0, -5.0, -3.0, -3.0, -7.0, -12.0, -11.0, -13.0, 0.0, -2.0, -31.0, -4.0, -31.0, -16.0, -20.0, -26.0, -13.0, -15.0, -3.0, -14.0, -5.0, -7.0, -14.0, -12.0, -15.0, -18.0, -17.0, -19.0, -31.0, -13.0, -10.0, -10.0, -22.0, -15.0, 0.0, -5.0, -8.0, -15.0, -5.0, -9.0, -4.0, -12.0, -11.0, -24.0, -8.0, -15.0, -9.0], 'episode_lengths': [26, 6, 4, 31, 11, 4, 11, 11, 20, 7, 4, 10, 3, 5, 17, 8, 10, 10, 15, 15, 22, 31, 2, 6, 19, 26, 10, 14, 5, 6, 8, 8, 13, 12, 7, 17, 3, 13, 10, 17, 9, 31, 12, 6, 7, 9, 12, 21, 13, 13, 5, 9, 20, 5, 10, 11, 5, 20, 14, 12, 12, 11, 12, 14, 1, 12, 12, 17, 23, 16, 13, 17, 9, 31, 19, 12, 7, 12, 11, 1, 6, 17, 12, 20, 31, 4, 11, 11, 5, 1, 2, 3, 27, 5, 7, 10, 9, 31, 2, 6, 31, 20, 15, 31, 14, 14, 10, 21, 20, 31, 8, 13, 14, 6, 4, 4, 8, 13, 12, 14, 1, 3, 31, 5, 31, 17, 21, 27, 14, 16, 4, 15, 6, 8, 15, 13, 16, 19, 18, 20, 31, 14, 11, 11, 23, 16, 1, 6, 9, 16, 6, 10, 5, 13, 12, 25, 9, 16, 10]}, 'sampler_perf': {'mean_raw_obs_processing_ms': 0.24216095391303596, 'mean_inference_ms': 0.7872277016465942, 'mean_action_processing_ms': 0.07953765077043605, 'mean_env_wait_ms': 0.13475127277088958, 'mean_env_render_ms': 0.0}, 'num_faulty_episodes': 0, 'connector_metrics': {'ObsPreprocessorConnector_ms': 0.004210861973792502, 'StateBufferConnector_ms': 0.004176523700450202, 'ViewRequirementAgentConnector_ms': 0.07429767704609805}, 'num_episodes': 159, 'episode_return_max': 0.0, 'episode_return_min': -31.0, 'episode_return_mean': -11.962264150943396}\n",
      "[10]\n",
      "Checkpoint saved in directory /tmp/tmpyd1izp3y\n",
      "{'episode_reward_max': 0.0, 'episode_reward_min': -31.0, 'episode_reward_mean': -10.988372093023257, 'episode_len_mean': 11.924418604651162, 'episode_media': {}, 'episodes_this_iter': 172, 'episodes_timesteps_total': 2051, 'policy_reward_min': {}, 'policy_reward_max': {}, 'policy_reward_mean': {}, 'custom_metrics': {}, 'hist_stats': {'episode_reward': [-31.0, -3.0, -1.0, -31.0, -7.0, -9.0, -21.0, -13.0, -8.0, -9.0, -5.0, -1.0, -5.0, -4.0, -8.0, -8.0, -31.0, -1.0, -13.0, -11.0, -10.0, -4.0, -31.0, -8.0, -5.0, -14.0, -19.0, -5.0, -17.0, -5.0, -21.0, -17.0, -5.0, -11.0, -5.0, -11.0, -2.0, -11.0, -7.0, -6.0, -6.0, -24.0, -16.0, -17.0, 0.0, -3.0, -6.0, -31.0, -17.0, -1.0, -11.0, -17.0, -30.0, -17.0, -5.0, 0.0, -4.0, -18.0, -6.0, -13.0, -20.0, -4.0, -11.0, -9.0, -14.0, -7.0, -5.0, -18.0, -6.0, -9.0, -2.0, -5.0, -5.0, -15.0, 0.0, -10.0, -11.0, -19.0, -21.0, -1.0, -4.0, -6.0, -5.0, -9.0, 0.0, -31.0, -8.0, -13.0, -7.0, -13.0, -9.0, -12.0, -19.0, -11.0, -5.0, -10.0, -16.0, -4.0, -12.0, -2.0, -3.0, -2.0, -3.0, -11.0, -16.0, 0.0, -9.0, -5.0, -13.0, -4.0, -8.0, -27.0, -19.0, -31.0, 0.0, -11.0, -17.0, -8.0, -7.0, -2.0, -23.0, -11.0, -7.0, -17.0, -24.0, -3.0, -17.0, -9.0, -23.0, -1.0, -4.0, -8.0, -19.0, -13.0, -20.0, -19.0, -31.0, -20.0, -23.0, -16.0, -13.0, -6.0, -8.0, -7.0, -19.0, -1.0, -1.0, -13.0, -1.0, -31.0, -3.0, -7.0, -2.0, -8.0, -1.0, -4.0, -30.0, -11.0, -31.0, -9.0, -16.0, -2.0, -14.0, -4.0, -15.0, -6.0, -8.0, -7.0, -1.0, -31.0, -5.0, -7.0], 'episode_lengths': [31, 4, 2, 31, 8, 10, 22, 14, 9, 10, 6, 2, 6, 5, 9, 9, 31, 2, 14, 12, 11, 5, 31, 9, 6, 15, 20, 6, 18, 6, 22, 18, 6, 12, 6, 12, 3, 12, 8, 7, 7, 25, 17, 18, 1, 4, 7, 31, 18, 2, 12, 18, 31, 18, 6, 1, 5, 19, 7, 14, 21, 5, 12, 10, 15, 8, 6, 19, 7, 10, 3, 6, 6, 16, 1, 11, 12, 20, 22, 2, 5, 7, 6, 10, 1, 31, 9, 14, 8, 14, 10, 13, 20, 12, 6, 11, 17, 5, 13, 3, 4, 3, 4, 12, 17, 1, 10, 6, 14, 5, 9, 28, 20, 31, 1, 12, 18, 9, 8, 3, 24, 12, 8, 18, 25, 4, 18, 10, 24, 2, 5, 9, 20, 14, 21, 20, 31, 21, 24, 17, 14, 7, 9, 8, 20, 2, 2, 14, 2, 31, 4, 8, 3, 9, 2, 5, 31, 12, 31, 10, 17, 3, 15, 5, 16, 7, 9, 8, 2, 31, 6, 8]}, 'sampler_perf': {'mean_raw_obs_processing_ms': 0.24426628052569893, 'mean_inference_ms': 0.7864190218926602, 'mean_action_processing_ms': 0.07942089854242475, 'mean_env_wait_ms': 0.13422988315446774, 'mean_env_render_ms': 0.0}, 'num_faulty_episodes': 0, 'connector_metrics': {'ObsPreprocessorConnector_ms': 0.004340188447819199, 'StateBufferConnector_ms': 0.004702667857325355, 'ViewRequirementAgentConnector_ms': 0.07587116818095363}, 'num_episodes': 172, 'episode_return_max': 0.0, 'episode_return_min': -31.0, 'episode_return_mean': -10.988372093023257}\n",
      "[11]\n",
      "{'episode_reward_max': 0.0, 'episode_reward_min': -31.0, 'episode_reward_mean': -10.405555555555555, 'episode_len_mean': 11.383333333333333, 'episode_media': {}, 'episodes_this_iter': 180, 'episodes_timesteps_total': 2049, 'policy_reward_min': {}, 'policy_reward_max': {}, 'policy_reward_mean': {}, 'custom_metrics': {}, 'hist_stats': {'episode_reward': [-16.0, -5.0, -3.0, -18.0, -12.0, -8.0, -8.0, -3.0, -15.0, -25.0, -10.0, -19.0, -10.0, -11.0, -14.0, -12.0, -12.0, 0.0, -7.0, -5.0, 0.0, -21.0, -14.0, 0.0, -23.0, -12.0, -2.0, -10.0, -13.0, -17.0, -11.0, -17.0, 0.0, -7.0, -17.0, -19.0, -14.0, -9.0, -20.0, -13.0, -31.0, -10.0, -14.0, -15.0, -15.0, -5.0, -16.0, -13.0, -11.0, -11.0, -5.0, -31.0, -8.0, -7.0, -24.0, -14.0, -5.0, -9.0, -5.0, -10.0, -18.0, -17.0, -13.0, -6.0, 0.0, -9.0, -6.0, -14.0, -15.0, -3.0, -4.0, -4.0, -18.0, -12.0, -7.0, -4.0, -2.0, -3.0, -5.0, -9.0, -3.0, -7.0, -12.0, -10.0, 0.0, -9.0, -6.0, -8.0, -2.0, -3.0, -8.0, -12.0, -1.0, -1.0, -19.0, -7.0, -19.0, -8.0, -8.0, -9.0, -13.0, -13.0, -15.0, -18.0, 0.0, -7.0, -10.0, -16.0, -13.0, -5.0, -7.0, -1.0, -2.0, -12.0, -11.0, -8.0, -7.0, -12.0, -6.0, -12.0, -24.0, -16.0, -3.0, -18.0, -3.0, -2.0, -6.0, 0.0, -3.0, -8.0, -18.0, -17.0, -11.0, -9.0, -12.0, -8.0, -7.0, -25.0, -6.0, -25.0, -13.0, -23.0, -13.0, -7.0, -1.0, -6.0, -9.0, -6.0, -11.0, -24.0, -31.0, -11.0, -4.0, -11.0, -18.0, -25.0, -15.0, -13.0, -9.0, -4.0, -1.0, -7.0, -8.0, -4.0, -9.0, -7.0, -6.0, -2.0, -2.0, -10.0, -4.0, -4.0, -5.0, -2.0, -12.0, -22.0, -13.0, -25.0, -31.0, -4.0], 'episode_lengths': [17, 6, 4, 19, 13, 9, 9, 4, 16, 26, 11, 20, 11, 12, 15, 13, 13, 1, 8, 6, 1, 22, 15, 1, 24, 13, 3, 11, 14, 18, 12, 18, 1, 8, 18, 20, 15, 10, 21, 14, 31, 11, 15, 16, 16, 6, 17, 14, 12, 12, 6, 31, 9, 8, 25, 15, 6, 10, 6, 11, 19, 18, 14, 7, 1, 10, 7, 15, 16, 4, 5, 5, 19, 13, 8, 5, 3, 4, 6, 10, 4, 8, 13, 11, 1, 10, 7, 9, 3, 4, 9, 13, 2, 2, 20, 8, 20, 9, 9, 10, 14, 14, 16, 19, 1, 8, 11, 17, 14, 6, 8, 2, 3, 13, 12, 9, 8, 13, 7, 13, 25, 17, 4, 19, 4, 3, 7, 1, 4, 9, 19, 18, 12, 10, 13, 9, 8, 26, 7, 26, 14, 24, 14, 8, 2, 7, 10, 7, 12, 25, 31, 12, 5, 12, 19, 26, 16, 14, 10, 5, 2, 8, 9, 5, 10, 8, 7, 3, 3, 11, 5, 5, 6, 3, 13, 23, 14, 26, 31, 5]}, 'sampler_perf': {'mean_raw_obs_processing_ms': 0.2462017345876201, 'mean_inference_ms': 0.7869966802462727, 'mean_action_processing_ms': 0.07945065207324675, 'mean_env_wait_ms': 0.13395711513752112, 'mean_env_render_ms': 0.0}, 'num_faulty_episodes': 0, 'connector_metrics': {'ObsPreprocessorConnector_ms': 0.0046384334564208984, 'StateBufferConnector_ms': 0.004689958360460069, 'ViewRequirementAgentConnector_ms': 0.07818566428290473}, 'num_episodes': 180, 'episode_return_max': 0.0, 'episode_return_min': -31.0, 'episode_return_mean': -10.405555555555555}\n",
      "[12]\n",
      "{'episode_reward_max': 0.0, 'episode_reward_min': -31.0, 'episode_reward_mean': -10.480446927374302, 'episode_len_mean': 11.458100558659218, 'episode_media': {}, 'episodes_this_iter': 179, 'episodes_timesteps_total': 2051, 'policy_reward_min': {}, 'policy_reward_max': {}, 'policy_reward_mean': {}, 'custom_metrics': {}, 'hist_stats': {'episode_reward': [-16.0, -15.0, -10.0, -14.0, -9.0, -4.0, -9.0, -17.0, -10.0, -5.0, -9.0, -10.0, 0.0, -22.0, -9.0, -10.0, -7.0, -9.0, -25.0, -7.0, -2.0, -12.0, -9.0, -3.0, -9.0, -25.0, -6.0, 0.0, -7.0, -14.0, -9.0, -8.0, -6.0, 0.0, 0.0, -19.0, -6.0, 0.0, -19.0, -6.0, -14.0, -5.0, -7.0, -15.0, -3.0, -7.0, -7.0, -4.0, 0.0, 0.0, -12.0, 0.0, -4.0, -5.0, -10.0, -23.0, -1.0, -6.0, -9.0, -14.0, -11.0, -10.0, -4.0, -13.0, -9.0, -17.0, -15.0, -12.0, -6.0, -6.0, -7.0, -6.0, -6.0, -12.0, -10.0, -22.0, -16.0, -4.0, -10.0, -11.0, -5.0, -4.0, -13.0, -12.0, -1.0, -8.0, -14.0, -7.0, -11.0, -16.0, -31.0, -5.0, -23.0, -31.0, -28.0, -7.0, -6.0, -17.0, -12.0, -5.0, -1.0, -8.0, -8.0, -5.0, -18.0, -23.0, -12.0, -15.0, -31.0, -6.0, -4.0, -23.0, -8.0, -7.0, -15.0, -8.0, -15.0, -13.0, -16.0, -8.0, -17.0, -7.0, -12.0, -5.0, -5.0, -18.0, -27.0, -9.0, -7.0, -2.0, -8.0, -6.0, -9.0, -31.0, -9.0, -15.0, -6.0, -6.0, -6.0, -6.0, -5.0, -15.0, -7.0, -7.0, -12.0, -21.0, -29.0, -1.0, -14.0, -2.0, -17.0, -24.0, -24.0, -5.0, -13.0, -2.0, -7.0, -17.0, -1.0, -6.0, -5.0, -18.0, -26.0, -9.0, -8.0, -15.0, -20.0, -4.0, -1.0, -6.0, -5.0, -13.0, -9.0, -7.0, -13.0, -8.0, -17.0, -12.0, -10.0], 'episode_lengths': [17, 16, 11, 15, 10, 5, 10, 18, 11, 6, 10, 11, 1, 23, 10, 11, 8, 10, 26, 8, 3, 13, 10, 4, 10, 26, 7, 1, 8, 15, 10, 9, 7, 1, 1, 20, 7, 1, 20, 7, 15, 6, 8, 16, 4, 8, 8, 5, 1, 1, 13, 1, 5, 6, 11, 24, 2, 7, 10, 15, 12, 11, 5, 14, 10, 18, 16, 13, 7, 7, 8, 7, 7, 13, 11, 23, 17, 5, 11, 12, 6, 5, 14, 13, 2, 9, 15, 8, 12, 17, 31, 6, 24, 31, 29, 8, 7, 18, 13, 6, 2, 9, 9, 6, 19, 24, 13, 16, 31, 7, 5, 24, 9, 8, 16, 9, 16, 14, 17, 9, 18, 8, 13, 6, 6, 19, 28, 10, 8, 3, 9, 7, 10, 31, 10, 16, 7, 7, 7, 7, 6, 16, 8, 8, 13, 22, 30, 2, 15, 3, 18, 25, 25, 6, 14, 3, 8, 18, 2, 7, 6, 19, 27, 10, 9, 16, 21, 5, 2, 7, 6, 14, 10, 8, 14, 9, 18, 13, 11]}, 'sampler_perf': {'mean_raw_obs_processing_ms': 0.2485457269247304, 'mean_inference_ms': 0.7881593044005479, 'mean_action_processing_ms': 0.0795552899606085, 'mean_env_wait_ms': 0.1338054740266365, 'mean_env_render_ms': 0.0}, 'num_faulty_episodes': 0, 'connector_metrics': {'ObsPreprocessorConnector_ms': 0.00446894981341655, 'StateBufferConnector_ms': 0.004570577397692803, 'ViewRequirementAgentConnector_ms': 0.07989193474114274}, 'num_episodes': 179, 'episode_return_max': 0.0, 'episode_return_min': -31.0, 'episode_return_mean': -10.480446927374302}\n",
      "[13]\n",
      "{'episode_reward_max': 0.0, 'episode_reward_min': -31.0, 'episode_reward_mean': -9.188118811881187, 'episode_len_mean': 10.168316831683168, 'episode_media': {}, 'episodes_this_iter': 202, 'episodes_timesteps_total': 2054, 'policy_reward_min': {}, 'policy_reward_max': {}, 'policy_reward_mean': {}, 'custom_metrics': {}, 'hist_stats': {'episode_reward': [-9.0, -16.0, -8.0, -14.0, -8.0, -7.0, -2.0, -9.0, -12.0, -7.0, -2.0, -10.0, -12.0, -15.0, -10.0, -19.0, -10.0, -6.0, -4.0, -6.0, -7.0, -13.0, -18.0, -4.0, -5.0, -9.0, -15.0, -9.0, -2.0, -5.0, -1.0, -4.0, -4.0, -8.0, -5.0, -15.0, -6.0, -10.0, -7.0, -7.0, -19.0, -10.0, -6.0, -11.0, -3.0, -3.0, -9.0, -5.0, -9.0, -16.0, -4.0, -5.0, -9.0, -7.0, -11.0, -7.0, -4.0, -7.0, -16.0, -16.0, -10.0, -6.0, -3.0, -5.0, -14.0, -7.0, -31.0, -5.0, -9.0, -2.0, -7.0, -31.0, -9.0, -4.0, -5.0, -22.0, -10.0, -5.0, -12.0, -14.0, -16.0, -3.0, -10.0, -5.0, -7.0, -9.0, -1.0, -2.0, -21.0, -4.0, -8.0, -8.0, -3.0, -5.0, -8.0, -3.0, -21.0, -5.0, -17.0, -8.0, -14.0, -31.0, -11.0, -10.0, -1.0, -9.0, -2.0, -5.0, -4.0, -17.0, -13.0, -12.0, -15.0, -9.0, -25.0, -2.0, -6.0, -1.0, -13.0, -26.0, -23.0, -11.0, -15.0, -24.0, -7.0, -13.0, -14.0, -5.0, -21.0, -1.0, -13.0, -3.0, -8.0, -13.0, -4.0, -7.0, -9.0, -3.0, -7.0, -3.0, -9.0, -12.0, -5.0, -10.0, -20.0, -5.0, -5.0, -2.0, -6.0, -3.0, 0.0, -16.0, -10.0, -7.0, -6.0, -9.0, -9.0, -10.0, -3.0, 0.0, -4.0, -13.0, -5.0, -12.0, -6.0, -7.0, -2.0, -9.0, -2.0, -18.0, -24.0, -9.0, -20.0, -3.0, -4.0, -18.0, -4.0, -3.0, -4.0, -7.0, -3.0, -9.0, 0.0, -31.0, -8.0, -11.0, -7.0, -16.0, -9.0, -11.0, -4.0, -11.0, -3.0, -13.0, -8.0, -14.0, -16.0, -3.0, -6.0, -9.0, -12.0, -9.0], 'episode_lengths': [10, 17, 9, 15, 9, 8, 3, 10, 13, 8, 3, 11, 13, 16, 11, 20, 11, 7, 5, 7, 8, 14, 19, 5, 6, 10, 16, 10, 3, 6, 2, 5, 5, 9, 6, 16, 7, 11, 8, 8, 20, 11, 7, 12, 4, 4, 10, 6, 10, 17, 5, 6, 10, 8, 12, 8, 5, 8, 17, 17, 11, 7, 4, 6, 15, 8, 31, 6, 10, 3, 8, 31, 10, 5, 6, 23, 11, 6, 13, 15, 17, 4, 11, 6, 8, 10, 2, 3, 22, 5, 9, 9, 4, 6, 9, 4, 22, 6, 18, 9, 15, 31, 12, 11, 2, 10, 3, 6, 5, 18, 14, 13, 16, 10, 26, 3, 7, 2, 14, 27, 24, 12, 16, 25, 8, 14, 15, 6, 22, 2, 14, 4, 9, 14, 5, 8, 10, 4, 8, 4, 10, 13, 6, 11, 21, 6, 6, 3, 7, 4, 1, 17, 11, 8, 7, 10, 10, 11, 4, 1, 5, 14, 6, 13, 7, 8, 3, 10, 3, 19, 25, 10, 21, 4, 5, 19, 5, 4, 5, 8, 4, 10, 1, 31, 9, 12, 8, 17, 10, 12, 5, 12, 4, 14, 9, 15, 17, 4, 7, 10, 13, 10]}, 'sampler_perf': {'mean_raw_obs_processing_ms': 0.2502472790799133, 'mean_inference_ms': 0.7862217906458817, 'mean_action_processing_ms': 0.0794473319449294, 'mean_env_wait_ms': 0.13312493011334545, 'mean_env_render_ms': 0.0}, 'num_faulty_episodes': 0, 'connector_metrics': {'ObsPreprocessorConnector_ms': 0.004225438184077197, 'StateBufferConnector_ms': 0.0046058456496437, 'ViewRequirementAgentConnector_ms': 0.07485458166292398}, 'num_episodes': 202, 'episode_return_max': 0.0, 'episode_return_min': -31.0, 'episode_return_mean': -9.188118811881187}\n",
      "[14]\n",
      "{'episode_reward_max': 0.0, 'episode_reward_min': -31.0, 'episode_reward_mean': -9.433673469387756, 'episode_len_mean': 10.418367346938776, 'episode_media': {}, 'episodes_this_iter': 196, 'episodes_timesteps_total': 2042, 'policy_reward_min': {}, 'policy_reward_max': {}, 'policy_reward_mean': {}, 'custom_metrics': {}, 'hist_stats': {'episode_reward': [-31.0, -21.0, -13.0, 0.0, -19.0, -3.0, -11.0, -9.0, -8.0, -7.0, -7.0, -8.0, -8.0, -14.0, -5.0, -5.0, -7.0, -6.0, -21.0, -10.0, -5.0, -4.0, -31.0, -15.0, -12.0, -3.0, -5.0, -23.0, -12.0, -8.0, -4.0, -1.0, -18.0, -8.0, -14.0, -6.0, -8.0, -9.0, -4.0, -22.0, -3.0, -4.0, -7.0, -16.0, -9.0, -3.0, -10.0, 0.0, -4.0, -31.0, -17.0, -10.0, 0.0, -18.0, -7.0, -9.0, -4.0, -13.0, -9.0, -7.0, -12.0, -10.0, -4.0, -11.0, -17.0, -9.0, -1.0, -11.0, -9.0, -10.0, -11.0, -19.0, -7.0, -12.0, -10.0, 0.0, -16.0, -10.0, -9.0, -19.0, -9.0, -5.0, -11.0, -2.0, -10.0, -6.0, -3.0, -11.0, -11.0, -2.0, -12.0, -5.0, -10.0, -4.0, -12.0, -4.0, -13.0, -7.0, -19.0, -15.0, -3.0, -12.0, -9.0, -15.0, -12.0, -19.0, -7.0, -1.0, -18.0, -8.0, -1.0, -5.0, -4.0, -10.0, -9.0, -11.0, -8.0, -5.0, -11.0, 0.0, -12.0, -6.0, -4.0, -4.0, -16.0, -4.0, -9.0, -11.0, -18.0, -18.0, -6.0, -5.0, -5.0, -2.0, -7.0, -17.0, -3.0, -3.0, -3.0, -5.0, -26.0, -5.0, -6.0, -13.0, -14.0, 0.0, -3.0, -8.0, -9.0, -14.0, -24.0, -10.0, -12.0, -2.0, -3.0, -21.0, -8.0, -16.0, -12.0, -12.0, -11.0, -8.0, -7.0, -7.0, -4.0, -21.0, -2.0, -8.0, -9.0, -16.0, -4.0, -12.0, 0.0, -12.0, -4.0, -3.0, -4.0, -9.0, -11.0, -12.0, -6.0, -8.0, -4.0, 0.0, -5.0, -10.0, -15.0, -11.0, -10.0, -8.0, -29.0, -3.0, -8.0, -9.0, -14.0, -17.0], 'episode_lengths': [31, 22, 14, 1, 20, 4, 12, 10, 9, 8, 8, 9, 9, 15, 6, 6, 8, 7, 22, 11, 6, 5, 31, 16, 13, 4, 6, 24, 13, 9, 5, 2, 19, 9, 15, 7, 9, 10, 5, 23, 4, 5, 8, 17, 10, 4, 11, 1, 5, 31, 18, 11, 1, 19, 8, 10, 5, 14, 10, 8, 13, 11, 5, 12, 18, 10, 2, 12, 10, 11, 12, 20, 8, 13, 11, 1, 17, 11, 10, 20, 10, 6, 12, 3, 11, 7, 4, 12, 12, 3, 13, 6, 11, 5, 13, 5, 14, 8, 20, 16, 4, 13, 10, 16, 13, 20, 8, 2, 19, 9, 2, 6, 5, 11, 10, 12, 9, 6, 12, 1, 13, 7, 5, 5, 17, 5, 10, 12, 19, 19, 7, 6, 6, 3, 8, 18, 4, 4, 4, 6, 27, 6, 7, 14, 15, 1, 4, 9, 10, 15, 25, 11, 13, 3, 4, 22, 9, 17, 13, 13, 12, 9, 8, 8, 5, 22, 3, 9, 10, 17, 5, 13, 1, 13, 5, 4, 5, 10, 12, 13, 7, 9, 5, 1, 6, 11, 16, 12, 11, 9, 30, 4, 9, 10, 15, 18]}, 'sampler_perf': {'mean_raw_obs_processing_ms': 0.25160074226733053, 'mean_inference_ms': 0.7852271193314843, 'mean_action_processing_ms': 0.07940008779328613, 'mean_env_wait_ms': 0.13277953317645652, 'mean_env_render_ms': 0.0}, 'num_faulty_episodes': 0, 'connector_metrics': {'ObsPreprocessorConnector_ms': 0.00436658761939224, 'StateBufferConnector_ms': 0.00452143805367606, 'ViewRequirementAgentConnector_ms': 0.0754942699354522}, 'num_episodes': 196, 'episode_return_max': 0.0, 'episode_return_min': -31.0, 'episode_return_mean': -9.433673469387756}\n",
      "[15]\n",
      "Checkpoint saved in directory /tmp/tmp1teq9kd5\n",
      "{'episode_reward_max': 0.0, 'episode_reward_min': -31.0, 'episode_reward_mean': -9.034146341463414, 'episode_len_mean': 10.019512195121951, 'episode_media': {}, 'episodes_this_iter': 205, 'episodes_timesteps_total': 2054, 'policy_reward_min': {}, 'policy_reward_max': {}, 'policy_reward_mean': {}, 'custom_metrics': {}, 'hist_stats': {'episode_reward': [-30.0, -8.0, -3.0, -17.0, -11.0, -5.0, -21.0, -8.0, -4.0, -16.0, -6.0, -8.0, -16.0, -7.0, -2.0, -6.0, -7.0, -9.0, -1.0, -7.0, -9.0, -7.0, -4.0, -11.0, 0.0, -6.0, -16.0, -7.0, -12.0, -11.0, -20.0, -7.0, -9.0, -16.0, -2.0, -15.0, -9.0, -3.0, -9.0, -6.0, -1.0, -10.0, -19.0, -6.0, -3.0, -13.0, -8.0, -14.0, -2.0, -1.0, -11.0, -7.0, -9.0, -10.0, -4.0, -4.0, -12.0, -20.0, -10.0, -3.0, -11.0, -19.0, -31.0, -2.0, -7.0, -8.0, -6.0, 0.0, -6.0, -1.0, -13.0, -9.0, -5.0, -3.0, -6.0, -6.0, -2.0, -10.0, -8.0, -20.0, -6.0, -13.0, -6.0, -25.0, -5.0, -9.0, -8.0, 0.0, -15.0, -14.0, -14.0, -3.0, -4.0, -8.0, -12.0, -10.0, -6.0, -3.0, -5.0, -13.0, -13.0, -6.0, -17.0, -9.0, -1.0, -7.0, -14.0, -8.0, -18.0, -31.0, -5.0, -6.0, -4.0, -7.0, -17.0, -4.0, -15.0, -17.0, -16.0, -6.0, -7.0, -4.0, -4.0, -18.0, -1.0, -9.0, -5.0, -2.0, -9.0, -21.0, -11.0, -3.0, -4.0, -6.0, -2.0, -9.0, -1.0, -12.0, -22.0, -4.0, 0.0, -2.0, -18.0, -7.0, -5.0, -19.0, -25.0, -3.0, -7.0, -2.0, -11.0, -2.0, -7.0, -14.0, -6.0, -7.0, -3.0, -1.0, -13.0, -21.0, -10.0, -17.0, 0.0, -10.0, -5.0, -6.0, -14.0, -6.0, -6.0, -23.0, -7.0, -3.0, -5.0, -4.0, -14.0, -6.0, -5.0, -10.0, -2.0, -9.0, -2.0, -10.0, -16.0, -17.0, -18.0, -2.0, -6.0, -6.0, -6.0, -13.0, -9.0, -7.0, -9.0, -31.0, -11.0, -11.0, -2.0, -7.0, -11.0, -6.0, -23.0, -9.0, -7.0, -4.0, -9.0], 'episode_lengths': [31, 9, 4, 18, 12, 6, 22, 9, 5, 17, 7, 9, 17, 8, 3, 7, 8, 10, 2, 8, 10, 8, 5, 12, 1, 7, 17, 8, 13, 12, 21, 8, 10, 17, 3, 16, 10, 4, 10, 7, 2, 11, 20, 7, 4, 14, 9, 15, 3, 2, 12, 8, 10, 11, 5, 5, 13, 21, 11, 4, 12, 20, 31, 3, 8, 9, 7, 1, 7, 2, 14, 10, 6, 4, 7, 7, 3, 11, 9, 21, 7, 14, 7, 26, 6, 10, 9, 1, 16, 15, 15, 4, 5, 9, 13, 11, 7, 4, 6, 14, 14, 7, 18, 10, 2, 8, 15, 9, 19, 31, 6, 7, 5, 8, 18, 5, 16, 18, 17, 7, 8, 5, 5, 19, 2, 10, 6, 3, 10, 22, 12, 4, 5, 7, 3, 10, 2, 13, 23, 5, 1, 3, 19, 8, 6, 20, 26, 4, 8, 3, 12, 3, 8, 15, 7, 8, 4, 2, 14, 22, 11, 18, 1, 11, 6, 7, 15, 7, 7, 24, 8, 4, 6, 5, 15, 7, 6, 11, 3, 10, 3, 11, 17, 18, 19, 3, 7, 7, 7, 14, 10, 8, 10, 31, 12, 12, 3, 8, 12, 7, 24, 10, 8, 5, 10]}, 'sampler_perf': {'mean_raw_obs_processing_ms': 0.25333798616105085, 'mean_inference_ms': 0.7847060963297902, 'mean_action_processing_ms': 0.07941890845509815, 'mean_env_wait_ms': 0.13239142587759642, 'mean_env_render_ms': 0.0}, 'num_faulty_episodes': 0, 'connector_metrics': {'ObsPreprocessorConnector_ms': 0.00436364150628811, 'StateBufferConnector_ms': 0.004810472814048209, 'ViewRequirementAgentConnector_ms': 0.07745940510819598}, 'num_episodes': 205, 'episode_return_max': 0.0, 'episode_return_min': -31.0, 'episode_return_mean': -9.034146341463414}\n",
      "[16]\n",
      "{'episode_reward_max': 0.0, 'episode_reward_min': -31.0, 'episode_reward_mean': -8.850961538461538, 'episode_len_mean': 9.846153846153847, 'episode_media': {}, 'episodes_this_iter': 208, 'episodes_timesteps_total': 2048, 'policy_reward_min': {}, 'policy_reward_max': {}, 'policy_reward_mean': {}, 'custom_metrics': {}, 'hist_stats': {'episode_reward': [-18.0, -8.0, -6.0, -7.0, -4.0, -3.0, -9.0, -13.0, -6.0, -8.0, -4.0, -2.0, -7.0, 0.0, -2.0, -8.0, -19.0, -10.0, -5.0, -17.0, -11.0, 0.0, -8.0, -3.0, -15.0, -17.0, -13.0, -4.0, -6.0, -4.0, 0.0, -4.0, -12.0, -6.0, -11.0, -13.0, -11.0, -3.0, -2.0, -17.0, -6.0, -9.0, -15.0, -15.0, -7.0, -15.0, -26.0, -19.0, -10.0, -5.0, -19.0, -24.0, -3.0, -5.0, -2.0, -8.0, -12.0, -10.0, -3.0, -12.0, -28.0, -8.0, -10.0, -7.0, -6.0, -1.0, -8.0, -8.0, -6.0, -5.0, -12.0, -4.0, -6.0, -5.0, -4.0, -17.0, -10.0, -11.0, -5.0, -5.0, -10.0, -10.0, -1.0, -3.0, -10.0, -6.0, -6.0, 0.0, -14.0, 0.0, -1.0, -15.0, -10.0, -12.0, -15.0, -9.0, -11.0, -8.0, -9.0, -9.0, -9.0, -2.0, -13.0, -18.0, -9.0, -8.0, -16.0, -1.0, -3.0, -12.0, -9.0, -13.0, -15.0, 0.0, -6.0, -11.0, -12.0, -14.0, -21.0, -9.0, -9.0, -6.0, -10.0, -6.0, -13.0, -16.0, -7.0, -13.0, -9.0, -9.0, -12.0, -8.0, -10.0, -11.0, -11.0, -10.0, -12.0, -5.0, -4.0, -8.0, -4.0, 0.0, -15.0, -5.0, -5.0, -15.0, -11.0, -4.0, -2.0, -18.0, -9.0, -6.0, -2.0, -5.0, -5.0, -9.0, -22.0, -5.0, -17.0, -10.0, -5.0, -1.0, -22.0, -9.0, -2.0, -9.0, -3.0, -9.0, -10.0, 0.0, -11.0, -10.0, -23.0, -19.0, -3.0, -5.0, -9.0, -4.0, -17.0, -5.0, -31.0, -7.0, -2.0, -8.0, -2.0, -2.0, -13.0, -15.0, -6.0, -6.0, -2.0, -7.0, -3.0, -6.0, -10.0, -10.0, -14.0, -5.0, -8.0, -19.0, -3.0, -9.0, -7.0, -10.0, -16.0, -8.0, -2.0, -9.0], 'episode_lengths': [19, 9, 7, 8, 5, 4, 10, 14, 7, 9, 5, 3, 8, 1, 3, 9, 20, 11, 6, 18, 12, 1, 9, 4, 16, 18, 14, 5, 7, 5, 1, 5, 13, 7, 12, 14, 12, 4, 3, 18, 7, 10, 16, 16, 8, 16, 27, 20, 11, 6, 20, 25, 4, 6, 3, 9, 13, 11, 4, 13, 29, 9, 11, 8, 7, 2, 9, 9, 7, 6, 13, 5, 7, 6, 5, 18, 11, 12, 6, 6, 11, 11, 2, 4, 11, 7, 7, 1, 15, 1, 2, 16, 11, 13, 16, 10, 12, 9, 10, 10, 10, 3, 14, 19, 10, 9, 17, 2, 4, 13, 10, 14, 16, 1, 7, 12, 13, 15, 22, 10, 10, 7, 11, 7, 14, 17, 8, 14, 10, 10, 13, 9, 11, 12, 12, 11, 13, 6, 5, 9, 5, 1, 16, 6, 6, 16, 12, 5, 3, 19, 10, 7, 3, 6, 6, 10, 23, 6, 18, 11, 6, 2, 23, 10, 3, 10, 4, 10, 11, 1, 12, 11, 24, 20, 4, 6, 10, 5, 18, 6, 31, 8, 3, 9, 3, 3, 14, 16, 7, 7, 3, 8, 4, 7, 11, 11, 15, 6, 9, 20, 4, 10, 8, 11, 17, 9, 3, 10]}, 'sampler_perf': {'mean_raw_obs_processing_ms': 0.25458948333794046, 'mean_inference_ms': 0.7838543470624052, 'mean_action_processing_ms': 0.07932946362812968, 'mean_env_wait_ms': 0.13204637576876313, 'mean_env_render_ms': 0.0}, 'num_faulty_episodes': 0, 'connector_metrics': {'ObsPreprocessorConnector_ms': 0.004215423877422626, 'StateBufferConnector_ms': 0.00461431650015024, 'ViewRequirementAgentConnector_ms': 0.07556757101645836}, 'num_episodes': 208, 'episode_return_max': 0.0, 'episode_return_min': -31.0, 'episode_return_mean': -8.850961538461538}\n",
      "[17]\n",
      "{'episode_reward_max': 0.0, 'episode_reward_min': -31.0, 'episode_reward_mean': -8.304545454545455, 'episode_len_mean': 9.3, 'episode_media': {}, 'episodes_this_iter': 220, 'episodes_timesteps_total': 2046, 'policy_reward_min': {}, 'policy_reward_max': {}, 'policy_reward_mean': {}, 'custom_metrics': {}, 'hist_stats': {'episode_reward': [-2.0, -2.0, -19.0, -9.0, -6.0, -3.0, -19.0, -16.0, -5.0, -7.0, -9.0, -5.0, -31.0, -7.0, 0.0, -1.0, -1.0, -2.0, -6.0, -11.0, -3.0, -15.0, -4.0, -18.0, -15.0, -14.0, -22.0, -10.0, -9.0, -6.0, -10.0, -6.0, -7.0, -17.0, -10.0, -4.0, -4.0, -10.0, -18.0, -12.0, -3.0, -7.0, -1.0, -16.0, -2.0, -6.0, -11.0, -10.0, -3.0, -4.0, -5.0, -10.0, -3.0, -16.0, -22.0, -1.0, -4.0, -3.0, -16.0, -14.0, -5.0, -16.0, -18.0, -7.0, -5.0, -3.0, -5.0, -5.0, -6.0, -1.0, -7.0, -11.0, -5.0, -8.0, -19.0, -4.0, -1.0, -6.0, -6.0, -14.0, -9.0, -12.0, -12.0, -14.0, -9.0, -9.0, -6.0, -12.0, -4.0, -1.0, -18.0, -9.0, -5.0, -10.0, -16.0, -13.0, -5.0, -11.0, -4.0, -9.0, -7.0, -10.0, -5.0, -10.0, -30.0, -9.0, -1.0, -17.0, -6.0, -2.0, -9.0, -15.0, -10.0, -4.0, -10.0, -10.0, -10.0, -15.0, -4.0, -11.0, -6.0, -3.0, -1.0, -9.0, -7.0, -4.0, -9.0, -12.0, -8.0, -1.0, -13.0, -5.0, -9.0, -4.0, -4.0, -2.0, -8.0, -17.0, -15.0, -8.0, -6.0, 0.0, -5.0, -7.0, -11.0, -9.0, -22.0, -8.0, -1.0, -7.0, -11.0, -8.0, -5.0, -1.0, -9.0, -3.0, -9.0, -4.0, 0.0, -8.0, -6.0, -11.0, -4.0, -6.0, -6.0, -7.0, -13.0, -6.0, -1.0, -3.0, -1.0, -11.0, -10.0, -8.0, -6.0, -4.0, -10.0, -1.0, -7.0, -6.0, -8.0, -6.0, -11.0, -13.0, -7.0, -13.0, -8.0, -22.0, -13.0, -15.0, -12.0, -6.0, -5.0, -10.0, -7.0, -9.0, -10.0, -7.0, -10.0, -9.0, -1.0, -5.0, -7.0, -8.0, -24.0, -11.0, -12.0, -15.0, -4.0, -11.0, -13.0, -3.0, -11.0, 0.0, -7.0, -2.0, -7.0, -5.0, -5.0, -2.0], 'episode_lengths': [3, 3, 20, 10, 7, 4, 20, 17, 6, 8, 10, 6, 31, 8, 1, 2, 2, 3, 7, 12, 4, 16, 5, 19, 16, 15, 23, 11, 10, 7, 11, 7, 8, 18, 11, 5, 5, 11, 19, 13, 4, 8, 2, 17, 3, 7, 12, 11, 4, 5, 6, 11, 4, 17, 23, 2, 5, 4, 17, 15, 6, 17, 19, 8, 6, 4, 6, 6, 7, 2, 8, 12, 6, 9, 20, 5, 2, 7, 7, 15, 10, 13, 13, 15, 10, 10, 7, 13, 5, 2, 19, 10, 6, 11, 17, 14, 6, 12, 5, 10, 8, 11, 6, 11, 31, 10, 2, 18, 7, 3, 10, 16, 11, 5, 11, 11, 11, 16, 5, 12, 7, 4, 2, 10, 8, 5, 10, 13, 9, 2, 14, 6, 10, 5, 5, 3, 9, 18, 16, 9, 7, 1, 6, 8, 12, 10, 23, 9, 2, 8, 12, 9, 6, 2, 10, 4, 10, 5, 1, 9, 7, 12, 5, 7, 7, 8, 14, 7, 2, 4, 2, 12, 11, 9, 7, 5, 11, 2, 8, 7, 9, 7, 12, 14, 8, 14, 9, 23, 14, 16, 13, 7, 6, 11, 8, 10, 11, 8, 11, 10, 2, 6, 8, 9, 25, 12, 13, 16, 5, 12, 14, 4, 12, 1, 8, 3, 8, 6, 6, 3]}, 'sampler_perf': {'mean_raw_obs_processing_ms': 0.2564844064269178, 'mean_inference_ms': 0.7847595474627054, 'mean_action_processing_ms': 0.07933321392006489, 'mean_env_wait_ms': 0.1318865555417757, 'mean_env_render_ms': 0.0}, 'num_faulty_episodes': 0, 'connector_metrics': {'ObsPreprocessorConnector_ms': 0.004606030204079368, 'StateBufferConnector_ms': 0.00491554086858576, 'ViewRequirementAgentConnector_ms': 0.07799311117692427}, 'num_episodes': 220, 'episode_return_max': 0.0, 'episode_return_min': -31.0, 'episode_return_mean': -8.304545454545455}\n",
      "[18]\n",
      "{'episode_reward_max': 0.0, 'episode_reward_min': -31.0, 'episode_reward_mean': -8.36986301369863, 'episode_len_mean': 9.351598173515981, 'episode_media': {}, 'episodes_this_iter': 219, 'episodes_timesteps_total': 2048, 'policy_reward_min': {}, 'policy_reward_max': {}, 'policy_reward_mean': {}, 'custom_metrics': {}, 'hist_stats': {'episode_reward': [-9.0, -8.0, -6.0, -11.0, -11.0, -15.0, -3.0, -8.0, -4.0, -4.0, 0.0, -8.0, -10.0, -3.0, -2.0, -26.0, -2.0, -8.0, -9.0, -13.0, -15.0, -7.0, -9.0, -6.0, -6.0, -9.0, -7.0, -14.0, -14.0, -15.0, -8.0, -5.0, -7.0, -22.0, 0.0, -3.0, -10.0, -1.0, -8.0, -9.0, -14.0, -3.0, -31.0, -6.0, -11.0, -13.0, -3.0, -16.0, -10.0, 0.0, -3.0, -9.0, -3.0, -13.0, -3.0, -10.0, -6.0, -8.0, -2.0, -10.0, -1.0, -8.0, -24.0, -8.0, -13.0, -3.0, -14.0, -6.0, 0.0, -7.0, -31.0, -9.0, -6.0, -18.0, -11.0, -11.0, -14.0, -12.0, -10.0, -4.0, -14.0, -14.0, -7.0, -4.0, -12.0, -12.0, -8.0, -4.0, 0.0, -6.0, -8.0, -4.0, -10.0, -4.0, -2.0, -10.0, -5.0, -5.0, -12.0, -9.0, -9.0, -17.0, -5.0, -8.0, -9.0, -5.0, -14.0, -10.0, 0.0, -8.0, -13.0, -14.0, -17.0, -2.0, -6.0, -10.0, -10.0, -7.0, -31.0, -9.0, -8.0, -9.0, -2.0, -6.0, -10.0, -7.0, -2.0, 0.0, -6.0, -4.0, -9.0, -10.0, -3.0, -5.0, -7.0, -3.0, 0.0, -11.0, -4.0, -13.0, -13.0, -3.0, -2.0, -2.0, -4.0, -2.0, -8.0, -9.0, -14.0, -1.0, -6.0, -11.0, -5.0, -3.0, -13.0, -3.0, -4.0, 0.0, -7.0, -18.0, -13.0, -11.0, -15.0, -6.0, -6.0, -13.0, -11.0, -3.0, -7.0, -1.0, -10.0, -11.0, -1.0, -16.0, -14.0, -9.0, -3.0, -6.0, -3.0, -7.0, -6.0, -14.0, -6.0, -7.0, -24.0, -10.0, -14.0, -5.0, -8.0, -5.0, -19.0, -3.0, -6.0, -18.0, -7.0, -6.0, -31.0, -4.0, -3.0, -4.0, -6.0, -8.0, -3.0, -3.0, 0.0, -6.0, -13.0, -7.0, -15.0, -11.0, -6.0, -12.0, -10.0, -7.0, -7.0, -7.0, -3.0, -13.0, -10.0], 'episode_lengths': [10, 9, 7, 12, 12, 16, 4, 9, 5, 5, 1, 9, 11, 4, 3, 27, 3, 9, 10, 14, 16, 8, 10, 7, 7, 10, 8, 15, 15, 16, 9, 6, 8, 23, 1, 4, 11, 2, 9, 10, 15, 4, 31, 7, 12, 14, 4, 17, 11, 1, 4, 10, 4, 14, 4, 11, 7, 9, 3, 11, 2, 9, 25, 9, 14, 4, 15, 7, 1, 8, 31, 10, 7, 19, 12, 12, 15, 13, 11, 5, 15, 15, 8, 5, 13, 13, 9, 5, 1, 7, 9, 5, 11, 5, 3, 11, 6, 6, 13, 10, 10, 18, 6, 9, 10, 6, 15, 11, 1, 9, 14, 15, 18, 3, 7, 11, 11, 8, 31, 10, 9, 10, 3, 7, 11, 8, 3, 1, 7, 5, 10, 11, 4, 6, 8, 4, 1, 12, 5, 14, 14, 4, 3, 3, 5, 3, 9, 10, 15, 2, 7, 12, 6, 4, 14, 4, 5, 1, 8, 19, 14, 12, 16, 7, 7, 14, 12, 4, 8, 2, 11, 12, 2, 17, 15, 10, 4, 7, 4, 8, 7, 15, 7, 8, 25, 11, 15, 6, 9, 6, 20, 4, 7, 19, 8, 7, 31, 5, 4, 5, 7, 9, 4, 4, 1, 7, 14, 8, 16, 12, 7, 13, 11, 8, 8, 8, 4, 14, 11]}, 'sampler_perf': {'mean_raw_obs_processing_ms': 0.2576995182891821, 'mean_inference_ms': 0.7845882570739776, 'mean_action_processing_ms': 0.0792484591241862, 'mean_env_wait_ms': 0.13156490270515395, 'mean_env_render_ms': 0.0}, 'num_faulty_episodes': 0, 'connector_metrics': {'ObsPreprocessorConnector_ms': 0.004052027175415597, 'StateBufferConnector_ms': 0.004943538474165686, 'ViewRequirementAgentConnector_ms': 0.07459562118739298}, 'num_episodes': 219, 'episode_return_max': 0.0, 'episode_return_min': -31.0, 'episode_return_mean': -8.36986301369863}\n",
      "[19]\n"
     ]
    }
   ],
   "source": [
    "from ray.rllib.algorithms.ppo import PPOConfig\n",
    "from ray.tune.logger import pretty_print\n",
    "from ray.tune.registry import register_env\n",
    "from gymnasium.wrappers.time_limit import TimeLimit\n",
    "\n",
    "\n",
    "register_env(\"my_env\", lambda _: PointCoverageEnv({\"height\": 10, \"width\": 10, \"n_agents\": 1, \"n_targets\": 1, \"max_steps\": 30}))\n",
    "\n",
    "algo = (\n",
    "    PPOConfig()\n",
    "    .training(gamma=0.99, lr=0.001, kl_coeff=0.5, train_batch_size=2048, sgd_minibatch_size=256, num_sgd_iter=10)\n",
    "    .env_runners(num_env_runners=1)\n",
    "    .resources(num_gpus=0)\n",
    "    .environment(env=\"my_env\")\n",
    "    .build()\n",
    ")\n",
    "\n",
    "for i in range(20):\n",
    "    result = algo.train()\n",
    "    print(result[\"sampler_results\"])\n",
    "    #print(result[\"info\"][\"learner\"][\"default_policy\"][\"learner_stats\"][\"total_loss\"])\n",
    "    print(f\"[{i}]\")\n",
    "    if i % 5 == 0:\n",
    "        checkpoint_dir = algo.save().checkpoint.path\n",
    "        print(f\"Checkpoint saved in directory {checkpoint_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[11]\n",
      "____________\n",
      "|          |\n",
      "|          |\n",
      "|          |\n",
      "|        * |\n",
      "|          |\n",
      "|          |\n",
      "|          |\n",
      "|          |\n",
      "|          |\n",
      "|          |\n",
      "‾‾‾‾‾‾‾‾‾‾‾‾\n",
      "{}\n",
      "{}\n"
     ]
    }
   ],
   "source": [
    "from IPython.display import clear_output\n",
    "import time\n",
    "import torch\n",
    "from gymnasium.spaces.utils import flatten\n",
    "\n",
    "env = PointCoverageEnv({\"height\": 10, \"width\": 10, \"n_agents\": 1, \"n_targets\": 1, \"max_steps\": 30})\n",
    "obs_space = env.observation_space\n",
    "obs, _ = env.reset()\n",
    "env.render()\n",
    "\n",
    "for i in range(100):\n",
    "    actions = algo.compute_actions({agent: o for agent, o in obs.items()})\n",
    "    print(actions, \"\\n\")\n",
    "    \n",
    "    obs, reward, terminated, truncated, info = env.step(actions)\n",
    "    clear_output()\n",
    "    print(f\"[{i}]\")\n",
    "    env.render()\n",
    "    print(obs)\n",
    "    print(reward)\n",
    "    time.sleep(0.5)\n",
    "\n",
    "    if terminated['__all__'] or truncated['__all__']:\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Two Agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-16 12:02:48,071\tWARNING deprecation.py:50 -- DeprecationWarning: `WorkerSet(num_workers=... OR local_worker=...)` has been deprecated. Use `EnvRunnerGroup(num_env_runners=... AND local_env_runner=...)` instead. This will raise an error in the future!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of different environment steps: 20480\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-16 12:03:00,806\tINFO trainable.py:161 -- Trainable.setup took 12.736 seconds. If your trainable is slow to initialize, consider setting reuse_actors=True to reduce actor creation overheads.\n",
      "2024-05-16 12:03:00,808\tWARNING util.py:61 -- Install gputil for GPU system monitoring.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "agent_timesteps_total: 2677\n",
      "connector_metrics:\n",
      "  ObsPreprocessorConnector_ms: 0.0050098176986452135\n",
      "  StateBufferConnector_ms: 0.0049023401169549854\n",
      "  ViewRequirementAgentConnector_ms: 0.10862577529180617\n",
      "counters:\n",
      "  num_agent_steps_sampled: 2677\n",
      "  num_agent_steps_trained: 2677\n",
      "  num_env_steps_sampled: 2048\n",
      "  num_env_steps_trained: 2048\n",
      "custom_metrics: {}\n",
      "date: 2024-05-16_12-03-05\n",
      "done: false\n",
      "env_runner_results:\n",
      "  connector_metrics:\n",
      "    ObsPreprocessorConnector_ms: 0.0050098176986452135\n",
      "    StateBufferConnector_ms: 0.0049023401169549854\n",
      "    ViewRequirementAgentConnector_ms: 0.10862577529180617\n",
      "  custom_metrics: {}\n",
      "  episode_len_mean: 32.34920634920635\n",
      "  episode_media: {}\n",
      "  episode_return_max: 26.0\n",
      "  episode_return_mean: -29.650793650793652\n",
      "  episode_return_min: -102.0\n",
      "  episode_reward_max: 26.0\n",
      "  episode_reward_mean: -29.650793650793652\n",
      "  episode_reward_min: -102.0\n",
      "  episodes_this_iter: 63\n",
      "  episodes_timesteps_total: 2038\n",
      "  hist_stats:\n",
      "    episode_lengths: [51, 4, 46, 17, 29, 16, 11, 51, 22, 14, 51, 51, 51, 51, 48, 51,\n",
      "      51, 51, 49, 29, 8, 51, 7, 51, 51, 1, 28, 51, 51, 51, 51, 4, 36, 4, 15, 22, 14,\n",
      "      7, 23, 51, 51, 44, 48, 9, 8, 51, 51, 16, 16, 2, 51, 45, 31, 36, 20, 51, 22,\n",
      "      49, 51, 4, 37, 9, 15]\n",
      "    episode_reward: [-45.0, 5.0, -35.0, -21.0, -20.0, -12.0, -5.0, -47.0, -12.0, -3.0,\n",
      "      -41.0, -42.0, -67.0, -67.0, -43.0, -46.0, -41.0, -102.0, -58.0, -20.0, 15.0,\n",
      "      -57.0, -1.0, -42.0, -41.0, 19.0, -34.0, -53.0, -47.0, -46.0, -49.0, 23.0, -38.0,\n",
      "      6.0, -5.0, -27.0, -12.0, 4.0, -27.0, -41.0, -44.0, -36.0, -59.0, 2.0, -1.0,\n",
      "      -58.0, -50.0, -16.0, -16.0, 9.0, -41.0, -43.0, -29.0, -55.0, -13.0, -41.0, -25.0,\n",
      "      -69.0, -102.0, 26.0, -35.0, 2.0, 1.0]\n",
      "  num_episodes: 63\n",
      "  num_faulty_episodes: 0\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10971932250619106\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.2175390923994818\n",
      "    mean_inference_ms: 0.9886346950130736\n",
      "    mean_raw_obs_processing_ms: 0.33313184322061185\n",
      "episode_len_mean: 32.34920634920635\n",
      "episode_media: {}\n",
      "episode_return_max: 26.0\n",
      "episode_return_mean: -29.650793650793652\n",
      "episode_return_min: -102.0\n",
      "episode_reward_max: 26.0\n",
      "episode_reward_mean: -29.650793650793652\n",
      "episode_reward_min: -102.0\n",
      "episodes_this_iter: 63\n",
      "episodes_timesteps_total: 2038\n",
      "episodes_total: 63\n",
      "hostname: LAPTOP-9AD2MD1C\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      diff_num_grad_updates_vs_sampler_policy: 49.5\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.5\n",
      "        cur_lr: 0.009999999999999998\n",
      "        entropy: 1.6083052623271943\n",
      "        entropy_coeff: 1.0\n",
      "        grad_gnorm: 2.9359652841091157\n",
      "        kl: 0.0009878574980054012\n",
      "        policy_loss: 0.029244988495483994\n",
      "        total_loss: 7.170420908927918\n",
      "        vf_explained_var: -0.0003442436456680298\n",
      "        vf_loss: 8.748987274169922\n",
      "      model: {}\n",
      "      num_agent_steps_trained: 256.0\n",
      "      num_grad_updates_lifetime: 50.5\n",
      "  num_agent_steps_sampled: 2677\n",
      "  num_agent_steps_trained: 2677\n",
      "  num_env_steps_sampled: 2048\n",
      "  num_env_steps_trained: 2048\n",
      "iterations_since_restore: 1\n",
      "node_ip: 172.18.164.188\n",
      "num_agent_steps_sampled: 2677\n",
      "num_agent_steps_sampled_lifetime: 2677\n",
      "num_agent_steps_trained: 2677\n",
      "num_env_steps_sampled: 2048\n",
      "num_env_steps_sampled_lifetime: 2048\n",
      "num_env_steps_sampled_this_iter: 2048\n",
      "num_env_steps_sampled_throughput_per_sec: 491.9369878325704\n",
      "num_env_steps_trained: 2048\n",
      "num_env_steps_trained_this_iter: 2048\n",
      "num_env_steps_trained_throughput_per_sec: 491.9369878325704\n",
      "num_episodes: 63\n",
      "num_faulty_episodes: 0\n",
      "num_healthy_workers: 1\n",
      "num_in_flight_async_reqs: 0\n",
      "num_remote_worker_restarts: 0\n",
      "num_steps_trained_this_iter: 2048\n",
      "perf:\n",
      "  cpu_util_percent: 21.183333333333334\n",
      "  ram_util_percent: 81.56666666666666\n",
      "pid: 722\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.10971932250619106\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.2175390923994818\n",
      "  mean_inference_ms: 0.9886346950130736\n",
      "  mean_raw_obs_processing_ms: 0.33313184322061185\n",
      "sampler_results:\n",
      "  connector_metrics:\n",
      "    ObsPreprocessorConnector_ms: 0.0050098176986452135\n",
      "    StateBufferConnector_ms: 0.0049023401169549854\n",
      "    ViewRequirementAgentConnector_ms: 0.10862577529180617\n",
      "  custom_metrics: {}\n",
      "  episode_len_mean: 32.34920634920635\n",
      "  episode_media: {}\n",
      "  episode_return_max: 26.0\n",
      "  episode_return_mean: -29.650793650793652\n",
      "  episode_return_min: -102.0\n",
      "  episode_reward_max: 26.0\n",
      "  episode_reward_mean: -29.650793650793652\n",
      "  episode_reward_min: -102.0\n",
      "  episodes_this_iter: 63\n",
      "  episodes_timesteps_total: 2038\n",
      "  hist_stats:\n",
      "    episode_lengths: [51, 4, 46, 17, 29, 16, 11, 51, 22, 14, 51, 51, 51, 51, 48, 51,\n",
      "      51, 51, 49, 29, 8, 51, 7, 51, 51, 1, 28, 51, 51, 51, 51, 4, 36, 4, 15, 22, 14,\n",
      "      7, 23, 51, 51, 44, 48, 9, 8, 51, 51, 16, 16, 2, 51, 45, 31, 36, 20, 51, 22,\n",
      "      49, 51, 4, 37, 9, 15]\n",
      "    episode_reward: [-45.0, 5.0, -35.0, -21.0, -20.0, -12.0, -5.0, -47.0, -12.0, -3.0,\n",
      "      -41.0, -42.0, -67.0, -67.0, -43.0, -46.0, -41.0, -102.0, -58.0, -20.0, 15.0,\n",
      "      -57.0, -1.0, -42.0, -41.0, 19.0, -34.0, -53.0, -47.0, -46.0, -49.0, 23.0, -38.0,\n",
      "      6.0, -5.0, -27.0, -12.0, 4.0, -27.0, -41.0, -44.0, -36.0, -59.0, 2.0, -1.0,\n",
      "      -58.0, -50.0, -16.0, -16.0, 9.0, -41.0, -43.0, -29.0, -55.0, -13.0, -41.0, -25.0,\n",
      "      -69.0, -102.0, 26.0, -35.0, 2.0, 1.0]\n",
      "  num_episodes: 63\n",
      "  num_faulty_episodes: 0\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10971932250619106\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.2175390923994818\n",
      "    mean_inference_ms: 0.9886346950130736\n",
      "    mean_raw_obs_processing_ms: 0.33313184322061185\n",
      "time_since_restore: 4.170497417449951\n",
      "time_this_iter_s: 4.170497417449951\n",
      "time_total_s: 4.170497417449951\n",
      "timers:\n",
      "  learn_throughput: 2766.489\n",
      "  learn_time_ms: 740.288\n",
      "  load_throughput: 1860098.439\n",
      "  load_time_ms: 1.101\n",
      "  restore_workers_time_ms: 0.024\n",
      "  sample_time_ms: 3416.84\n",
      "  synch_weights_time_ms: 4.405\n",
      "  training_iteration_time_ms: 4163.161\n",
      "  training_step_time_ms: 4163.061\n",
      "timestamp: 1715853785\n",
      "timesteps_total: 2048\n",
      "training_iteration: 1\n",
      "trial_id: default\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "[0]\n",
      "Checkpoint saved in directory /tmp/tmpkh004xrr\n",
      "agent_timesteps_total: 5272\n",
      "connector_metrics:\n",
      "  ObsPreprocessorConnector_ms: 0.0053517818450927734\n",
      "  StateBufferConnector_ms: 0.004912376403808594\n",
      "  ViewRequirementAgentConnector_ms: 0.10784149169921875\n",
      "counters:\n",
      "  num_agent_steps_sampled: 5272\n",
      "  num_agent_steps_trained: 5272\n",
      "  num_env_steps_sampled: 4096\n",
      "  num_env_steps_trained: 4096\n",
      "custom_metrics: {}\n",
      "date: 2024-05-16_12-03-09\n",
      "done: false\n",
      "env_runner_results:\n",
      "  connector_metrics:\n",
      "    ObsPreprocessorConnector_ms: 0.0053517818450927734\n",
      "    StateBufferConnector_ms: 0.004912376403808594\n",
      "    ViewRequirementAgentConnector_ms: 0.10784149169921875\n",
      "  custom_metrics: {}\n",
      "  episode_len_mean: 32.75\n",
      "  episode_media: {}\n",
      "  episode_return_max: 26.0\n",
      "  episode_return_mean: -29.72\n",
      "  episode_return_min: -102.0\n",
      "  episode_reward_max: 26.0\n",
      "  episode_reward_mean: -29.72\n",
      "  episode_reward_min: -102.0\n",
      "  episodes_this_iter: 59\n",
      "  episodes_timesteps_total: 3275\n",
      "  hist_stats:\n",
      "    episode_lengths: [7, 51, 51, 1, 28, 51, 51, 51, 51, 4, 36, 4, 15, 22, 14, 7, 23,\n",
      "      51, 51, 44, 48, 9, 8, 51, 51, 16, 16, 2, 51, 45, 31, 36, 20, 51, 22, 49, 51,\n",
      "      4, 37, 9, 15, 50, 7, 47, 48, 51, 51, 51, 4, 30, 51, 29, 51, 30, 7, 51, 51, 45,\n",
      "      51, 51, 51, 51, 8, 51, 51, 14, 51, 20, 18, 51, 8, 51, 11, 42, 51, 50, 6, 51,\n",
      "      51, 19, 25, 47, 51, 25, 51, 12, 17, 15, 51, 7, 15, 47, 2, 51, 6, 51, 51, 4,\n",
      "      6, 44]\n",
      "    episode_reward: [-1.0, -42.0, -41.0, 19.0, -34.0, -53.0, -47.0, -46.0, -49.0,\n",
      "      23.0, -38.0, 6.0, -5.0, -27.0, -12.0, 4.0, -27.0, -41.0, -44.0, -36.0, -59.0,\n",
      "      2.0, -1.0, -58.0, -50.0, -16.0, -16.0, 9.0, -41.0, -43.0, -29.0, -55.0, -13.0,\n",
      "      -41.0, -25.0, -69.0, -102.0, 26.0, -35.0, 2.0, 1.0, -41.0, 0.0, -36.0, -45.0,\n",
      "      -46.0, -68.0, -42.0, 5.0, -22.0, -44.0, -28.0, -42.0, -23.0, 1.0, -49.0, -47.0,\n",
      "      -25.0, -41.0, -57.0, -45.0, -48.0, 3.0, -47.0, -78.0, 3.0, -50.0, -11.0, -8.0,\n",
      "      -81.0, 1.0, -59.0, -9.0, -51.0, -41.0, -44.0, 2.0, -43.0, -45.0, -36.0, -35.0,\n",
      "      -76.0, -53.0, -18.0, -55.0, -11.0, -19.0, -4.0, -42.0, 3.0, -15.0, -46.0, 9.0,\n",
      "      -48.0, 19.0, -45.0, -72.0, 7.0, 5.0, -35.0]\n",
      "  num_episodes: 59\n",
      "  num_faulty_episodes: 0\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.1090134880419246\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.2171017930840516\n",
      "    mean_inference_ms: 0.9890292847056498\n",
      "    mean_raw_obs_processing_ms: 0.32826732273702114\n",
      "episode_len_mean: 32.75\n",
      "episode_media: {}\n",
      "episode_return_max: 26.0\n",
      "episode_return_mean: -29.72\n",
      "episode_return_min: -102.0\n",
      "episode_reward_max: 26.0\n",
      "episode_reward_mean: -29.72\n",
      "episode_reward_min: -102.0\n",
      "episodes_this_iter: 59\n",
      "episodes_timesteps_total: 3275\n",
      "episodes_total: 122\n",
      "hostname: LAPTOP-9AD2MD1C\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      diff_num_grad_updates_vs_sampler_policy: 49.5\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.25\n",
      "        cur_lr: 0.0010000000000000002\n",
      "        entropy: 1.5999012100696564\n",
      "        entropy_coeff: 0.8692871093749995\n",
      "        grad_gnorm: 1.290707049369812\n",
      "        kl: 0.00813795069552528\n",
      "        policy_loss: -0.011220068223774433\n",
      "        total_loss: 7.084713854789734\n",
      "        vf_explained_var: 0.0017953014373779297\n",
      "        vf_loss: 8.484672980308533\n",
      "      model: {}\n",
      "      num_agent_steps_trained: 256.0\n",
      "      num_grad_updates_lifetime: 150.5\n",
      "  num_agent_steps_sampled: 5272\n",
      "  num_agent_steps_trained: 5272\n",
      "  num_env_steps_sampled: 4096\n",
      "  num_env_steps_trained: 4096\n",
      "iterations_since_restore: 2\n",
      "node_ip: 172.18.164.188\n",
      "num_agent_steps_sampled: 5272\n",
      "num_agent_steps_sampled_lifetime: 5272\n",
      "num_agent_steps_trained: 5272\n",
      "num_env_steps_sampled: 4096\n",
      "num_env_steps_sampled_lifetime: 4096\n",
      "num_env_steps_sampled_this_iter: 2048\n",
      "num_env_steps_sampled_throughput_per_sec: 504.5434567645549\n",
      "num_env_steps_trained: 4096\n",
      "num_env_steps_trained_this_iter: 2048\n",
      "num_env_steps_trained_throughput_per_sec: 504.5434567645549\n",
      "num_episodes: 59\n",
      "num_faulty_episodes: 0\n",
      "num_healthy_workers: 1\n",
      "num_in_flight_async_reqs: 0\n",
      "num_remote_worker_restarts: 0\n",
      "num_steps_trained_this_iter: 2048\n",
      "perf:\n",
      "  cpu_util_percent: 24.833333333333332\n",
      "  ram_util_percent: 81.76666666666667\n",
      "pid: 722\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.1090134880419246\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.2171017930840516\n",
      "  mean_inference_ms: 0.9890292847056498\n",
      "  mean_raw_obs_processing_ms: 0.32826732273702114\n",
      "sampler_results:\n",
      "  connector_metrics:\n",
      "    ObsPreprocessorConnector_ms: 0.0053517818450927734\n",
      "    StateBufferConnector_ms: 0.004912376403808594\n",
      "    ViewRequirementAgentConnector_ms: 0.10784149169921875\n",
      "  custom_metrics: {}\n",
      "  episode_len_mean: 32.75\n",
      "  episode_media: {}\n",
      "  episode_return_max: 26.0\n",
      "  episode_return_mean: -29.72\n",
      "  episode_return_min: -102.0\n",
      "  episode_reward_max: 26.0\n",
      "  episode_reward_mean: -29.72\n",
      "  episode_reward_min: -102.0\n",
      "  episodes_this_iter: 59\n",
      "  episodes_timesteps_total: 3275\n",
      "  hist_stats:\n",
      "    episode_lengths: [7, 51, 51, 1, 28, 51, 51, 51, 51, 4, 36, 4, 15, 22, 14, 7, 23,\n",
      "      51, 51, 44, 48, 9, 8, 51, 51, 16, 16, 2, 51, 45, 31, 36, 20, 51, 22, 49, 51,\n",
      "      4, 37, 9, 15, 50, 7, 47, 48, 51, 51, 51, 4, 30, 51, 29, 51, 30, 7, 51, 51, 45,\n",
      "      51, 51, 51, 51, 8, 51, 51, 14, 51, 20, 18, 51, 8, 51, 11, 42, 51, 50, 6, 51,\n",
      "      51, 19, 25, 47, 51, 25, 51, 12, 17, 15, 51, 7, 15, 47, 2, 51, 6, 51, 51, 4,\n",
      "      6, 44]\n",
      "    episode_reward: [-1.0, -42.0, -41.0, 19.0, -34.0, -53.0, -47.0, -46.0, -49.0,\n",
      "      23.0, -38.0, 6.0, -5.0, -27.0, -12.0, 4.0, -27.0, -41.0, -44.0, -36.0, -59.0,\n",
      "      2.0, -1.0, -58.0, -50.0, -16.0, -16.0, 9.0, -41.0, -43.0, -29.0, -55.0, -13.0,\n",
      "      -41.0, -25.0, -69.0, -102.0, 26.0, -35.0, 2.0, 1.0, -41.0, 0.0, -36.0, -45.0,\n",
      "      -46.0, -68.0, -42.0, 5.0, -22.0, -44.0, -28.0, -42.0, -23.0, 1.0, -49.0, -47.0,\n",
      "      -25.0, -41.0, -57.0, -45.0, -48.0, 3.0, -47.0, -78.0, 3.0, -50.0, -11.0, -8.0,\n",
      "      -81.0, 1.0, -59.0, -9.0, -51.0, -41.0, -44.0, 2.0, -43.0, -45.0, -36.0, -35.0,\n",
      "      -76.0, -53.0, -18.0, -55.0, -11.0, -19.0, -4.0, -42.0, 3.0, -15.0, -46.0, 9.0,\n",
      "      -48.0, 19.0, -45.0, -72.0, 7.0, 5.0, -35.0]\n",
      "  num_episodes: 59\n",
      "  num_faulty_episodes: 0\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.1090134880419246\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.2171017930840516\n",
      "    mean_inference_ms: 0.9890292847056498\n",
      "    mean_raw_obs_processing_ms: 0.32826732273702114\n",
      "time_since_restore: 8.234557867050171\n",
      "time_this_iter_s: 4.06406044960022\n",
      "time_total_s: 8.234557867050171\n",
      "timers:\n",
      "  learn_throughput: 2878.012\n",
      "  learn_time_ms: 711.602\n",
      "  load_throughput: 2448670.066\n",
      "  load_time_ms: 0.836\n",
      "  restore_workers_time_ms: 0.018\n",
      "  sample_time_ms: 3394.062\n",
      "  synch_weights_time_ms: 4.236\n",
      "  training_iteration_time_ms: 4111.143\n",
      "  training_step_time_ms: 4111.075\n",
      "timestamp: 1715853789\n",
      "timesteps_total: 4096\n",
      "training_iteration: 2\n",
      "trial_id: default\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "[1]\n",
      "agent_timesteps_total: 7729\n",
      "connector_metrics:\n",
      "  ObsPreprocessorConnector_ms: 0.0053255558013916016\n",
      "  StateBufferConnector_ms: 0.005186319351196289\n",
      "  ViewRequirementAgentConnector_ms: 0.11048126220703125\n",
      "counters:\n",
      "  num_agent_steps_sampled: 7729\n",
      "  num_agent_steps_trained: 7729\n",
      "  num_env_steps_sampled: 6144\n",
      "  num_env_steps_trained: 6144\n",
      "custom_metrics: {}\n",
      "date: 2024-05-16_12-03-13\n",
      "done: false\n",
      "env_runner_results:\n",
      "  connector_metrics:\n",
      "    ObsPreprocessorConnector_ms: 0.0053255558013916016\n",
      "    StateBufferConnector_ms: 0.005186319351196289\n",
      "    ViewRequirementAgentConnector_ms: 0.11048126220703125\n",
      "  custom_metrics: {}\n",
      "  episode_len_mean: 29.24\n",
      "  episode_media: {}\n",
      "  episode_return_max: 19.0\n",
      "  episode_return_mean: -23.94\n",
      "  episode_return_min: -76.0\n",
      "  episode_reward_max: 19.0\n",
      "  episode_reward_mean: -23.94\n",
      "  episode_reward_min: -76.0\n",
      "  episodes_this_iter: 72\n",
      "  episodes_timesteps_total: 2924\n",
      "  hist_stats:\n",
      "    episode_lengths: [11, 42, 51, 50, 6, 51, 51, 19, 25, 47, 51, 25, 51, 12, 17, 15,\n",
      "      51, 7, 15, 47, 2, 51, 6, 51, 51, 4, 6, 44, 44, 3, 6, 17, 13, 39, 38, 13, 51,\n",
      "      35, 19, 6, 23, 51, 16, 31, 51, 7, 49, 46, 39, 29, 6, 5, 43, 51, 8, 37, 51, 12,\n",
      "      13, 51, 23, 39, 22, 51, 42, 23, 40, 6, 48, 41, 19, 51, 34, 51, 31, 51, 51, 41,\n",
      "      51, 14, 16, 51, 51, 8, 25, 51, 12, 18, 8, 7, 12, 51, 5, 28, 3, 23, 25, 3, 4,\n",
      "      32]\n",
      "    episode_reward: [-9.0, -51.0, -41.0, -44.0, 2.0, -43.0, -45.0, -36.0, -35.0, -76.0,\n",
      "      -53.0, -18.0, -55.0, -11.0, -19.0, -4.0, -42.0, 3.0, -15.0, -46.0, 9.0, -48.0,\n",
      "      19.0, -45.0, -72.0, 7.0, 5.0, -35.0, -33.0, 8.0, 5.0, -9.0, -9.0, -43.0, -37.0,\n",
      "      -3.0, -43.0, -25.0, -11.0, 1.0, -16.0, -41.0, -5.0, -25.0, -42.0, 0.0, -38.0,\n",
      "      -35.0, -48.0, -18.0, 1.0, 6.0, -34.0, -42.0, 1.0, -37.0, -43.0, -11.0, -2.0,\n",
      "      -45.0, -18.0, -31.0, -15.0, -50.0, -31.0, -13.0, -33.0, 5.0, -41.0, -65.0, -25.0,\n",
      "      -42.0, -45.0, -41.0, -22.0, -41.0, -41.0, -30.0, -32.0, -14.0, -9.0, -52.0,\n",
      "      -44.0, -1.0, -15.0, -45.0, -3.0, -14.0, 2.0, 3.0, -1.0, -41.0, 3.0, -17.0, -4.0,\n",
      "      -12.0, -19.0, 8.0, 6.0, -43.0]\n",
      "  num_episodes: 72\n",
      "  num_faulty_episodes: 0\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.1077364678309446\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.2140318028499651\n",
      "    mean_inference_ms: 1.0048153863468199\n",
      "    mean_raw_obs_processing_ms: 0.32329749151531606\n",
      "episode_len_mean: 29.24\n",
      "episode_media: {}\n",
      "episode_return_max: 19.0\n",
      "episode_return_mean: -23.94\n",
      "episode_return_min: -76.0\n",
      "episode_reward_max: 19.0\n",
      "episode_reward_mean: -23.94\n",
      "episode_reward_min: -76.0\n",
      "episodes_this_iter: 72\n",
      "episodes_timesteps_total: 2924\n",
      "episodes_total: 194\n",
      "hostname: LAPTOP-9AD2MD1C\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      diff_num_grad_updates_vs_sampler_policy: 44.5\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.25\n",
      "        cur_lr: 0.0010000000000000002\n",
      "        entropy: 1.5965936581293743\n",
      "        entropy_coeff: 0.7425781249999996\n",
      "        grad_gnorm: 1.5446042117145327\n",
      "        kl: 0.011032601648234337\n",
      "        policy_loss: 0.015455249482248393\n",
      "        total_loss: 7.058772643407186\n",
      "        vf_explained_var: 0.0022429744402567547\n",
      "        vf_loss: 8.226154730055068\n",
      "      model: {}\n",
      "      num_agent_steps_trained: 256.0\n",
      "      num_grad_updates_lifetime: 245.5\n",
      "  num_agent_steps_sampled: 7729\n",
      "  num_agent_steps_trained: 7729\n",
      "  num_env_steps_sampled: 6144\n",
      "  num_env_steps_trained: 6144\n",
      "iterations_since_restore: 3\n",
      "node_ip: 172.18.164.188\n",
      "num_agent_steps_sampled: 7729\n",
      "num_agent_steps_sampled_lifetime: 7729\n",
      "num_agent_steps_trained: 7729\n",
      "num_env_steps_sampled: 6144\n",
      "num_env_steps_sampled_lifetime: 6144\n",
      "num_env_steps_sampled_this_iter: 2048\n",
      "num_env_steps_sampled_throughput_per_sec: 498.8830563330376\n",
      "num_env_steps_trained: 6144\n",
      "num_env_steps_trained_this_iter: 2048\n",
      "num_env_steps_trained_throughput_per_sec: 498.8830563330376\n",
      "num_episodes: 72\n",
      "num_faulty_episodes: 0\n",
      "num_healthy_workers: 1\n",
      "num_in_flight_async_reqs: 0\n",
      "num_remote_worker_restarts: 0\n",
      "num_steps_trained_this_iter: 2048\n",
      "perf:\n",
      "  cpu_util_percent: 22.2\n",
      "  ram_util_percent: 81.53333333333335\n",
      "pid: 722\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.1077364678309446\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.2140318028499651\n",
      "  mean_inference_ms: 1.0048153863468199\n",
      "  mean_raw_obs_processing_ms: 0.32329749151531606\n",
      "sampler_results:\n",
      "  connector_metrics:\n",
      "    ObsPreprocessorConnector_ms: 0.0053255558013916016\n",
      "    StateBufferConnector_ms: 0.005186319351196289\n",
      "    ViewRequirementAgentConnector_ms: 0.11048126220703125\n",
      "  custom_metrics: {}\n",
      "  episode_len_mean: 29.24\n",
      "  episode_media: {}\n",
      "  episode_return_max: 19.0\n",
      "  episode_return_mean: -23.94\n",
      "  episode_return_min: -76.0\n",
      "  episode_reward_max: 19.0\n",
      "  episode_reward_mean: -23.94\n",
      "  episode_reward_min: -76.0\n",
      "  episodes_this_iter: 72\n",
      "  episodes_timesteps_total: 2924\n",
      "  hist_stats:\n",
      "    episode_lengths: [11, 42, 51, 50, 6, 51, 51, 19, 25, 47, 51, 25, 51, 12, 17, 15,\n",
      "      51, 7, 15, 47, 2, 51, 6, 51, 51, 4, 6, 44, 44, 3, 6, 17, 13, 39, 38, 13, 51,\n",
      "      35, 19, 6, 23, 51, 16, 31, 51, 7, 49, 46, 39, 29, 6, 5, 43, 51, 8, 37, 51, 12,\n",
      "      13, 51, 23, 39, 22, 51, 42, 23, 40, 6, 48, 41, 19, 51, 34, 51, 31, 51, 51, 41,\n",
      "      51, 14, 16, 51, 51, 8, 25, 51, 12, 18, 8, 7, 12, 51, 5, 28, 3, 23, 25, 3, 4,\n",
      "      32]\n",
      "    episode_reward: [-9.0, -51.0, -41.0, -44.0, 2.0, -43.0, -45.0, -36.0, -35.0, -76.0,\n",
      "      -53.0, -18.0, -55.0, -11.0, -19.0, -4.0, -42.0, 3.0, -15.0, -46.0, 9.0, -48.0,\n",
      "      19.0, -45.0, -72.0, 7.0, 5.0, -35.0, -33.0, 8.0, 5.0, -9.0, -9.0, -43.0, -37.0,\n",
      "      -3.0, -43.0, -25.0, -11.0, 1.0, -16.0, -41.0, -5.0, -25.0, -42.0, 0.0, -38.0,\n",
      "      -35.0, -48.0, -18.0, 1.0, 6.0, -34.0, -42.0, 1.0, -37.0, -43.0, -11.0, -2.0,\n",
      "      -45.0, -18.0, -31.0, -15.0, -50.0, -31.0, -13.0, -33.0, 5.0, -41.0, -65.0, -25.0,\n",
      "      -42.0, -45.0, -41.0, -22.0, -41.0, -41.0, -30.0, -32.0, -14.0, -9.0, -52.0,\n",
      "      -44.0, -1.0, -15.0, -45.0, -3.0, -14.0, 2.0, 3.0, -1.0, -41.0, 3.0, -17.0, -4.0,\n",
      "      -12.0, -19.0, 8.0, 6.0, -43.0]\n",
      "  num_episodes: 72\n",
      "  num_faulty_episodes: 0\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.1077364678309446\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.2140318028499651\n",
      "    mean_inference_ms: 1.0048153863468199\n",
      "    mean_raw_obs_processing_ms: 0.32329749151531606\n",
      "time_since_restore: 12.344887256622314\n",
      "time_this_iter_s: 4.1103293895721436\n",
      "time_total_s: 12.344887256622314\n",
      "timers:\n",
      "  learn_throughput: 3004.011\n",
      "  learn_time_ms: 681.755\n",
      "  load_throughput: 2871287.329\n",
      "  load_time_ms: 0.713\n",
      "  restore_workers_time_ms: 0.024\n",
      "  sample_time_ms: 3422.067\n",
      "  synch_weights_time_ms: 4.22\n",
      "  training_iteration_time_ms: 4109.16\n",
      "  training_step_time_ms: 4109.089\n",
      "timestamp: 1715853793\n",
      "timesteps_total: 6144\n",
      "training_iteration: 3\n",
      "trial_id: default\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "[2]\n",
      "agent_timesteps_total: 10222\n",
      "connector_metrics:\n",
      "  ObsPreprocessorConnector_ms: 0.005243778228759766\n",
      "  StateBufferConnector_ms: 0.0050694942474365234\n",
      "  ViewRequirementAgentConnector_ms: 0.11172842979431152\n",
      "counters:\n",
      "  num_agent_steps_sampled: 10222\n",
      "  num_agent_steps_trained: 10222\n",
      "  num_env_steps_sampled: 8192\n",
      "  num_env_steps_trained: 8192\n",
      "custom_metrics: {}\n",
      "date: 2024-05-16_12-03-17\n",
      "done: false\n",
      "env_runner_results:\n",
      "  connector_metrics:\n",
      "    ObsPreprocessorConnector_ms: 0.005243778228759766\n",
      "    StateBufferConnector_ms: 0.0050694942474365234\n",
      "    ViewRequirementAgentConnector_ms: 0.11172842979431152\n",
      "  custom_metrics: {}\n",
      "  episode_len_mean: 26.34\n",
      "  episode_media: {}\n",
      "  episode_return_max: 23.0\n",
      "  episode_return_mean: -19.46\n",
      "  episode_return_min: -59.0\n",
      "  episode_reward_max: 23.0\n",
      "  episode_reward_mean: -19.46\n",
      "  episode_reward_min: -59.0\n",
      "  episodes_this_iter: 76\n",
      "  episodes_timesteps_total: 2634\n",
      "  hist_stats:\n",
      "    episode_lengths: [51, 41, 51, 14, 16, 51, 51, 8, 25, 51, 12, 18, 8, 7, 12, 51,\n",
      "      5, 28, 3, 23, 25, 3, 4, 32, 5, 51, 10, 15, 10, 51, 12, 51, 44, 40, 51, 19, 16,\n",
      "      30, 51, 45, 51, 51, 6, 13, 49, 18, 21, 6, 21, 27, 50, 45, 20, 37, 14, 37, 14,\n",
      "      5, 25, 32, 26, 2, 2, 32, 51, 23, 21, 41, 51, 2, 31, 36, 6, 33, 14, 51, 25, 10,\n",
      "      51, 41, 24, 33, 50, 14, 41, 4, 7, 26, 6, 24, 28, 51, 16, 19, 36, 23, 5, 18,\n",
      "      13, 14]\n",
      "    episode_reward: [-41.0, -30.0, -32.0, -14.0, -9.0, -52.0, -44.0, -1.0, -15.0,\n",
      "      -45.0, -3.0, -14.0, 2.0, 3.0, -1.0, -41.0, 3.0, -17.0, -4.0, -12.0, -19.0, 8.0,\n",
      "      6.0, -43.0, 5.0, -44.0, -4.0, 1.0, 0.0, -42.0, -2.0, -55.0, -45.0, -30.0, -49.0,\n",
      "      -12.0, -10.0, -19.0, -42.0, -46.0, -42.0, -43.0, 4.0, -11.0, -40.0, -10.0, -15.0,\n",
      "      1.0, -10.0, -16.0, -46.0, -45.0, -11.0, -26.0, -8.0, -34.0, -15.0, 6.0, -19.0,\n",
      "      -21.0, -16.0, 9.0, 9.0, -28.0, -40.0, -14.0, -23.0, -32.0, -48.0, 9.0, -25.0,\n",
      "      -27.0, 5.0, -44.0, -13.0, -41.0, -15.0, 1.0, -38.0, -51.0, -13.0, -22.0, -44.0,\n",
      "      -10.0, -37.0, 23.0, 3.0, -23.0, -10.0, -14.0, -17.0, -59.0, -14.0, -11.0, -26.0,\n",
      "      -19.0, 6.0, -12.0, -6.0, -9.0]\n",
      "  num_episodes: 76\n",
      "  num_faulty_episodes: 0\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10823698055094927\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.2137792846328535\n",
      "    mean_inference_ms: 1.0190860478637846\n",
      "    mean_raw_obs_processing_ms: 0.3254848795544521\n",
      "episode_len_mean: 26.34\n",
      "episode_media: {}\n",
      "episode_return_max: 23.0\n",
      "episode_return_mean: -19.46\n",
      "episode_return_min: -59.0\n",
      "episode_reward_max: 23.0\n",
      "episode_reward_mean: -19.46\n",
      "episode_reward_min: -59.0\n",
      "episodes_this_iter: 76\n",
      "episodes_timesteps_total: 2634\n",
      "episodes_total: 270\n",
      "hostname: LAPTOP-9AD2MD1C\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      diff_num_grad_updates_vs_sampler_policy: 44.5\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.25\n",
      "        cur_lr: 0.0010000000000000002\n",
      "        entropy: 1.5804998450809056\n",
      "        entropy_coeff: 0.6226074218750002\n",
      "        grad_gnorm: 1.8065952168570625\n",
      "        kl: 0.0123153244687521\n",
      "        policy_loss: 0.017455923557281493\n",
      "        total_loss: 7.0768531534406875\n",
      "        vf_explained_var: 0.009803189833958944\n",
      "        vf_loss: 8.040349298053318\n",
      "      model: {}\n",
      "      num_agent_steps_trained: 256.0\n",
      "      num_grad_updates_lifetime: 335.5\n",
      "  num_agent_steps_sampled: 10222\n",
      "  num_agent_steps_trained: 10222\n",
      "  num_env_steps_sampled: 8192\n",
      "  num_env_steps_trained: 8192\n",
      "iterations_since_restore: 4\n",
      "node_ip: 172.18.164.188\n",
      "num_agent_steps_sampled: 10222\n",
      "num_agent_steps_sampled_lifetime: 10222\n",
      "num_agent_steps_trained: 10222\n",
      "num_env_steps_sampled: 8192\n",
      "num_env_steps_sampled_lifetime: 8192\n",
      "num_env_steps_sampled_this_iter: 2048\n",
      "num_env_steps_sampled_throughput_per_sec: 490.76072158943697\n",
      "num_env_steps_trained: 8192\n",
      "num_env_steps_trained_this_iter: 2048\n",
      "num_env_steps_trained_throughput_per_sec: 490.76072158943697\n",
      "num_episodes: 76\n",
      "num_faulty_episodes: 0\n",
      "num_healthy_workers: 1\n",
      "num_in_flight_async_reqs: 0\n",
      "num_remote_worker_restarts: 0\n",
      "num_steps_trained_this_iter: 2048\n",
      "perf:\n",
      "  cpu_util_percent: 23.299999999999997\n",
      "  ram_util_percent: 81.64999999999999\n",
      "pid: 722\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.10823698055094927\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.2137792846328535\n",
      "  mean_inference_ms: 1.0190860478637846\n",
      "  mean_raw_obs_processing_ms: 0.3254848795544521\n",
      "sampler_results:\n",
      "  connector_metrics:\n",
      "    ObsPreprocessorConnector_ms: 0.005243778228759766\n",
      "    StateBufferConnector_ms: 0.0050694942474365234\n",
      "    ViewRequirementAgentConnector_ms: 0.11172842979431152\n",
      "  custom_metrics: {}\n",
      "  episode_len_mean: 26.34\n",
      "  episode_media: {}\n",
      "  episode_return_max: 23.0\n",
      "  episode_return_mean: -19.46\n",
      "  episode_return_min: -59.0\n",
      "  episode_reward_max: 23.0\n",
      "  episode_reward_mean: -19.46\n",
      "  episode_reward_min: -59.0\n",
      "  episodes_this_iter: 76\n",
      "  episodes_timesteps_total: 2634\n",
      "  hist_stats:\n",
      "    episode_lengths: [51, 41, 51, 14, 16, 51, 51, 8, 25, 51, 12, 18, 8, 7, 12, 51,\n",
      "      5, 28, 3, 23, 25, 3, 4, 32, 5, 51, 10, 15, 10, 51, 12, 51, 44, 40, 51, 19, 16,\n",
      "      30, 51, 45, 51, 51, 6, 13, 49, 18, 21, 6, 21, 27, 50, 45, 20, 37, 14, 37, 14,\n",
      "      5, 25, 32, 26, 2, 2, 32, 51, 23, 21, 41, 51, 2, 31, 36, 6, 33, 14, 51, 25, 10,\n",
      "      51, 41, 24, 33, 50, 14, 41, 4, 7, 26, 6, 24, 28, 51, 16, 19, 36, 23, 5, 18,\n",
      "      13, 14]\n",
      "    episode_reward: [-41.0, -30.0, -32.0, -14.0, -9.0, -52.0, -44.0, -1.0, -15.0,\n",
      "      -45.0, -3.0, -14.0, 2.0, 3.0, -1.0, -41.0, 3.0, -17.0, -4.0, -12.0, -19.0, 8.0,\n",
      "      6.0, -43.0, 5.0, -44.0, -4.0, 1.0, 0.0, -42.0, -2.0, -55.0, -45.0, -30.0, -49.0,\n",
      "      -12.0, -10.0, -19.0, -42.0, -46.0, -42.0, -43.0, 4.0, -11.0, -40.0, -10.0, -15.0,\n",
      "      1.0, -10.0, -16.0, -46.0, -45.0, -11.0, -26.0, -8.0, -34.0, -15.0, 6.0, -19.0,\n",
      "      -21.0, -16.0, 9.0, 9.0, -28.0, -40.0, -14.0, -23.0, -32.0, -48.0, 9.0, -25.0,\n",
      "      -27.0, 5.0, -44.0, -13.0, -41.0, -15.0, 1.0, -38.0, -51.0, -13.0, -22.0, -44.0,\n",
      "      -10.0, -37.0, 23.0, 3.0, -23.0, -10.0, -14.0, -17.0, -59.0, -14.0, -11.0, -26.0,\n",
      "      -19.0, 6.0, -12.0, -6.0, -9.0]\n",
      "  num_episodes: 76\n",
      "  num_faulty_episodes: 0\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10823698055094927\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.2137792846328535\n",
      "    mean_inference_ms: 1.0190860478637846\n",
      "    mean_raw_obs_processing_ms: 0.3254848795544521\n",
      "time_since_restore: 16.525810956954956\n",
      "time_this_iter_s: 4.180923700332642\n",
      "time_total_s: 16.525810956954956\n",
      "timers:\n",
      "  learn_throughput: 3084.216\n",
      "  learn_time_ms: 664.026\n",
      "  load_throughput: 3046076.096\n",
      "  load_time_ms: 0.672\n",
      "  restore_workers_time_ms: 0.023\n",
      "  sample_time_ms: 3455.824\n",
      "  synch_weights_time_ms: 4.245\n",
      "  training_iteration_time_ms: 4125.151\n",
      "  training_step_time_ms: 4125.086\n",
      "timestamp: 1715853797\n",
      "timesteps_total: 8192\n",
      "training_iteration: 4\n",
      "trial_id: default\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "[3]\n",
      "agent_timesteps_total: 12650\n",
      "connector_metrics:\n",
      "  ObsPreprocessorConnector_ms: 0.005014181137084961\n",
      "  StateBufferConnector_ms: 0.0045011043548583984\n",
      "  ViewRequirementAgentConnector_ms: 0.104461669921875\n",
      "counters:\n",
      "  num_agent_steps_sampled: 12650\n",
      "  num_agent_steps_trained: 12650\n",
      "  num_env_steps_sampled: 10240\n",
      "  num_env_steps_trained: 10240\n",
      "custom_metrics: {}\n",
      "date: 2024-05-16_12-03-21\n",
      "done: false\n",
      "env_runner_results:\n",
      "  connector_metrics:\n",
      "    ObsPreprocessorConnector_ms: 0.005014181137084961\n",
      "    StateBufferConnector_ms: 0.0045011043548583984\n",
      "    ViewRequirementAgentConnector_ms: 0.104461669921875\n",
      "  custom_metrics: {}\n",
      "  episode_len_mean: 27.42\n",
      "  episode_media: {}\n",
      "  episode_return_max: 23.0\n",
      "  episode_return_mean: -20.32\n",
      "  episode_return_min: -59.0\n",
      "  episode_reward_max: 23.0\n",
      "  episode_reward_mean: -20.32\n",
      "  episode_reward_min: -59.0\n",
      "  episodes_this_iter: 71\n",
      "  episodes_timesteps_total: 2742\n",
      "  hist_stats:\n",
      "    episode_lengths: [36, 6, 33, 14, 51, 25, 10, 51, 41, 24, 33, 50, 14, 41, 4, 7,\n",
      "      26, 6, 24, 28, 51, 16, 19, 36, 23, 5, 18, 13, 14, 15, 10, 51, 51, 46, 17, 26,\n",
      "      51, 42, 48, 1, 12, 47, 14, 51, 21, 28, 25, 24, 7, 12, 15, 26, 28, 22, 6, 9,\n",
      "      4, 13, 7, 51, 11, 22, 8, 51, 23, 6, 39, 19, 4, 46, 51, 18, 51, 51, 39, 35, 40,\n",
      "      12, 19, 46, 51, 32, 51, 19, 47, 19, 6, 51, 51, 5, 27, 33, 40, 9, 36, 51, 51,\n",
      "      51, 17, 5]\n",
      "    episode_reward: [-27.0, 5.0, -44.0, -13.0, -41.0, -15.0, 1.0, -38.0, -51.0, -13.0,\n",
      "      -22.0, -44.0, -10.0, -37.0, 23.0, 3.0, -23.0, -10.0, -14.0, -17.0, -59.0, -14.0,\n",
      "      -11.0, -26.0, -19.0, 6.0, -12.0, -6.0, -9.0, -4.0, 1.0, -47.0, -47.0, -35.0,\n",
      "      -19.0, -16.0, -47.0, -40.0, -41.0, 0.0, -4.0, -36.0, -8.0, -50.0, -11.0, -26.0,\n",
      "      -14.0, -14.0, 17.0, -9.0, -11.0, -16.0, -17.0, -13.0, 5.0, 1.0, 7.0, -3.0, -1.0,\n",
      "      -44.0, -9.0, -23.0, -2.0, -42.0, -13.0, 5.0, -28.0, -10.0, 7.0, -39.0, -42.0,\n",
      "      -7.0, -55.0, -47.0, -29.0, -27.0, -37.0, -1.0, -12.0, -39.0, -43.0, -32.0, -50.0,\n",
      "      -18.0, -38.0, -17.0, 19.0, -42.0, -55.0, 5.0, -16.0, -26.0, -49.0, 2.0, -25.0,\n",
      "      -43.0, -46.0, -44.0, -6.0, 21.0]\n",
      "  num_episodes: 71\n",
      "  num_faulty_episodes: 0\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10743312093971964\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.21202741635499586\n",
      "    mean_inference_ms: 1.0135206820953178\n",
      "    mean_raw_obs_processing_ms: 0.3240627290159888\n",
      "episode_len_mean: 27.42\n",
      "episode_media: {}\n",
      "episode_return_max: 23.0\n",
      "episode_return_mean: -20.32\n",
      "episode_return_min: -59.0\n",
      "episode_reward_max: 23.0\n",
      "episode_reward_mean: -20.32\n",
      "episode_reward_min: -59.0\n",
      "episodes_this_iter: 71\n",
      "episodes_timesteps_total: 2742\n",
      "episodes_total: 341\n",
      "hostname: LAPTOP-9AD2MD1C\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      diff_num_grad_updates_vs_sampler_policy: 44.5\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.25\n",
      "        cur_lr: 0.0010000000000000002\n",
      "        entropy: 1.5739524179034763\n",
      "        entropy_coeff: 0.5008789062499999\n",
      "        grad_gnorm: 1.9678345501422883\n",
      "        kl: 0.01514219961242137\n",
      "        policy_loss: -0.04258039179258048\n",
      "        total_loss: 7.223295280668471\n",
      "        vf_explained_var: 0.009111502435472277\n",
      "        vf_loss: 8.05044977400038\n",
      "      model: {}\n",
      "      num_agent_steps_trained: 256.0\n",
      "      num_grad_updates_lifetime: 425.5\n",
      "  num_agent_steps_sampled: 12650\n",
      "  num_agent_steps_trained: 12650\n",
      "  num_env_steps_sampled: 10240\n",
      "  num_env_steps_trained: 10240\n",
      "iterations_since_restore: 5\n",
      "node_ip: 172.18.164.188\n",
      "num_agent_steps_sampled: 12650\n",
      "num_agent_steps_sampled_lifetime: 12650\n",
      "num_agent_steps_trained: 12650\n",
      "num_env_steps_sampled: 10240\n",
      "num_env_steps_sampled_lifetime: 10240\n",
      "num_env_steps_sampled_this_iter: 2048\n",
      "num_env_steps_sampled_throughput_per_sec: 523.858272510831\n",
      "num_env_steps_trained: 10240\n",
      "num_env_steps_trained_this_iter: 2048\n",
      "num_env_steps_trained_throughput_per_sec: 523.858272510831\n",
      "num_episodes: 71\n",
      "num_faulty_episodes: 0\n",
      "num_healthy_workers: 1\n",
      "num_in_flight_async_reqs: 0\n",
      "num_remote_worker_restarts: 0\n",
      "num_steps_trained_this_iter: 2048\n",
      "perf:\n",
      "  cpu_util_percent: 25.183333333333334\n",
      "  ram_util_percent: 81.7\n",
      "pid: 722\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.10743312093971964\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.21202741635499586\n",
      "  mean_inference_ms: 1.0135206820953178\n",
      "  mean_raw_obs_processing_ms: 0.3240627290159888\n",
      "sampler_results:\n",
      "  connector_metrics:\n",
      "    ObsPreprocessorConnector_ms: 0.005014181137084961\n",
      "    StateBufferConnector_ms: 0.0045011043548583984\n",
      "    ViewRequirementAgentConnector_ms: 0.104461669921875\n",
      "  custom_metrics: {}\n",
      "  episode_len_mean: 27.42\n",
      "  episode_media: {}\n",
      "  episode_return_max: 23.0\n",
      "  episode_return_mean: -20.32\n",
      "  episode_return_min: -59.0\n",
      "  episode_reward_max: 23.0\n",
      "  episode_reward_mean: -20.32\n",
      "  episode_reward_min: -59.0\n",
      "  episodes_this_iter: 71\n",
      "  episodes_timesteps_total: 2742\n",
      "  hist_stats:\n",
      "    episode_lengths: [36, 6, 33, 14, 51, 25, 10, 51, 41, 24, 33, 50, 14, 41, 4, 7,\n",
      "      26, 6, 24, 28, 51, 16, 19, 36, 23, 5, 18, 13, 14, 15, 10, 51, 51, 46, 17, 26,\n",
      "      51, 42, 48, 1, 12, 47, 14, 51, 21, 28, 25, 24, 7, 12, 15, 26, 28, 22, 6, 9,\n",
      "      4, 13, 7, 51, 11, 22, 8, 51, 23, 6, 39, 19, 4, 46, 51, 18, 51, 51, 39, 35, 40,\n",
      "      12, 19, 46, 51, 32, 51, 19, 47, 19, 6, 51, 51, 5, 27, 33, 40, 9, 36, 51, 51,\n",
      "      51, 17, 5]\n",
      "    episode_reward: [-27.0, 5.0, -44.0, -13.0, -41.0, -15.0, 1.0, -38.0, -51.0, -13.0,\n",
      "      -22.0, -44.0, -10.0, -37.0, 23.0, 3.0, -23.0, -10.0, -14.0, -17.0, -59.0, -14.0,\n",
      "      -11.0, -26.0, -19.0, 6.0, -12.0, -6.0, -9.0, -4.0, 1.0, -47.0, -47.0, -35.0,\n",
      "      -19.0, -16.0, -47.0, -40.0, -41.0, 0.0, -4.0, -36.0, -8.0, -50.0, -11.0, -26.0,\n",
      "      -14.0, -14.0, 17.0, -9.0, -11.0, -16.0, -17.0, -13.0, 5.0, 1.0, 7.0, -3.0, -1.0,\n",
      "      -44.0, -9.0, -23.0, -2.0, -42.0, -13.0, 5.0, -28.0, -10.0, 7.0, -39.0, -42.0,\n",
      "      -7.0, -55.0, -47.0, -29.0, -27.0, -37.0, -1.0, -12.0, -39.0, -43.0, -32.0, -50.0,\n",
      "      -18.0, -38.0, -17.0, 19.0, -42.0, -55.0, 5.0, -16.0, -26.0, -49.0, 2.0, -25.0,\n",
      "      -43.0, -46.0, -44.0, -6.0, 21.0]\n",
      "  num_episodes: 71\n",
      "  num_faulty_episodes: 0\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10743312093971964\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.21202741635499586\n",
      "    mean_inference_ms: 1.0135206820953178\n",
      "    mean_raw_obs_processing_ms: 0.3240627290159888\n",
      "time_since_restore: 20.442721605300903\n",
      "time_this_iter_s: 3.9169106483459473\n",
      "time_total_s: 20.442721605300903\n",
      "timers:\n",
      "  learn_throughput: 3101.876\n",
      "  learn_time_ms: 660.246\n",
      "  load_throughput: 3271359.049\n",
      "  load_time_ms: 0.626\n",
      "  restore_workers_time_ms: 0.021\n",
      "  sample_time_ms: 3416.336\n",
      "  synch_weights_time_ms: 4.437\n",
      "  training_iteration_time_ms: 4082.014\n",
      "  training_step_time_ms: 4081.955\n",
      "timestamp: 1715853801\n",
      "timesteps_total: 10240\n",
      "training_iteration: 5\n",
      "trial_id: default\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "[4]\n",
      "agent_timesteps_total: 15237\n",
      "connector_metrics:\n",
      "  ObsPreprocessorConnector_ms: 0.004440723079265935\n",
      "  StateBufferConnector_ms: 0.0046529392204662364\n",
      "  ViewRequirementAgentConnector_ms: 0.09615492112565749\n",
      "counters:\n",
      "  num_agent_steps_sampled: 15237\n",
      "  num_agent_steps_trained: 15237\n",
      "  num_env_steps_sampled: 12288\n",
      "  num_env_steps_trained: 12288\n",
      "custom_metrics: {}\n",
      "date: 2024-05-16_12-03-25\n",
      "done: false\n",
      "env_runner_results:\n",
      "  connector_metrics:\n",
      "    ObsPreprocessorConnector_ms: 0.004440723079265935\n",
      "    StateBufferConnector_ms: 0.0046529392204662364\n",
      "    ViewRequirementAgentConnector_ms: 0.09615492112565749\n",
      "  custom_metrics: {}\n",
      "  episode_len_mean: 20.346534653465348\n",
      "  episode_media: {}\n",
      "  episode_return_max: 27.0\n",
      "  episode_return_mean: -12.96039603960396\n",
      "  episode_return_min: -88.0\n",
      "  episode_reward_max: 27.0\n",
      "  episode_reward_mean: -12.96039603960396\n",
      "  episode_reward_min: -88.0\n",
      "  episodes_this_iter: 101\n",
      "  episodes_timesteps_total: 2055\n",
      "  hist_stats:\n",
      "    episode_lengths: [51, 11, 6, 30, 38, 26, 11, 38, 27, 11, 4, 3, 44, 16, 26, 20,\n",
      "      34, 12, 43, 3, 13, 21, 4, 12, 14, 13, 6, 51, 18, 18, 18, 3, 1, 14, 31, 5, 10,\n",
      "      51, 14, 22, 28, 20, 38, 20, 7, 11, 12, 13, 3, 17, 14, 13, 35, 17, 3, 30, 23,\n",
      "      15, 47, 37, 38, 51, 8, 21, 7, 20, 11, 4, 51, 6, 33, 11, 4, 29, 10, 11, 20, 24,\n",
      "      3, 37, 22, 51, 30, 22, 15, 27, 7, 51, 2, 7, 3, 29, 6, 51, 7, 7, 36, 3, 10, 23,\n",
      "      51]\n",
      "    episode_reward: [-47.0, -2.0, 5.0, -35.0, -27.0, -30.0, -3.0, -35.0, -31.0, 0.0,\n",
      "      6.0, 7.0, -34.0, -12.0, -15.0, -12.0, -24.0, -10.0, -32.0, 7.0, -6.0, -11.0,\n",
      "      23.0, -7.0, -3.0, -4.0, 5.0, -43.0, -10.0, -10.0, -7.0, 8.0, 19.0, -8.0, -26.0,\n",
      "      4.0, -4.0, -88.0, -3.0, 7.0, -42.0, -17.0, -35.0, -12.0, 1.0, -5.0, -1.0, -4.0,\n",
      "      -4.0, -6.0, -7.0, -6.0, -27.0, -16.0, 8.0, -21.0, -28.0, -4.0, -36.0, -33.0,\n",
      "      -27.0, -43.0, 2.0, -15.0, 4.0, -14.0, -5.0, 7.0, -43.0, 19.0, -22.0, -4.0, 5.0,\n",
      "      -18.0, -2.0, -9.0, -11.0, -18.0, 7.0, -28.0, -23.0, -42.0, -21.0, -13.0, -6.0,\n",
      "      -16.0, 4.0, -45.0, 27.0, 2.0, -4.0, -24.0, -10.0, -46.0, 4.0, 0.0, -30.0, -4.0,\n",
      "      11.0, -29.0, -46.0]\n",
      "  num_episodes: 101\n",
      "  num_faulty_episodes: 0\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10533654492868927\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.20775202089120814\n",
      "    mean_inference_ms: 0.985796549155507\n",
      "    mean_raw_obs_processing_ms: 0.32181978051099236\n",
      "episode_len_mean: 20.346534653465348\n",
      "episode_media: {}\n",
      "episode_return_max: 27.0\n",
      "episode_return_mean: -12.96039603960396\n",
      "episode_return_min: -88.0\n",
      "episode_reward_max: 27.0\n",
      "episode_reward_mean: -12.96039603960396\n",
      "episode_reward_min: -88.0\n",
      "episodes_this_iter: 101\n",
      "episodes_timesteps_total: 2055\n",
      "episodes_total: 442\n",
      "hostname: LAPTOP-9AD2MD1C\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      diff_num_grad_updates_vs_sampler_policy: 49.5\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.25\n",
      "        cur_lr: 0.0010000000000000002\n",
      "        entropy: 1.5513447701931\n",
      "        entropy_coeff: 0.38232421875\n",
      "        grad_gnorm: 1.6798913598060607\n",
      "        kl: 0.014717428826075376\n",
      "        policy_loss: -0.04205482948105782\n",
      "        total_loss: 6.565159869194031\n",
      "        vf_explained_var: 0.0432019305229187\n",
      "        vf_loss: 7.1966521310806275\n",
      "      model: {}\n",
      "      num_agent_steps_trained: 256.0\n",
      "      num_grad_updates_lifetime: 520.5\n",
      "  num_agent_steps_sampled: 15237\n",
      "  num_agent_steps_trained: 15237\n",
      "  num_env_steps_sampled: 12288\n",
      "  num_env_steps_trained: 12288\n",
      "iterations_since_restore: 6\n",
      "node_ip: 172.18.164.188\n",
      "num_agent_steps_sampled: 15237\n",
      "num_agent_steps_sampled_lifetime: 15237\n",
      "num_agent_steps_trained: 15237\n",
      "num_env_steps_sampled: 12288\n",
      "num_env_steps_sampled_lifetime: 12288\n",
      "num_env_steps_sampled_this_iter: 2048\n",
      "num_env_steps_sampled_throughput_per_sec: 551.9720035611783\n",
      "num_env_steps_trained: 12288\n",
      "num_env_steps_trained_this_iter: 2048\n",
      "num_env_steps_trained_throughput_per_sec: 551.9720035611783\n",
      "num_episodes: 101\n",
      "num_faulty_episodes: 0\n",
      "num_healthy_workers: 1\n",
      "num_in_flight_async_reqs: 0\n",
      "num_remote_worker_restarts: 0\n",
      "num_steps_trained_this_iter: 2048\n",
      "perf:\n",
      "  cpu_util_percent: 21.560000000000002\n",
      "  ram_util_percent: 81.7\n",
      "pid: 722\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.10533654492868927\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.20775202089120814\n",
      "  mean_inference_ms: 0.985796549155507\n",
      "  mean_raw_obs_processing_ms: 0.32181978051099236\n",
      "sampler_results:\n",
      "  connector_metrics:\n",
      "    ObsPreprocessorConnector_ms: 0.004440723079265935\n",
      "    StateBufferConnector_ms: 0.0046529392204662364\n",
      "    ViewRequirementAgentConnector_ms: 0.09615492112565749\n",
      "  custom_metrics: {}\n",
      "  episode_len_mean: 20.346534653465348\n",
      "  episode_media: {}\n",
      "  episode_return_max: 27.0\n",
      "  episode_return_mean: -12.96039603960396\n",
      "  episode_return_min: -88.0\n",
      "  episode_reward_max: 27.0\n",
      "  episode_reward_mean: -12.96039603960396\n",
      "  episode_reward_min: -88.0\n",
      "  episodes_this_iter: 101\n",
      "  episodes_timesteps_total: 2055\n",
      "  hist_stats:\n",
      "    episode_lengths: [51, 11, 6, 30, 38, 26, 11, 38, 27, 11, 4, 3, 44, 16, 26, 20,\n",
      "      34, 12, 43, 3, 13, 21, 4, 12, 14, 13, 6, 51, 18, 18, 18, 3, 1, 14, 31, 5, 10,\n",
      "      51, 14, 22, 28, 20, 38, 20, 7, 11, 12, 13, 3, 17, 14, 13, 35, 17, 3, 30, 23,\n",
      "      15, 47, 37, 38, 51, 8, 21, 7, 20, 11, 4, 51, 6, 33, 11, 4, 29, 10, 11, 20, 24,\n",
      "      3, 37, 22, 51, 30, 22, 15, 27, 7, 51, 2, 7, 3, 29, 6, 51, 7, 7, 36, 3, 10, 23,\n",
      "      51]\n",
      "    episode_reward: [-47.0, -2.0, 5.0, -35.0, -27.0, -30.0, -3.0, -35.0, -31.0, 0.0,\n",
      "      6.0, 7.0, -34.0, -12.0, -15.0, -12.0, -24.0, -10.0, -32.0, 7.0, -6.0, -11.0,\n",
      "      23.0, -7.0, -3.0, -4.0, 5.0, -43.0, -10.0, -10.0, -7.0, 8.0, 19.0, -8.0, -26.0,\n",
      "      4.0, -4.0, -88.0, -3.0, 7.0, -42.0, -17.0, -35.0, -12.0, 1.0, -5.0, -1.0, -4.0,\n",
      "      -4.0, -6.0, -7.0, -6.0, -27.0, -16.0, 8.0, -21.0, -28.0, -4.0, -36.0, -33.0,\n",
      "      -27.0, -43.0, 2.0, -15.0, 4.0, -14.0, -5.0, 7.0, -43.0, 19.0, -22.0, -4.0, 5.0,\n",
      "      -18.0, -2.0, -9.0, -11.0, -18.0, 7.0, -28.0, -23.0, -42.0, -21.0, -13.0, -6.0,\n",
      "      -16.0, 4.0, -45.0, 27.0, 2.0, -4.0, -24.0, -10.0, -46.0, 4.0, 0.0, -30.0, -4.0,\n",
      "      11.0, -29.0, -46.0]\n",
      "  num_episodes: 101\n",
      "  num_faulty_episodes: 0\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10533654492868927\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.20775202089120814\n",
      "    mean_inference_ms: 0.985796549155507\n",
      "    mean_raw_obs_processing_ms: 0.32181978051099236\n",
      "time_since_restore: 24.160478353500366\n",
      "time_this_iter_s: 3.717756748199463\n",
      "time_total_s: 24.160478353500366\n",
      "timers:\n",
      "  learn_throughput: 3091.383\n",
      "  learn_time_ms: 662.487\n",
      "  load_throughput: 3545896.632\n",
      "  load_time_ms: 0.578\n",
      "  restore_workers_time_ms: 0.019\n",
      "  sample_time_ms: 3352.211\n",
      "  synch_weights_time_ms: 4.432\n",
      "  training_iteration_time_ms: 4020.069\n",
      "  training_step_time_ms: 4020.014\n",
      "timestamp: 1715853805\n",
      "timesteps_total: 12288\n",
      "training_iteration: 6\n",
      "trial_id: default\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "[5]\n",
      "Checkpoint saved in directory /tmp/tmpkrgoeyw6\n",
      "agent_timesteps_total: 17856\n",
      "connector_metrics:\n",
      "  ObsPreprocessorConnector_ms: 0.004900353295462472\n",
      "  StateBufferConnector_ms: 0.005202846867697579\n",
      "  ViewRequirementAgentConnector_ms: 0.1041584781238011\n",
      "counters:\n",
      "  num_agent_steps_sampled: 17856\n",
      "  num_agent_steps_trained: 17856\n",
      "  num_env_steps_sampled: 14336\n",
      "  num_env_steps_trained: 14336\n",
      "custom_metrics: {}\n",
      "date: 2024-05-16_12-03-29\n",
      "done: false\n",
      "env_runner_results:\n",
      "  connector_metrics:\n",
      "    ObsPreprocessorConnector_ms: 0.004900353295462472\n",
      "    StateBufferConnector_ms: 0.005202846867697579\n",
      "    ViewRequirementAgentConnector_ms: 0.1041584781238011\n",
      "  custom_metrics: {}\n",
      "  episode_len_mean: 18.4375\n",
      "  episode_media: {}\n",
      "  episode_return_max: 36.0\n",
      "  episode_return_mean: -10.3125\n",
      "  episode_return_min: -61.0\n",
      "  episode_reward_max: 36.0\n",
      "  episode_reward_mean: -10.3125\n",
      "  episode_reward_min: -61.0\n",
      "  episodes_this_iter: 112\n",
      "  episodes_timesteps_total: 2065\n",
      "  hist_stats:\n",
      "    episode_lengths: [46, 23, 11, 12, 31, 23, 31, 6, 5, 23, 15, 16, 14, 5, 2, 25,\n",
      "      25, 19, 16, 12, 17, 7, 11, 29, 12, 7, 27, 51, 8, 34, 28, 11, 41, 2, 9, 28, 14,\n",
      "      23, 26, 18, 15, 8, 22, 10, 7, 11, 10, 8, 8, 16, 9, 19, 8, 15, 13, 31, 3, 31,\n",
      "      17, 10, 1, 3, 7, 51, 8, 20, 29, 12, 12, 11, 21, 20, 15, 34, 20, 22, 16, 2, 18,\n",
      "      16, 5, 42, 19, 14, 35, 3, 8, 42, 14, 40, 23, 31, 10, 46, 17, 14, 51, 6, 12,\n",
      "      18, 43, 9, 7, 28, 9, 36, 3, 20, 37, 14, 14, 23]\n",
      "    episode_reward: [-39.0, -19.0, -1.0, -22.0, -23.0, -16.0, -23.0, 2.0, 21.0, -12.0,\n",
      "      -6.0, -11.0, -6.0, 21.0, 9.0, -19.0, -16.0, -18.0, -5.0, -1.0, -12.0, 2.0, -4.0,\n",
      "      -21.0, -1.0, 1.0, -17.0, -41.0, 3.0, -31.0, -20.0, -5.0, -31.0, -2.0, -1.0,\n",
      "      -20.0, 6.0, -15.0, -27.0, -7.0, -5.0, -2.0, -15.0, 0.0, 0.0, -1.0, -1.0, -14.0,\n",
      "      -2.0, -5.0, 13.0, -8.0, -3.0, -17.0, 5.0, -20.0, 7.0, -34.0, -11.0, 0.0, 19.0,\n",
      "      7.0, 4.0, -49.0, 15.0, -9.0, -18.0, -1.0, -2.0, 0.0, -13.0, -19.0, -13.0, -26.0,\n",
      "      -9.0, -16.0, -8.0, 36.0, -22.0, -12.0, 4.0, -31.0, -9.0, -10.0, -25.0, -4.0,\n",
      "      3.0, -33.0, -3.0, -44.0, -16.0, -20.0, -2.0, -50.0, -12.0, -8.0, -47.0, 19.0,\n",
      "      -3.0, -7.0, -32.0, 1.0, 17.0, -35.0, 1.0, -43.0, 8.0, -14.0, -61.0, -6.0, -5.0,\n",
      "      -12.0]\n",
      "  num_episodes: 112\n",
      "  num_faulty_episodes: 0\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10495151927666228\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.20611968808421247\n",
      "    mean_inference_ms: 0.9769814495196557\n",
      "    mean_raw_obs_processing_ms: 0.32477810549637903\n",
      "episode_len_mean: 18.4375\n",
      "episode_media: {}\n",
      "episode_return_max: 36.0\n",
      "episode_return_mean: -10.3125\n",
      "episode_return_min: -61.0\n",
      "episode_reward_max: 36.0\n",
      "episode_reward_mean: -10.3125\n",
      "episode_reward_min: -61.0\n",
      "episodes_this_iter: 112\n",
      "episodes_timesteps_total: 2065\n",
      "episodes_total: 554\n",
      "hostname: LAPTOP-9AD2MD1C\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      diff_num_grad_updates_vs_sampler_policy: 49.5\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.25\n",
      "        cur_lr: 0.0010000000000000002\n",
      "        entropy: 1.5218621134757995\n",
      "        entropy_coeff: 0.2560058593750001\n",
      "        grad_gnorm: 1.9204861223697662\n",
      "        kl: 0.017213270208354805\n",
      "        policy_loss: -0.04426620413083583\n",
      "        total_loss: 6.231049437522888\n",
      "        vf_explained_var: 0.04961710631847382\n",
      "        vf_loss: 6.660618028640747\n",
      "      model: {}\n",
      "      num_agent_steps_trained: 256.0\n",
      "      num_grad_updates_lifetime: 620.5\n",
      "  num_agent_steps_sampled: 17856\n",
      "  num_agent_steps_trained: 17856\n",
      "  num_env_steps_sampled: 14336\n",
      "  num_env_steps_trained: 14336\n",
      "iterations_since_restore: 7\n",
      "node_ip: 172.18.164.188\n",
      "num_agent_steps_sampled: 17856\n",
      "num_agent_steps_sampled_lifetime: 17856\n",
      "num_agent_steps_trained: 17856\n",
      "num_env_steps_sampled: 14336\n",
      "num_env_steps_sampled_lifetime: 14336\n",
      "num_env_steps_sampled_this_iter: 2048\n",
      "num_env_steps_sampled_throughput_per_sec: 519.2073795948012\n",
      "num_env_steps_trained: 14336\n",
      "num_env_steps_trained_this_iter: 2048\n",
      "num_env_steps_trained_throughput_per_sec: 519.2073795948012\n",
      "num_episodes: 112\n",
      "num_faulty_episodes: 0\n",
      "num_healthy_workers: 1\n",
      "num_in_flight_async_reqs: 0\n",
      "num_remote_worker_restarts: 0\n",
      "num_steps_trained_this_iter: 2048\n",
      "perf:\n",
      "  cpu_util_percent: 25.61666666666667\n",
      "  ram_util_percent: 81.78333333333333\n",
      "pid: 722\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.10495151927666228\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.20611968808421247\n",
      "  mean_inference_ms: 0.9769814495196557\n",
      "  mean_raw_obs_processing_ms: 0.32477810549637903\n",
      "sampler_results:\n",
      "  connector_metrics:\n",
      "    ObsPreprocessorConnector_ms: 0.004900353295462472\n",
      "    StateBufferConnector_ms: 0.005202846867697579\n",
      "    ViewRequirementAgentConnector_ms: 0.1041584781238011\n",
      "  custom_metrics: {}\n",
      "  episode_len_mean: 18.4375\n",
      "  episode_media: {}\n",
      "  episode_return_max: 36.0\n",
      "  episode_return_mean: -10.3125\n",
      "  episode_return_min: -61.0\n",
      "  episode_reward_max: 36.0\n",
      "  episode_reward_mean: -10.3125\n",
      "  episode_reward_min: -61.0\n",
      "  episodes_this_iter: 112\n",
      "  episodes_timesteps_total: 2065\n",
      "  hist_stats:\n",
      "    episode_lengths: [46, 23, 11, 12, 31, 23, 31, 6, 5, 23, 15, 16, 14, 5, 2, 25,\n",
      "      25, 19, 16, 12, 17, 7, 11, 29, 12, 7, 27, 51, 8, 34, 28, 11, 41, 2, 9, 28, 14,\n",
      "      23, 26, 18, 15, 8, 22, 10, 7, 11, 10, 8, 8, 16, 9, 19, 8, 15, 13, 31, 3, 31,\n",
      "      17, 10, 1, 3, 7, 51, 8, 20, 29, 12, 12, 11, 21, 20, 15, 34, 20, 22, 16, 2, 18,\n",
      "      16, 5, 42, 19, 14, 35, 3, 8, 42, 14, 40, 23, 31, 10, 46, 17, 14, 51, 6, 12,\n",
      "      18, 43, 9, 7, 28, 9, 36, 3, 20, 37, 14, 14, 23]\n",
      "    episode_reward: [-39.0, -19.0, -1.0, -22.0, -23.0, -16.0, -23.0, 2.0, 21.0, -12.0,\n",
      "      -6.0, -11.0, -6.0, 21.0, 9.0, -19.0, -16.0, -18.0, -5.0, -1.0, -12.0, 2.0, -4.0,\n",
      "      -21.0, -1.0, 1.0, -17.0, -41.0, 3.0, -31.0, -20.0, -5.0, -31.0, -2.0, -1.0,\n",
      "      -20.0, 6.0, -15.0, -27.0, -7.0, -5.0, -2.0, -15.0, 0.0, 0.0, -1.0, -1.0, -14.0,\n",
      "      -2.0, -5.0, 13.0, -8.0, -3.0, -17.0, 5.0, -20.0, 7.0, -34.0, -11.0, 0.0, 19.0,\n",
      "      7.0, 4.0, -49.0, 15.0, -9.0, -18.0, -1.0, -2.0, 0.0, -13.0, -19.0, -13.0, -26.0,\n",
      "      -9.0, -16.0, -8.0, 36.0, -22.0, -12.0, 4.0, -31.0, -9.0, -10.0, -25.0, -4.0,\n",
      "      3.0, -33.0, -3.0, -44.0, -16.0, -20.0, -2.0, -50.0, -12.0, -8.0, -47.0, 19.0,\n",
      "      -3.0, -7.0, -32.0, 1.0, 17.0, -35.0, 1.0, -43.0, 8.0, -14.0, -61.0, -6.0, -5.0,\n",
      "      -12.0]\n",
      "  num_episodes: 112\n",
      "  num_faulty_episodes: 0\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10495151927666228\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.20611968808421247\n",
      "    mean_inference_ms: 0.9769814495196557\n",
      "    mean_raw_obs_processing_ms: 0.32477810549637903\n",
      "time_since_restore: 28.110915184020996\n",
      "time_this_iter_s: 3.95043683052063\n",
      "time_total_s: 28.110915184020996\n",
      "timers:\n",
      "  learn_throughput: 3066.581\n",
      "  learn_time_ms: 667.845\n",
      "  load_throughput: 3719736.6\n",
      "  load_time_ms: 0.551\n",
      "  restore_workers_time_ms: 0.018\n",
      "  sample_time_ms: 3336.03\n",
      "  synch_weights_time_ms: 4.494\n",
      "  training_iteration_time_ms: 4009.271\n",
      "  training_step_time_ms: 4009.22\n",
      "timestamp: 1715853809\n",
      "timesteps_total: 14336\n",
      "training_iteration: 7\n",
      "trial_id: default\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "[6]\n",
      "agent_timesteps_total: 20515\n",
      "connector_metrics:\n",
      "  ObsPreprocessorConnector_ms: 0.004821029498422746\n",
      "  StateBufferConnector_ms: 0.006156859638022004\n",
      "  ViewRequirementAgentConnector_ms: 0.10858422560657528\n",
      "counters:\n",
      "  num_agent_steps_sampled: 20515\n",
      "  num_agent_steps_trained: 20515\n",
      "  num_env_steps_sampled: 16384\n",
      "  num_env_steps_trained: 16384\n",
      "custom_metrics: {}\n",
      "date: 2024-05-16_12-03-33\n",
      "done: false\n",
      "env_runner_results:\n",
      "  connector_metrics:\n",
      "    ObsPreprocessorConnector_ms: 0.004821029498422746\n",
      "    StateBufferConnector_ms: 0.006156859638022004\n",
      "    ViewRequirementAgentConnector_ms: 0.10858422560657528\n",
      "  custom_metrics: {}\n",
      "  episode_len_mean: 14.726618705035971\n",
      "  episode_media: {}\n",
      "  episode_return_max: 30.0\n",
      "  episode_return_mean: -6.374100719424461\n",
      "  episode_return_min: -43.0\n",
      "  episode_reward_max: 30.0\n",
      "  episode_reward_mean: -6.374100719424461\n",
      "  episode_reward_min: -43.0\n",
      "  episodes_this_iter: 139\n",
      "  episodes_timesteps_total: 2047\n",
      "  hist_stats:\n",
      "    episode_lengths: [34, 10, 26, 4, 17, 8, 16, 22, 1, 28, 18, 3, 11, 3, 20, 8, 19,\n",
      "      11, 24, 5, 23, 6, 15, 2, 12, 28, 25, 6, 19, 8, 26, 18, 33, 22, 9, 4, 3, 16,\n",
      "      4, 2, 5, 1, 27, 10, 7, 23, 5, 6, 8, 13, 8, 11, 29, 32, 8, 13, 7, 18, 5, 26,\n",
      "      42, 6, 1, 27, 9, 21, 21, 22, 6, 22, 4, 6, 6, 10, 13, 33, 36, 14, 51, 15, 10,\n",
      "      24, 2, 15, 12, 6, 18, 14, 6, 2, 22, 13, 24, 1, 14, 23, 13, 13, 7, 29, 8, 6,\n",
      "      12, 51, 10, 20, 1, 7, 46, 12, 7, 9, 6, 7, 16, 29, 2, 6, 14, 37, 6, 2, 12, 8,\n",
      "      39, 3, 6, 22, 14, 9, 6, 16, 19, 12, 14, 40, 8, 23, 18]\n",
      "    episode_reward: [-27.0, -3.0, -20.0, 6.0, -13.0, -2.0, -5.0, -20.0, 0.0, -37.0,\n",
      "      -10.0, 8.0, -4.0, 8.0, -17.0, 2.0, -9.0, -3.0, -13.0, 6.0, -15.0, 1.0, -5.0,\n",
      "      9.0, -2.0, -37.0, -24.0, 5.0, -8.0, 2.0, -23.0, -16.0, -23.0, -16.0, -3.0, -6.0,\n",
      "      7.0, -5.0, 23.0, 27.0, 21.0, 19.0, -26.0, -2.0, 3.0, -15.0, 3.0, 5.0, 2.0, -3.0,\n",
      "      -3.0, -2.0, -20.0, -21.0, 2.0, -2.0, -1.0, -14.0, 30.0, -20.0, -43.0, 5.0, 19.0,\n",
      "      -36.0, -2.0, -12.0, -13.0, -20.0, 5.0, -27.0, 5.0, 4.0, 2.0, 1.0, -12.0, -28.0,\n",
      "      -36.0, -4.0, -41.0, -5.0, 1.0, -21.0, 9.0, -4.0, -1.0, 5.0, -9.0, -3.0, 5.0,\n",
      "      9.0, -20.0, -6.0, -22.0, 19.0, -9.0, -16.0, -11.0, -9.0, -1.0, -21.0, 3.0, 5.0,\n",
      "      -3.0, -43.0, -2.0, -14.0, 19.0, 4.0, -35.0, -22.0, 4.0, 1.0, 3.0, 3.0, -5.0,\n",
      "      -28.0, 9.0, -10.0, -6.0, -27.0, 19.0, -2.0, -2.0, 0.0, -30.0, 8.0, 1.0, -18.0,\n",
      "      -10.0, 2.0, 5.0, -6.0, -11.0, -10.0, -5.0, -29.0, 15.0, -12.0, -8.0]\n",
      "  num_episodes: 139\n",
      "  num_faulty_episodes: 0\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10511065387929543\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.20529198726576056\n",
      "    mean_inference_ms: 0.9777220713499303\n",
      "    mean_raw_obs_processing_ms: 0.33209534145398295\n",
      "episode_len_mean: 14.726618705035971\n",
      "episode_media: {}\n",
      "episode_return_max: 30.0\n",
      "episode_return_mean: -6.374100719424461\n",
      "episode_return_min: -43.0\n",
      "episode_reward_max: 30.0\n",
      "episode_reward_mean: -6.374100719424461\n",
      "episode_reward_min: -43.0\n",
      "episodes_this_iter: 139\n",
      "episodes_timesteps_total: 2047\n",
      "episodes_total: 693\n",
      "hostname: LAPTOP-9AD2MD1C\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      diff_num_grad_updates_vs_sampler_policy: 49.5\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.25\n",
      "        cur_lr: 0.0010000000000000002\n",
      "        entropy: 1.4902705109119416\n",
      "        entropy_coeff: 0.12812500000000007\n",
      "        grad_gnorm: 2.627486537694931\n",
      "        kl: 0.021445403786916256\n",
      "        policy_loss: -0.0571377164311707\n",
      "        total_loss: 6.0236940717697145\n",
      "        vf_explained_var: 0.04032330095767975\n",
      "        vf_loss: 6.266411347389221\n",
      "      model: {}\n",
      "      num_agent_steps_trained: 256.0\n",
      "      num_grad_updates_lifetime: 720.5\n",
      "  num_agent_steps_sampled: 20515\n",
      "  num_agent_steps_trained: 20515\n",
      "  num_env_steps_sampled: 16384\n",
      "  num_env_steps_trained: 16384\n",
      "iterations_since_restore: 8\n",
      "node_ip: 172.18.164.188\n",
      "num_agent_steps_sampled: 20515\n",
      "num_agent_steps_sampled_lifetime: 20515\n",
      "num_agent_steps_trained: 20515\n",
      "num_env_steps_sampled: 16384\n",
      "num_env_steps_sampled_lifetime: 16384\n",
      "num_env_steps_sampled_this_iter: 2048\n",
      "num_env_steps_sampled_throughput_per_sec: 491.4939763778987\n",
      "num_env_steps_trained: 16384\n",
      "num_env_steps_trained_this_iter: 2048\n",
      "num_env_steps_trained_throughput_per_sec: 491.4939763778987\n",
      "num_episodes: 139\n",
      "num_faulty_episodes: 0\n",
      "num_healthy_workers: 1\n",
      "num_in_flight_async_reqs: 0\n",
      "num_remote_worker_restarts: 0\n",
      "num_steps_trained_this_iter: 2048\n",
      "perf:\n",
      "  cpu_util_percent: 23.183333333333337\n",
      "  ram_util_percent: 81.7\n",
      "pid: 722\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.10511065387929543\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.20529198726576056\n",
      "  mean_inference_ms: 0.9777220713499303\n",
      "  mean_raw_obs_processing_ms: 0.33209534145398295\n",
      "sampler_results:\n",
      "  connector_metrics:\n",
      "    ObsPreprocessorConnector_ms: 0.004821029498422746\n",
      "    StateBufferConnector_ms: 0.006156859638022004\n",
      "    ViewRequirementAgentConnector_ms: 0.10858422560657528\n",
      "  custom_metrics: {}\n",
      "  episode_len_mean: 14.726618705035971\n",
      "  episode_media: {}\n",
      "  episode_return_max: 30.0\n",
      "  episode_return_mean: -6.374100719424461\n",
      "  episode_return_min: -43.0\n",
      "  episode_reward_max: 30.0\n",
      "  episode_reward_mean: -6.374100719424461\n",
      "  episode_reward_min: -43.0\n",
      "  episodes_this_iter: 139\n",
      "  episodes_timesteps_total: 2047\n",
      "  hist_stats:\n",
      "    episode_lengths: [34, 10, 26, 4, 17, 8, 16, 22, 1, 28, 18, 3, 11, 3, 20, 8, 19,\n",
      "      11, 24, 5, 23, 6, 15, 2, 12, 28, 25, 6, 19, 8, 26, 18, 33, 22, 9, 4, 3, 16,\n",
      "      4, 2, 5, 1, 27, 10, 7, 23, 5, 6, 8, 13, 8, 11, 29, 32, 8, 13, 7, 18, 5, 26,\n",
      "      42, 6, 1, 27, 9, 21, 21, 22, 6, 22, 4, 6, 6, 10, 13, 33, 36, 14, 51, 15, 10,\n",
      "      24, 2, 15, 12, 6, 18, 14, 6, 2, 22, 13, 24, 1, 14, 23, 13, 13, 7, 29, 8, 6,\n",
      "      12, 51, 10, 20, 1, 7, 46, 12, 7, 9, 6, 7, 16, 29, 2, 6, 14, 37, 6, 2, 12, 8,\n",
      "      39, 3, 6, 22, 14, 9, 6, 16, 19, 12, 14, 40, 8, 23, 18]\n",
      "    episode_reward: [-27.0, -3.0, -20.0, 6.0, -13.0, -2.0, -5.0, -20.0, 0.0, -37.0,\n",
      "      -10.0, 8.0, -4.0, 8.0, -17.0, 2.0, -9.0, -3.0, -13.0, 6.0, -15.0, 1.0, -5.0,\n",
      "      9.0, -2.0, -37.0, -24.0, 5.0, -8.0, 2.0, -23.0, -16.0, -23.0, -16.0, -3.0, -6.0,\n",
      "      7.0, -5.0, 23.0, 27.0, 21.0, 19.0, -26.0, -2.0, 3.0, -15.0, 3.0, 5.0, 2.0, -3.0,\n",
      "      -3.0, -2.0, -20.0, -21.0, 2.0, -2.0, -1.0, -14.0, 30.0, -20.0, -43.0, 5.0, 19.0,\n",
      "      -36.0, -2.0, -12.0, -13.0, -20.0, 5.0, -27.0, 5.0, 4.0, 2.0, 1.0, -12.0, -28.0,\n",
      "      -36.0, -4.0, -41.0, -5.0, 1.0, -21.0, 9.0, -4.0, -1.0, 5.0, -9.0, -3.0, 5.0,\n",
      "      9.0, -20.0, -6.0, -22.0, 19.0, -9.0, -16.0, -11.0, -9.0, -1.0, -21.0, 3.0, 5.0,\n",
      "      -3.0, -43.0, -2.0, -14.0, 19.0, 4.0, -35.0, -22.0, 4.0, 1.0, 3.0, 3.0, -5.0,\n",
      "      -28.0, 9.0, -10.0, -6.0, -27.0, 19.0, -2.0, -2.0, 0.0, -30.0, 8.0, 1.0, -18.0,\n",
      "      -10.0, 2.0, 5.0, -6.0, -11.0, -10.0, -5.0, -29.0, 15.0, -12.0, -8.0]\n",
      "  num_episodes: 139\n",
      "  num_faulty_episodes: 0\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10511065387929543\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.20529198726576056\n",
      "    mean_inference_ms: 0.9777220713499303\n",
      "    mean_raw_obs_processing_ms: 0.33209534145398295\n",
      "time_since_restore: 32.284212827682495\n",
      "time_this_iter_s: 4.173297643661499\n",
      "time_total_s: 32.284212827682495\n",
      "timers:\n",
      "  learn_throughput: 3046.688\n",
      "  learn_time_ms: 672.205\n",
      "  load_throughput: 3970158.688\n",
      "  load_time_ms: 0.516\n",
      "  restore_workers_time_ms: 0.017\n",
      "  sample_time_ms: 3351.375\n",
      "  synch_weights_time_ms: 4.528\n",
      "  training_iteration_time_ms: 4028.974\n",
      "  training_step_time_ms: 4028.925\n",
      "timestamp: 1715853813\n",
      "timesteps_total: 16384\n",
      "training_iteration: 8\n",
      "trial_id: default\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "[7]\n",
      "agent_timesteps_total: 23228\n",
      "connector_metrics:\n",
      "  ObsPreprocessorConnector_ms: 0.005427859295373675\n",
      "  StateBufferConnector_ms: 0.0066670878180142105\n",
      "  ViewRequirementAgentConnector_ms: 0.11792991353177476\n",
      "counters:\n",
      "  num_agent_steps_sampled: 23228\n",
      "  num_agent_steps_trained: 23228\n",
      "  num_env_steps_sampled: 18432\n",
      "  num_env_steps_trained: 18432\n",
      "custom_metrics: {}\n",
      "date: 2024-05-16_12-03-37\n",
      "done: false\n",
      "env_runner_results:\n",
      "  connector_metrics:\n",
      "    ObsPreprocessorConnector_ms: 0.005427859295373675\n",
      "    StateBufferConnector_ms: 0.0066670878180142105\n",
      "    ViewRequirementAgentConnector_ms: 0.11792991353177476\n",
      "  custom_metrics: {}\n",
      "  episode_len_mean: 11.758620689655173\n",
      "  episode_media: {}\n",
      "  episode_return_max: 34.0\n",
      "  episode_return_mean: -2.442528735632184\n",
      "  episode_return_min: -45.0\n",
      "  episode_reward_max: 34.0\n",
      "  episode_reward_mean: -2.442528735632184\n",
      "  episode_reward_min: -45.0\n",
      "  episodes_this_iter: 174\n",
      "  episodes_timesteps_total: 2046\n",
      "  hist_stats:\n",
      "    episode_lengths: [16, 33, 12, 10, 3, 7, 8, 15, 9, 16, 13, 13, 9, 18, 11, 2, 2,\n",
      "      8, 2, 17, 26, 10, 15, 11, 13, 3, 8, 9, 4, 9, 4, 33, 38, 5, 6, 1, 12, 25, 8,\n",
      "      18, 27, 9, 7, 8, 13, 10, 10, 9, 22, 5, 12, 9, 2, 2, 6, 7, 13, 8, 12, 13, 10,\n",
      "      2, 4, 29, 26, 2, 8, 11, 16, 8, 16, 6, 16, 2, 1, 2, 10, 21, 7, 11, 6, 25, 5,\n",
      "      20, 32, 20, 31, 10, 18, 9, 16, 11, 7, 24, 29, 9, 13, 4, 23, 3, 4, 2, 2, 8, 4,\n",
      "      1, 7, 7, 11, 13, 23, 1, 12, 8, 33, 1, 21, 5, 5, 18, 8, 22, 24, 38, 16, 18, 5,\n",
      "      12, 14, 4, 5, 8, 8, 31, 21, 5, 12, 15, 1, 5, 11, 12, 4, 14, 17, 7, 12, 22, 20,\n",
      "      11, 14, 5, 1, 7, 12, 7, 16, 8, 11, 36, 10, 4, 23, 5, 23, 2, 8, 13, 5, 3, 8,\n",
      "      1, 14, 13]\n",
      "    episode_reward: [-6.0, -23.0, -4.0, 1.0, -4.0, 4.0, -1.0, -4.0, 13.0, -11.0, -2.0,\n",
      "      -8.0, -4.0, -13.0, -2.0, 9.0, 27.0, 3.0, 27.0, -16.0, -18.0, 1.0, -9.0, -3.0,\n",
      "      5.0, 7.0, -1.0, 1.0, 5.0, 1.0, 7.0, -4.0, -28.0, 34.0, 3.0, 19.0, -4.0, -23.0,\n",
      "      15.0, -9.0, -20.0, 0.0, 2.0, 1.0, -3.0, 11.0, -3.0, 2.0, -11.0, -8.0, -11.0,\n",
      "      1.0, 9.0, 9.0, 1.0, 3.0, -3.0, 3.0, 7.0, -5.0, 0.0, 9.0, -6.0, -32.0, -23.0,\n",
      "      9.0, -3.0, -3.0, -6.0, 2.0, -5.0, -10.0, -5.0, -2.0, 0.0, -2.0, -4.0, -15.0,\n",
      "      4.0, -1.0, 4.0, -14.0, 3.0, -13.0, -21.0, -9.0, -20.0, -4.0, -7.0, -4.0, -5.0,\n",
      "      -20.0, 1.0, -17.0, -19.0, 0.0, -4.0, 7.0, -17.0, 8.0, 6.0, 27.0, 9.0, 1.0, -6.0,\n",
      "      0.0, 3.0, 3.0, -2.0, -8.0, -12.0, 19.0, -9.0, -2.0, -23.0, 19.0, -10.0, 5.0,\n",
      "      4.0, -8.0, 2.0, -25.0, -17.0, -45.0, -8.0, -9.0, 5.0, -2.0, 3.0, 5.0, 6.0, 10.0,\n",
      "      1.0, -26.0, -18.0, 5.0, -1.0, -9.0, 19.0, 5.0, -5.0, -1.0, 7.0, -11.0, -6.0,\n",
      "      3.0, -6.0, -24.0, -10.0, -2.0, -3.0, 21.0, 0.0, -12.0, -3.0, 3.0, -5.0, 3.0,\n",
      "      9.0, -37.0, -6.0, 6.0, -12.0, 4.0, -14.0, 9.0, 3.0, -2.0, 4.0, 8.0, 0.0, 19.0,\n",
      "      -4.0, -4.0]\n",
      "  num_episodes: 174\n",
      "  num_faulty_episodes: 0\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10582586251304739\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.20555101356471167\n",
      "    mean_inference_ms: 0.9816113396810515\n",
      "    mean_raw_obs_processing_ms: 0.34172560940638336\n",
      "episode_len_mean: 11.758620689655173\n",
      "episode_media: {}\n",
      "episode_return_max: 34.0\n",
      "episode_return_mean: -2.442528735632184\n",
      "episode_return_min: -45.0\n",
      "episode_reward_max: 34.0\n",
      "episode_reward_mean: -2.442528735632184\n",
      "episode_reward_min: -45.0\n",
      "episodes_this_iter: 174\n",
      "episodes_timesteps_total: 2046\n",
      "episodes_total: 867\n",
      "hostname: LAPTOP-9AD2MD1C\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      diff_num_grad_updates_vs_sampler_policy: 49.5\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.375\n",
      "        cur_lr: 0.0010000000000000002\n",
      "        entropy: 1.4267903554439545\n",
      "        entropy_coeff: 0.0\n",
      "        grad_gnorm: 3.4616033375263213\n",
      "        kl: 0.02589604080181112\n",
      "        policy_loss: -0.06010157256387174\n",
      "        total_loss: 5.737624902725219\n",
      "        vf_explained_var: 0.06918606519699097\n",
      "        vf_loss: 5.788015513420105\n",
      "      model: {}\n",
      "      num_agent_steps_trained: 256.0\n",
      "      num_grad_updates_lifetime: 820.5\n",
      "  num_agent_steps_sampled: 23228\n",
      "  num_agent_steps_trained: 23228\n",
      "  num_env_steps_sampled: 18432\n",
      "  num_env_steps_trained: 18432\n",
      "iterations_since_restore: 9\n",
      "node_ip: 172.18.164.188\n",
      "num_agent_steps_sampled: 23228\n",
      "num_agent_steps_sampled_lifetime: 23228\n",
      "num_agent_steps_trained: 23228\n",
      "num_env_steps_sampled: 18432\n",
      "num_env_steps_sampled_lifetime: 18432\n",
      "num_env_steps_sampled_this_iter: 2048\n",
      "num_env_steps_sampled_throughput_per_sec: 476.13527488623936\n",
      "num_env_steps_trained: 18432\n",
      "num_env_steps_trained_this_iter: 2048\n",
      "num_env_steps_trained_throughput_per_sec: 476.13527488623936\n",
      "num_episodes: 174\n",
      "num_faulty_episodes: 0\n",
      "num_healthy_workers: 1\n",
      "num_in_flight_async_reqs: 0\n",
      "num_remote_worker_restarts: 0\n",
      "num_steps_trained_this_iter: 2048\n",
      "perf:\n",
      "  cpu_util_percent: 22.866666666666664\n",
      "  ram_util_percent: 81.7\n",
      "pid: 722\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.10582586251304739\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.20555101356471167\n",
      "  mean_inference_ms: 0.9816113396810515\n",
      "  mean_raw_obs_processing_ms: 0.34172560940638336\n",
      "sampler_results:\n",
      "  connector_metrics:\n",
      "    ObsPreprocessorConnector_ms: 0.005427859295373675\n",
      "    StateBufferConnector_ms: 0.0066670878180142105\n",
      "    ViewRequirementAgentConnector_ms: 0.11792991353177476\n",
      "  custom_metrics: {}\n",
      "  episode_len_mean: 11.758620689655173\n",
      "  episode_media: {}\n",
      "  episode_return_max: 34.0\n",
      "  episode_return_mean: -2.442528735632184\n",
      "  episode_return_min: -45.0\n",
      "  episode_reward_max: 34.0\n",
      "  episode_reward_mean: -2.442528735632184\n",
      "  episode_reward_min: -45.0\n",
      "  episodes_this_iter: 174\n",
      "  episodes_timesteps_total: 2046\n",
      "  hist_stats:\n",
      "    episode_lengths: [16, 33, 12, 10, 3, 7, 8, 15, 9, 16, 13, 13, 9, 18, 11, 2, 2,\n",
      "      8, 2, 17, 26, 10, 15, 11, 13, 3, 8, 9, 4, 9, 4, 33, 38, 5, 6, 1, 12, 25, 8,\n",
      "      18, 27, 9, 7, 8, 13, 10, 10, 9, 22, 5, 12, 9, 2, 2, 6, 7, 13, 8, 12, 13, 10,\n",
      "      2, 4, 29, 26, 2, 8, 11, 16, 8, 16, 6, 16, 2, 1, 2, 10, 21, 7, 11, 6, 25, 5,\n",
      "      20, 32, 20, 31, 10, 18, 9, 16, 11, 7, 24, 29, 9, 13, 4, 23, 3, 4, 2, 2, 8, 4,\n",
      "      1, 7, 7, 11, 13, 23, 1, 12, 8, 33, 1, 21, 5, 5, 18, 8, 22, 24, 38, 16, 18, 5,\n",
      "      12, 14, 4, 5, 8, 8, 31, 21, 5, 12, 15, 1, 5, 11, 12, 4, 14, 17, 7, 12, 22, 20,\n",
      "      11, 14, 5, 1, 7, 12, 7, 16, 8, 11, 36, 10, 4, 23, 5, 23, 2, 8, 13, 5, 3, 8,\n",
      "      1, 14, 13]\n",
      "    episode_reward: [-6.0, -23.0, -4.0, 1.0, -4.0, 4.0, -1.0, -4.0, 13.0, -11.0, -2.0,\n",
      "      -8.0, -4.0, -13.0, -2.0, 9.0, 27.0, 3.0, 27.0, -16.0, -18.0, 1.0, -9.0, -3.0,\n",
      "      5.0, 7.0, -1.0, 1.0, 5.0, 1.0, 7.0, -4.0, -28.0, 34.0, 3.0, 19.0, -4.0, -23.0,\n",
      "      15.0, -9.0, -20.0, 0.0, 2.0, 1.0, -3.0, 11.0, -3.0, 2.0, -11.0, -8.0, -11.0,\n",
      "      1.0, 9.0, 9.0, 1.0, 3.0, -3.0, 3.0, 7.0, -5.0, 0.0, 9.0, -6.0, -32.0, -23.0,\n",
      "      9.0, -3.0, -3.0, -6.0, 2.0, -5.0, -10.0, -5.0, -2.0, 0.0, -2.0, -4.0, -15.0,\n",
      "      4.0, -1.0, 4.0, -14.0, 3.0, -13.0, -21.0, -9.0, -20.0, -4.0, -7.0, -4.0, -5.0,\n",
      "      -20.0, 1.0, -17.0, -19.0, 0.0, -4.0, 7.0, -17.0, 8.0, 6.0, 27.0, 9.0, 1.0, -6.0,\n",
      "      0.0, 3.0, 3.0, -2.0, -8.0, -12.0, 19.0, -9.0, -2.0, -23.0, 19.0, -10.0, 5.0,\n",
      "      4.0, -8.0, 2.0, -25.0, -17.0, -45.0, -8.0, -9.0, 5.0, -2.0, 3.0, 5.0, 6.0, 10.0,\n",
      "      1.0, -26.0, -18.0, 5.0, -1.0, -9.0, 19.0, 5.0, -5.0, -1.0, 7.0, -11.0, -6.0,\n",
      "      3.0, -6.0, -24.0, -10.0, -2.0, -3.0, 21.0, 0.0, -12.0, -3.0, 3.0, -5.0, 3.0,\n",
      "      9.0, -37.0, -6.0, 6.0, -12.0, 4.0, -14.0, 9.0, 3.0, -2.0, 4.0, 8.0, 0.0, 19.0,\n",
      "      -4.0, -4.0]\n",
      "  num_episodes: 174\n",
      "  num_faulty_episodes: 0\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10582586251304739\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.20555101356471167\n",
      "    mean_inference_ms: 0.9816113396810515\n",
      "    mean_raw_obs_processing_ms: 0.34172560940638336\n",
      "time_since_restore: 36.594966173172\n",
      "time_this_iter_s: 4.310753345489502\n",
      "time_total_s: 36.594966173172\n",
      "timers:\n",
      "  learn_throughput: 3045.783\n",
      "  learn_time_ms: 672.405\n",
      "  load_throughput: 3905107.407\n",
      "  load_time_ms: 0.524\n",
      "  restore_workers_time_ms: 0.016\n",
      "  sample_time_ms: 3381.438\n",
      "  synch_weights_time_ms: 4.527\n",
      "  training_iteration_time_ms: 4059.233\n",
      "  training_step_time_ms: 4059.187\n",
      "timestamp: 1715853817\n",
      "timesteps_total: 18432\n",
      "training_iteration: 9\n",
      "trial_id: default\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "[8]\n",
      "agent_timesteps_total: 25923\n",
      "connector_metrics:\n",
      "  ObsPreprocessorConnector_ms: 0.0053725812746130905\n",
      "  StateBufferConnector_ms: 0.006957805675009023\n",
      "  ViewRequirementAgentConnector_ms: 0.1186718111452849\n",
      "counters:\n",
      "  num_agent_steps_sampled: 25923\n",
      "  num_agent_steps_trained: 25923\n",
      "  num_env_steps_sampled: 20480\n",
      "  num_env_steps_trained: 20480\n",
      "custom_metrics: {}\n",
      "date: 2024-05-16_12-03-42\n",
      "done: false\n",
      "env_runner_results:\n",
      "  connector_metrics:\n",
      "    ObsPreprocessorConnector_ms: 0.0053725812746130905\n",
      "    StateBufferConnector_ms: 0.006957805675009023\n",
      "    ViewRequirementAgentConnector_ms: 0.1186718111452849\n",
      "  custom_metrics: {}\n",
      "  episode_len_mean: 11.059782608695652\n",
      "  episode_media: {}\n",
      "  episode_return_max: 38.0\n",
      "  episode_return_mean: -1.8641304347826086\n",
      "  episode_return_min: -45.0\n",
      "  episode_reward_max: 38.0\n",
      "  episode_reward_mean: -1.8641304347826086\n",
      "  episode_reward_min: -45.0\n",
      "  episodes_this_iter: 184\n",
      "  episodes_timesteps_total: 2035\n",
      "  hist_stats:\n",
      "    episode_lengths: [13, 10, 19, 8, 8, 20, 6, 2, 5, 6, 17, 3, 5, 7, 8, 14, 3, 7,\n",
      "      8, 4, 21, 23, 15, 5, 23, 23, 7, 10, 19, 9, 1, 2, 2, 5, 6, 10, 22, 10, 15, 2,\n",
      "      14, 5, 11, 2, 20, 7, 13, 36, 4, 12, 13, 2, 15, 5, 16, 2, 14, 1, 10, 1, 5, 6,\n",
      "      8, 18, 2, 10, 15, 17, 16, 10, 15, 11, 6, 14, 6, 47, 6, 21, 2, 5, 6, 6, 5, 5,\n",
      "      16, 6, 7, 4, 13, 20, 11, 11, 13, 11, 11, 2, 14, 1, 8, 5, 22, 8, 13, 8, 4, 1,\n",
      "      9, 16, 6, 7, 18, 9, 15, 15, 5, 8, 15, 7, 23, 13, 6, 2, 4, 2, 32, 9, 5, 11, 15,\n",
      "      12, 6, 7, 11, 8, 51, 7, 6, 17, 4, 51, 16, 1, 9, 4, 12, 9, 10, 9, 6, 4, 22, 33,\n",
      "      13, 11, 10, 10, 11, 23, 4, 11, 24, 11, 33, 7, 16, 13, 27, 1, 10, 20, 8, 11,\n",
      "      8, 14, 7, 15, 5, 2, 27, 9, 11, 6, 7, 6]\n",
      "    episode_reward: [-7.0, 8.0, -12.0, -14.0, -2.0, -20.0, 19.0, 9.0, 5.0, 3.0, -6.0,\n",
      "      7.0, 4.0, -1.0, 0.0, -7.0, 8.0, 3.0, 3.0, 14.0, -13.0, -13.0, -13.0, 4.0, -13.0,\n",
      "      -30.0, 3.0, -2.0, -12.0, -1.0, 19.0, -2.0, 9.0, 6.0, 3.0, 0.0, -11.0, -18.0,\n",
      "      -8.0, 27.0, -8.0, 5.0, -3.0, 9.0, -9.0, 2.0, -6.0, -31.0, 7.0, -7.0, -2.0, 9.0,\n",
      "      -7.0, 6.0, -6.0, -2.0, -7.0, 19.0, -6.0, 19.0, 3.0, 1.0, 3.0, -7.0, -2.0, -1.0,\n",
      "      -5.0, -6.0, -6.0, -5.0, -7.0, -1.0, 5.0, -7.0, 5.0, -38.0, 3.0, -13.0, -2.0,\n",
      "      6.0, 5.0, 5.0, 4.0, 5.0, -9.0, 4.0, 3.0, 6.0, -3.0, -12.0, -5.0, 0.0, -5.0,\n",
      "      0.0, 0.0, 27.0, -3.0, 38.0, -1.0, 4.0, -22.0, 1.0, -2.0, 0.0, 6.0, 19.0, -2.0,\n",
      "      -7.0, 13.0, 0.0, -14.0, 2.0, -5.0, -6.0, 3.0, 15.0, -4.0, 3.0, -15.0, -6.0,\n",
      "      19.0, 9.0, 5.0, 9.0, -31.0, -1.0, 4.0, -3.0, -6.0, -3.0, 5.0, 4.0, 0.0, -1.0,\n",
      "      -45.0, 4.0, 4.0, -6.0, 7.0, -45.0, -5.0, 0.0, 0.0, 7.0, -3.0, -5.0, -2.0, -2.0,\n",
      "      1.0, 7.0, -13.0, -32.0, -5.0, -1.0, -1.0, -7.0, -8.0, -12.0, 5.0, 0.0, -14.0,\n",
      "      0.0, -25.0, -12.0, -13.0, -9.0, -16.0, 0.0, 1.0, -12.0, 11.0, -5.0, 2.0, -9.0,\n",
      "      2.0, -14.0, 4.0, 9.0, -23.0, 0.0, 0.0, 19.0, 4.0, 5.0]\n",
      "  num_episodes: 184\n",
      "  num_faulty_episodes: 0\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10641265745467023\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.20573776067802022\n",
      "    mean_inference_ms: 0.9864707994458733\n",
      "    mean_raw_obs_processing_ms: 0.35081229389869195\n",
      "episode_len_mean: 11.059782608695652\n",
      "episode_media: {}\n",
      "episode_return_max: 38.0\n",
      "episode_return_mean: -1.8641304347826086\n",
      "episode_return_min: -45.0\n",
      "episode_reward_max: 38.0\n",
      "episode_reward_mean: -1.8641304347826086\n",
      "episode_reward_min: -45.0\n",
      "episodes_this_iter: 184\n",
      "episodes_timesteps_total: 2035\n",
      "episodes_total: 1051\n",
      "hostname: LAPTOP-9AD2MD1C\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      diff_num_grad_updates_vs_sampler_policy: 49.5\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.5625\n",
      "        cur_lr: 0.0010000000000000002\n",
      "        entropy: 1.384682846069336\n",
      "        entropy_coeff: 0.0\n",
      "        grad_gnorm: 2.591031185388565\n",
      "        kl: 0.02081248024540704\n",
      "        policy_loss: -0.06032394622568973\n",
      "        total_loss: 5.321783022880554\n",
      "        vf_explained_var: 0.09161368548870087\n",
      "        vf_loss: 5.3703999328613286\n",
      "      model: {}\n",
      "      num_agent_steps_trained: 256.0\n",
      "      num_grad_updates_lifetime: 920.5\n",
      "  num_agent_steps_sampled: 25923\n",
      "  num_agent_steps_trained: 25923\n",
      "  num_env_steps_sampled: 20480\n",
      "  num_env_steps_trained: 20480\n",
      "iterations_since_restore: 10\n",
      "node_ip: 172.18.164.188\n",
      "num_agent_steps_sampled: 25923\n",
      "num_agent_steps_sampled_lifetime: 25923\n",
      "num_agent_steps_trained: 25923\n",
      "num_env_steps_sampled: 20480\n",
      "num_env_steps_sampled_lifetime: 20480\n",
      "num_env_steps_sampled_this_iter: 2048\n",
      "num_env_steps_sampled_throughput_per_sec: 474.5053929688592\n",
      "num_env_steps_trained: 20480\n",
      "num_env_steps_trained_this_iter: 2048\n",
      "num_env_steps_trained_throughput_per_sec: 474.5053929688592\n",
      "num_episodes: 184\n",
      "num_faulty_episodes: 0\n",
      "num_healthy_workers: 1\n",
      "num_in_flight_async_reqs: 0\n",
      "num_remote_worker_restarts: 0\n",
      "num_steps_trained_this_iter: 2048\n",
      "perf:\n",
      "  cpu_util_percent: 25.400000000000002\n",
      "  ram_util_percent: 81.7\n",
      "pid: 722\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.10641265745467023\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.20573776067802022\n",
      "  mean_inference_ms: 0.9864707994458733\n",
      "  mean_raw_obs_processing_ms: 0.35081229389869195\n",
      "sampler_results:\n",
      "  connector_metrics:\n",
      "    ObsPreprocessorConnector_ms: 0.0053725812746130905\n",
      "    StateBufferConnector_ms: 0.006957805675009023\n",
      "    ViewRequirementAgentConnector_ms: 0.1186718111452849\n",
      "  custom_metrics: {}\n",
      "  episode_len_mean: 11.059782608695652\n",
      "  episode_media: {}\n",
      "  episode_return_max: 38.0\n",
      "  episode_return_mean: -1.8641304347826086\n",
      "  episode_return_min: -45.0\n",
      "  episode_reward_max: 38.0\n",
      "  episode_reward_mean: -1.8641304347826086\n",
      "  episode_reward_min: -45.0\n",
      "  episodes_this_iter: 184\n",
      "  episodes_timesteps_total: 2035\n",
      "  hist_stats:\n",
      "    episode_lengths: [13, 10, 19, 8, 8, 20, 6, 2, 5, 6, 17, 3, 5, 7, 8, 14, 3, 7,\n",
      "      8, 4, 21, 23, 15, 5, 23, 23, 7, 10, 19, 9, 1, 2, 2, 5, 6, 10, 22, 10, 15, 2,\n",
      "      14, 5, 11, 2, 20, 7, 13, 36, 4, 12, 13, 2, 15, 5, 16, 2, 14, 1, 10, 1, 5, 6,\n",
      "      8, 18, 2, 10, 15, 17, 16, 10, 15, 11, 6, 14, 6, 47, 6, 21, 2, 5, 6, 6, 5, 5,\n",
      "      16, 6, 7, 4, 13, 20, 11, 11, 13, 11, 11, 2, 14, 1, 8, 5, 22, 8, 13, 8, 4, 1,\n",
      "      9, 16, 6, 7, 18, 9, 15, 15, 5, 8, 15, 7, 23, 13, 6, 2, 4, 2, 32, 9, 5, 11, 15,\n",
      "      12, 6, 7, 11, 8, 51, 7, 6, 17, 4, 51, 16, 1, 9, 4, 12, 9, 10, 9, 6, 4, 22, 33,\n",
      "      13, 11, 10, 10, 11, 23, 4, 11, 24, 11, 33, 7, 16, 13, 27, 1, 10, 20, 8, 11,\n",
      "      8, 14, 7, 15, 5, 2, 27, 9, 11, 6, 7, 6]\n",
      "    episode_reward: [-7.0, 8.0, -12.0, -14.0, -2.0, -20.0, 19.0, 9.0, 5.0, 3.0, -6.0,\n",
      "      7.0, 4.0, -1.0, 0.0, -7.0, 8.0, 3.0, 3.0, 14.0, -13.0, -13.0, -13.0, 4.0, -13.0,\n",
      "      -30.0, 3.0, -2.0, -12.0, -1.0, 19.0, -2.0, 9.0, 6.0, 3.0, 0.0, -11.0, -18.0,\n",
      "      -8.0, 27.0, -8.0, 5.0, -3.0, 9.0, -9.0, 2.0, -6.0, -31.0, 7.0, -7.0, -2.0, 9.0,\n",
      "      -7.0, 6.0, -6.0, -2.0, -7.0, 19.0, -6.0, 19.0, 3.0, 1.0, 3.0, -7.0, -2.0, -1.0,\n",
      "      -5.0, -6.0, -6.0, -5.0, -7.0, -1.0, 5.0, -7.0, 5.0, -38.0, 3.0, -13.0, -2.0,\n",
      "      6.0, 5.0, 5.0, 4.0, 5.0, -9.0, 4.0, 3.0, 6.0, -3.0, -12.0, -5.0, 0.0, -5.0,\n",
      "      0.0, 0.0, 27.0, -3.0, 38.0, -1.0, 4.0, -22.0, 1.0, -2.0, 0.0, 6.0, 19.0, -2.0,\n",
      "      -7.0, 13.0, 0.0, -14.0, 2.0, -5.0, -6.0, 3.0, 15.0, -4.0, 3.0, -15.0, -6.0,\n",
      "      19.0, 9.0, 5.0, 9.0, -31.0, -1.0, 4.0, -3.0, -6.0, -3.0, 5.0, 4.0, 0.0, -1.0,\n",
      "      -45.0, 4.0, 4.0, -6.0, 7.0, -45.0, -5.0, 0.0, 0.0, 7.0, -3.0, -5.0, -2.0, -2.0,\n",
      "      1.0, 7.0, -13.0, -32.0, -5.0, -1.0, -1.0, -7.0, -8.0, -12.0, 5.0, 0.0, -14.0,\n",
      "      0.0, -25.0, -12.0, -13.0, -9.0, -16.0, 0.0, 1.0, -12.0, 11.0, -5.0, 2.0, -9.0,\n",
      "      2.0, -14.0, 4.0, 9.0, -23.0, 0.0, 0.0, 19.0, 4.0, 5.0]\n",
      "  num_episodes: 184\n",
      "  num_faulty_episodes: 0\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10641265745467023\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.20573776067802022\n",
      "    mean_inference_ms: 0.9864707994458733\n",
      "    mean_raw_obs_processing_ms: 0.35081229389869195\n",
      "time_since_restore: 40.91842555999756\n",
      "time_this_iter_s: 4.3234593868255615\n",
      "time_total_s: 40.91842555999756\n",
      "timers:\n",
      "  learn_throughput: 3067.162\n",
      "  learn_time_ms: 667.718\n",
      "  load_throughput: 3871432.573\n",
      "  load_time_ms: 0.529\n",
      "  restore_workers_time_ms: 0.016\n",
      "  sample_time_ms: 3411.856\n",
      "  synch_weights_time_ms: 4.473\n",
      "  training_iteration_time_ms: 4084.918\n",
      "  training_step_time_ms: 4084.873\n",
      "timestamp: 1715853822\n",
      "timesteps_total: 20480\n",
      "training_iteration: 10\n",
      "trial_id: default\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "[9]\n"
     ]
    },
    {
     "ename": "SyntaxError",
     "evalue": "unterminated string literal (detected at line 40) (3292121196.py, line 40)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[93], line 40\u001b[0;36m\u001b[0m\n\u001b[0;31m    print(f\"iteration [{result['training_iteration']}] =>\u001b[0m\n\u001b[0m          ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m unterminated string literal (detected at line 40)\n"
     ]
    }
   ],
   "source": [
    "from ray.rllib.algorithms.ppo import PPOConfig\n",
    "from ray.tune.logger import pretty_print\n",
    "from ray.tune.registry import register_env\n",
    "from gymnasium.wrappers.time_limit import TimeLimit\n",
    "\n",
    "\n",
    "register_env(\"my_env\", lambda _: PointCoverageEnv({\"height\": 4, \"width\": 4, \"n_agents\": 2, \"n_targets\": 2, \"max_steps\": 50}))\n",
    "\n",
    "train_batch_size = 2048\n",
    "sgd_minibatch_size = 256\n",
    "num_sgd_iter = 10\n",
    "trainings = 10\n",
    "\n",
    "print(f\"number of different environment steps: {trainings*train_batch_size}\")\n",
    "\n",
    "algo2 = (\n",
    "    PPOConfig()\n",
    "    .training(gamma=0.99, \n",
    "              #lr=0.001,\n",
    "              lr_schedule=[\n",
    "                [0, 0.01],  \n",
    "                [1000, 0.001],  \n",
    "                [10000, 0.001],  \n",
    "              ],\n",
    "              kl_coeff=0.5, \n",
    "              train_batch_size=train_batch_size, \n",
    "              sgd_minibatch_size=sgd_minibatch_size, \n",
    "              num_sgd_iter=num_sgd_iter, \n",
    "              entropy_coeff_schedule = [\n",
    "                [0, 1],  # Start with relatively high entropy coefficient\n",
    "                [20480, 0],  # Gradually decrease entropy coefficient over 10,000 iterations\n",
    "              ])\n",
    "    .env_runners(num_env_runners=1)\n",
    "    .resources(num_gpus=0)\n",
    "    .environment(env=\"my_env\")\n",
    "    .build()\n",
    ")\n",
    "\n",
    "def customResultPrint(result):\n",
    "    print(f\"iteration [{result['training_iteration']}] => \n",
    "          episode_reward_mean: {result['sampler_results']['episode_reward_mean']}, \n",
    "          episode_len_mean: {result['sampler_results']['episode_len_mean']},\n",
    "          agent_steps_trained: {result['info']['num_agent_steps_trained']},\n",
    "          env_steps_trained: {result['info']['num_env_steps_trained']}\n",
    "          \")\n",
    "\n",
    "for i in range(trainings):\n",
    "    result = algo2.train()\n",
    "    customResultPrint(result)\n",
    "    #print(result[\"sampler_results\"])\n",
    "    #print(result[\"info\"][\"learner\"][\"default_policy\"][\"learner_stats\"][\"total_loss\"])\n",
    "    print(f\"[{i}]\")\n",
    "    if i % 5 == 0:\n",
    "        checkpoint_dir = algo2.save().checkpoint.path\n",
    "        print(f\"Checkpoint saved in directory {checkpoint_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[31]\n",
      "____________\n",
      "|          |\n",
      "|          |\n",
      "|          |\n",
      "|         o|\n",
      "|         x|\n",
      "|          |\n",
      "|          |\n",
      "|          |\n",
      "|         *|\n",
      "|          |\n",
      "‾‾‾‾‾‾‾‾‾‾‾‾\n",
      "{'agent-1': array([9, 8, 9, 3, 9, 8, 9, 4], dtype=int32)}\n",
      "{'agent-1': -1}\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[87], line 17\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28mprint\u001b[39m(obs)\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28mprint\u001b[39m(reward)\n\u001b[0;32m---> 17\u001b[0m time\u001b[38;5;241m.\u001b[39msleep(\u001b[38;5;241m0.5\u001b[39m)\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m terminated[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__all__\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;129;01mor\u001b[39;00m truncated[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__all__\u001b[39m\u001b[38;5;124m'\u001b[39m]:\n\u001b[1;32m     20\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from IPython.display import clear_output\n",
    "import time\n",
    "\n",
    "env = PointCoverageEnv({\"height\": 10, \"width\": 10, \"n_agents\": 2, \"n_targets\": 2})\n",
    "obs, _ = env.reset()\n",
    "env.render()\n",
    "\n",
    "for i in range(100):\n",
    "    actions = algo2.compute_actions(obs)\n",
    "    print(actions, \"\\n\")\n",
    "    obs, reward, terminated, truncated, info = env.step(actions)\n",
    "    clear_output()\n",
    "    print(f\"[{i}]\")\n",
    "    env.render()\n",
    "    print(obs)\n",
    "    print(reward)\n",
    "    time.sleep(0.5)\n",
    "\n",
    "    if terminated['__all__'] or truncated['__all__']:\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n"
     ]
    },
    {
     "ename": "NotImplementedError",
     "evalue": "Nested observation spaces are not supported (Tuple/Dict space inside Tuple/Dict space).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 6\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mstable_baselines3\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcommon\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01menv_util\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m make_vec_env\n\u001b[1;32m      5\u001b[0m env \u001b[38;5;241m=\u001b[39m PointCoverageEnv({\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mheight\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m10\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwidth\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m50\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mn_agents\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m2\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mn_targets\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m2\u001b[39m})\n\u001b[0;32m----> 6\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mPPO\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mMlpPolicy\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      7\u001b[0m model\u001b[38;5;241m.\u001b[39mlearn(total_timesteps\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m25000\u001b[39m)\n\u001b[1;32m      9\u001b[0m obs, _ \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mreset()\n",
      "File \u001b[0;32m~/anaconda3/envs/rayEnv/lib/python3.11/site-packages/stable_baselines3/ppo/ppo.py:109\u001b[0m, in \u001b[0;36mPPO.__init__\u001b[0;34m(self, policy, env, learning_rate, n_steps, batch_size, n_epochs, gamma, gae_lambda, clip_range, clip_range_vf, normalize_advantage, ent_coef, vf_coef, max_grad_norm, use_sde, sde_sample_freq, rollout_buffer_class, rollout_buffer_kwargs, target_kl, stats_window_size, tensorboard_log, policy_kwargs, verbose, seed, device, _init_setup_model)\u001b[0m\n\u001b[1;32m     80\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\n\u001b[1;32m     81\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m     82\u001b[0m     policy: Union[\u001b[38;5;28mstr\u001b[39m, Type[ActorCriticPolicy]],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    107\u001b[0m     _init_setup_model: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    108\u001b[0m ):\n\u001b[0;32m--> 109\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m    110\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpolicy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    111\u001b[0m \u001b[43m        \u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    112\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlearning_rate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    113\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    114\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgamma\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgamma\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    115\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgae_lambda\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgae_lambda\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    116\u001b[0m \u001b[43m        \u001b[49m\u001b[43ment_coef\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43ment_coef\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    117\u001b[0m \u001b[43m        \u001b[49m\u001b[43mvf_coef\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvf_coef\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    118\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_grad_norm\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_grad_norm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    119\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_sde\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_sde\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    120\u001b[0m \u001b[43m        \u001b[49m\u001b[43msde_sample_freq\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msde_sample_freq\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    121\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrollout_buffer_class\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrollout_buffer_class\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    122\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrollout_buffer_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrollout_buffer_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    123\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstats_window_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstats_window_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    124\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtensorboard_log\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtensorboard_log\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    125\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpolicy_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpolicy_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    126\u001b[0m \u001b[43m        \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    127\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    128\u001b[0m \u001b[43m        \u001b[49m\u001b[43mseed\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mseed\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    129\u001b[0m \u001b[43m        \u001b[49m\u001b[43m_init_setup_model\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    130\u001b[0m \u001b[43m        \u001b[49m\u001b[43msupported_action_spaces\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m    131\u001b[0m \u001b[43m            \u001b[49m\u001b[43mspaces\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mBox\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    132\u001b[0m \u001b[43m            \u001b[49m\u001b[43mspaces\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mDiscrete\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    133\u001b[0m \u001b[43m            \u001b[49m\u001b[43mspaces\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mMultiDiscrete\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    134\u001b[0m \u001b[43m            \u001b[49m\u001b[43mspaces\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mMultiBinary\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    135\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    136\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    138\u001b[0m     \u001b[38;5;66;03m# Sanity check, otherwise it will lead to noisy gradient and NaN\u001b[39;00m\n\u001b[1;32m    139\u001b[0m     \u001b[38;5;66;03m# because of the advantage normalization\u001b[39;00m\n\u001b[1;32m    140\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m normalize_advantage:\n",
      "File \u001b[0;32m~/anaconda3/envs/rayEnv/lib/python3.11/site-packages/stable_baselines3/common/on_policy_algorithm.py:85\u001b[0m, in \u001b[0;36mOnPolicyAlgorithm.__init__\u001b[0;34m(self, policy, env, learning_rate, n_steps, gamma, gae_lambda, ent_coef, vf_coef, max_grad_norm, use_sde, sde_sample_freq, rollout_buffer_class, rollout_buffer_kwargs, stats_window_size, tensorboard_log, monitor_wrapper, policy_kwargs, verbose, seed, device, _init_setup_model, supported_action_spaces)\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\n\u001b[1;32m     61\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m     62\u001b[0m     policy: Union[\u001b[38;5;28mstr\u001b[39m, Type[ActorCriticPolicy]],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     83\u001b[0m     supported_action_spaces: Optional[Tuple[Type[spaces\u001b[38;5;241m.\u001b[39mSpace], \u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m     84\u001b[0m ):\n\u001b[0;32m---> 85\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m     86\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpolicy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpolicy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     87\u001b[0m \u001b[43m        \u001b[49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     88\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlearning_rate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     89\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpolicy_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpolicy_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     90\u001b[0m \u001b[43m        \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     91\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     92\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_sde\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_sde\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     93\u001b[0m \u001b[43m        \u001b[49m\u001b[43msde_sample_freq\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msde_sample_freq\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     94\u001b[0m \u001b[43m        \u001b[49m\u001b[43msupport_multi_env\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     95\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmonitor_wrapper\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmonitor_wrapper\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     96\u001b[0m \u001b[43m        \u001b[49m\u001b[43mseed\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mseed\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     97\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstats_window_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstats_window_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     98\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtensorboard_log\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtensorboard_log\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     99\u001b[0m \u001b[43m        \u001b[49m\u001b[43msupported_action_spaces\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msupported_action_spaces\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    100\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    102\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_steps \u001b[38;5;241m=\u001b[39m n_steps\n\u001b[1;32m    103\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgamma \u001b[38;5;241m=\u001b[39m gamma\n",
      "File \u001b[0;32m~/anaconda3/envs/rayEnv/lib/python3.11/site-packages/stable_baselines3/common/base_class.py:169\u001b[0m, in \u001b[0;36mBaseAlgorithm.__init__\u001b[0;34m(self, policy, env, learning_rate, policy_kwargs, stats_window_size, tensorboard_log, verbose, device, support_multi_env, monitor_wrapper, seed, use_sde, sde_sample_freq, supported_action_spaces)\u001b[0m\n\u001b[1;32m    167\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m env \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    168\u001b[0m     env \u001b[38;5;241m=\u001b[39m maybe_make_env(env, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose)\n\u001b[0;32m--> 169\u001b[0m     env \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_wrap_env\u001b[49m\u001b[43m(\u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmonitor_wrapper\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    171\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobservation_space \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mobservation_space\n\u001b[1;32m    172\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maction_space \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39maction_space\n",
      "File \u001b[0;32m~/anaconda3/envs/rayEnv/lib/python3.11/site-packages/stable_baselines3/common/base_class.py:223\u001b[0m, in \u001b[0;36mBaseAlgorithm._wrap_env\u001b[0;34m(env, verbose, monitor_wrapper)\u001b[0m\n\u001b[1;32m    221\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m verbose \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m    222\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWrapping the env in a DummyVecEnv.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 223\u001b[0m     env \u001b[38;5;241m=\u001b[39m \u001b[43mDummyVecEnv\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43menv\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore[list-item, return-value]\u001b[39;00m\n\u001b[1;32m    225\u001b[0m \u001b[38;5;66;03m# Make sure that dict-spaces are not nested (not supported)\u001b[39;00m\n\u001b[1;32m    226\u001b[0m check_for_nested_spaces(env\u001b[38;5;241m.\u001b[39mobservation_space)\n",
      "File \u001b[0;32m~/anaconda3/envs/rayEnv/lib/python3.11/site-packages/stable_baselines3/common/vec_env/dummy_vec_env.py:44\u001b[0m, in \u001b[0;36mDummyVecEnv.__init__\u001b[0;34m(self, env_fns)\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mlen\u001b[39m(env_fns), env\u001b[38;5;241m.\u001b[39mobservation_space, env\u001b[38;5;241m.\u001b[39maction_space)\n\u001b[1;32m     43\u001b[0m obs_space \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mobservation_space\n\u001b[0;32m---> 44\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkeys, shapes, dtypes \u001b[38;5;241m=\u001b[39m \u001b[43mobs_space_info\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobs_space\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     46\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuf_obs \u001b[38;5;241m=\u001b[39m OrderedDict([(k, np\u001b[38;5;241m.\u001b[39mzeros((\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_envs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;28mtuple\u001b[39m(shapes[k])), dtype\u001b[38;5;241m=\u001b[39mdtypes[k])) \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkeys])\n\u001b[1;32m     47\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuf_dones \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mzeros((\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_envs,), dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mbool\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/envs/rayEnv/lib/python3.11/site-packages/stable_baselines3/common/vec_env/util.py:61\u001b[0m, in \u001b[0;36mobs_space_info\u001b[0;34m(obs_space)\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mobs_space_info\u001b[39m(obs_space: spaces\u001b[38;5;241m.\u001b[39mSpace) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[List[\u001b[38;5;28mstr\u001b[39m], Dict[Any, Tuple[\u001b[38;5;28mint\u001b[39m, \u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m]], Dict[Any, np\u001b[38;5;241m.\u001b[39mdtype]]:\n\u001b[1;32m     48\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     49\u001b[0m \u001b[38;5;124;03m    Get dict-structured information about a gym.Space.\u001b[39;00m\n\u001b[1;32m     50\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[38;5;124;03m        dtypes: a dict mapping keys to dtypes.\u001b[39;00m\n\u001b[1;32m     60\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 61\u001b[0m     \u001b[43mcheck_for_nested_spaces\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobs_space\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     62\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(obs_space, spaces\u001b[38;5;241m.\u001b[39mDict):\n\u001b[1;32m     63\u001b[0m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(obs_space\u001b[38;5;241m.\u001b[39mspaces, OrderedDict), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDict space must have ordered subspaces\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[0;32m~/anaconda3/envs/rayEnv/lib/python3.11/site-packages/stable_baselines3/common/preprocessing.py:225\u001b[0m, in \u001b[0;36mcheck_for_nested_spaces\u001b[0;34m(obs_space)\u001b[0m\n\u001b[1;32m    223\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m sub_space \u001b[38;5;129;01min\u001b[39;00m sub_spaces:\n\u001b[1;32m    224\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(sub_space, (spaces\u001b[38;5;241m.\u001b[39mDict, spaces\u001b[38;5;241m.\u001b[39mTuple)):\n\u001b[0;32m--> 225\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m(\n\u001b[1;32m    226\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNested observation spaces are not supported (Tuple/Dict space inside Tuple/Dict space).\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    227\u001b[0m         )\n",
      "\u001b[0;31mNotImplementedError\u001b[0m: Nested observation spaces are not supported (Tuple/Dict space inside Tuple/Dict space)."
     ]
    }
   ],
   "source": [
    "import gymnasium as gym\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "\n",
    "env = PointCoverageEnv({\"height\": 10, \"width\": 50, \"n_agents\": 2, \"n_targets\": 2})\n",
    "model = PPO(\"MlpPolicy\", env, verbose=1)\n",
    "model.learn(total_timesteps=25000)\n",
    "\n",
    "obs, _ = env.reset()\n",
    "while True:\n",
    "    action, _states = model.predict(obs)\n",
    "    obs, rewards, dones, info = env.step(action)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rayEnv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
