{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nicolo/anaconda3/envs/tianEnv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2024-05-17 10:58:42,008\tINFO util.py:154 -- Missing packages: ['ipywidgets']. Run `pip install -U ipywidgets`, then restart the notebook server for rich notebook output.\n",
      "2024-05-17 10:58:46,602\tINFO util.py:154 -- Missing packages: ['ipywidgets']. Run `pip install -U ipywidgets`, then restart the notebook server for rich notebook output.\n"
     ]
    }
   ],
   "source": [
    "from typing import Set\n",
    "from ray.rllib.env.multi_agent_env import MultiAgentEnv\n",
    "import random as rnd\n",
    "import math\n",
    "from gymnasium.spaces import Discrete, Box, Sequence, Dict\n",
    "from gymnasium.spaces.utils import flatten, flatten_space\n",
    "import numpy as np\n",
    "from IPython.display import clear_output\n",
    "\n",
    "from ray.rllib.utils.typing import AgentID\n",
    "\n",
    "class PointCoverageEnv(MultiAgentEnv):\n",
    "\n",
    "    actions_dict = [(0,-1),(0,1),(1,0),(-1,0),(0,0)]\n",
    "\n",
    "    def __init__(self, config):\n",
    "        self.observations_memory = config[\"observations_memory\"] if \"observations_memory\" in config.keys() else 1\n",
    "        self.width = config[\"width\"]\n",
    "        self.height = config[\"height\"]\n",
    "        self.n_agents = config[\"n_agents\"]\n",
    "        self.n_targets = config[\"n_targets\"]\n",
    "        self.max_steps = config[\"max_steps\"] if \"max_steps\" in config.keys() else None\n",
    "        self.use_nested_observation = config[\"use_nested_observation\"] if \"use_nested_observation\" in config.keys() else False\n",
    "        self.agents = ['agent-' + str(i) for i in range(self.n_agents)]\n",
    "        self.observation_space = self.observation_space('agent-0')\n",
    "        self.action_space = Discrete(5)\n",
    "\n",
    "    def unflatten_observation_space(self, agent):\n",
    "        coordinates_space = Box(low=np.array([0.0, 0.0], dtype=np.float32), high=np.array([1.0, 1.0], dtype=np.float32), dtype=np.float32)\n",
    "        obs_space = {\"position\": coordinates_space,\n",
    "                     \"targets\": Dict({f\"target-{i}\": coordinates_space for i in range(self.n_targets)})}\n",
    "        if self.n_agents > 1:\n",
    "            obs_space = {\"position\": coordinates_space,\n",
    "                        \"other_agents\": Dict({f\"other_agent-{i}\": coordinates_space for i in range(self.n_agents-1)}),\n",
    "                        \"targets\": Dict({f\"target-{i}\": coordinates_space for i in range(self.n_targets)})}\n",
    "        \n",
    "        obs_space = Dict(obs_space)\n",
    "\n",
    "        if self.observations_memory > 1:\n",
    "            return Dict({f\"t(-{i})\": obs_space for i in range(self.observations_memory)})\n",
    "        return obs_space\n",
    "\n",
    "    def observation_space(self, agent):\n",
    "       if self.use_nested_observation:\n",
    "           return self.unflatten_observation_space(agent)\n",
    "       return flatten_space(self.unflatten_observation_space(agent))\n",
    "\n",
    "    def action_space(self, agent):\n",
    "        return Discrete(5)\n",
    "    \n",
    "    def __get_other_agents(self, agent):\n",
    "        return [other for other in self.agents if other != agent]\n",
    "\n",
    "    def __get_random_point(self):\n",
    "        return (rnd.randint(0, self.width-1), rnd.randint(0, self.height-1))\n",
    "    \n",
    "    def __get_normalized_position(self, position):\n",
    "        return (position[0]/self.width, position[1]/self.height)\n",
    "\n",
    "    def __get_unflatten_time_t_observation(self, agent):\n",
    "        time_t_obs = {\"position\": self.__get_normalized_position(self.agent_pos[agent]),\n",
    "               \"targets\": {f\"target-{i}\": self.__get_normalized_position(pos) for i, pos in enumerate(self.targets)}}\n",
    "        if self.n_agents > 1:\n",
    "            time_t_obs = {\"position\": self.__get_normalized_position(self.agent_pos[agent]),\n",
    "               \"other_agents\": {f\"other_agent-{i}\": self.__get_normalized_position(self.agent_pos[other]) for i, other in enumerate(self.__get_other_agents(agent))},\n",
    "               \"targets\": {f\"target-{i}\": self.__get_normalized_position(pos) for i, pos in enumerate(self.targets)}}\n",
    "        return time_t_obs\n",
    "\n",
    "    def __get_observation(self, agent):\n",
    "        time_t_obs = self.__get_unflatten_time_t_observation(agent)\n",
    "\n",
    "        obs = {}\n",
    "        if self.observations_memory > 1:\n",
    "            self.agents_memory[agent].pop(0)\n",
    "            self.agents_memory[agent].append(time_t_obs)\n",
    "            obs = {f\"t(-{i})\": self.agents_memory[agent][self.observations_memory-1-i] for i in range(self.observations_memory)}\n",
    "        else:\n",
    "            obs = time_t_obs\n",
    "\n",
    "        if self.use_nested_observation:\n",
    "            return obs\n",
    "        return flatten(self.unflatten_observation_space(agent), obs)\n",
    "\n",
    "    def __get_not_covered_targets(self):\n",
    "        return set(self.targets) - set(self.agent_pos.values())\n",
    "\n",
    "    def __is_target_contended(self, target):\n",
    "        return len([t for t in self.agent_pos.values() if target == t]) > 1\n",
    "\n",
    "    def __get_reward(self, agent):\n",
    "        return -1 + self.__get_global_reward()\n",
    "        if self.agent_pos[agent] in self.targets:\n",
    "            if self.agent_pos[agent] in [pos[1] for pos in self.old_agent_pos if pos[0] != agent]:\n",
    "                return -1 # someone was already covering the target -> no +10 reward\n",
    "            if self.__is_target_contended(self.agent_pos[agent]):\n",
    "                return -2 # someone arrived at the target at the same time of me -> someone has to leave\n",
    "            return 10\n",
    "        else:\n",
    "            return -1\n",
    "    \n",
    "    def __get_global_reward(self):\n",
    "        return (len(self.not_covered_target) - len(set(self.not_covered_target) - set(self.agent_pos.values())))*10\n",
    "    \n",
    "    def __update_agent_position(self, agent, x, y):\n",
    "        self.agent_pos[agent] = (max(min(self.agent_pos[agent][0] + x, self.width-1), 0),\n",
    "                                 max(min(self.agent_pos[agent][1] + y, self.height-1), 0))\n",
    "\n",
    "    def reset(self, seed=None, options=None):\n",
    "        self.agent_pos = {agent: self.__get_random_point() for agent in self.agents}\n",
    "        self.targets = [self.__get_random_point() for _ in range(self.n_targets)]\n",
    "        self.not_covered_target = self.targets.copy()\n",
    "        self.steps = 0;\n",
    "        self.agents_memory = {agent: [self.__get_unflatten_time_t_observation(agent)]*self.observations_memory for agent in self.agents}\n",
    "        return {agent: self.__get_observation(agent) for agent in self.agents}, {}\n",
    "     \n",
    "    def step(self, actions):\n",
    "        self.steps += 1\n",
    "        observations, rewards, terminated, truncated, infos = {}, {}, {}, {}, {}\n",
    "\n",
    "        self.old_agent_pos = self.agent_pos.copy()\n",
    "        for agent, action in actions.items():\n",
    "            self.__update_agent_position(agent, self.actions_dict[action][0], self.actions_dict[action][1])\n",
    "\n",
    "        for agent in actions.keys():\n",
    "            if not (self.agent_pos[agent] in self.targets and not self.__is_target_contended(self.agent_pos[agent])):\n",
    "                observations[agent] = self.__get_observation(agent)\n",
    "                rewards[agent] = self.__get_reward(agent)\n",
    "                terminated[agent] = False\n",
    "                truncated[agent] = False\n",
    "                infos[agent] = {}\n",
    "        \n",
    "        if self.max_steps != None and self.steps > self.max_steps:\n",
    "            truncated['__all__'] = True\n",
    "        else:\n",
    "            truncated['__all__'] = False\n",
    "\n",
    "        self.not_covered_target = list(set(self.not_covered_target) - set(self.agent_pos.values())) \n",
    "\n",
    "        terminated['__all__'] = len(self.__get_not_covered_targets()) == 0\n",
    "        return observations, rewards, terminated, truncated, infos\n",
    "     \n",
    "    def render(self, mode='text'):\n",
    "        str = '_' * (self.width+2) + '\\n'\n",
    "        for i in range(self.height):\n",
    "            str = str + \"|\"\n",
    "            for j in range(self.width):\n",
    "                if (j,i) in self.agent_pos.values() and (j,i) in self.targets:\n",
    "                    str = str + '*'\n",
    "                elif (j,i) in self.agent_pos.values():\n",
    "                    str = str + 'o'\n",
    "                elif (j,i) in self.targets:\n",
    "                    str = str + 'x'\n",
    "                else:\n",
    "                    str = str + ' '\n",
    "            str = str + '|\\n'\n",
    "        str = str + '‾' * (self.width+2)\n",
    "        print(str)\n",
    "\n",
    "    def get_agent_ids(self):\n",
    "       return self.agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'agent-0': array([0.3, 0.2, 0.8, 0.8, 0.1, 0.3, 0.1, 0.8, 0.3, 0.2, 0.8, 0.8, 0.1,\n",
      "       0.3, 0.1, 0.8], dtype=float32), 'agent-1': array([0.8, 0.8, 0.3, 0.2, 0.1, 0.3, 0.1, 0.8, 0.8, 0.8, 0.3, 0.2, 0.1,\n",
      "       0.3, 0.1, 0.8], dtype=float32)}\n",
      "____________\n",
      "|          |\n",
      "|          |\n",
      "|   o      |\n",
      "| x        |\n",
      "|          |\n",
      "|          |\n",
      "|          |\n",
      "|          |\n",
      "| x      o |\n",
      "|          |\n",
      "‾‾‾‾‾‾‾‾‾‾‾‾\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "observations_memory = 2\n",
    "\n",
    "env = PointCoverageEnv({\"height\": 10, \"width\": 10, \"n_agents\": 2, \"n_targets\": 2, \"max_steps\": 100, \"use_nested_observation\": False, \"observations_memory\": observations_memory})\n",
    "obs, _ = env.reset() \n",
    "print(obs)\n",
    "#print(json.dumps(obs['agent-0'], indent=2))\n",
    "env.render()\n",
    "\n",
    "#obs, _, _, _, _ = env.step({'agent-0': 1, 'agent-1': 2})\n",
    "#print(json.dumps(obs['agent-0'], indent=2))\n",
    "\n",
    "#obs, _, _, _, _ = env.step({'agent-0': 1, 'agent-1': 2})\n",
    "#print(json.dumps(obs['agent-0'], indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import ray\n",
    "\n",
    "def customResultPrint(result):\n",
    "    print(f\"iteration [{result['training_iteration']}] => \" +\n",
    "          f\"episode_reward_mean: {result['sampler_results']['episode_reward_mean']}, \" +\n",
    "          f\"episode_len_mean: {result['sampler_results']['episode_len_mean']}, \" +\n",
    "          f\"agent_steps_trained: {result['info']['num_agent_steps_trained']}, \" +\n",
    "          f\"env_steps_trained: {result['info']['num_env_steps_trained']}, \" + \n",
    "          f\"entropy: {result['info']['learner']['default_policy']['learner_stats']['entropy']}, \" +\n",
    "          f\"learning_rate: {result['info']['learner']['default_policy']['learner_stats']['cur_lr']}\")\n",
    "\n",
    "#ray.shutdown()\n",
    "#ray.init()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Single agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### no memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nicolo/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/rllib/algorithms/algorithm.py:521: RayDeprecationWarning: This API is deprecated and may be removed in future Ray releases. You could suppress this warning by setting env variable PYTHONWARNINGS=\"ignore::DeprecationWarning\"\n",
      "`UnifiedLogger` will be removed in Ray 2.7.\n",
      "  return UnifiedLogger(config, logdir, loggers=None)\n",
      "/home/nicolo/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/tune/logger/unified.py:53: RayDeprecationWarning: This API is deprecated and may be removed in future Ray releases. You could suppress this warning by setting env variable PYTHONWARNINGS=\"ignore::DeprecationWarning\"\n",
      "The `JsonLogger interface is deprecated in favor of the `ray.tune.json.JsonLoggerCallback` interface and will be removed in Ray 2.7.\n",
      "  self._loggers.append(cls(self.config, self.logdir, self.trial))\n",
      "/home/nicolo/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/tune/logger/unified.py:53: RayDeprecationWarning: This API is deprecated and may be removed in future Ray releases. You could suppress this warning by setting env variable PYTHONWARNINGS=\"ignore::DeprecationWarning\"\n",
      "The `CSVLogger interface is deprecated in favor of the `ray.tune.csv.CSVLoggerCallback` interface and will be removed in Ray 2.7.\n",
      "  self._loggers.append(cls(self.config, self.logdir, self.trial))\n",
      "/home/nicolo/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/tune/logger/unified.py:53: RayDeprecationWarning: This API is deprecated and may be removed in future Ray releases. You could suppress this warning by setting env variable PYTHONWARNINGS=\"ignore::DeprecationWarning\"\n",
      "The `TBXLogger interface is deprecated in favor of the `ray.tune.tensorboardx.TBXLoggerCallback` interface and will be removed in Ray 2.7.\n",
      "  self._loggers.append(cls(self.config, self.logdir, self.trial))\n",
      "2024-05-17 08:50:20,067\tWARNING deprecation.py:50 -- DeprecationWarning: `WorkerSet(num_workers=... OR local_worker=...)` has been deprecated. Use `EnvRunnerGroup(num_env_runners=... AND local_env_runner=...)` instead. This will raise an error in the future!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of different environment steps: 61440\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-17 08:50:24,898\tWARNING util.py:61 -- Install gputil for GPU system monitoring.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration [1] => episode_reward_mean: -85.78260869565217, episode_len_mean: 86.1304347826087, agent_steps_trained: 2048, env_steps_trained: 2048, entropy: 1.6025252640247345, learning_rate: 0.0010000000000000002\n",
      "Checkpoint saved in directory /tmp/tmp7jl06sar\n",
      "iteration [2] => episode_reward_mean: -86.38297872340425, episode_len_mean: 86.65957446808511, agent_steps_trained: 4096, env_steps_trained: 4096, entropy: 1.5903215453028678, learning_rate: 0.0010000000000000002\n",
      "iteration [3] => episode_reward_mean: -76.10126582278481, episode_len_mean: 76.51898734177215, agent_steps_trained: 6144, env_steps_trained: 6144, entropy: 1.5736982330679894, learning_rate: 0.0010000000000000002\n",
      "iteration [4] => episode_reward_mean: -72.35, episode_len_mean: 72.82, agent_steps_trained: 8192, env_steps_trained: 8192, entropy: 1.5585408955812454, learning_rate: 0.0010000000000000002\n",
      "iteration [5] => episode_reward_mean: -66.82, episode_len_mean: 67.4, agent_steps_trained: 10240, env_steps_trained: 10240, entropy: 1.5345567747950555, learning_rate: 0.0010000000000000002\n",
      "iteration [6] => episode_reward_mean: -54.03, episode_len_mean: 54.79, agent_steps_trained: 12288, env_steps_trained: 12288, entropy: 1.5172324672341346, learning_rate: 0.0010000000000000002\n",
      "Checkpoint saved in directory /tmp/tmpqmdp7_gl\n",
      "iteration [7] => episode_reward_mean: -39.08, episode_len_mean: 39.99, agent_steps_trained: 14336, env_steps_trained: 14336, entropy: 1.4866899073123931, learning_rate: 0.0010000000000000002\n",
      "iteration [8] => episode_reward_mean: -30.35, episode_len_mean: 31.3, agent_steps_trained: 16384, env_steps_trained: 16384, entropy: 1.4231449097394944, learning_rate: 0.0010000000000000002\n",
      "iteration [9] => episode_reward_mean: -21.13, episode_len_mean: 22.13, agent_steps_trained: 18432, env_steps_trained: 18432, entropy: 1.3257878571748734, learning_rate: 0.0010000000000000002\n",
      "iteration [10] => episode_reward_mean: -14.73076923076923, episode_len_mean: 15.73076923076923, agent_steps_trained: 20480, env_steps_trained: 20480, entropy: 1.226938509941101, learning_rate: 0.0010000000000000002\n",
      "iteration [11] => episode_reward_mean: -11.701863354037267, episode_len_mean: 12.701863354037267, agent_steps_trained: 22528, env_steps_trained: 22528, entropy: 1.1298109516501427, learning_rate: 0.0010000000000000002\n",
      "Checkpoint saved in directory /tmp/tmpt35gj2tq\n",
      "iteration [12] => episode_reward_mean: -8.64319248826291, episode_len_mean: 9.64319248826291, agent_steps_trained: 24576, env_steps_trained: 24576, entropy: 0.9789892189204693, learning_rate: 0.0010000000000000002\n",
      "iteration [13] => episode_reward_mean: -7.641350210970464, episode_len_mean: 8.641350210970463, agent_steps_trained: 26624, env_steps_trained: 26624, entropy: 0.8242164947092533, learning_rate: 0.0010000000000000002\n",
      "iteration [14] => episode_reward_mean: -7.258064516129032, episode_len_mean: 8.258064516129032, agent_steps_trained: 28672, env_steps_trained: 28672, entropy: 0.7006458029150963, learning_rate: 0.0010000000000000002\n",
      "iteration [15] => episode_reward_mean: -6.487179487179487, episode_len_mean: 7.487179487179487, agent_steps_trained: 30720, env_steps_trained: 30720, entropy: 0.5994768179953098, learning_rate: 0.0010000000000000002\n",
      "iteration [16] => episode_reward_mean: -6.465454545454546, episode_len_mean: 7.465454545454546, agent_steps_trained: 32768, env_steps_trained: 32768, entropy: 0.5519505832344294, learning_rate: 0.0010000000000000002\n",
      "Checkpoint saved in directory /tmp/tmp2uk157is\n",
      "iteration [17] => episode_reward_mean: -6.311827956989247, episode_len_mean: 7.311827956989247, agent_steps_trained: 34816, env_steps_trained: 34816, entropy: 0.4836220685392618, learning_rate: 0.0010000000000000002\n",
      "iteration [18] => episode_reward_mean: -5.915824915824916, episode_len_mean: 6.915824915824916, agent_steps_trained: 36864, env_steps_trained: 36864, entropy: 0.45069163851439953, learning_rate: 0.0010000000000000002\n",
      "iteration [19] => episode_reward_mean: -5.979522184300341, episode_len_mean: 6.979522184300341, agent_steps_trained: 38912, env_steps_trained: 38912, entropy: 0.4269075568765402, learning_rate: 0.0010000000000000002\n",
      "iteration [20] => episode_reward_mean: -5.662337662337662, episode_len_mean: 6.662337662337662, agent_steps_trained: 40960, env_steps_trained: 40960, entropy: 0.39127405397593973, learning_rate: 0.0010000000000000002\n",
      "iteration [21] => episode_reward_mean: -5.492063492063492, episode_len_mean: 6.492063492063492, agent_steps_trained: 43008, env_steps_trained: 43008, entropy: 0.37814905643463137, learning_rate: 0.0010000000000000002\n",
      "Checkpoint saved in directory /tmp/tmp2hp9e6he\n",
      "iteration [22] => episode_reward_mean: -5.504761904761905, episode_len_mean: 6.504761904761905, agent_steps_trained: 45056, env_steps_trained: 45056, entropy: 0.3701599590480328, learning_rate: 0.0010000000000000002\n",
      "iteration [23] => episode_reward_mean: -5.823333333333333, episode_len_mean: 6.823333333333333, agent_steps_trained: 47104, env_steps_trained: 47104, entropy: 0.37212825305759906, learning_rate: 0.0010000000000000002\n",
      "iteration [24] => episode_reward_mean: -5.918918918918919, episode_len_mean: 6.918918918918919, agent_steps_trained: 49152, env_steps_trained: 49152, entropy: 0.3675144866108894, learning_rate: 0.0010000000000000002\n",
      "iteration [25] => episode_reward_mean: -5.446540880503145, episode_len_mean: 6.446540880503145, agent_steps_trained: 51200, env_steps_trained: 51200, entropy: 0.35586721301078794, learning_rate: 0.0010000000000000002\n",
      "iteration [26] => episode_reward_mean: -5.669934640522876, episode_len_mean: 6.669934640522876, agent_steps_trained: 53248, env_steps_trained: 53248, entropy: 0.3546208631247282, learning_rate: 0.0010000000000000002\n",
      "Checkpoint saved in directory /tmp/tmp6uowr_9f\n",
      "iteration [27] => episode_reward_mean: -5.215151515151515, episode_len_mean: 6.215151515151515, agent_steps_trained: 55296, env_steps_trained: 55296, entropy: 0.3612247839570045, learning_rate: 0.0010000000000000002\n",
      "iteration [28] => episode_reward_mean: -5.429467084639499, episode_len_mean: 6.429467084639499, agent_steps_trained: 57344, env_steps_trained: 57344, entropy: 0.3629027891904116, learning_rate: 0.0010000000000000002\n",
      "iteration [29] => episode_reward_mean: -5.721311475409836, episode_len_mean: 6.721311475409836, agent_steps_trained: 59392, env_steps_trained: 59392, entropy: 0.3818366225808859, learning_rate: 0.0010000000000000002\n",
      "iteration [30] => episode_reward_mean: -5.698360655737705, episode_len_mean: 6.698360655737705, agent_steps_trained: 61440, env_steps_trained: 61440, entropy: 0.3709181480109692, learning_rate: 0.0010000000000000002\n"
     ]
    }
   ],
   "source": [
    "from ray.rllib.algorithms.ppo import PPOConfig\n",
    "from ray.tune.logger import pretty_print\n",
    "from ray.tune.registry import register_env\n",
    "from gymnasium.wrappers.time_limit import TimeLimit\n",
    "\n",
    "observations_memory = 1\n",
    "register_env(\"my_env\", lambda _: PointCoverageEnv({\"height\": 10, \"width\": 10, \"n_agents\": 1, \"n_targets\": 1, \"max_steps\": 100, \"use_nested_observation\": False, \"observations_memory\": observations_memory}))\n",
    "\n",
    "train_batch_size = 2048\n",
    "sgd_minibatch_size = 256\n",
    "num_sgd_iter = 10\n",
    "trainings = 30\n",
    "\n",
    "total_env_steps = trainings*train_batch_size\n",
    "\n",
    "print(f\"number of different environment steps: {total_env_steps}\")\n",
    "\n",
    "\n",
    "algo = (\n",
    "    PPOConfig()\n",
    "    .training(gamma=0.99, \n",
    "              lr=0.001,\n",
    "              kl_coeff=0.5, \n",
    "              train_batch_size=train_batch_size, \n",
    "              sgd_minibatch_size=sgd_minibatch_size, \n",
    "              num_sgd_iter=num_sgd_iter)\n",
    "    .env_runners(num_env_runners=1)\n",
    "    .resources(num_gpus=0)\n",
    "    .environment(env=\"my_env\")\n",
    "    .build()\n",
    ")\n",
    "\n",
    "for i in range(trainings):\n",
    "    result = algo.train()\n",
    "    customResultPrint(result)\n",
    "    if i % 5 == 0:\n",
    "        checkpoint_dir = algo.save().checkpoint.path\n",
    "        print(f\"Checkpoint saved in directory {checkpoint_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### memory = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-17 08:52:10,055\tWARNING deprecation.py:50 -- DeprecationWarning: `WorkerSet(num_workers=... OR local_worker=...)` has been deprecated. Use `EnvRunnerGroup(num_env_runners=... AND local_env_runner=...)` instead. This will raise an error in the future!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of different environment steps: 61440\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-17 08:52:13,732\tWARNING util.py:61 -- Install gputil for GPU system monitoring.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration [1] => episode_reward_mean: -86.17391304347827, episode_len_mean: 86.43478260869566, agent_steps_trained: 2048, env_steps_trained: 2048, entropy: 1.6000442996621131, learning_rate: 0.0010000000000000002\n",
      "Checkpoint saved in directory /tmp/tmp5lv0z5ba\n",
      "iteration [2] => episode_reward_mean: -75.31481481481481, episode_len_mean: 75.70370370370371, agent_steps_trained: 4096, env_steps_trained: 4096, entropy: 1.5908732578158378, learning_rate: 0.0010000000000000002\n",
      "iteration [3] => episode_reward_mean: -76.29113924050633, episode_len_mean: 76.65822784810126, agent_steps_trained: 6144, env_steps_trained: 6144, entropy: 1.5680509522557258, learning_rate: 0.0010000000000000002\n",
      "iteration [4] => episode_reward_mean: -74.5, episode_len_mean: 74.91, agent_steps_trained: 8192, env_steps_trained: 8192, entropy: 1.5522387847304344, learning_rate: 0.0010000000000000002\n",
      "iteration [5] => episode_reward_mean: -68.86, episode_len_mean: 69.36, agent_steps_trained: 10240, env_steps_trained: 10240, entropy: 1.5177503615617751, learning_rate: 0.0010000000000000002\n",
      "iteration [6] => episode_reward_mean: -58.18, episode_len_mean: 58.86, agent_steps_trained: 12288, env_steps_trained: 12288, entropy: 1.5156457915902137, learning_rate: 0.0010000000000000002\n",
      "Checkpoint saved in directory /tmp/tmpqnrbfvar\n",
      "iteration [7] => episode_reward_mean: -43.15, episode_len_mean: 44.04, agent_steps_trained: 14336, env_steps_trained: 14336, entropy: 1.4703963965177536, learning_rate: 0.0010000000000000002\n",
      "iteration [8] => episode_reward_mean: -29.9, episode_len_mean: 30.9, agent_steps_trained: 16384, env_steps_trained: 16384, entropy: 1.4203141137957573, learning_rate: 0.0010000000000000002\n",
      "iteration [9] => episode_reward_mean: -18.805825242718445, episode_len_mean: 19.805825242718445, agent_steps_trained: 18432, env_steps_trained: 18432, entropy: 1.3312469080090523, learning_rate: 0.0010000000000000002\n",
      "iteration [10] => episode_reward_mean: -14.373134328358208, episode_len_mean: 15.373134328358208, agent_steps_trained: 20480, env_steps_trained: 20480, entropy: 1.2653407901525497, learning_rate: 0.0010000000000000002\n",
      "iteration [11] => episode_reward_mean: -11.406060606060606, episode_len_mean: 12.406060606060606, agent_steps_trained: 22528, env_steps_trained: 22528, entropy: 1.1287067592144013, learning_rate: 0.0010000000000000002\n",
      "Checkpoint saved in directory /tmp/tmpyyalivzf\n",
      "iteration [12] => episode_reward_mean: -9.26, episode_len_mean: 10.26, agent_steps_trained: 24576, env_steps_trained: 24576, entropy: 1.0199550978839398, learning_rate: 0.0010000000000000002\n",
      "iteration [13] => episode_reward_mean: -9.083743842364532, episode_len_mean: 10.083743842364532, agent_steps_trained: 26624, env_steps_trained: 26624, entropy: 0.9199140965938568, learning_rate: 0.0010000000000000002\n",
      "iteration [14] => episode_reward_mean: -7.682203389830509, episode_len_mean: 8.682203389830509, agent_steps_trained: 28672, env_steps_trained: 28672, entropy: 0.7803926788270473, learning_rate: 0.0010000000000000002\n",
      "iteration [15] => episode_reward_mean: -6.9921875, episode_len_mean: 7.9921875, agent_steps_trained: 30720, env_steps_trained: 30720, entropy: 0.6797437109053135, learning_rate: 0.0010000000000000002\n",
      "iteration [16] => episode_reward_mean: -6.638059701492537, episode_len_mean: 7.638059701492537, agent_steps_trained: 32768, env_steps_trained: 32768, entropy: 0.5933313466608524, learning_rate: 0.0010000000000000002\n",
      "Checkpoint saved in directory /tmp/tmp0pged8qb\n",
      "iteration [17] => episode_reward_mean: -6.498168498168498, episode_len_mean: 7.498168498168498, agent_steps_trained: 34816, env_steps_trained: 34816, entropy: 0.5466358903795481, learning_rate: 0.0010000000000000002\n",
      "iteration [18] => episode_reward_mean: -6.354838709677419, episode_len_mean: 7.354838709677419, agent_steps_trained: 36864, env_steps_trained: 36864, entropy: 0.47229198813438417, learning_rate: 0.0010000000000000002\n",
      "iteration [19] => episode_reward_mean: -5.888888888888889, episode_len_mean: 6.888888888888889, agent_steps_trained: 38912, env_steps_trained: 38912, entropy: 0.4191170774400234, learning_rate: 0.0010000000000000002\n",
      "iteration [20] => episode_reward_mean: -5.865771812080537, episode_len_mean: 6.865771812080537, agent_steps_trained: 40960, env_steps_trained: 40960, entropy: 0.4240105677396059, learning_rate: 0.0010000000000000002\n",
      "iteration [21] => episode_reward_mean: -5.803986710963455, episode_len_mean: 6.803986710963455, agent_steps_trained: 43008, env_steps_trained: 43008, entropy: 0.3924148317426443, learning_rate: 0.0010000000000000002\n",
      "Checkpoint saved in directory /tmp/tmpr1jn8dkx\n",
      "iteration [22] => episode_reward_mean: -5.4637223974763405, episode_len_mean: 6.4637223974763405, agent_steps_trained: 45056, env_steps_trained: 45056, entropy: 0.3684031777083874, learning_rate: 0.0010000000000000002\n",
      "iteration [23] => episode_reward_mean: -5.765676567656766, episode_len_mean: 6.765676567656766, agent_steps_trained: 47104, env_steps_trained: 47104, entropy: 0.35473692417144775, learning_rate: 0.0010000000000000002\n",
      "iteration [24] => episode_reward_mean: -5.764900662251655, episode_len_mean: 6.764900662251655, agent_steps_trained: 49152, env_steps_trained: 49152, entropy: 0.35000532269477846, learning_rate: 0.0010000000000000002\n",
      "iteration [25] => episode_reward_mean: -5.523809523809524, episode_len_mean: 6.523809523809524, agent_steps_trained: 51200, env_steps_trained: 51200, entropy: 0.35265416093170643, learning_rate: 0.0010000000000000002\n",
      "iteration [26] => episode_reward_mean: -5.611650485436893, episode_len_mean: 6.611650485436893, agent_steps_trained: 53248, env_steps_trained: 53248, entropy: 0.317489979788661, learning_rate: 0.0010000000000000002\n",
      "Checkpoint saved in directory /tmp/tmpoja03hme\n",
      "iteration [27] => episode_reward_mean: -5.373831775700935, episode_len_mean: 6.373831775700935, agent_steps_trained: 55296, env_steps_trained: 55296, entropy: 0.3263606283813715, learning_rate: 0.0010000000000000002\n",
      "iteration [28] => episode_reward_mean: -5.583333333333333, episode_len_mean: 6.583333333333333, agent_steps_trained: 57344, env_steps_trained: 57344, entropy: 0.3252231139689684, learning_rate: 0.0010000000000000002\n",
      "iteration [29] => episode_reward_mean: -5.6742671009771986, episode_len_mean: 6.6742671009771986, agent_steps_trained: 59392, env_steps_trained: 59392, entropy: 0.3212065491825342, learning_rate: 0.0010000000000000002\n",
      "iteration [30] => episode_reward_mean: -5.732673267326732, episode_len_mean: 6.732673267326732, agent_steps_trained: 61440, env_steps_trained: 61440, entropy: 0.3227567713707685, learning_rate: 0.0010000000000000002\n"
     ]
    }
   ],
   "source": [
    "from ray.rllib.algorithms.ppo import PPOConfig\n",
    "from ray.tune.logger import pretty_print\n",
    "from ray.tune.registry import register_env\n",
    "from gymnasium.wrappers.time_limit import TimeLimit\n",
    "\n",
    "observations_memory = 2\n",
    "register_env(\"my_env\", lambda _: PointCoverageEnv({\"height\": 10, \"width\": 10, \"n_agents\": 1, \"n_targets\": 1, \"max_steps\": 100, \"observations_memory\": observations_memory}))\n",
    "\n",
    "train_batch_size = 2048\n",
    "sgd_minibatch_size = 256\n",
    "num_sgd_iter = 10\n",
    "trainings = 30\n",
    "\n",
    "total_env_steps = trainings*train_batch_size\n",
    "\n",
    "print(f\"number of different environment steps: {total_env_steps}\")\n",
    "\n",
    "\n",
    "algo = (\n",
    "    PPOConfig()\n",
    "    .training(gamma=0.99, \n",
    "              lr=0.001,\n",
    "              kl_coeff=0.5, \n",
    "              train_batch_size=train_batch_size, \n",
    "              sgd_minibatch_size=sgd_minibatch_size, \n",
    "              num_sgd_iter=num_sgd_iter)\n",
    "    .env_runners(num_env_runners=1)\n",
    "    .resources(num_gpus=0)\n",
    "    .environment(env=\"my_env\")\n",
    "    .build()\n",
    ")\n",
    "\n",
    "for i in range(trainings):\n",
    "    result = algo.train()\n",
    "    customResultPrint(result)\n",
    "    if i % 5 == 0:\n",
    "        checkpoint_dir = algo.save().checkpoint.path\n",
    "        print(f\"Checkpoint saved in directory {checkpoint_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### memory = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-17 08:53:52,927\tWARNING deprecation.py:50 -- DeprecationWarning: `WorkerSet(num_workers=... OR local_worker=...)` has been deprecated. Use `EnvRunnerGroup(num_env_runners=... AND local_env_runner=...)` instead. This will raise an error in the future!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of different environment steps: 61440\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-17 08:53:55,942\tWARNING util.py:61 -- Install gputil for GPU system monitoring.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration [1] => episode_reward_mean: -86.95652173913044, episode_len_mean: 87.1304347826087, agent_steps_trained: 2048, env_steps_trained: 2048, entropy: 1.6030417501926422, learning_rate: 0.0010000000000000002\n",
      "Checkpoint saved in directory /tmp/tmp_dyj7nff\n",
      "iteration [2] => episode_reward_mean: -82.06122448979592, episode_len_mean: 82.3265306122449, agent_steps_trained: 4096, env_steps_trained: 4096, entropy: 1.5916752144694328, learning_rate: 0.0010000000000000002\n",
      "iteration [3] => episode_reward_mean: -75.23456790123457, episode_len_mean: 75.62962962962963, agent_steps_trained: 6144, env_steps_trained: 6144, entropy: 1.57153390198946, learning_rate: 0.0010000000000000002\n",
      "iteration [4] => episode_reward_mean: -74.33, episode_len_mean: 74.77, agent_steps_trained: 8192, env_steps_trained: 8192, entropy: 1.5667954951524734, learning_rate: 0.0010000000000000002\n",
      "iteration [5] => episode_reward_mean: -64.91, episode_len_mean: 65.53, agent_steps_trained: 10240, env_steps_trained: 10240, entropy: 1.5275139719247819, learning_rate: 0.0010000000000000002\n",
      "iteration [6] => episode_reward_mean: -58.03, episode_len_mean: 58.77, agent_steps_trained: 12288, env_steps_trained: 12288, entropy: 1.4923106491565705, learning_rate: 0.0010000000000000002\n",
      "Checkpoint saved in directory /tmp/tmpt4owihl8\n",
      "iteration [7] => episode_reward_mean: -39.37, episode_len_mean: 40.29, agent_steps_trained: 14336, env_steps_trained: 14336, entropy: 1.4456908091902734, learning_rate: 0.0010000000000000002\n",
      "iteration [8] => episode_reward_mean: -21.16, episode_len_mean: 22.16, agent_steps_trained: 16384, env_steps_trained: 16384, entropy: 1.3467460095882415, learning_rate: 0.0010000000000000002\n",
      "iteration [9] => episode_reward_mean: -15.11023622047244, episode_len_mean: 16.11023622047244, agent_steps_trained: 18432, env_steps_trained: 18432, entropy: 1.2614080786705018, learning_rate: 0.0010000000000000002\n",
      "iteration [10] => episode_reward_mean: -12.486842105263158, episode_len_mean: 13.486842105263158, agent_steps_trained: 20480, env_steps_trained: 20480, entropy: 1.129934936761856, learning_rate: 0.0010000000000000002\n",
      "iteration [11] => episode_reward_mean: -9.28643216080402, episode_len_mean: 10.28643216080402, agent_steps_trained: 22528, env_steps_trained: 22528, entropy: 1.009160263836384, learning_rate: 0.0010000000000000002\n",
      "Checkpoint saved in directory /tmp/tmp3v71gs5s\n",
      "iteration [12] => episode_reward_mean: -9.299492385786802, episode_len_mean: 10.299492385786802, agent_steps_trained: 24576, env_steps_trained: 24576, entropy: 0.9056655578315258, learning_rate: 0.0010000000000000002\n",
      "iteration [13] => episode_reward_mean: -8.16, episode_len_mean: 9.16, agent_steps_trained: 26624, env_steps_trained: 26624, entropy: 0.8214868605136871, learning_rate: 0.0010000000000000002\n",
      "iteration [14] => episode_reward_mean: -7.303643724696356, episode_len_mean: 8.303643724696355, agent_steps_trained: 28672, env_steps_trained: 28672, entropy: 0.7711746297776699, learning_rate: 0.0010000000000000002\n",
      "iteration [15] => episode_reward_mean: -7.303643724696356, episode_len_mean: 8.303643724696355, agent_steps_trained: 30720, env_steps_trained: 30720, entropy: 0.7319523341953754, learning_rate: 0.0010000000000000002\n",
      "iteration [16] => episode_reward_mean: -7.1673306772908365, episode_len_mean: 8.167330677290837, agent_steps_trained: 32768, env_steps_trained: 32768, entropy: 0.7025453463196755, learning_rate: 0.0010000000000000002\n",
      "Checkpoint saved in directory /tmp/tmpl4bjskqq\n",
      "iteration [17] => episode_reward_mean: -6.289285714285715, episode_len_mean: 7.289285714285715, agent_steps_trained: 34816, env_steps_trained: 34816, entropy: 0.6211981348693371, learning_rate: 0.0010000000000000002\n",
      "iteration [18] => episode_reward_mean: -6.329749103942652, episode_len_mean: 7.329749103942652, agent_steps_trained: 36864, env_steps_trained: 36864, entropy: 0.6346237197518348, learning_rate: 0.0010000000000000002\n",
      "iteration [19] => episode_reward_mean: -6.5661764705882355, episode_len_mean: 7.5661764705882355, agent_steps_trained: 38912, env_steps_trained: 38912, entropy: 0.5541387125849724, learning_rate: 0.0010000000000000002\n",
      "iteration [20] => episode_reward_mean: -6.3, episode_len_mean: 7.3, agent_steps_trained: 40960, env_steps_trained: 40960, entropy: 0.5034220796078444, learning_rate: 0.0010000000000000002\n",
      "iteration [21] => episode_reward_mean: -6.142857142857143, episode_len_mean: 7.142857142857143, agent_steps_trained: 43008, env_steps_trained: 43008, entropy: 0.43309403508901595, learning_rate: 0.0010000000000000002\n",
      "Checkpoint saved in directory /tmp/tmpaiqlkb_o\n",
      "iteration [22] => episode_reward_mean: -5.621359223300971, episode_len_mean: 6.621359223300971, agent_steps_trained: 45056, env_steps_trained: 45056, entropy: 0.3871869742870331, learning_rate: 0.0010000000000000002\n",
      "iteration [23] => episode_reward_mean: -6.0446735395189, episode_len_mean: 7.0446735395189, agent_steps_trained: 47104, env_steps_trained: 47104, entropy: 0.40698475353419783, learning_rate: 0.0010000000000000002\n",
      "iteration [24] => episode_reward_mean: -5.583870967741936, episode_len_mean: 6.583870967741936, agent_steps_trained: 49152, env_steps_trained: 49152, entropy: 0.38641208671033384, learning_rate: 0.0010000000000000002\n",
      "iteration [25] => episode_reward_mean: -5.612903225806452, episode_len_mean: 6.612903225806452, agent_steps_trained: 51200, env_steps_trained: 51200, entropy: 0.3562379330396652, learning_rate: 0.0010000000000000002\n",
      "iteration [26] => episode_reward_mean: -5.803986710963455, episode_len_mean: 6.803986710963455, agent_steps_trained: 53248, env_steps_trained: 53248, entropy: 0.3546981953084469, learning_rate: 0.0010000000000000002\n",
      "Checkpoint saved in directory /tmp/tmpg6p6_wny\n",
      "iteration [27] => episode_reward_mean: -5.672077922077922, episode_len_mean: 6.672077922077922, agent_steps_trained: 55296, env_steps_trained: 55296, entropy: 0.33260118775069714, learning_rate: 0.0010000000000000002\n",
      "iteration [28] => episode_reward_mean: -5.651465798045603, episode_len_mean: 6.651465798045603, agent_steps_trained: 57344, env_steps_trained: 57344, entropy: 0.350455267727375, learning_rate: 0.0010000000000000002\n",
      "iteration [29] => episode_reward_mean: -5.772277227722772, episode_len_mean: 6.772277227722772, agent_steps_trained: 59392, env_steps_trained: 59392, entropy: 0.36157243363559244, learning_rate: 0.0010000000000000002\n",
      "iteration [30] => episode_reward_mean: -5.474683544303797, episode_len_mean: 6.474683544303797, agent_steps_trained: 61440, env_steps_trained: 61440, entropy: 0.358021317794919, learning_rate: 0.0010000000000000002\n"
     ]
    }
   ],
   "source": [
    "from ray.rllib.algorithms.ppo import PPOConfig\n",
    "from ray.tune.logger import pretty_print\n",
    "from ray.tune.registry import register_env\n",
    "from gymnasium.wrappers.time_limit import TimeLimit\n",
    "\n",
    "observations_memory = 3\n",
    "register_env(\"my_env\", lambda _: PointCoverageEnv({\"height\": 10, \"width\": 10, \"n_agents\": 1, \"n_targets\": 1, \"max_steps\": 100, \"observations_memory\": observations_memory}))\n",
    "\n",
    "train_batch_size = 2048\n",
    "sgd_minibatch_size = 256\n",
    "num_sgd_iter = 10\n",
    "trainings = 30\n",
    "\n",
    "total_env_steps = trainings*train_batch_size\n",
    "\n",
    "print(f\"number of different environment steps: {total_env_steps}\")\n",
    "\n",
    "\n",
    "algo = (\n",
    "    PPOConfig()\n",
    "    .training(gamma=0.99, \n",
    "              lr=0.001,\n",
    "              kl_coeff=0.5, \n",
    "              train_batch_size=train_batch_size, \n",
    "              sgd_minibatch_size=sgd_minibatch_size, \n",
    "              num_sgd_iter=num_sgd_iter)\n",
    "    .env_runners(num_env_runners=1)\n",
    "    .resources(num_gpus=0)\n",
    "    .environment(env=\"my_env\")\n",
    "    .build()\n",
    ")\n",
    "\n",
    "for i in range(trainings):\n",
    "    result = algo.train()\n",
    "    customResultPrint(result)\n",
    "    if i % 5 == 0:\n",
    "        checkpoint_dir = algo.save().checkpoint.path\n",
    "        print(f\"Checkpoint saved in directory {checkpoint_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[40]\n",
      "______________________________________________________________________________________________________\n",
      "|                                                                                                    |\n",
      "|                                                                                                    |\n",
      "|                                                                                                    |\n",
      "|                                                                                                    |\n",
      "|                                                                                                    |\n",
      "|                                                                                                    |\n",
      "|                                                                                                    |\n",
      "|                                                                                                    |\n",
      "|                                                                                                    |\n",
      "|                                                                                                    |\n",
      "|                                                                                                    |\n",
      "|                                                                                                    |\n",
      "|                                                                                                    |\n",
      "|                                                                                                    |\n",
      "|                                                                                                    |\n",
      "|                              *                                                                     |\n",
      "|                                                                                                    |\n",
      "|                                                                                                    |\n",
      "|                                                                                                    |\n",
      "|                                                                                                    |\n",
      "‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾\n",
      "{}\n",
      "{}\n"
     ]
    }
   ],
   "source": [
    "from IPython.display import clear_output\n",
    "import time\n",
    "import torch\n",
    "from gymnasium.spaces.utils import flatten\n",
    "\n",
    "env = PointCoverageEnv({\"height\": 20, \"width\": 100, \"n_agents\": 1, \"n_targets\": 1, \"observations_memory\": observations_memory})\n",
    "obs_space = env.observation_space\n",
    "obs, _ = env.reset()\n",
    "print(obs)\n",
    "env.render()\n",
    "\n",
    "for i in range(100):\n",
    "    actions = algo.compute_actions({agent: o for agent, o in obs.items()})\n",
    "    print(actions, \"\\n\")\n",
    "    \n",
    "    obs, reward, terminated, truncated, info = env.step(actions)\n",
    "    clear_output()\n",
    "    print(f\"[{i}]\")\n",
    "    env.render()\n",
    "    print(obs)\n",
    "    print(reward)\n",
    "    time.sleep(0.5)\n",
    "\n",
    "    if terminated['__all__'] or truncated['__all__']:\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Two Agents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### no memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nicolo/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/rllib/algorithms/algorithm.py:521: RayDeprecationWarning: This API is deprecated and may be removed in future Ray releases. You could suppress this warning by setting env variable PYTHONWARNINGS=\"ignore::DeprecationWarning\"\n",
      "`UnifiedLogger` will be removed in Ray 2.7.\n",
      "  return UnifiedLogger(config, logdir, loggers=None)\n",
      "/home/nicolo/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/tune/logger/unified.py:53: RayDeprecationWarning: This API is deprecated and may be removed in future Ray releases. You could suppress this warning by setting env variable PYTHONWARNINGS=\"ignore::DeprecationWarning\"\n",
      "The `JsonLogger interface is deprecated in favor of the `ray.tune.json.JsonLoggerCallback` interface and will be removed in Ray 2.7.\n",
      "  self._loggers.append(cls(self.config, self.logdir, self.trial))\n",
      "/home/nicolo/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/tune/logger/unified.py:53: RayDeprecationWarning: This API is deprecated and may be removed in future Ray releases. You could suppress this warning by setting env variable PYTHONWARNINGS=\"ignore::DeprecationWarning\"\n",
      "The `CSVLogger interface is deprecated in favor of the `ray.tune.csv.CSVLoggerCallback` interface and will be removed in Ray 2.7.\n",
      "  self._loggers.append(cls(self.config, self.logdir, self.trial))\n",
      "/home/nicolo/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/tune/logger/unified.py:53: RayDeprecationWarning: This API is deprecated and may be removed in future Ray releases. You could suppress this warning by setting env variable PYTHONWARNINGS=\"ignore::DeprecationWarning\"\n",
      "The `TBXLogger interface is deprecated in favor of the `ray.tune.tensorboardx.TBXLoggerCallback` interface and will be removed in Ray 2.7.\n",
      "  self._loggers.append(cls(self.config, self.logdir, self.trial))\n",
      "2024-05-17 08:57:16,799\tWARNING deprecation.py:50 -- DeprecationWarning: `WorkerSet(num_workers=... OR local_worker=...)` has been deprecated. Use `EnvRunnerGroup(num_env_runners=... AND local_env_runner=...)` instead. This will raise an error in the future!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of different environment steps: 102400\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-17 08:57:20,990\tWARNING util.py:61 -- Install gputil for GPU system monitoring.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration [1] => episode_reward_mean: -30.39189189189189, episode_len_mean: 27.60810810810811, agent_steps_trained: 3010, env_steps_trained: 2048, entropy: 1.6090558713132685, learning_rate: 0.004999999999999999\n",
      "Checkpoint saved in directory /tmp/tmp_7zdus8e\n",
      "iteration [2] => episode_reward_mean: -28.07, episode_len_mean: 26.64, agent_steps_trained: 5954, env_steps_trained: 4096, entropy: 1.5991164966063065, learning_rate: 0.0010000000000000005\n",
      "iteration [3] => episode_reward_mean: -27.16, episode_len_mean: 27.11, agent_steps_trained: 8806, env_steps_trained: 6144, entropy: 1.5966651743108575, learning_rate: 0.0010000000000000005\n",
      "iteration [4] => episode_reward_mean: -20.95, episode_len_mean: 24.35, agent_steps_trained: 11559, env_steps_trained: 8192, entropy: 1.5901681077480316, learning_rate: 0.0010000000000000002\n",
      "iteration [5] => episode_reward_mean: -21.06, episode_len_mean: 23.63, agent_steps_trained: 14426, env_steps_trained: 10240, entropy: 1.5812660033052617, learning_rate: 0.0010000000000000005\n",
      "iteration [6] => episode_reward_mean: -19.85, episode_len_mean: 22.84, agent_steps_trained: 17219, env_steps_trained: 12288, entropy: 1.5711101615428924, learning_rate: 0.0010000000000000002\n",
      "Checkpoint saved in directory /tmp/tmpoeiscai7\n",
      "iteration [7] => episode_reward_mean: -17.27, episode_len_mean: 21.76, agent_steps_trained: 19958, env_steps_trained: 14336, entropy: 1.5661015176773072, learning_rate: 0.0010000000000000002\n",
      "iteration [8] => episode_reward_mean: -18.51, episode_len_mean: 22.63, agent_steps_trained: 22727, env_steps_trained: 16384, entropy: 1.5661302387714386, learning_rate: 0.0010000000000000002\n",
      "iteration [9] => episode_reward_mean: -17.28, episode_len_mean: 21.84, agent_steps_trained: 25429, env_steps_trained: 18432, entropy: 1.566563730239868, learning_rate: 0.0010000000000000002\n",
      "iteration [10] => episode_reward_mean: -17.46, episode_len_mean: 22.08, agent_steps_trained: 28201, env_steps_trained: 20480, entropy: 1.5514474499225617, learning_rate: 0.0010000000000000002\n",
      "iteration [11] => episode_reward_mean: -16.86, episode_len_mean: 22.2, agent_steps_trained: 30938, env_steps_trained: 22528, entropy: 1.5460438406467438, learning_rate: 0.0010000000000000002\n",
      "Checkpoint saved in directory /tmp/tmptcul4m_v\n",
      "iteration [12] => episode_reward_mean: -13.36, episode_len_mean: 20.57, agent_steps_trained: 33528, env_steps_trained: 24576, entropy: 1.5300699877738952, learning_rate: 0.0010000000000000002\n",
      "iteration [13] => episode_reward_mean: -12.131578947368421, episode_len_mean: 18.06140350877193, agent_steps_trained: 36352, env_steps_trained: 26624, entropy: 1.5113409746776927, learning_rate: 0.0010000000000000005\n",
      "iteration [14] => episode_reward_mean: -13.554545454545455, episode_len_mean: 18.536363636363635, agent_steps_trained: 39203, env_steps_trained: 28672, entropy: 1.484479575807398, learning_rate: 0.0010000000000000005\n",
      "iteration [15] => episode_reward_mean: -8.126865671641792, episode_len_mean: 15.343283582089553, agent_steps_trained: 41985, env_steps_trained: 30720, entropy: 1.451199643611908, learning_rate: 0.0010000000000000002\n",
      "iteration [16] => episode_reward_mean: -7.13986013986014, episode_len_mean: 14.321678321678322, agent_steps_trained: 44820, env_steps_trained: 32768, entropy: 1.4267808654091574, learning_rate: 0.0010000000000000005\n",
      "Checkpoint saved in directory /tmp/tmpppy_clfc\n",
      "iteration [17] => episode_reward_mean: -5.993548387096774, episode_len_mean: 13.225806451612904, agent_steps_trained: 47613, env_steps_trained: 34816, entropy: 1.380331621170044, learning_rate: 0.0010000000000000002\n",
      "iteration [18] => episode_reward_mean: -2.0591397849462365, episode_len_mean: 10.96774193548387, agent_steps_trained: 50409, env_steps_trained: 36864, entropy: 1.3439351963996886, learning_rate: 0.0010000000000000002\n",
      "iteration [19] => episode_reward_mean: -2.481675392670157, episode_len_mean: 10.732984293193716, agent_steps_trained: 53292, env_steps_trained: 38912, entropy: 1.3114652785387906, learning_rate: 0.0010000000000000005\n",
      "iteration [20] => episode_reward_mean: -0.8082191780821918, episode_len_mean: 9.360730593607306, agent_steps_trained: 56206, env_steps_trained: 40960, entropy: 1.2448629866946828, learning_rate: 0.0010000000000000005\n",
      "iteration [21] => episode_reward_mean: -1.3203463203463204, episode_len_mean: 8.865800865800866, agent_steps_trained: 59225, env_steps_trained: 43008, entropy: 1.2314793760126288, learning_rate: 0.0010000000000000005\n",
      "Checkpoint saved in directory /tmp/tmpr3_69zhf\n",
      "iteration [22] => episode_reward_mean: 0.3836734693877551, episode_len_mean: 8.326530612244898, agent_steps_trained: 62156, env_steps_trained: 45056, entropy: 1.1997897126457908, learning_rate: 0.0010000000000000005\n",
      "iteration [23] => episode_reward_mean: 0.8918918918918919, episode_len_mean: 7.934362934362935, agent_steps_trained: 65155, env_steps_trained: 47104, entropy: 1.1493902748281306, learning_rate: 0.0010000000000000005\n",
      "iteration [24] => episode_reward_mean: 1.325088339222615, episode_len_mean: 7.240282685512367, agent_steps_trained: 68226, env_steps_trained: 49152, entropy: 1.1120187835259872, learning_rate: 0.0010000000000000005\n",
      "iteration [25] => episode_reward_mean: 1.0680272108843538, episode_len_mean: 6.965986394557823, agent_steps_trained: 71408, env_steps_trained: 51200, entropy: 1.0977041761080424, learning_rate: 0.0010000000000000005\n",
      "iteration [26] => episode_reward_mean: 1.4467353951890034, episode_len_mean: 7.013745704467354, agent_steps_trained: 74539, env_steps_trained: 53248, entropy: 1.0782014101743698, learning_rate: 0.0010000000000000005\n",
      "Checkpoint saved in directory /tmp/tmp_bxlt46n\n",
      "iteration [27] => episode_reward_mean: 2.691131498470948, episode_len_mean: 6.290519877675841, agent_steps_trained: 77662, env_steps_trained: 55296, entropy: 1.0238485510150592, learning_rate: 0.0010000000000000005\n",
      "iteration [28] => episode_reward_mean: 2.7018072289156625, episode_len_mean: 6.174698795180723, agent_steps_trained: 80759, env_steps_trained: 57344, entropy: 1.0058910702665647, learning_rate: 0.0010000000000000005\n",
      "iteration [29] => episode_reward_mean: 2.0952380952380953, episode_len_mean: 6.498412698412698, agent_steps_trained: 83864, env_steps_trained: 59392, entropy: 0.9937953104575475, learning_rate: 0.0010000000000000005\n",
      "iteration [30] => episode_reward_mean: 2.206896551724138, episode_len_mean: 6.407523510971787, agent_steps_trained: 87116, env_steps_trained: 61440, entropy: 1.0002395828564963, learning_rate: 0.0010000000000000005\n",
      "iteration [31] => episode_reward_mean: 2.0384615384615383, episode_len_mean: 6.573717948717949, agent_steps_trained: 90313, env_steps_trained: 63488, entropy: 0.9934808472792308, learning_rate: 0.0010000000000000005\n",
      "Checkpoint saved in directory /tmp/tmp7ahxwjlz\n",
      "iteration [32] => episode_reward_mean: 2.174698795180723, episode_len_mean: 6.168674698795181, agent_steps_trained: 93550, env_steps_trained: 65536, entropy: 0.9575420205791791, learning_rate: 0.0010000000000000005\n",
      "iteration [33] => episode_reward_mean: 1.884498480243161, episode_len_mean: 6.206686930091186, agent_steps_trained: 96811, env_steps_trained: 67584, entropy: 0.9369750405351321, learning_rate: 0.0010000000000000005\n",
      "iteration [34] => episode_reward_mean: 2.7078313253012047, episode_len_mean: 6.1897590361445785, agent_steps_trained: 100036, env_steps_trained: 69632, entropy: 0.9150742014249166, learning_rate: 0.0010000000000000005\n",
      "iteration [35] => episode_reward_mean: 1.8975903614457832, episode_len_mean: 6.171686746987952, agent_steps_trained: 103230, env_steps_trained: 71680, entropy: 0.9168093875050545, learning_rate: 0.0010000000000000005\n",
      "iteration [36] => episode_reward_mean: 2.6785714285714284, episode_len_mean: 6.083333333333333, agent_steps_trained: 106515, env_steps_trained: 73728, entropy: 0.9115428283810616, learning_rate: 0.0010000000000000005\n",
      "Checkpoint saved in directory /tmp/tmpvdzliu09\n",
      "iteration [37] => episode_reward_mean: 2.6666666666666665, episode_len_mean: 6.212121212121212, agent_steps_trained: 109692, env_steps_trained: 75776, entropy: 0.8942843283216159, learning_rate: 0.0010000000000000005\n",
      "iteration [38] => episode_reward_mean: 1.5598802395209581, episode_len_mean: 6.131736526946108, agent_steps_trained: 112977, env_steps_trained: 77824, entropy: 0.8989933118224144, learning_rate: 0.0010000000000000005\n",
      "iteration [39] => episode_reward_mean: 2.588235294117647, episode_len_mean: 6.008823529411765, agent_steps_trained: 116233, env_steps_trained: 79872, entropy: 0.875792478521665, learning_rate: 0.0010000000000000005\n",
      "iteration [40] => episode_reward_mean: 2.006172839506173, episode_len_mean: 6.339506172839506, agent_steps_trained: 119475, env_steps_trained: 81920, entropy: 0.879810756444931, learning_rate: 0.0010000000000000005\n",
      "iteration [41] => episode_reward_mean: 2.5698005698005697, episode_len_mean: 5.831908831908832, agent_steps_trained: 122785, env_steps_trained: 83968, entropy: 0.8373332157731056, learning_rate: 0.0010000000000000005\n",
      "Checkpoint saved in directory /tmp/tmpfndlnkcr\n",
      "iteration [42] => episode_reward_mean: 2.561959654178674, episode_len_mean: 5.899135446685879, agent_steps_trained: 126071, env_steps_trained: 86016, entropy: 0.8353710889816284, learning_rate: 0.0010000000000000005\n",
      "iteration [43] => episode_reward_mean: 3.0304709141274238, episode_len_mean: 5.6814404432132966, agent_steps_trained: 129384, env_steps_trained: 88064, entropy: 0.82929301460584, learning_rate: 0.0010000000000000005\n",
      "iteration [44] => episode_reward_mean: 2.409090909090909, episode_len_mean: 5.815340909090909, agent_steps_trained: 132663, env_steps_trained: 90112, entropy: 0.8212777038415273, learning_rate: 0.0010000000000000005\n",
      "iteration [45] => episode_reward_mean: 3.096317280453258, episode_len_mean: 5.798866855524079, agent_steps_trained: 135928, env_steps_trained: 92160, entropy: 0.8431536793708801, learning_rate: 0.0010000000000000005\n",
      "iteration [46] => episode_reward_mean: 2.5819209039548023, episode_len_mean: 5.788135593220339, agent_steps_trained: 139223, env_steps_trained: 94208, entropy: 0.8218075330058734, learning_rate: 0.0010000000000000005\n",
      "Checkpoint saved in directory /tmp/tmpu4fmacce\n",
      "iteration [47] => episode_reward_mean: 2.272727272727273, episode_len_mean: 5.815340909090909, agent_steps_trained: 142538, env_steps_trained: 96256, entropy: 0.8071840400497119, learning_rate: 0.0010000000000000005\n",
      "iteration [48] => episode_reward_mean: 2.0638297872340425, episode_len_mean: 6.21580547112462, agent_steps_trained: 145814, env_steps_trained: 98304, entropy: 0.8264349266886711, learning_rate: 0.0010000000000000005\n",
      "iteration [49] => episode_reward_mean: 3.26027397260274, episode_len_mean: 5.6219178082191785, agent_steps_trained: 149120, env_steps_trained: 100352, entropy: 0.794453023870786, learning_rate: 0.0010000000000000005\n",
      "iteration [50] => episode_reward_mean: 1.9941176470588236, episode_len_mean: 6.017647058823529, agent_steps_trained: 152448, env_steps_trained: 102400, entropy: 0.813424572119346, learning_rate: 0.001\n"
     ]
    }
   ],
   "source": [
    "from ray.rllib.algorithms.ppo import PPOConfig\n",
    "from ray.tune.logger import pretty_print\n",
    "from ray.tune.registry import register_env\n",
    "from gymnasium.wrappers.time_limit import TimeLimit\n",
    "\n",
    "observations_memory = 1\n",
    "register_env(\"my_env\", lambda _: PointCoverageEnv({\"height\": 5, \"width\": 5, \"n_agents\": 2, \"n_targets\": 2, \"max_steps\": 30, \"observations_memory\": observations_memory}))\n",
    "\n",
    "train_batch_size = 2048\n",
    "sgd_minibatch_size = 256\n",
    "num_sgd_iter = 10\n",
    "trainings = 50\n",
    "\n",
    "total_env_steps = trainings*train_batch_size\n",
    "\n",
    "print(f\"number of different environment steps: {total_env_steps}\")\n",
    "\n",
    "\n",
    "algo2 = (\n",
    "    PPOConfig()\n",
    "    .training(gamma=0.99, \n",
    "              #lr=0.001,\n",
    "              lr_schedule=[\n",
    "                [0, 0.005],  \n",
    "                [1000, 0.001],  \n",
    "                [10000, 0.001],  \n",
    "              ],\n",
    "              kl_coeff=0.5, \n",
    "              train_batch_size=train_batch_size, \n",
    "              sgd_minibatch_size=sgd_minibatch_size, \n",
    "              num_sgd_iter=num_sgd_iter, \n",
    "              entropy_coeff_schedule = [\n",
    "                [0, 0.8],  # Start with relatively high entropy coefficient\n",
    "                [40480, 0],  # Gradually decrease entropy coefficient over 10,000 iterations\n",
    "              ])\n",
    "    .env_runners(num_env_runners=1)\n",
    "    .resources(num_gpus=0)\n",
    "    .environment(env=\"my_env\")\n",
    "    .build()\n",
    ")\n",
    "\n",
    "for i in range(trainings):\n",
    "    result = algo2.train()\n",
    "    customResultPrint(result)\n",
    "    if i % 5 == 0:\n",
    "        checkpoint_dir = algo2.save().checkpoint.path\n",
    "        print(f\"Checkpoint saved in directory {checkpoint_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## memory = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nicolo/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/rllib/algorithms/algorithm.py:521: RayDeprecationWarning: This API is deprecated and may be removed in future Ray releases. You could suppress this warning by setting env variable PYTHONWARNINGS=\"ignore::DeprecationWarning\"\n",
      "`UnifiedLogger` will be removed in Ray 2.7.\n",
      "  return UnifiedLogger(config, logdir, loggers=None)\n",
      "/home/nicolo/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/tune/logger/unified.py:53: RayDeprecationWarning: This API is deprecated and may be removed in future Ray releases. You could suppress this warning by setting env variable PYTHONWARNINGS=\"ignore::DeprecationWarning\"\n",
      "The `JsonLogger interface is deprecated in favor of the `ray.tune.json.JsonLoggerCallback` interface and will be removed in Ray 2.7.\n",
      "  self._loggers.append(cls(self.config, self.logdir, self.trial))\n",
      "/home/nicolo/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/tune/logger/unified.py:53: RayDeprecationWarning: This API is deprecated and may be removed in future Ray releases. You could suppress this warning by setting env variable PYTHONWARNINGS=\"ignore::DeprecationWarning\"\n",
      "The `CSVLogger interface is deprecated in favor of the `ray.tune.csv.CSVLoggerCallback` interface and will be removed in Ray 2.7.\n",
      "  self._loggers.append(cls(self.config, self.logdir, self.trial))\n",
      "/home/nicolo/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/tune/logger/unified.py:53: RayDeprecationWarning: This API is deprecated and may be removed in future Ray releases. You could suppress this warning by setting env variable PYTHONWARNINGS=\"ignore::DeprecationWarning\"\n",
      "The `TBXLogger interface is deprecated in favor of the `ray.tune.tensorboardx.TBXLoggerCallback` interface and will be removed in Ray 2.7.\n",
      "  self._loggers.append(cls(self.config, self.logdir, self.trial))\n",
      "2024-05-17 10:15:52,146\tWARNING deprecation.py:50 -- DeprecationWarning: `WorkerSet(num_workers=... OR local_worker=...)` has been deprecated. Use `EnvRunnerGroup(num_env_runners=... AND local_env_runner=...)` instead. This will raise an error in the future!\n",
      "2024-05-17 10:15:52,146\tWARNING deprecation.py:50 -- DeprecationWarning: `max_num_worker_restarts` has been deprecated. Use `AlgorithmConfig.max_num_env_runner_restarts` instead. This will raise an error in the future!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of different environment steps: 204800\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-17 10:15:55,338\tINFO worker.py:1749 -- Started a local Ray instance.\n",
      "2024-05-17 10:16:09,711\tINFO trainable.py:161 -- Trainable.setup took 17.566 seconds. If your trainable is slow to initialize, consider setting reuse_actors=True to reduce actor creation overheads.\n",
      "2024-05-17 10:16:09,713\tWARNING util.py:61 -- Install gputil for GPU system monitoring.\n",
      "2024-05-17 10:16:09,738\tERROR actor_manager.py:519 -- Ray error, taking actor 1 out of service. \u001b[36mray::RolloutWorker.apply()\u001b[39m (pid=32151, ip=172.18.171.36, actor_id=3803ba9c2b5304c3c2171c6101000000, repr=<ray.rllib.evaluation.rollout_worker.RolloutWorker object at 0x7f4ae8f2d1d0>)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/nicolo/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/rllib/utils/actor_manager.py\", line 189, in apply\n",
      "    raise e\n",
      "  File \"/home/nicolo/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/rllib/utils/actor_manager.py\", line 178, in apply\n",
      "    return func(self, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/nicolo/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/rllib/execution/rollout_ops.py\", line 99, in <lambda>\n",
      "    (lambda w: w.sample())\n",
      "               ^^^^^^^^^^\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/nicolo/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/rllib/evaluation/rollout_worker.py\", line 685, in sample\n",
      "    batches = [self.input_reader.next()]\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/nicolo/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/rllib/evaluation/sampler.py\", line 91, in next\n",
      "    batches = [self.get_data()]\n",
      "               ^^^^^^^^^^^^^^^\n",
      "  File \"/home/nicolo/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/rllib/evaluation/sampler.py\", line 273, in get_data\n",
      "    item = next(self._env_runner)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/nicolo/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/rllib/evaluation/env_runner_v2.py\", line 348, in run\n",
      "    outputs = self.step()\n",
      "              ^^^^^^^^^^^\n",
      "  File \"/home/nicolo/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/rllib/evaluation/env_runner_v2.py\", line 374, in step\n",
      "    active_envs, to_eval, outputs = self._process_observations(\n",
      "                                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/nicolo/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/rllib/evaluation/env_runner_v2.py\", line 540, in _process_observations\n",
      "    policy_id: PolicyID = episode.policy_for(agent_id)\n",
      "                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/nicolo/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/rllib/evaluation/episode_v2.py\", line 119, in policy_for\n",
      "    policy_id = self._agent_to_policy[agent_id] = self.policy_mapping_fn(\n",
      "                                                  ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "TypeError: my_policy_mapping_fn() got an unexpected keyword argument 'worker'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    },
    {
     "ename": "RayTaskError(TypeError)",
     "evalue": "\u001b[36mray::RolloutWorker.apply()\u001b[39m (pid=32151, ip=172.18.171.36, actor_id=3803ba9c2b5304c3c2171c6101000000, repr=<ray.rllib.evaluation.rollout_worker.RolloutWorker object at 0x7f4ae8f2d1d0>)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/nicolo/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/rllib/utils/actor_manager.py\", line 189, in apply\n    raise e\n  File \"/home/nicolo/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/rllib/utils/actor_manager.py\", line 178, in apply\n    return func(self, *args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/nicolo/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/rllib/execution/rollout_ops.py\", line 99, in <lambda>\n    (lambda w: w.sample())\n               ^^^^^^^^^^\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/nicolo/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/rllib/evaluation/rollout_worker.py\", line 685, in sample\n    batches = [self.input_reader.next()]\n               ^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/nicolo/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/rllib/evaluation/sampler.py\", line 91, in next\n    batches = [self.get_data()]\n               ^^^^^^^^^^^^^^^\n  File \"/home/nicolo/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/rllib/evaluation/sampler.py\", line 273, in get_data\n    item = next(self._env_runner)\n           ^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/nicolo/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/rllib/evaluation/env_runner_v2.py\", line 348, in run\n    outputs = self.step()\n              ^^^^^^^^^^^\n  File \"/home/nicolo/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/rllib/evaluation/env_runner_v2.py\", line 374, in step\n    active_envs, to_eval, outputs = self._process_observations(\n                                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/nicolo/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/rllib/evaluation/env_runner_v2.py\", line 540, in _process_observations\n    policy_id: PolicyID = episode.policy_for(agent_id)\n                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/nicolo/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/rllib/evaluation/episode_v2.py\", line 119, in policy_for\n    policy_id = self._agent_to_policy[agent_id] = self.policy_mapping_fn(\n                                                  ^^^^^^^^^^^^^^^^^^^^^^^\nTypeError: my_policy_mapping_fn() got an unexpected keyword argument 'worker'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRayTaskError(TypeError)\u001b[0m                   Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 41\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[38;5;28mprint\u001b[39m(algo2\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mis_multi_agent())\n\u001b[1;32m     40\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(trainings):\n\u001b[0;32m---> 41\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43malgo2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     42\u001b[0m     customResultPrint(result)\n\u001b[1;32m     43\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m5\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m~/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/tune/trainable/trainable.py:331\u001b[0m, in \u001b[0;36mTrainable.train\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    329\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    330\u001b[0m     skipped \u001b[38;5;241m=\u001b[39m skip_exceptions(e)\n\u001b[0;32m--> 331\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m skipped \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mexception_cause\u001b[39;00m(skipped)\n\u001b[1;32m    333\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(result, \u001b[38;5;28mdict\u001b[39m), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstep() needs to return a dict.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    335\u001b[0m \u001b[38;5;66;03m# We do not modify internal state nor update this result if duplicate.\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/tune/trainable/trainable.py:328\u001b[0m, in \u001b[0;36mTrainable.train\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    326\u001b[0m start \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m    327\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 328\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    329\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    330\u001b[0m     skipped \u001b[38;5;241m=\u001b[39m skip_exceptions(e)\n",
      "File \u001b[0;32m~/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/rllib/algorithms/algorithm.py:873\u001b[0m, in \u001b[0;36mAlgorithm.step\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    863\u001b[0m     (\n\u001b[1;32m    864\u001b[0m         train_results,\n\u001b[1;32m    865\u001b[0m         eval_results,\n\u001b[1;32m    866\u001b[0m         train_iter_ctx,\n\u001b[1;32m    867\u001b[0m     ) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_run_one_training_iteration_and_evaluation_in_parallel()\n\u001b[1;32m    869\u001b[0m \u001b[38;5;66;03m# - No evaluation necessary, just run the next training iteration.\u001b[39;00m\n\u001b[1;32m    870\u001b[0m \u001b[38;5;66;03m# - We have to evaluate in this training iteration, but no parallelism ->\u001b[39;00m\n\u001b[1;32m    871\u001b[0m \u001b[38;5;66;03m#   evaluate after the training iteration is entirely done.\u001b[39;00m\n\u001b[1;32m    872\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 873\u001b[0m     train_results, train_iter_ctx \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_one_training_iteration\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    875\u001b[0m \u001b[38;5;66;03m# Sequential: Train (already done above), then evaluate.\u001b[39;00m\n\u001b[1;32m    876\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m evaluate_this_iter \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mevaluation_parallel_to_training:\n",
      "File \u001b[0;32m~/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/rllib/algorithms/algorithm.py:3156\u001b[0m, in \u001b[0;36mAlgorithm._run_one_training_iteration\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   3154\u001b[0m             \u001b[38;5;66;03m# Try to train one step.\u001b[39;00m\n\u001b[1;32m   3155\u001b[0m             \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_timers[TRAINING_STEP_TIMER]:\n\u001b[0;32m-> 3156\u001b[0m                 results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3158\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m results, train_iter_ctx\n",
      "File \u001b[0;32m~/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/rllib/algorithms/ppo/ppo.py:428\u001b[0m, in \u001b[0;36mPPO.training_step\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    424\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_training_step_new_api_stack()\n\u001b[1;32m    425\u001b[0m \u001b[38;5;66;03m# Old and hybrid API stacks (Policy, RolloutWorker, Connector, maybe RLModule,\u001b[39;00m\n\u001b[1;32m    426\u001b[0m \u001b[38;5;66;03m# maybe Learner).\u001b[39;00m\n\u001b[1;32m    427\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 428\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_training_step_old_and_hybrid_api_stacks\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/rllib/algorithms/ppo/ppo.py:562\u001b[0m, in \u001b[0;36mPPO._training_step_old_and_hybrid_api_stacks\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    557\u001b[0m     train_batch \u001b[38;5;241m=\u001b[39m synchronous_parallel_sample(\n\u001b[1;32m    558\u001b[0m         worker_set\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mworkers,\n\u001b[1;32m    559\u001b[0m         max_agent_steps\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mtotal_train_batch_size,\n\u001b[1;32m    560\u001b[0m     )\n\u001b[1;32m    561\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 562\u001b[0m     train_batch \u001b[38;5;241m=\u001b[39m \u001b[43msynchronous_parallel_sample\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    563\u001b[0m \u001b[43m        \u001b[49m\u001b[43mworker_set\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mworkers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    564\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_env_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtotal_train_batch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    565\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    566\u001b[0m train_batch \u001b[38;5;241m=\u001b[39m train_batch\u001b[38;5;241m.\u001b[39mas_multi_agent()\n\u001b[1;32m    567\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_counters[NUM_AGENT_STEPS_SAMPLED] \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m train_batch\u001b[38;5;241m.\u001b[39magent_steps()\n",
      "File \u001b[0;32m~/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/rllib/execution/rollout_ops.py:97\u001b[0m, in \u001b[0;36msynchronous_parallel_sample\u001b[0;34m(worker_set, max_agent_steps, max_env_steps, concat, sample_timeout_s, _uses_new_env_runners, _return_metrics)\u001b[0m\n\u001b[1;32m     94\u001b[0m         stats_dicts \u001b[38;5;241m=\u001b[39m [worker_set\u001b[38;5;241m.\u001b[39mlocal_worker()\u001b[38;5;241m.\u001b[39mget_metrics()]\n\u001b[1;32m     95\u001b[0m \u001b[38;5;66;03m# Loop over remote workers' `sample()` method in parallel.\u001b[39;00m\n\u001b[1;32m     96\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 97\u001b[0m     sampled_data \u001b[38;5;241m=\u001b[39m \u001b[43mworker_set\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforeach_worker\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     98\u001b[0m \u001b[43m        \u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     99\u001b[0m \u001b[43m            \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mw\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mw\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msample\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    100\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m_return_metrics\u001b[49m\n\u001b[1;32m    101\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mw\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mw\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msample\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mw\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_metrics\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    102\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    103\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlocal_worker\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    104\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout_seconds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample_timeout_s\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    105\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    106\u001b[0m     \u001b[38;5;66;03m# Nothing was returned (maybe all workers are stalling) or no healthy\u001b[39;00m\n\u001b[1;32m    107\u001b[0m     \u001b[38;5;66;03m# remote workers left: Break.\u001b[39;00m\n\u001b[1;32m    108\u001b[0m     \u001b[38;5;66;03m# There is no point staying in this loop, since we will not be able to\u001b[39;00m\n\u001b[1;32m    109\u001b[0m     \u001b[38;5;66;03m# get any new samples if we don't have any healthy remote workers left.\u001b[39;00m\n\u001b[1;32m    110\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m sampled_data \u001b[38;5;129;01mor\u001b[39;00m worker_set\u001b[38;5;241m.\u001b[39mnum_healthy_remote_workers() \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m~/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/rllib/env/env_runner_group.py:849\u001b[0m, in \u001b[0;36mEnvRunnerGroup.foreach_worker\u001b[0;34m(self, func, local_worker, healthy_only, remote_worker_ids, timeout_seconds, return_obj_refs, mark_healthy)\u001b[0m\n\u001b[1;32m    838\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m local_result\n\u001b[1;32m    840\u001b[0m remote_results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_worker_manager\u001b[38;5;241m.\u001b[39mforeach_actor(\n\u001b[1;32m    841\u001b[0m     func,\n\u001b[1;32m    842\u001b[0m     healthy_only\u001b[38;5;241m=\u001b[39mhealthy_only,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    846\u001b[0m     mark_healthy\u001b[38;5;241m=\u001b[39mmark_healthy,\n\u001b[1;32m    847\u001b[0m )\n\u001b[0;32m--> 849\u001b[0m \u001b[43m_handle_remote_call_result_errors\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    850\u001b[0m \u001b[43m    \u001b[49m\u001b[43mremote_results\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_ignore_env_runner_failures\u001b[49m\n\u001b[1;32m    851\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    853\u001b[0m \u001b[38;5;66;03m# With application errors handled, return good results.\u001b[39;00m\n\u001b[1;32m    854\u001b[0m remote_results \u001b[38;5;241m=\u001b[39m [r\u001b[38;5;241m.\u001b[39mget() \u001b[38;5;28;01mfor\u001b[39;00m r \u001b[38;5;129;01min\u001b[39;00m remote_results\u001b[38;5;241m.\u001b[39mignore_errors()]\n",
      "File \u001b[0;32m~/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/rllib/env/env_runner_group.py:1184\u001b[0m, in \u001b[0;36m_handle_remote_call_result_errors\u001b[0;34m(results, ignore_env_runner_failures)\u001b[0m\n\u001b[1;32m   1182\u001b[0m     logger\u001b[38;5;241m.\u001b[39mexception(r\u001b[38;5;241m.\u001b[39mget())\n\u001b[1;32m   1183\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1184\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m r\u001b[38;5;241m.\u001b[39mget()\n",
      "\u001b[0;31mRayTaskError(TypeError)\u001b[0m: \u001b[36mray::RolloutWorker.apply()\u001b[39m (pid=32151, ip=172.18.171.36, actor_id=3803ba9c2b5304c3c2171c6101000000, repr=<ray.rllib.evaluation.rollout_worker.RolloutWorker object at 0x7f4ae8f2d1d0>)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/nicolo/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/rllib/utils/actor_manager.py\", line 189, in apply\n    raise e\n  File \"/home/nicolo/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/rllib/utils/actor_manager.py\", line 178, in apply\n    return func(self, *args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/nicolo/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/rllib/execution/rollout_ops.py\", line 99, in <lambda>\n    (lambda w: w.sample())\n               ^^^^^^^^^^\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/nicolo/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/rllib/evaluation/rollout_worker.py\", line 685, in sample\n    batches = [self.input_reader.next()]\n               ^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/nicolo/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/rllib/evaluation/sampler.py\", line 91, in next\n    batches = [self.get_data()]\n               ^^^^^^^^^^^^^^^\n  File \"/home/nicolo/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/rllib/evaluation/sampler.py\", line 273, in get_data\n    item = next(self._env_runner)\n           ^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/nicolo/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/rllib/evaluation/env_runner_v2.py\", line 348, in run\n    outputs = self.step()\n              ^^^^^^^^^^^\n  File \"/home/nicolo/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/rllib/evaluation/env_runner_v2.py\", line 374, in step\n    active_envs, to_eval, outputs = self._process_observations(\n                                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/nicolo/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/rllib/evaluation/env_runner_v2.py\", line 540, in _process_observations\n    policy_id: PolicyID = episode.policy_for(agent_id)\n                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/nicolo/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/rllib/evaluation/episode_v2.py\", line 119, in policy_for\n    policy_id = self._agent_to_policy[agent_id] = self.policy_mapping_fn(\n                                                  ^^^^^^^^^^^^^^^^^^^^^^^\nTypeError: my_policy_mapping_fn() got an unexpected keyword argument 'worker'"
     ]
    }
   ],
   "source": [
    "from ray.rllib.algorithms.ppo import PPOConfig\n",
    "from ray.tune.logger import pretty_print\n",
    "from ray.tune.registry import register_env\n",
    "from gymnasium.wrappers.time_limit import TimeLimit\n",
    "\n",
    "observations_memory = 2\n",
    "env = PointCoverageEnv({\"height\": 5, \"width\": 5, \"n_agents\": 2, \"n_targets\": 2, \"max_steps\": 30, \"observations_memory\": observations_memory})\n",
    "register_env(\"my_env\", lambda _: PointCoverageEnv({\"height\": 5, \"width\": 5, \"n_agents\": 2, \"n_targets\": 2, \"max_steps\": 30, \"observations_memory\": observations_memory}))\n",
    "\n",
    "train_batch_size = 4096\n",
    "sgd_minibatch_size = 256\n",
    "num_sgd_iter = 20\n",
    "trainings = 50\n",
    "\n",
    "total_env_steps = trainings*train_batch_size\n",
    "\n",
    "print(f\"number of different environment steps: {total_env_steps}\")\n",
    "\n",
    "#def my_policy_mapping_fn(agent_id, episode):\n",
    "    # return \"agent-policy\"\n",
    "\n",
    "algo2 = (\n",
    "    PPOConfig()\n",
    "    .training(gamma=0.99, \n",
    "              lr=0.0005,\n",
    "              kl_coeff=0.2, \n",
    "              train_batch_size=train_batch_size, \n",
    "              sgd_minibatch_size=sgd_minibatch_size, \n",
    "              num_sgd_iter=num_sgd_iter)\n",
    "    .env_runners(num_env_runners=1)\n",
    "    #.multi_agent(policies={\"agent-policy\": (None, env.observation_space, env.action_space, {})},\n",
    "    #             policy_mapping_fn=my_policy_mapping_fn)\n",
    "    .resources(num_gpus=0)\n",
    "    .environment(env=\"my_env\")\n",
    "    .build()\n",
    ")\n",
    "\n",
    "#print(algo2.config.is_multi_agent())\n",
    "\n",
    "for i in range(trainings):\n",
    "    result = algo2.train()\n",
    "    customResultPrint(result)\n",
    "    if i % 5 == 0:\n",
    "        checkpoint_dir = algo2.save().checkpoint.path\n",
    "        print(f\"Checkpoint saved in directory {checkpoint_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_______\n",
      "|o    |\n",
      "|   x |\n",
      "|  o  |\n",
      "|    x|\n",
      "|     |\n",
      "‾‾‾‾‾‾‾\n",
      "deepmind\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (2x16 and 24x256)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 12\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28mprint\u001b[39m(algo2\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mpreprocessor_pref)\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m100\u001b[39m):\n\u001b[0;32m---> 12\u001b[0m     actions \u001b[38;5;241m=\u001b[39m \u001b[43malgo2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_actions\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     13\u001b[0m     \u001b[38;5;28mprint\u001b[39m(actions, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     14\u001b[0m     obs, reward, terminated, truncated, info \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mstep(actions)\n",
      "File \u001b[0;32m~/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/rllib/algorithms/algorithm.py:1965\u001b[0m, in \u001b[0;36mAlgorithm.compute_actions\u001b[0;34m(self, observations, state, prev_action, prev_reward, info, policy_id, full_fetch, explore, timestep, episodes, unsquash_actions, clip_actions, **kwargs)\u001b[0m\n\u001b[1;32m   1962\u001b[0m     input_dict[\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstate_in_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m s\n\u001b[1;32m   1964\u001b[0m \u001b[38;5;66;03m# Batch compute actions\u001b[39;00m\n\u001b[0;32m-> 1965\u001b[0m actions, states, infos \u001b[38;5;241m=\u001b[39m \u001b[43mpolicy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_actions_from_input_dict\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1966\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1967\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexplore\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexplore\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1968\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimestep\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimestep\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1969\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepisodes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mepisodes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1970\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1972\u001b[0m \u001b[38;5;66;03m# Unbatch actions for the environment into a multi-agent dict.\u001b[39;00m\n\u001b[1;32m   1973\u001b[0m single_actions \u001b[38;5;241m=\u001b[39m space_utils\u001b[38;5;241m.\u001b[39munbatch(actions)\n",
      "File \u001b[0;32m~/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/rllib/policy/torch_policy.py:323\u001b[0m, in \u001b[0;36mTorchPolicy.compute_actions_from_input_dict\u001b[0;34m(self, input_dict, explore, timestep, **kwargs)\u001b[0m\n\u001b[1;32m    312\u001b[0m \u001b[38;5;66;03m# Calculate RNN sequence lengths.\u001b[39;00m\n\u001b[1;32m    313\u001b[0m seq_lens \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    314\u001b[0m     torch\u001b[38;5;241m.\u001b[39mtensor(\n\u001b[1;32m    315\u001b[0m         [\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mlen\u001b[39m(state_batches[\u001b[38;5;241m0\u001b[39m]),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    320\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    321\u001b[0m )\n\u001b[0;32m--> 323\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_compute_action_helper\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    324\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstate_batches\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mseq_lens\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexplore\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimestep\u001b[49m\n\u001b[1;32m    325\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/rllib/utils/threading.py:24\u001b[0m, in \u001b[0;36mwith_lock.<locals>.wrapper\u001b[0;34m(self, *a, **k)\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     23\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[0;32m---> 24\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mk\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     26\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas no attribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_lock\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m e\u001b[38;5;241m.\u001b[39margs[\u001b[38;5;241m0\u001b[39m]:\n",
      "File \u001b[0;32m~/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/rllib/policy/torch_policy.py:960\u001b[0m, in \u001b[0;36mTorchPolicy._compute_action_helper\u001b[0;34m(self, input_dict, state_batches, seq_lens, explore, timestep)\u001b[0m\n\u001b[1;32m    956\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maction_distribution_fn:\n\u001b[1;32m    957\u001b[0m     \u001b[38;5;66;03m# Try new action_distribution_fn signature, supporting\u001b[39;00m\n\u001b[1;32m    958\u001b[0m     \u001b[38;5;66;03m# state_batches and seq_lens.\u001b[39;00m\n\u001b[1;32m    959\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 960\u001b[0m         dist_inputs, dist_class, state_out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maction_distribution_fn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    961\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    962\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    963\u001b[0m \u001b[43m            \u001b[49m\u001b[43minput_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    964\u001b[0m \u001b[43m            \u001b[49m\u001b[43mstate_batches\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstate_batches\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    965\u001b[0m \u001b[43m            \u001b[49m\u001b[43mseq_lens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mseq_lens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    966\u001b[0m \u001b[43m            \u001b[49m\u001b[43mexplore\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexplore\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    967\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtimestep\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimestep\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    968\u001b[0m \u001b[43m            \u001b[49m\u001b[43mis_training\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    969\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    970\u001b[0m     \u001b[38;5;66;03m# Trying the old way (to stay backward compatible).\u001b[39;00m\n\u001b[1;32m    971\u001b[0m     \u001b[38;5;66;03m# TODO: Remove in future.\u001b[39;00m\n\u001b[1;32m    972\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/rllib/algorithms/dqn/dqn_torch_policy.py:239\u001b[0m, in \u001b[0;36mget_distribution_inputs_and_class\u001b[0;34m(policy, model, input_dict, explore, is_training, **kwargs)\u001b[0m\n\u001b[1;32m    230\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_distribution_inputs_and_class\u001b[39m(\n\u001b[1;32m    231\u001b[0m     policy: Policy,\n\u001b[1;32m    232\u001b[0m     model: ModelV2,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    237\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[1;32m    238\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[TensorType, \u001b[38;5;28mtype\u001b[39m, List[TensorType]]:\n\u001b[0;32m--> 239\u001b[0m     q_vals \u001b[38;5;241m=\u001b[39m \u001b[43mcompute_q_values\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    240\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpolicy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexplore\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexplore\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_training\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_training\u001b[49m\n\u001b[1;32m    241\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    242\u001b[0m     q_vals \u001b[38;5;241m=\u001b[39m q_vals[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(q_vals, \u001b[38;5;28mtuple\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m q_vals\n\u001b[1;32m    244\u001b[0m     model\u001b[38;5;241m.\u001b[39mtower_stats[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mq_values\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m q_vals\n",
      "File \u001b[0;32m~/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/rllib/algorithms/dqn/dqn_torch_policy.py:429\u001b[0m, in \u001b[0;36mcompute_q_values\u001b[0;34m(policy, model, input_dict, state_batches, seq_lens, explore, is_training)\u001b[0m\n\u001b[1;32m    418\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_q_values\u001b[39m(\n\u001b[1;32m    419\u001b[0m     policy: Policy,\n\u001b[1;32m    420\u001b[0m     model: ModelV2,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    425\u001b[0m     is_training: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    426\u001b[0m ):\n\u001b[1;32m    427\u001b[0m     config \u001b[38;5;241m=\u001b[39m policy\u001b[38;5;241m.\u001b[39mconfig\n\u001b[0;32m--> 429\u001b[0m     model_out, state \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstate_batches\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mseq_lens\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    431\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m config[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnum_atoms\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m    432\u001b[0m         (\n\u001b[1;32m    433\u001b[0m             action_scores,\n\u001b[1;32m    434\u001b[0m             z,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    437\u001b[0m             probs_or_logits,\n\u001b[1;32m    438\u001b[0m         ) \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mget_q_value_distributions(model_out)\n",
      "File \u001b[0;32m~/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/rllib/models/modelv2.py:255\u001b[0m, in \u001b[0;36mModelV2.__call__\u001b[0;34m(self, input_dict, state, seq_lens)\u001b[0m\n\u001b[1;32m    252\u001b[0m         restored[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mobs_flat\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m input_dict[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mobs\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m    254\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontext():\n\u001b[0;32m--> 255\u001b[0m     res \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrestored\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstate\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mseq_lens\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    257\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(input_dict, SampleBatch):\n\u001b[1;32m    258\u001b[0m     input_dict\u001b[38;5;241m.\u001b[39maccessed_keys \u001b[38;5;241m=\u001b[39m restored\u001b[38;5;241m.\u001b[39maccessed_keys \u001b[38;5;241m-\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mobs_flat\u001b[39m\u001b[38;5;124m\"\u001b[39m}\n",
      "File \u001b[0;32m~/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/rllib/models/torch/fcnet.py:147\u001b[0m, in \u001b[0;36mFullyConnectedNetwork.forward\u001b[0;34m(self, input_dict, state, seq_lens)\u001b[0m\n\u001b[1;32m    145\u001b[0m obs \u001b[38;5;241m=\u001b[39m input_dict[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mobs_flat\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mfloat()\n\u001b[1;32m    146\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_last_flat_in \u001b[38;5;241m=\u001b[39m obs\u001b[38;5;241m.\u001b[39mreshape(obs\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m--> 147\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_features \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_hidden_layers\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_last_flat_in\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    148\u001b[0m logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_logits(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_features) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_logits \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_features\n\u001b[1;32m    149\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfree_log_std:\n",
      "File \u001b[0;32m~/anaconda3/envs/tianEnv/lib/python3.11/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/tianEnv/lib/python3.11/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/tianEnv/lib/python3.11/site-packages/torch/nn/modules/container.py:217\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 217\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    218\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m~/anaconda3/envs/tianEnv/lib/python3.11/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/tianEnv/lib/python3.11/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/rllib/models/torch/misc.py:297\u001b[0m, in \u001b[0;36mSlimFC.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    296\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x: TensorType) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m TensorType:\n\u001b[0;32m--> 297\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/tianEnv/lib/python3.11/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/tianEnv/lib/python3.11/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/tianEnv/lib/python3.11/site-packages/torch/nn/modules/container.py:217\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 217\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    218\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m~/anaconda3/envs/tianEnv/lib/python3.11/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/tianEnv/lib/python3.11/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/tianEnv/lib/python3.11/site-packages/torch/nn/modules/linear.py:116\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 116\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (2x16 and 24x256)"
     ]
    }
   ],
   "source": [
    "from IPython.display import clear_output\n",
    "import time\n",
    "\n",
    "observations_memory = 2\n",
    "env = PointCoverageEnv({\"height\": 5, \"width\": 5, \"n_agents\": 2, \"n_targets\": 2, \"observations_memory\": observations_memory})\n",
    "obs, _ = env.reset()\n",
    "env.render()\n",
    "\n",
    "for i in range(100):\n",
    "    actions = algo2.compute_actions(obs)\n",
    "    print(actions, \"\\n\")\n",
    "    obs, reward, terminated, truncated, info = env.step(actions)\n",
    "    clear_output()\n",
    "    print(f\"[{i}]\")\n",
    "    env.render()\n",
    "    #print(obs)\n",
    "    print(reward)\n",
    "    time.sleep(0.5)\n",
    "\n",
    "    if terminated['__all__'] or truncated['__all__']:\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Three Agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-16 15:37:09,231\tWARNING deprecation.py:50 -- DeprecationWarning: `WorkerSet(num_workers=... OR local_worker=...)` has been deprecated. Use `EnvRunnerGroup(num_env_runners=... AND local_env_runner=...)` instead. This will raise an error in the future!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of different environment steps: 102400\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-16 15:37:16,447\tWARNING util.py:61 -- Install gputil for GPU system monitoring.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration [1] => episode_reward_mean: -18.105263157894736, episode_len_mean: 26.92105263157895, agent_steps_trained: 3826, env_steps_trained: 2048, entropy: 1.609091877085822, learning_rate: 0.005\n",
      "Checkpoint saved in directory /tmp/tmpxadcgia4\n",
      "iteration [2] => episode_reward_mean: -18.36, episode_len_mean: 27.42, agent_steps_trained: 7508, env_steps_trained: 4096, entropy: 1.603918524299349, learning_rate: 0.001\n",
      "iteration [3] => episode_reward_mean: -13.75, episode_len_mean: 27.31, agent_steps_trained: 10915, env_steps_trained: 6144, entropy: 1.595703953046065, learning_rate: 0.001\n",
      "iteration [4] => episode_reward_mean: -11.53, episode_len_mean: 26.55, agent_steps_trained: 14476, env_steps_trained: 8192, entropy: 1.5948491188196035, learning_rate: 0.001\n",
      "iteration [5] => episode_reward_mean: -11.56, episode_len_mean: 26.83, agent_steps_trained: 18006, env_steps_trained: 10240, entropy: 1.5882749713384188, learning_rate: 0.001\n",
      "iteration [6] => episode_reward_mean: -12.65, episode_len_mean: 26.72, agent_steps_trained: 21569, env_steps_trained: 12288, entropy: 1.5847441049722524, learning_rate: 0.001\n",
      "Checkpoint saved in directory /tmp/tmp5n6y4f0g\n",
      "iteration [7] => episode_reward_mean: -7.28, episode_len_mean: 25.18, agent_steps_trained: 25052, env_steps_trained: 14336, entropy: 1.5751991326992327, learning_rate: 0.001\n",
      "iteration [8] => episode_reward_mean: -4.96, episode_len_mean: 23.55, agent_steps_trained: 28497, env_steps_trained: 16384, entropy: 1.5711096974519583, learning_rate: 0.001\n",
      "iteration [9] => episode_reward_mean: -7.05, episode_len_mean: 24.23, agent_steps_trained: 32077, env_steps_trained: 18432, entropy: 1.5599557207180903, learning_rate: 0.001\n",
      "iteration [10] => episode_reward_mean: -6.78, episode_len_mean: 23.66, agent_steps_trained: 35518, env_steps_trained: 20480, entropy: 1.5410287261009217, learning_rate: 0.001\n",
      "iteration [11] => episode_reward_mean: -5.97, episode_len_mean: 23.4, agent_steps_trained: 38975, env_steps_trained: 22528, entropy: 1.5350211079304035, learning_rate: 0.001\n",
      "Checkpoint saved in directory /tmp/tmpg6k86ha6\n",
      "iteration [12] => episode_reward_mean: -2.05, episode_len_mean: 23.08, agent_steps_trained: 42201, env_steps_trained: 24576, entropy: 1.516617618004481, learning_rate: 0.0010000000000000005\n",
      "iteration [13] => episode_reward_mean: -1.69, episode_len_mean: 22.05, agent_steps_trained: 45604, env_steps_trained: 26624, entropy: 1.5038254921252912, learning_rate: 0.001\n",
      "iteration [14] => episode_reward_mean: -1.54, episode_len_mean: 22.39, agent_steps_trained: 49023, env_steps_trained: 28672, entropy: 1.4954009927236116, learning_rate: 0.001\n",
      "iteration [15] => episode_reward_mean: 1.5096153846153846, episode_len_mean: 19.64423076923077, agent_steps_trained: 52426, env_steps_trained: 30720, entropy: 1.4482355365386377, learning_rate: 0.001\n",
      "iteration [16] => episode_reward_mean: 1.5, episode_len_mean: 20.41, agent_steps_trained: 55893, env_steps_trained: 32768, entropy: 1.4323068407865671, learning_rate: 0.001\n",
      "Checkpoint saved in directory /tmp/tmpfzvr9up8\n",
      "iteration [17] => episode_reward_mean: -1.62, episode_len_mean: 21.26, agent_steps_trained: 59292, env_steps_trained: 34816, entropy: 1.4145525299585782, learning_rate: 0.001\n",
      "iteration [18] => episode_reward_mean: 2.018018018018018, episode_len_mean: 18.63963963963964, agent_steps_trained: 62892, env_steps_trained: 36864, entropy: 1.3837242713996343, learning_rate: 0.001\n",
      "iteration [19] => episode_reward_mean: 5.930434782608696, episode_len_mean: 17.817391304347826, agent_steps_trained: 66256, env_steps_trained: 38912, entropy: 1.3427495525433466, learning_rate: 0.001\n",
      "iteration [20] => episode_reward_mean: 2.088235294117647, episode_len_mean: 20.098039215686274, agent_steps_trained: 69699, env_steps_trained: 40960, entropy: 1.3669304214991056, learning_rate: 0.001\n",
      "iteration [21] => episode_reward_mean: 1.3177570093457944, episode_len_mean: 19.074766355140188, agent_steps_trained: 73173, env_steps_trained: 43008, entropy: 1.353693867646731, learning_rate: 0.001\n",
      "Checkpoint saved in directory /tmp/tmp368k_4qq\n",
      "iteration [22] => episode_reward_mean: 3.4864864864864864, episode_len_mean: 18.405405405405407, agent_steps_trained: 76681, env_steps_trained: 45056, entropy: 1.3415880450835596, learning_rate: 0.001\n",
      "iteration [23] => episode_reward_mean: 4.672, episode_len_mean: 16.392, agent_steps_trained: 80408, env_steps_trained: 47104, entropy: 1.3384717438902174, learning_rate: 0.001\n",
      "iteration [24] => episode_reward_mean: 3.5130434782608697, episode_len_mean: 17.756521739130434, agent_steps_trained: 83908, env_steps_trained: 49152, entropy: 1.298675979100741, learning_rate: 0.001\n",
      "iteration [25] => episode_reward_mean: 9.030075187969924, episode_len_mean: 15.481203007518797, agent_steps_trained: 87374, env_steps_trained: 51200, entropy: 1.2948517698508042, learning_rate: 0.001\n",
      "iteration [26] => episode_reward_mean: 3.5213675213675213, episode_len_mean: 17.53846153846154, agent_steps_trained: 90856, env_steps_trained: 53248, entropy: 1.297991864497845, learning_rate: 0.001\n",
      "Checkpoint saved in directory /tmp/tmpmyjnpw9a\n",
      "iteration [27] => episode_reward_mean: 5.551724137931035, episode_len_mean: 17.620689655172413, agent_steps_trained: 94397, env_steps_trained: 55296, entropy: 1.3107950018002437, learning_rate: 0.001\n",
      "iteration [28] => episode_reward_mean: 7.7109375, episode_len_mean: 15.9453125, agent_steps_trained: 97881, env_steps_trained: 57344, entropy: 1.288069210602687, learning_rate: 0.001\n",
      "iteration [29] => episode_reward_mean: 6.4609375, episode_len_mean: 16.046875, agent_steps_trained: 101436, env_steps_trained: 59392, entropy: 1.3011834364670973, learning_rate: 0.001\n",
      "iteration [30] => episode_reward_mean: 3.8482142857142856, episode_len_mean: 18.142857142857142, agent_steps_trained: 104867, env_steps_trained: 61440, entropy: 1.2857761905743526, learning_rate: 0.001\n",
      "iteration [31] => episode_reward_mean: 8.088888888888889, episode_len_mean: 15.348148148148148, agent_steps_trained: 108468, env_steps_trained: 63488, entropy: 1.2534972088677543, learning_rate: 0.001\n",
      "Checkpoint saved in directory /tmp/tmp1xjiqlns\n",
      "iteration [32] => episode_reward_mean: 7.511278195488722, episode_len_mean: 15.38345864661654, agent_steps_trained: 111946, env_steps_trained: 65536, entropy: 1.2504174342522254, learning_rate: 0.001\n",
      "iteration [33] => episode_reward_mean: 7.503759398496241, episode_len_mean: 15.263157894736842, agent_steps_trained: 115511, env_steps_trained: 67584, entropy: 1.2323498221544118, learning_rate: 0.001\n",
      "iteration [34] => episode_reward_mean: 6.428571428571429, episode_len_mean: 16.357142857142858, agent_steps_trained: 118921, env_steps_trained: 69632, entropy: 1.2288158783545862, learning_rate: 0.001\n",
      "iteration [35] => episode_reward_mean: 8.338235294117647, episode_len_mean: 14.904411764705882, agent_steps_trained: 122655, env_steps_trained: 71680, entropy: 1.2182922857148306, learning_rate: 0.001\n",
      "iteration [36] => episode_reward_mean: 9.0, episode_len_mean: 15.22962962962963, agent_steps_trained: 126212, env_steps_trained: 73728, entropy: 1.2283447751632104, learning_rate: 0.001\n",
      "Checkpoint saved in directory /tmp/tmpaqk52zfm\n",
      "iteration [37] => episode_reward_mean: 11.875862068965517, episode_len_mean: 14.2, agent_steps_trained: 129745, env_steps_trained: 75776, entropy: 1.1843390639011677, learning_rate: 0.001\n",
      "iteration [38] => episode_reward_mean: 8.462686567164178, episode_len_mean: 15.313432835820896, agent_steps_trained: 133256, env_steps_trained: 77824, entropy: 1.19593833226424, learning_rate: 0.001\n",
      "iteration [39] => episode_reward_mean: 11.324503311258278, episode_len_mean: 13.596026490066226, agent_steps_trained: 136836, env_steps_trained: 79872, entropy: 1.1926280901982234, learning_rate: 0.001\n",
      "iteration [40] => episode_reward_mean: 12.175, episode_len_mean: 12.61875, agent_steps_trained: 140536, env_steps_trained: 81920, entropy: 1.1804586972509112, learning_rate: 0.001\n",
      "iteration [41] => episode_reward_mean: 12.35483870967742, episode_len_mean: 13.361290322580645, agent_steps_trained: 144121, env_steps_trained: 83968, entropy: 1.1589094749518802, learning_rate: 0.001\n",
      "Checkpoint saved in directory /tmp/tmpym3vutnq\n",
      "iteration [42] => episode_reward_mean: 9.977941176470589, episode_len_mean: 14.963235294117647, agent_steps_trained: 147543, env_steps_trained: 86016, entropy: 1.1730037707548875, learning_rate: 0.001\n",
      "iteration [43] => episode_reward_mean: 14.107142857142858, episode_len_mean: 12.244047619047619, agent_steps_trained: 151048, env_steps_trained: 88064, entropy: 1.1505446195602418, learning_rate: 0.001\n",
      "iteration [44] => episode_reward_mean: 13.564935064935066, episode_len_mean: 13.292207792207792, agent_steps_trained: 154622, env_steps_trained: 90112, entropy: 1.1569711804389953, learning_rate: 0.001\n",
      "iteration [45] => episode_reward_mean: 11.69811320754717, episode_len_mean: 12.924528301886792, agent_steps_trained: 158272, env_steps_trained: 92160, entropy: 1.1446772038936615, learning_rate: 0.001\n",
      "iteration [46] => episode_reward_mean: 13.75925925925926, episode_len_mean: 12.62962962962963, agent_steps_trained: 161831, env_steps_trained: 94208, entropy: 1.119362914103728, learning_rate: 0.001\n",
      "Checkpoint saved in directory /tmp/tmpn0y3edjc\n",
      "iteration [47] => episode_reward_mean: 12.974025974025974, episode_len_mean: 13.285714285714286, agent_steps_trained: 165434, env_steps_trained: 96256, entropy: 1.1366191055093493, learning_rate: 0.001\n",
      "iteration [48] => episode_reward_mean: 11.993506493506494, episode_len_mean: 13.168831168831169, agent_steps_trained: 168877, env_steps_trained: 98304, entropy: 1.152009488069094, learning_rate: 0.001\n",
      "iteration [49] => episode_reward_mean: 13.38888888888889, episode_len_mean: 12.75925925925926, agent_steps_trained: 172426, env_steps_trained: 100352, entropy: 1.1355909989430355, learning_rate: 0.001\n",
      "iteration [50] => episode_reward_mean: 13.844155844155845, episode_len_mean: 13.311688311688311, agent_steps_trained: 175958, env_steps_trained: 102400, entropy: 1.1074482697706955, learning_rate: 0.001\n"
     ]
    }
   ],
   "source": [
    "from ray.rllib.algorithms.ppo import PPOConfig\n",
    "from ray.tune.logger import pretty_print\n",
    "from ray.tune.registry import register_env\n",
    "from gymnasium.wrappers.time_limit import TimeLimit\n",
    "\n",
    "\n",
    "register_env(\"my_env\", lambda _: PointCoverageEnv({\"height\": 5, \"width\": 5, \"n_agents\": 3, \"n_targets\": 3, \"max_steps\": 30}))\n",
    "\n",
    "train_batch_size = 2048\n",
    "sgd_minibatch_size = 256\n",
    "num_sgd_iter = 10\n",
    "trainings = 50\n",
    "\n",
    "total_env_steps = trainings*train_batch_size\n",
    "\n",
    "print(f\"number of different environment steps: {total_env_steps}\")\n",
    "\n",
    "\n",
    "algo3 = (\n",
    "    PPOConfig()\n",
    "    .training(gamma=0.99, \n",
    "              #lr=0.001,\n",
    "              lr_schedule=[\n",
    "                [0, 0.005],  \n",
    "                [1000, 0.001],  \n",
    "                [10000, 0.001],  \n",
    "              ],\n",
    "              kl_coeff=0.5, \n",
    "              train_batch_size=train_batch_size, \n",
    "              sgd_minibatch_size=sgd_minibatch_size, \n",
    "              num_sgd_iter=num_sgd_iter, \n",
    "              entropy_coeff_schedule = [\n",
    "                [0, 0.8],  # Start with relatively high entropy coefficient\n",
    "                [40480, 0],  # Gradually decrease entropy coefficient over 10,000 iterations\n",
    "              ])\n",
    "    .env_runners(num_env_runners=1)\n",
    "    .resources(num_gpus=0)\n",
    "    .environment(env=\"my_env\")\n",
    "    .build()\n",
    ")\n",
    "\n",
    "for i in range(trainings):\n",
    "    result = algo3.train()\n",
    "    customResultPrint(result)\n",
    "    if i % 5 == 0:\n",
    "        checkpoint_dir = algo3.save().checkpoint.path\n",
    "        print(f\"Checkpoint saved in directory {checkpoint_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[99]\n",
      "____________\n",
      "|   o      |\n",
      "|          |\n",
      "|      x   |\n",
      "| x        |\n",
      "|          |\n",
      "|          |\n",
      "|        o |\n",
      "|          |\n",
      "|      o   |\n",
      "|         x|\n",
      "‾‾‾‾‾‾‾‾‾‾‾‾\n",
      "{'agent-0': array([0.8, 0.6, 0.3, 0. , 0.6, 0.8, 0.1, 0.3, 0.6, 0.2, 0.9, 0.9],\n",
      "      dtype=float32), 'agent-1': array([0.6, 0.8, 0.3, 0. , 0.8, 0.6, 0.1, 0.3, 0.6, 0.2, 0.9, 0.9],\n",
      "      dtype=float32), 'agent-2': array([0.6, 0.8, 0.8, 0.6, 0.3, 0. , 0.1, 0.3, 0.6, 0.2, 0.9, 0.9],\n",
      "      dtype=float32)}\n",
      "{'agent-0': -1, 'agent-1': -1, 'agent-2': -1}\n"
     ]
    }
   ],
   "source": [
    "from IPython.display import clear_output\n",
    "import time\n",
    "\n",
    "env = PointCoverageEnv({\"height\": 10, \"width\": 10, \"n_agents\": 3, \"n_targets\": 3})\n",
    "obs, _ = env.reset()\n",
    "env.render()\n",
    "\n",
    "for i in range(100):\n",
    "    actions = algo3.compute_actions(obs)\n",
    "    print(actions, \"\\n\")\n",
    "    obs, reward, terminated, truncated, info = env.step(actions)\n",
    "    clear_output()\n",
    "    print(f\"[{i}]\")\n",
    "    env.render()\n",
    "    print(obs)\n",
    "    print(reward)\n",
    "    time.sleep(0.5)\n",
    "\n",
    "    if terminated['__all__'] or truncated['__all__']:\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Two agents, DQN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nicolo/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/rllib/algorithms/algorithm.py:521: RayDeprecationWarning: This API is deprecated and may be removed in future Ray releases. You could suppress this warning by setting env variable PYTHONWARNINGS=\"ignore::DeprecationWarning\"\n",
      "`UnifiedLogger` will be removed in Ray 2.7.\n",
      "  return UnifiedLogger(config, logdir, loggers=None)\n",
      "/home/nicolo/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/tune/logger/unified.py:53: RayDeprecationWarning: This API is deprecated and may be removed in future Ray releases. You could suppress this warning by setting env variable PYTHONWARNINGS=\"ignore::DeprecationWarning\"\n",
      "The `JsonLogger interface is deprecated in favor of the `ray.tune.json.JsonLoggerCallback` interface and will be removed in Ray 2.7.\n",
      "  self._loggers.append(cls(self.config, self.logdir, self.trial))\n",
      "/home/nicolo/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/tune/logger/unified.py:53: RayDeprecationWarning: This API is deprecated and may be removed in future Ray releases. You could suppress this warning by setting env variable PYTHONWARNINGS=\"ignore::DeprecationWarning\"\n",
      "The `CSVLogger interface is deprecated in favor of the `ray.tune.csv.CSVLoggerCallback` interface and will be removed in Ray 2.7.\n",
      "  self._loggers.append(cls(self.config, self.logdir, self.trial))\n",
      "/home/nicolo/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/tune/logger/unified.py:53: RayDeprecationWarning: This API is deprecated and may be removed in future Ray releases. You could suppress this warning by setting env variable PYTHONWARNINGS=\"ignore::DeprecationWarning\"\n",
      "The `TBXLogger interface is deprecated in favor of the `ray.tune.tensorboardx.TBXLoggerCallback` interface and will be removed in Ray 2.7.\n",
      "  self._loggers.append(cls(self.config, self.logdir, self.trial))\n",
      "2024-05-17 11:42:40,671\tWARNING deprecation.py:50 -- DeprecationWarning: `WorkerSet(num_workers=... OR local_worker=...)` has been deprecated. Use `EnvRunnerGroup(num_env_runners=... AND local_env_runner=...)` instead. This will raise an error in the future!\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 24\u001b[0m\n\u001b[1;32m     22\u001b[0m config\u001b[38;5;241m.\u001b[39m_disable_preprocessor_api \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m     23\u001b[0m config\u001b[38;5;241m.\u001b[39msample_timeout_s \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m5\u001b[39m \n\u001b[0;32m---> 24\u001b[0m algo2 \u001b[38;5;241m=\u001b[39m \u001b[43mDQN\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m30\u001b[39m):\n\u001b[1;32m     26\u001b[0m     result \u001b[38;5;241m=\u001b[39m algo2\u001b[38;5;241m.\u001b[39mtrain()\n",
      "File \u001b[0;32m~/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/rllib/algorithms/algorithm.py:554\u001b[0m, in \u001b[0;36mAlgorithm.__init__\u001b[0;34m(self, config, env, logger_creator, **kwargs)\u001b[0m\n\u001b[1;32m    536\u001b[0m \u001b[38;5;66;03m# Initialize common evaluation_metrics to nan, before they become\u001b[39;00m\n\u001b[1;32m    537\u001b[0m \u001b[38;5;66;03m# available. We want to make sure the metrics are always present\u001b[39;00m\n\u001b[1;32m    538\u001b[0m \u001b[38;5;66;03m# (although their values may be nan), so that Tune does not complain\u001b[39;00m\n\u001b[1;32m    539\u001b[0m \u001b[38;5;66;03m# when we use these as stopping criteria.\u001b[39;00m\n\u001b[1;32m    540\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mevaluation_metrics \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    541\u001b[0m     \u001b[38;5;66;03m# TODO: Don't dump sampler results into top-level.\u001b[39;00m\n\u001b[1;32m    542\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mevaluation\u001b[39m\u001b[38;5;124m\"\u001b[39m: {\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    551\u001b[0m     },\n\u001b[1;32m    552\u001b[0m }\n\u001b[0;32m--> 554\u001b[0m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m    555\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    556\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlogger_creator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlogger_creator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    557\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    558\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/tune/trainable/trainable.py:158\u001b[0m, in \u001b[0;36mTrainable.__init__\u001b[0;34m(self, config, logger_creator, storage)\u001b[0m\n\u001b[1;32m    154\u001b[0m     logger\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStorageContext on the TRAINABLE:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mstorage\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    156\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_open_logfiles(stdout_file, stderr_file)\n\u001b[0;32m--> 158\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msetup\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdeepcopy\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    159\u001b[0m setup_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_start_time\n\u001b[1;32m    160\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m setup_time \u001b[38;5;241m>\u001b[39m SETUP_TIME_THRESHOLD:\n",
      "File \u001b[0;32m~/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/rllib/algorithms/algorithm.py:640\u001b[0m, in \u001b[0;36mAlgorithm.setup\u001b[0;34m(self, config)\u001b[0m\n\u001b[1;32m    637\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39moff_policy_estimation_methods \u001b[38;5;241m=\u001b[39m ope_dict\n\u001b[1;32m    639\u001b[0m \u001b[38;5;66;03m# Create a set of env runner actors via a EnvRunnerGroup.\u001b[39;00m\n\u001b[0;32m--> 640\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mworkers \u001b[38;5;241m=\u001b[39m \u001b[43mEnvRunnerGroup\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    641\u001b[0m \u001b[43m    \u001b[49m\u001b[43menv_creator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv_creator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    642\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalidate_env\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalidate_env\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    643\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdefault_policy_class\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_default_policy_class\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    644\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    645\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_workers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnum_env_runners\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    646\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlocal_worker\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    647\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlogdir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlogdir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    648\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    650\u001b[0m \u001b[38;5;66;03m# Ensure remote workers are initially in sync with the local worker.\u001b[39;00m\n\u001b[1;32m    651\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mworkers\u001b[38;5;241m.\u001b[39msync_weights(inference_only\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m~/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/rllib/env/env_runner_group.py:169\u001b[0m, in \u001b[0;36mEnvRunnerGroup.__init__\u001b[0;34m(self, env_creator, validate_env, default_policy_class, config, num_env_runners, local_env_runner, logdir, _setup, num_workers, local_worker)\u001b[0m\n\u001b[1;32m    167\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _setup:\n\u001b[1;32m    168\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 169\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_setup\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    170\u001b[0m \u001b[43m            \u001b[49m\u001b[43mvalidate_env\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalidate_env\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    171\u001b[0m \u001b[43m            \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    172\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnum_env_runners\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_env_runners\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    173\u001b[0m \u001b[43m            \u001b[49m\u001b[43mlocal_env_runner\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_env_runner\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    174\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    175\u001b[0m     \u001b[38;5;66;03m# EnvRunnerGroup creation possibly fails, if some (remote) workers cannot\u001b[39;00m\n\u001b[1;32m    176\u001b[0m     \u001b[38;5;66;03m# be initialized properly (due to some errors in the EnvRunners's\u001b[39;00m\n\u001b[1;32m    177\u001b[0m     \u001b[38;5;66;03m# constructor).\u001b[39;00m\n\u001b[1;32m    178\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m RayActorError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    179\u001b[0m         \u001b[38;5;66;03m# In case of an actor (remote worker) init failure, the remote worker\u001b[39;00m\n\u001b[1;32m    180\u001b[0m         \u001b[38;5;66;03m# may still exist and will be accessible, however, e.g. calling\u001b[39;00m\n\u001b[1;32m    181\u001b[0m         \u001b[38;5;66;03m# its `sample.remote()` would result in strange \"property not found\"\u001b[39;00m\n\u001b[1;32m    182\u001b[0m         \u001b[38;5;66;03m# errors.\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/rllib/env/env_runner_group.py:239\u001b[0m, in \u001b[0;36mEnvRunnerGroup._setup\u001b[0;34m(self, validate_env, config, num_env_runners, local_env_runner)\u001b[0m\n\u001b[1;32m    236\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ds_shards \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    238\u001b[0m \u001b[38;5;66;03m# Create a number of @ray.remote workers.\u001b[39;00m\n\u001b[0;32m--> 239\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd_workers\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    240\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_env_runners\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    241\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalidate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalidate_env_runners_after_construction\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    242\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    244\u001b[0m \u001b[38;5;66;03m# If num_workers > 0 and we don't have an env on the local worker,\u001b[39;00m\n\u001b[1;32m    245\u001b[0m \u001b[38;5;66;03m# get the observation- and action spaces for each policy from\u001b[39;00m\n\u001b[1;32m    246\u001b[0m \u001b[38;5;66;03m# the first remote worker (which does have an env).\u001b[39;00m\n\u001b[1;32m    247\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    248\u001b[0m     local_env_runner\n\u001b[1;32m    249\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_worker_manager\u001b[38;5;241m.\u001b[39mnum_actors() \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    252\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (\u001b[38;5;129;01mnot\u001b[39;00m config\u001b[38;5;241m.\u001b[39mobservation_space \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m config\u001b[38;5;241m.\u001b[39maction_space)\n\u001b[1;32m    253\u001b[0m ):\n",
      "File \u001b[0;32m~/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/rllib/env/env_runner_group.py:748\u001b[0m, in \u001b[0;36mEnvRunnerGroup.add_workers\u001b[0;34m(self, num_workers, validate)\u001b[0m\n\u001b[1;32m    745\u001b[0m \u001b[38;5;66;03m# Validate here, whether all remote workers have been constructed properly\u001b[39;00m\n\u001b[1;32m    746\u001b[0m \u001b[38;5;66;03m# and are \"up and running\". Establish initial states.\u001b[39;00m\n\u001b[1;32m    747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m validate:\n\u001b[0;32m--> 748\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m result \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_worker_manager\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforeach_actor\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    749\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mw\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mw\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43massert_healthy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    750\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m    751\u001b[0m         \u001b[38;5;66;03m# Simiply raise the error, which will get handled by the try-except\u001b[39;00m\n\u001b[1;32m    752\u001b[0m         \u001b[38;5;66;03m# clause around the _setup().\u001b[39;00m\n\u001b[1;32m    753\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m result\u001b[38;5;241m.\u001b[39mok:\n\u001b[1;32m    754\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m result\u001b[38;5;241m.\u001b[39mget()\n",
      "File \u001b[0;32m~/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/rllib/utils/actor_manager.py:622\u001b[0m, in \u001b[0;36mFaultTolerantActorManager.foreach_actor\u001b[0;34m(self, func, healthy_only, remote_actor_ids, timeout_seconds, return_obj_refs, mark_healthy)\u001b[0m\n\u001b[1;32m    616\u001b[0m remote_calls \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_actors(\n\u001b[1;32m    617\u001b[0m     func\u001b[38;5;241m=\u001b[39mfunc,\n\u001b[1;32m    618\u001b[0m     remote_actor_ids\u001b[38;5;241m=\u001b[39mremote_actor_ids,\n\u001b[1;32m    619\u001b[0m )\n\u001b[1;32m    621\u001b[0m \u001b[38;5;66;03m# Collect remote request results (if available given timeout and/or errors).\u001b[39;00m\n\u001b[0;32m--> 622\u001b[0m _, remote_results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fetch_result\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    623\u001b[0m \u001b[43m    \u001b[49m\u001b[43mremote_actor_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mremote_actor_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    624\u001b[0m \u001b[43m    \u001b[49m\u001b[43mremote_calls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mremote_calls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    625\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtags\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mremote_calls\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    626\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout_seconds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout_seconds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    627\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_obj_refs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_obj_refs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    628\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmark_healthy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmark_healthy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    629\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    631\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m remote_results\n",
      "File \u001b[0;32m~/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/rllib/utils/actor_manager.py:476\u001b[0m, in \u001b[0;36mFaultTolerantActorManager._fetch_result\u001b[0;34m(self, remote_actor_ids, remote_calls, tags, timeout_seconds, return_obj_refs, mark_healthy)\u001b[0m\n\u001b[1;32m    473\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m remote_calls:\n\u001b[1;32m    474\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [], RemoteCallResults()\n\u001b[0;32m--> 476\u001b[0m ready, _ \u001b[38;5;241m=\u001b[39m \u001b[43mray\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    477\u001b[0m \u001b[43m    \u001b[49m\u001b[43mremote_calls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    478\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_returns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mremote_calls\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    479\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    480\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# Make sure remote results are fetched locally in parallel.\u001b[39;49;00m\n\u001b[1;32m    481\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfetch_local\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mreturn_obj_refs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    482\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    484\u001b[0m \u001b[38;5;66;03m# Remote data should already be fetched to local object store at this point.\u001b[39;00m\n\u001b[1;32m    485\u001b[0m remote_results \u001b[38;5;241m=\u001b[39m RemoteCallResults()\n",
      "File \u001b[0;32m~/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/_private/auto_init_hook.py:21\u001b[0m, in \u001b[0;36mwrap_auto_init.<locals>.auto_init_wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(fn)\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mauto_init_wrapper\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     20\u001b[0m     auto_init_ray()\n\u001b[0;32m---> 21\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/_private/client_mode_hook.py:103\u001b[0m, in \u001b[0;36mclient_mode_hook.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    101\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m func\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minit\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m is_client_mode_enabled_by_default:\n\u001b[1;32m    102\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(ray, func\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m)(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m--> 103\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/tianEnv/lib/python3.11/site-packages/ray/_private/worker.py:2854\u001b[0m, in \u001b[0;36mwait\u001b[0;34m(ray_waitables, num_returns, timeout, fetch_local)\u001b[0m\n\u001b[1;32m   2852\u001b[0m timeout \u001b[38;5;241m=\u001b[39m timeout \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;241m10\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m6\u001b[39m\n\u001b[1;32m   2853\u001b[0m timeout_milliseconds \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(timeout \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m1000\u001b[39m)\n\u001b[0;32m-> 2854\u001b[0m ready_ids, remaining_ids \u001b[38;5;241m=\u001b[39m \u001b[43mworker\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcore_worker\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2855\u001b[0m \u001b[43m    \u001b[49m\u001b[43mray_waitables\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2856\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_returns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2857\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout_milliseconds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2858\u001b[0m \u001b[43m    \u001b[49m\u001b[43mworker\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcurrent_task_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2859\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfetch_local\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2860\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2861\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m ready_ids, remaining_ids\n",
      "File \u001b[0;32mpython/ray/_raylet.pyx:3812\u001b[0m, in \u001b[0;36mray._raylet.CoreWorker.wait\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpython/ray/_raylet.pyx:571\u001b[0m, in \u001b[0;36mray._raylet.check_status\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from ray.rllib.algorithms.dqn.dqn import DQNConfig\n",
    "from ray.rllib.algorithms import DQN\n",
    "from ray.tune.registry import register_env\n",
    "\n",
    "observations_memory = 3\n",
    "register_env(\"my_env\", lambda _: PointCoverageEnv({\"height\": 5, \"width\": 5, \"n_agents\": 2, \"n_targets\": 2, \"max_steps\": 30, \"observations_memory\": observations_memory}))\n",
    "\n",
    "config = DQNConfig()\n",
    "\n",
    "replay_config = {\n",
    "        \"type\": \"MultiAgentPrioritizedReplayBuffer\",\n",
    "        \"capacity\": 60000,\n",
    "        \"prioritized_replay_alpha\": 0.5,\n",
    "        \"prioritized_replay_beta\": 0.5,\n",
    "        \"prioritized_replay_eps\": 3e-6,\n",
    "    }\n",
    "\n",
    "config = config.training(replay_buffer_config=replay_config)\n",
    "config = config.resources(num_gpus=0)\n",
    "config = config.env_runners(num_env_runners=1)\n",
    "config = config.environment(\"my_env\")\n",
    "config.sample_timeout_s *= 5 \n",
    "algo2 = DQN(config=config)\n",
    "for i in range(30):\n",
    "    result = algo2.train()\n",
    "    print(f\"[{i}] mean_reward: {result['sampler_results']['episode_reward_mean']}, mean_len: {result['sampler_results']['episode_len_mean']}\")\n",
    "    #customResultPrint(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tianshou"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0.0\n"
     ]
    }
   ],
   "source": [
    "import tianshou\n",
    "import torch\n",
    "from tianshou.highlevel.config import SamplingConfig\n",
    "from tianshou.highlevel.env import (\n",
    "    EnvFactoryRegistered,\n",
    "    VectorEnvType,\n",
    ")\n",
    "from tianshou.highlevel.experiment import DQNExperimentBuilder, ExperimentConfig,  PPOExperimentBuilder\n",
    "from tianshou.highlevel.params.policy_params import DQNParams\n",
    "from tianshou.highlevel.trainer import (\n",
    "    EpochTestCallbackDQNSetEps,\n",
    "    EpochTrainCallbackDQNSetEps,\n",
    "    EpochStopCallbackRewardThreshold\n",
    ")\n",
    "from tianshou.env import DummyVectorEnv\n",
    "from tianshou.utils.net.discrete import (Actor, Net, Critic)\n",
    "from tianshou.utils.net.common import ActorCritic\n",
    "from tianshou.policy.base import BasePolicy\n",
    "from tianshou.policy.modelfree.ppo import PPOPolicy\n",
    "from tianshou.data.collector import Collector\n",
    "from tianshou.data.buffer.vecbuf import VectorReplayBuffer\n",
    "from tianshou.trainer.base import OnpolicyTrainer\n",
    "from tianshou.data.batch import Batch \n",
    "from tianshou.policy.multiagent.mapolicy import MultiAgentPolicyManager\n",
    "print(tianshou.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://tianshou.org/en/stable/02_notebooks/L7_Experiment.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'agent-0': array([0.7, 0.8, 0.8, 0. , 0.9, 0.5, 0.1, 0.3], dtype=float32), 'agent-1': array([0.8, 0. , 0.7, 0.8, 0.9, 0.5, 0.1, 0.3], dtype=float32)}\n",
      "Batch(\n",
      "    agent-0: array([0.7, 0.8, 0.8, 0. , 0.9, 0.5, 0.1, 0.3], dtype=float32),\n",
      "    agent-1: array([0.8, 0. , 0.7, 0.8, 0.9, 0.5, 0.1, 0.3], dtype=float32),\n",
      ")\n",
      "(8,)\n"
     ]
    }
   ],
   "source": [
    "env = PointCoverageEnv({\"height\": 10, \"width\": 10, \"n_agents\": 2, \"n_targets\": 2, \"max_steps\": 100, \"use_nested_observation\": False})\n",
    "obs, _ = env.reset()\n",
    "print(obs)\n",
    "print(Batch(obs)) \n",
    "print(env.observation_space.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'dict' object has no attribute 'agent_id'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 52\u001b[0m\n\u001b[1;32m     34\u001b[0m train_collector \u001b[38;5;241m=\u001b[39m Collector(\n\u001b[1;32m     35\u001b[0m     policy\u001b[38;5;241m=\u001b[39mmapolicy_manager,\n\u001b[1;32m     36\u001b[0m     env\u001b[38;5;241m=\u001b[39mtrain_envs,\n\u001b[1;32m     37\u001b[0m     buffer\u001b[38;5;241m=\u001b[39mVectorReplayBuffer(\u001b[38;5;241m20000\u001b[39m, \u001b[38;5;28mlen\u001b[39m(train_envs)),\n\u001b[1;32m     38\u001b[0m )\n\u001b[1;32m     39\u001b[0m test_collector \u001b[38;5;241m=\u001b[39m Collector(policy\u001b[38;5;241m=\u001b[39mmapolicy_manager, env\u001b[38;5;241m=\u001b[39mtest_envs)\n\u001b[1;32m     41\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mOnpolicyTrainer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     42\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpolicy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmapolicy_manager\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     43\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_collector\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_collector\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     44\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtest_collector\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtest_collector\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     45\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_epoch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     46\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstep_per_epoch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m50000\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     47\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrepeat_per_collect\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     48\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepisode_per_test\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     49\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m256\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     50\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstep_per_collect\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2000\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     51\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstop_fn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mmean_reward\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmean_reward\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m>\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m195\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m---> 52\u001b[0m \u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     54\u001b[0m result\u001b[38;5;241m.\u001b[39mpprint_asdict()\n",
      "File \u001b[0;32m~/anaconda3/envs/tianEnv/lib/python3.11/site-packages/tianshou/trainer/base.py:557\u001b[0m, in \u001b[0;36mBaseTrainer.run\u001b[0;34m(self, reset_prior_to_run)\u001b[0m\n\u001b[1;32m    551\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Consume iterator.\u001b[39;00m\n\u001b[1;32m    552\u001b[0m \n\u001b[1;32m    553\u001b[0m \u001b[38;5;124;03mSee itertools - recipes. Use functions that consume iterators at C speed\u001b[39;00m\n\u001b[1;32m    554\u001b[0m \u001b[38;5;124;03m(feed the entire iterator into a zero-length deque).\u001b[39;00m\n\u001b[1;32m    555\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    556\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m reset_prior_to_run:\n\u001b[0;32m--> 557\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreset\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    558\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    559\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_run \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/tianEnv/lib/python3.11/site-packages/tianshou/trainer/base.py:261\u001b[0m, in \u001b[0;36mBaseTrainer.reset\u001b[0;34m(self, reset_collectors, reset_buffer)\u001b[0m\n\u001b[1;32m    259\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mepisode_per_test \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    260\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtest_collector, AsyncCollector)  \u001b[38;5;66;03m# Issue 700\u001b[39;00m\n\u001b[0;32m--> 261\u001b[0m test_result \u001b[38;5;241m=\u001b[39m \u001b[43mtest_episode\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    262\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtest_collector\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    263\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtest_fn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    264\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstart_epoch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    265\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mepisode_per_test\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    266\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlogger\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    267\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv_step\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    268\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreward_metric\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    269\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    270\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m test_result\u001b[38;5;241m.\u001b[39mreturns_stat \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m  \u001b[38;5;66;03m# for mypy\u001b[39;00m\n\u001b[1;32m    271\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbest_epoch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstart_epoch\n",
      "File \u001b[0;32m~/anaconda3/envs/tianEnv/lib/python3.11/site-packages/tianshou/trainer/utils.py:30\u001b[0m, in \u001b[0;36mtest_episode\u001b[0;34m(collector, test_fn, epoch, n_episode, logger, global_step, reward_metric)\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m test_fn:\n\u001b[1;32m     29\u001b[0m     test_fn(epoch, global_step)\n\u001b[0;32m---> 30\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mcollector\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollect\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn_episode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_episode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m reward_metric:  \u001b[38;5;66;03m# TODO: move into collector\u001b[39;00m\n\u001b[1;32m     32\u001b[0m     rew \u001b[38;5;241m=\u001b[39m reward_metric(result\u001b[38;5;241m.\u001b[39mreturns)\n",
      "File \u001b[0;32m~/anaconda3/envs/tianEnv/lib/python3.11/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/tianEnv/lib/python3.11/site-packages/tianshou/data/collector.py:304\u001b[0m, in \u001b[0;36mBaseCollector.collect\u001b[0;34m(self, n_step, n_episode, random, render, reset_before_collect, gym_reset_kwargs)\u001b[0m\n\u001b[1;32m    301\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreset(reset_buffer\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, gym_reset_kwargs\u001b[38;5;241m=\u001b[39mgym_reset_kwargs)\n\u001b[1;32m    303\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch_train_mode(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpolicy, \u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[0;32m--> 304\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_collect\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    305\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_step\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_step\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    306\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_episode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_episode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    307\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrandom\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrandom\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    308\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrender\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrender\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    309\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgym_reset_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgym_reset_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    310\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/tianEnv/lib/python3.11/site-packages/tianshou/data/collector.py:507\u001b[0m, in \u001b[0;36mCollector._collect\u001b[0;34m(self, n_step, n_episode, random, render, gym_reset_kwargs)\u001b[0m\n\u001b[1;32m    487\u001b[0m last_hidden_state_RH \u001b[38;5;241m=\u001b[39m _nullable_slice(\n\u001b[1;32m    488\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pre_collect_hidden_state_RH,\n\u001b[1;32m    489\u001b[0m     ready_env_ids_R,\n\u001b[1;32m    490\u001b[0m )\n\u001b[1;32m    492\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m    493\u001b[0m     \u001b[38;5;66;03m# todo check if we need this when using cur_rollout_batch\u001b[39;00m\n\u001b[1;32m    494\u001b[0m     \u001b[38;5;66;03m# if len(cur_rollout_batch) != len(ready_env_ids):\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    500\u001b[0m \n\u001b[1;32m    501\u001b[0m     \u001b[38;5;66;03m# get the next action\u001b[39;00m\n\u001b[1;32m    502\u001b[0m     (\n\u001b[1;32m    503\u001b[0m         act_RA,\n\u001b[1;32m    504\u001b[0m         act_normalized_RA,\n\u001b[1;32m    505\u001b[0m         policy_R,\n\u001b[1;32m    506\u001b[0m         hidden_state_RH,\n\u001b[0;32m--> 507\u001b[0m     ) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_compute_action_policy_hidden\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    508\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrandom\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrandom\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    509\u001b[0m \u001b[43m        \u001b[49m\u001b[43mready_env_ids_R\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mready_env_ids_R\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    510\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlast_obs_RO\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlast_obs_RO\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    511\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlast_info_R\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlast_info_R\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    512\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlast_hidden_state_RH\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlast_hidden_state_RH\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    513\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    515\u001b[0m     obs_next_RO, rew_R, terminated_R, truncated_R, info_R \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv\u001b[38;5;241m.\u001b[39mstep(\n\u001b[1;32m    516\u001b[0m         act_normalized_RA,\n\u001b[1;32m    517\u001b[0m         ready_env_ids_R,\n\u001b[1;32m    518\u001b[0m     )\n\u001b[1;32m    519\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(info_R, \u001b[38;5;28mdict\u001b[39m):  \u001b[38;5;66;03m# type: ignore[unreachable]\u001b[39;00m\n\u001b[1;32m    520\u001b[0m         \u001b[38;5;66;03m# This can happen if the env is an envpool env. Then the info returned by step is a dict\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/tianEnv/lib/python3.11/site-packages/tianshou/data/collector.py:419\u001b[0m, in \u001b[0;36mCollector._compute_action_policy_hidden\u001b[0;34m(self, random, ready_env_ids_R, last_obs_RO, last_info_R, last_hidden_state_RH)\u001b[0m\n\u001b[1;32m    416\u001b[0m info_batch \u001b[38;5;241m=\u001b[39m _HACKY_create_info_batch(last_info_R)\n\u001b[1;32m    417\u001b[0m obs_batch_R \u001b[38;5;241m=\u001b[39m cast(ObsBatchProtocol, Batch(obs\u001b[38;5;241m=\u001b[39mlast_obs_RO, info\u001b[38;5;241m=\u001b[39minfo_batch))\n\u001b[0;32m--> 419\u001b[0m act_batch_RA \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpolicy\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    420\u001b[0m \u001b[43m    \u001b[49m\u001b[43mobs_batch_R\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    421\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlast_hidden_state_RH\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    422\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    424\u001b[0m act_RA \u001b[38;5;241m=\u001b[39m to_numpy(act_batch_RA\u001b[38;5;241m.\u001b[39mact)\n\u001b[1;32m    425\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexploration_noise:\n",
      "File \u001b[0;32m~/anaconda3/envs/tianEnv/lib/python3.11/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/tianEnv/lib/python3.11/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/tianEnv/lib/python3.11/site-packages/tianshou/policy/multiagent/mapolicy.py:222\u001b[0m, in \u001b[0;36mMultiAgentPolicyManager.forward\u001b[0;34m(self, batch, state, **kwargs)\u001b[0m\n\u001b[1;32m    213\u001b[0m results: \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mtuple\u001b[39m[\u001b[38;5;28mbool\u001b[39m, np\u001b[38;5;241m.\u001b[39mndarray, Batch, np\u001b[38;5;241m.\u001b[39mndarray \u001b[38;5;241m|\u001b[39m Batch, Batch]] \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    214\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m agent_id, policy \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpolicies\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m    215\u001b[0m     \u001b[38;5;66;03m# This part of code is difficult to understand.\u001b[39;00m\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;66;03m# Let's follow an example with two agents\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    220\u001b[0m     \u001b[38;5;66;03m# agent_index for agent 2 is [1, 3, 5]\u001b[39;00m\n\u001b[1;32m    221\u001b[0m     \u001b[38;5;66;03m# we separate the transition of each agent according to agent_id\u001b[39;00m\n\u001b[0;32m--> 222\u001b[0m     agent_index \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mnonzero(\u001b[43mbatch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mobs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43magent_id\u001b[49m \u001b[38;5;241m==\u001b[39m agent_id)[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    223\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(agent_index) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    224\u001b[0m         \u001b[38;5;66;03m# (has_data, agent_index, out, act, state)\u001b[39;00m\n\u001b[1;32m    225\u001b[0m         results\u001b[38;5;241m.\u001b[39mappend((\u001b[38;5;28;01mFalse\u001b[39;00m, np\u001b[38;5;241m.\u001b[39marray([\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]), Batch(), Batch(), Batch()))\n",
      "File \u001b[0;32m~/anaconda3/envs/tianEnv/lib/python3.11/site-packages/tianshou/data/batch.py:474\u001b[0m, in \u001b[0;36mBatch.__getattr__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    472\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getattr__\u001b[39m(\u001b[38;5;28mself\u001b[39m, key: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m    473\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Return self.key. The \"Any\" return type is needed for mypy.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 474\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__dict__\u001b[39m, key)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'dict' object has no attribute 'agent_id'"
     ]
    }
   ],
   "source": [
    "train_size, test_size = (20, 10)\n",
    "device = \"cpu\"\n",
    "\n",
    "env = PointCoverageEnv({\"height\": 10, \"width\": 10, \"n_agents\": 2, \"n_targets\": 2, \"max_steps\": 100, \"use_nested_observation\": False})\n",
    "train_envs = DummyVectorEnv([lambda: PointCoverageEnv({\"height\": 10, \"width\": 10, \"n_agents\": 2, \"n_targets\": 2, \"max_steps\": 100, \"use_nested_observation\": False}) for _ in range(train_size)])\n",
    "test_envs = DummyVectorEnv([lambda: PointCoverageEnv({\"height\": 10, \"width\": 10, \"n_agents\": 2, \"n_targets\": 2, \"max_steps\": 100, \"use_nested_observation\": False}) for _ in range(test_size)])\n",
    "\n",
    "assert env.observation_space.shape is not None\n",
    "assert isinstance(env.action_space, Discrete) \n",
    "\n",
    "net = Net(state_shape=env.observation_space.shape, hidden_sizes=[64, 64], device=device)\n",
    "actor = Actor(preprocess_net=net, action_shape=env.action_space.n, device=device).to(device)\n",
    "critic = Critic(preprocess_net=net, device=device).to(device)\n",
    "actor_critic = ActorCritic(actor=actor, critic=critic)\n",
    "\n",
    "# optimizer of the actor and the critic\n",
    "optim = torch.optim.Adam(actor_critic.parameters(), lr=0.0003)\n",
    "\n",
    "dist = torch.distributions.Categorical\n",
    "policy: BasePolicy\n",
    "policy = PPOPolicy(\n",
    "    actor=actor,\n",
    "    critic=critic,\n",
    "    optim=optim,\n",
    "    dist_fn=dist,\n",
    "    action_space=env.action_space,\n",
    "    deterministic_eval=True,\n",
    "    action_scaling=False,\n",
    ")\n",
    "\n",
    "mapolicy_manager = MultiAgentPolicyManager(policies=[policy, policy], env=env)\n",
    "\n",
    "\n",
    "train_collector = Collector(\n",
    "    policy=mapolicy_manager,\n",
    "    env=train_envs,\n",
    "    buffer=VectorReplayBuffer(20000, len(train_envs)),\n",
    ")\n",
    "test_collector = Collector(policy=mapolicy_manager, env=test_envs)\n",
    "\n",
    "result = OnpolicyTrainer(\n",
    "    policy=mapolicy_manager,\n",
    "    train_collector=train_collector,\n",
    "    test_collector=test_collector,\n",
    "    max_epoch=10,\n",
    "    step_per_epoch=50000,\n",
    "    repeat_per_collect=10,\n",
    "    episode_per_test=10,\n",
    "    batch_size=256,\n",
    "    step_per_collect=2000,\n",
    "    stop_fn=lambda mean_reward: mean_reward >= 195,\n",
    ").run()\n",
    "\n",
    "result.pprint_asdict()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rayEnv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
