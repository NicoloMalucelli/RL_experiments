{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Set\n",
    "from ray.rllib.env.multi_agent_env import MultiAgentEnv\n",
    "import random as rnd\n",
    "import math\n",
    "from gymnasium.spaces import Discrete, Box, Sequence, Dict\n",
    "from gymnasium.spaces.utils import flatten, flatten_space\n",
    "import numpy as np\n",
    "from IPython.display import clear_output\n",
    "\n",
    "from ray.rllib.utils.typing import AgentID\n",
    "\n",
    "class PointCoverageEnv(MultiAgentEnv):\n",
    "\n",
    "    actions_dict = [(0,-1),(0,1),(1,0),(-1,0),(0,0)]\n",
    "\n",
    "    def __init__(self, config):\n",
    "        self.width = config[\"width\"]\n",
    "        self.height = config[\"height\"]\n",
    "        self.n_agents = config[\"n_agents\"]\n",
    "        self.n_targets = config[\"n_targets\"]\n",
    "        self.max_steps = config[\"max_steps\"] if \"max_steps\" in config.keys() else None\n",
    "        self.agents = ['agent-' + str(i) for i in range(self.n_agents)]\n",
    "        self.observation_space = self.observation_space('agent-0')\n",
    "        self.action_space = Discrete(5)\n",
    "\n",
    "    def unflatten_observation_space(self, agent):\n",
    "       coordinates_space = Box(low=np.array([0, 0]), high=np.array([self.width-1, self.height-1]), dtype=np.int32)\n",
    "       if self.n_agents > 1:\n",
    "            return Dict({\n",
    "                \"position\": coordinates_space,\n",
    "                \"other_agents\": Dict({f\"other_agent-{i}\": coordinates_space for i in range(self.n_agents-1)}),\n",
    "                \"targets\": Dict({f\"target-{i}\": coordinates_space for i in range(self.n_targets)})\n",
    "            })\n",
    "       else:\n",
    "           return Dict({\n",
    "                \"position\": coordinates_space,\n",
    "                \"targets\": Dict({f\"target-{i}\": coordinates_space for i in range(self.n_targets)})\n",
    "            })\n",
    "\n",
    "    def observation_space(self, agent):\n",
    "       return flatten_space(self.unflatten_observation_space(agent))\n",
    "       #return self.unflatten_observation_space(agent)\n",
    "\n",
    "    def action_space(self, agent):\n",
    "        return Discrete(5)\n",
    "    \n",
    "    def __get_other_agents(self, agent):\n",
    "        return [other for other in self.agents if other != agent]\n",
    "\n",
    "    def __get_random_point(self):\n",
    "        return (rnd.randint(0, self.width-1), rnd.randint(0, self.height-1))\n",
    "    \n",
    "    def __get_observation(self, agent):\n",
    "        if self.n_agents > 1:\n",
    "            return flatten(self.unflatten_observation_space(agent), \n",
    "                {\n",
    "                    \"position\": self.agent_pos[agent],\n",
    "                    \"other_agents\": {f\"other_agent-{i}\": self.agent_pos[other] for i, other in enumerate(self.__get_other_agents(agent))},\n",
    "                    \"targets\": {f\"target-{i}\": pos for i, pos in enumerate(self.targets)}\n",
    "                }\n",
    "            )\n",
    "        else:\n",
    "            return flatten(self.unflatten_observation_space(agent), \n",
    "                {\n",
    "                    \"position\": self.agent_pos[agent],\n",
    "                    \"targets\": {f\"target-{i}\": pos for i, pos in enumerate(self.targets)}\n",
    "                }\n",
    "            )\n",
    "\n",
    "    def __get_not_covered_targets(self):\n",
    "        return set(self.targets) - set(self.agent_pos.values())\n",
    "\n",
    "    def __is_target_contended(self, target):\n",
    "        return len([t for t in self.agent_pos.values() if target == t]) > 1\n",
    "\n",
    "    def __get_reward(self, agent):\n",
    "        return -1 + self.__get_global_reward()\n",
    "        if self.agent_pos[agent] in self.targets:\n",
    "            if self.agent_pos[agent] in [pos[1] for pos in self.old_agent_pos if pos[0] != agent]:\n",
    "                return -1 # someone was already covering the target -> no +10 reward\n",
    "            if self.__is_target_contended(self.agent_pos[agent]):\n",
    "                return -2 # someone arrived at the target at the same time of me -> someone has to leave\n",
    "            return 10\n",
    "        else:\n",
    "            return -1\n",
    "    \n",
    "    def __get_global_reward(self):\n",
    "        return (len(self.not_covered_target) - len(set(self.not_covered_target) - set(self.agent_pos.values())))*10\n",
    "    \n",
    "    def __update_agent_position(self, agent, x, y):\n",
    "        self.agent_pos[agent] = (max(min(self.agent_pos[agent][0] + x, self.width-1), 0),\n",
    "                                 max(min(self.agent_pos[agent][1] + y, self.height-1), 0))\n",
    "\n",
    "    def reset(self, seed=None, options=None):\n",
    "        self.agent_pos = {agent: self.__get_random_point() for agent in self.agents}\n",
    "        self.targets = [self.__get_random_point() for _ in range(self.n_targets)]\n",
    "        self.not_covered_target = self.targets.copy()\n",
    "        self.steps = 0;\n",
    "        return {agent: self.__get_observation(agent) for agent in self.agents}, {}\n",
    "     \n",
    "    def step(self, actions):\n",
    "        self.steps += 1\n",
    "        observations, rewards, terminated, truncated, infos = {}, {}, {}, {}, {}\n",
    "\n",
    "        self.old_agent_pos = self.agent_pos.copy()\n",
    "        for agent, action in actions.items():\n",
    "            self.__update_agent_position(agent, self.actions_dict[action][0], self.actions_dict[action][1])\n",
    "\n",
    "        for agent in actions.keys():\n",
    "            if not (self.agent_pos[agent] in self.targets and not self.__is_target_contended(self.agent_pos[agent])):\n",
    "                observations[agent] = self.__get_observation(agent)\n",
    "                rewards[agent] = self.__get_reward(agent)\n",
    "                terminated[agent] = False\n",
    "                truncated[agent] = False\n",
    "                infos[agent] = {}\n",
    "        \n",
    "        if self.max_steps != None and self.steps > self.max_steps:\n",
    "            truncated['__all__'] = True\n",
    "        else:\n",
    "            truncated['__all__'] = False\n",
    "\n",
    "        self.not_covered_target = list(set(self.not_covered_target) - set(self.agent_pos.values())) \n",
    "\n",
    "        terminated['__all__'] = len(self.__get_not_covered_targets()) == 0\n",
    "        return observations, rewards, terminated, truncated, infos\n",
    "     \n",
    "    def render(self, mode='text'):\n",
    "        str = '_' * (self.width+2) + '\\n'\n",
    "        for i in range(self.height):\n",
    "            str = str + \"|\"\n",
    "            for j in range(self.width):\n",
    "                if (j,i) in self.agent_pos.values() and (j,i) in self.targets:\n",
    "                    str = str + '*'\n",
    "                elif (j,i) in self.agent_pos.values():\n",
    "                    str = str + 'o'\n",
    "                elif (j,i) in self.targets:\n",
    "                    str = str + 'x'\n",
    "                else:\n",
    "                    str = str + ' '\n",
    "            str = str + '|\\n'\n",
    "        str = str + '‾' * (self.width+2)\n",
    "        print(str)\n",
    "\n",
    "    def get_agent_ids(self):\n",
    "       return self.agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[7 2 4 7 1 2 6 4]\n",
      "[4 7 7 2 1 2 6 4]\n",
      "____________\n",
      "|          |\n",
      "|          |\n",
      "| x     o  |\n",
      "|          |\n",
      "|      x   |\n",
      "|          |\n",
      "|          |\n",
      "|    o     |\n",
      "|          |\n",
      "|          |\n",
      "‾‾‾‾‾‾‾‾‾‾‾‾\n",
      "[7 1 4 8 1 2 6 4]\n",
      "____________\n",
      "|          |\n",
      "|       o  |\n",
      "| x        |\n",
      "|          |\n",
      "|      x   |\n",
      "|          |\n",
      "|          |\n",
      "|          |\n",
      "|    o     |\n",
      "|          |\n",
      "‾‾‾‾‾‾‾‾‾‾‾‾\n"
     ]
    }
   ],
   "source": [
    "env = PointCoverageEnv({\"height\": 10, \"width\": 10, \"n_agents\": 2, \"n_targets\": 2})\n",
    "obs, _ = env.reset() \n",
    "print(obs['agent-0'])\n",
    "print(obs['agent-1'])\n",
    "env.render()\n",
    "\n",
    "actions = {\"agent-0\": 1, \"agent-1\": 0}\n",
    "obs, _, _, _, _ = env.step(actions)\n",
    "print(obs['agent-0'])\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-16 14:54:59,281\tINFO worker.py:1740 -- Started a local Ray instance. View the dashboard at \u001b[1m\u001b[32mhttp://127.0.0.1:8265 \u001b[39m\u001b[22m\n"
     ]
    }
   ],
   "source": [
    "import ray\n",
    "ray.shutdown()\n",
    "ray.init()\n",
    "\n",
    "def customResultPrint(result):\n",
    "    print(f\"iteration [{result['training_iteration']}] => \" +\n",
    "          f\"episode_reward_mean: {result['sampler_results']['episode_reward_mean']}, \" +\n",
    "          f\"episode_len_mean: {result['sampler_results']['episode_len_mean']}, \" +\n",
    "          f\"agent_steps_trained: {result['info']['num_agent_steps_trained']}, \" +\n",
    "          f\"env_steps_trained: {result['info']['num_env_steps_trained']}, \" + \n",
    "          f\"entropy: {result['info']['learner']['default_policy']['learner_stats']['entropy']}, \" +\n",
    "          f\"learning_rate: {result['info']['learner']['default_policy']['learner_stats']['cur_lr']}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Single agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nicolo/anaconda3/envs/rayEnv/lib/python3.11/site-packages/ray/rllib/algorithms/algorithm.py:521: RayDeprecationWarning: This API is deprecated and may be removed in future Ray releases. You could suppress this warning by setting env variable PYTHONWARNINGS=\"ignore::DeprecationWarning\"\n",
      "`UnifiedLogger` will be removed in Ray 2.7.\n",
      "  return UnifiedLogger(config, logdir, loggers=None)\n",
      "/home/nicolo/anaconda3/envs/rayEnv/lib/python3.11/site-packages/ray/tune/logger/unified.py:53: RayDeprecationWarning: This API is deprecated and may be removed in future Ray releases. You could suppress this warning by setting env variable PYTHONWARNINGS=\"ignore::DeprecationWarning\"\n",
      "The `JsonLogger interface is deprecated in favor of the `ray.tune.json.JsonLoggerCallback` interface and will be removed in Ray 2.7.\n",
      "  self._loggers.append(cls(self.config, self.logdir, self.trial))\n",
      "/home/nicolo/anaconda3/envs/rayEnv/lib/python3.11/site-packages/ray/tune/logger/unified.py:53: RayDeprecationWarning: This API is deprecated and may be removed in future Ray releases. You could suppress this warning by setting env variable PYTHONWARNINGS=\"ignore::DeprecationWarning\"\n",
      "The `CSVLogger interface is deprecated in favor of the `ray.tune.csv.CSVLoggerCallback` interface and will be removed in Ray 2.7.\n",
      "  self._loggers.append(cls(self.config, self.logdir, self.trial))\n",
      "/home/nicolo/anaconda3/envs/rayEnv/lib/python3.11/site-packages/ray/tune/logger/unified.py:53: RayDeprecationWarning: This API is deprecated and may be removed in future Ray releases. You could suppress this warning by setting env variable PYTHONWARNINGS=\"ignore::DeprecationWarning\"\n",
      "The `TBXLogger interface is deprecated in favor of the `ray.tune.tensorboardx.TBXLoggerCallback` interface and will be removed in Ray 2.7.\n",
      "  self._loggers.append(cls(self.config, self.logdir, self.trial))\n",
      "2024-05-16 14:57:44,859\tWARNING deprecation.py:50 -- DeprecationWarning: `WorkerSet(num_workers=... OR local_worker=...)` has been deprecated. Use `EnvRunnerGroup(num_env_runners=... AND local_env_runner=...)` instead. This will raise an error in the future!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of different environment steps: 61440\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-16 14:57:55,679\tINFO trainable.py:161 -- Trainable.setup took 10.821 seconds. If your trainable is slow to initialize, consider setting reuse_actors=True to reduce actor creation overheads.\n",
      "2024-05-16 14:57:55,687\tWARNING util.py:61 -- Install gputil for GPU system monitoring.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration [1] => episode_reward_mean: -87.82608695652173, episode_len_mean: 88.08695652173913, agent_steps_trained: 2048, env_steps_trained: 2048, entropy: 1.5966786593198776, learning_rate: 0.0010000000000000002\n",
      "Checkpoint saved in directory /tmp/tmp2fua4s14\n",
      "iteration [2] => episode_reward_mean: -81.38, episode_len_mean: 81.74, agent_steps_trained: 4096, env_steps_trained: 4096, entropy: 1.5709845513105392, learning_rate: 0.0010000000000000002\n",
      "iteration [3] => episode_reward_mean: -81.34666666666666, episode_len_mean: 81.72, agent_steps_trained: 6144, env_steps_trained: 6144, entropy: 1.5638435751199722, learning_rate: 0.0010000000000000002\n",
      "iteration [4] => episode_reward_mean: -77.6, episode_len_mean: 78.0, agent_steps_trained: 8192, env_steps_trained: 8192, entropy: 1.5468495473265649, learning_rate: 0.0010000000000000002\n",
      "iteration [5] => episode_reward_mean: -69.62, episode_len_mean: 70.12, agent_steps_trained: 10240, env_steps_trained: 10240, entropy: 1.5364061310887336, learning_rate: 0.0010000000000000002\n",
      "iteration [6] => episode_reward_mean: -55.11, episode_len_mean: 55.81, agent_steps_trained: 12288, env_steps_trained: 12288, entropy: 1.483174854516983, learning_rate: 0.0010000000000000002\n",
      "Checkpoint saved in directory /tmp/tmpwo7t7bxq\n",
      "iteration [7] => episode_reward_mean: -38.07, episode_len_mean: 38.98, agent_steps_trained: 14336, env_steps_trained: 14336, entropy: 1.4824659407138825, learning_rate: 0.0010000000000000002\n",
      "iteration [8] => episode_reward_mean: -36.31, episode_len_mean: 37.21, agent_steps_trained: 16384, env_steps_trained: 16384, entropy: 1.4453082725405693, learning_rate: 0.0010000000000000002\n",
      "iteration [9] => episode_reward_mean: -37.13, episode_len_mean: 38.05, agent_steps_trained: 18432, env_steps_trained: 18432, entropy: 1.4288546338677406, learning_rate: 0.0010000000000000002\n",
      "iteration [10] => episode_reward_mean: -28.74, episode_len_mean: 29.68, agent_steps_trained: 20480, env_steps_trained: 20480, entropy: 1.4042565017938613, learning_rate: 0.0010000000000000002\n",
      "iteration [11] => episode_reward_mean: -20.06, episode_len_mean: 21.04, agent_steps_trained: 22528, env_steps_trained: 22528, entropy: 1.354799933731556, learning_rate: 0.0010000000000000002\n",
      "Checkpoint saved in directory /tmp/tmp8fmlo1_o\n",
      "iteration [12] => episode_reward_mean: -15.958677685950413, episode_len_mean: 16.950413223140497, agent_steps_trained: 24576, env_steps_trained: 24576, entropy: 1.299217975139618, learning_rate: 0.0010000000000000002\n",
      "iteration [13] => episode_reward_mean: -13.76086956521739, episode_len_mean: 14.76086956521739, agent_steps_trained: 26624, env_steps_trained: 26624, entropy: 1.2416415601968764, learning_rate: 0.0010000000000000002\n",
      "iteration [14] => episode_reward_mean: -11.466666666666667, episode_len_mean: 12.466666666666667, agent_steps_trained: 28672, env_steps_trained: 28672, entropy: 1.1387612789869308, learning_rate: 0.0010000000000000002\n",
      "iteration [15] => episode_reward_mean: -9.590673575129534, episode_len_mean: 10.590673575129534, agent_steps_trained: 30720, env_steps_trained: 30720, entropy: 1.0145523726940155, learning_rate: 0.0010000000000000002\n",
      "iteration [16] => episode_reward_mean: -8.151785714285714, episode_len_mean: 9.151785714285714, agent_steps_trained: 32768, env_steps_trained: 32768, entropy: 0.9127302713692188, learning_rate: 0.0010000000000000002\n",
      "Checkpoint saved in directory /tmp/tmpffov5saz\n",
      "iteration [17] => episode_reward_mean: -7.564853556485356, episode_len_mean: 8.564853556485355, agent_steps_trained: 34816, env_steps_trained: 34816, entropy: 0.8007351629436016, learning_rate: 0.0010000000000000002\n",
      "iteration [18] => episode_reward_mean: -7.407407407407407, episode_len_mean: 8.407407407407407, agent_steps_trained: 36864, env_steps_trained: 36864, entropy: 0.7158502042293549, learning_rate: 0.0010000000000000002\n",
      "iteration [19] => episode_reward_mean: -6.40072202166065, episode_len_mean: 7.40072202166065, agent_steps_trained: 38912, env_steps_trained: 38912, entropy: 0.6098644331097602, learning_rate: 0.0010000000000000002\n",
      "iteration [20] => episode_reward_mean: -6.427536231884058, episode_len_mean: 7.427536231884058, agent_steps_trained: 40960, env_steps_trained: 40960, entropy: 0.561473386734724, learning_rate: 0.0010000000000000002\n",
      "iteration [21] => episode_reward_mean: -6.344086021505376, episode_len_mean: 7.344086021505376, agent_steps_trained: 43008, env_steps_trained: 43008, entropy: 0.50241026468575, learning_rate: 0.0010000000000000002\n",
      "Checkpoint saved in directory /tmp/tmp57mkoqfy\n",
      "iteration [22] => episode_reward_mean: -6.103806228373703, episode_len_mean: 7.103806228373703, agent_steps_trained: 45056, env_steps_trained: 45056, entropy: 0.46649055406451223, learning_rate: 0.0010000000000000002\n",
      "iteration [23] => episode_reward_mean: -5.986348122866894, episode_len_mean: 6.986348122866894, agent_steps_trained: 47104, env_steps_trained: 47104, entropy: 0.4390166413038969, learning_rate: 0.0010000000000000002\n",
      "iteration [24] => episode_reward_mean: -6.0588235294117645, episode_len_mean: 7.0588235294117645, agent_steps_trained: 49152, env_steps_trained: 49152, entropy: 0.40648901015520095, learning_rate: 0.0010000000000000002\n",
      "iteration [25] => episode_reward_mean: -5.526984126984127, episode_len_mean: 6.526984126984127, agent_steps_trained: 51200, env_steps_trained: 51200, entropy: 0.3803903989493847, learning_rate: 0.0010000000000000002\n",
      "iteration [26] => episode_reward_mean: -6.041237113402062, episode_len_mean: 7.041237113402062, agent_steps_trained: 53248, env_steps_trained: 53248, entropy: 0.3783162530511618, learning_rate: 0.0010000000000000002\n",
      "Checkpoint saved in directory /tmp/tmp_fowrvn3\n",
      "iteration [27] => episode_reward_mean: -6.013698630136986, episode_len_mean: 7.013698630136986, agent_steps_trained: 55296, env_steps_trained: 55296, entropy: 0.36471176855266096, learning_rate: 0.0010000000000000002\n",
      "iteration [28] => episode_reward_mean: -5.73841059602649, episode_len_mean: 6.73841059602649, agent_steps_trained: 57344, env_steps_trained: 57344, entropy: 0.35031734555959704, learning_rate: 0.0010000000000000002\n",
      "iteration [29] => episode_reward_mean: -6.347670250896058, episode_len_mean: 7.347670250896058, agent_steps_trained: 59392, env_steps_trained: 59392, entropy: 0.3632784832268953, learning_rate: 0.0010000000000000002\n",
      "iteration [30] => episode_reward_mean: -5.414330218068536, episode_len_mean: 6.414330218068536, agent_steps_trained: 61440, env_steps_trained: 61440, entropy: 0.3422633707523346, learning_rate: 0.0010000000000000002\n"
     ]
    }
   ],
   "source": [
    "from ray.rllib.algorithms.ppo import PPOConfig\n",
    "from ray.tune.logger import pretty_print\n",
    "from ray.tune.registry import register_env\n",
    "from gymnasium.wrappers.time_limit import TimeLimit\n",
    "\n",
    "\n",
    "register_env(\"my_env\", lambda _: PointCoverageEnv({\"height\": 10, \"width\": 10, \"n_agents\": 1, \"n_targets\": 1, \"max_steps\": 100}))\n",
    "\n",
    "train_batch_size = 2048\n",
    "sgd_minibatch_size = 256\n",
    "num_sgd_iter = 10\n",
    "trainings = 30\n",
    "\n",
    "total_env_steps = trainings*train_batch_size\n",
    "\n",
    "print(f\"number of different environment steps: {total_env_steps}\")\n",
    "\n",
    "\n",
    "algo = (\n",
    "    PPOConfig()\n",
    "    .training(gamma=0.99, \n",
    "              lr=0.001,\n",
    "              kl_coeff=0.5, \n",
    "              train_batch_size=train_batch_size, \n",
    "              sgd_minibatch_size=sgd_minibatch_size, \n",
    "              num_sgd_iter=num_sgd_iter)\n",
    "    .env_runners(num_env_runners=1)\n",
    "    .resources(num_gpus=0)\n",
    "    .environment(env=\"my_env\")\n",
    "    .build()\n",
    ")\n",
    "\n",
    "for i in range(trainings):\n",
    "    result = algo.train()\n",
    "    customResultPrint(result)\n",
    "    if i % 5 == 0:\n",
    "        checkpoint_dir = algo.save().checkpoint.path\n",
    "        print(f\"Checkpoint saved in directory {checkpoint_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[29]\n",
      "____________________________________________________\n",
      "|                                                  |\n",
      "|                                                  |\n",
      "|                                                  |\n",
      "|                                                  |\n",
      "|                        *                         |\n",
      "|                                                  |\n",
      "|                                                  |\n",
      "|                                                  |\n",
      "|                                                  |\n",
      "|                                                  |\n",
      "‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾\n",
      "{}\n",
      "{}\n"
     ]
    }
   ],
   "source": [
    "from IPython.display import clear_output\n",
    "import time\n",
    "import torch\n",
    "from gymnasium.spaces.utils import flatten\n",
    "\n",
    "env = PointCoverageEnv({\"height\": 10, \"width\": 50, \"n_agents\": 1, \"n_targets\": 1, \"max_steps\": 30})\n",
    "obs_space = env.observation_space\n",
    "obs, _ = env.reset()\n",
    "env.render()\n",
    "\n",
    "for i in range(100):\n",
    "    actions = algo.compute_actions({agent: o for agent, o in obs.items()})\n",
    "    print(actions, \"\\n\")\n",
    "    \n",
    "    obs, reward, terminated, truncated, info = env.step(actions)\n",
    "    clear_output()\n",
    "    print(f\"[{i}]\")\n",
    "    env.render()\n",
    "    print(obs)\n",
    "    print(reward)\n",
    "    time.sleep(0.5)\n",
    "\n",
    "    if terminated['__all__'] or truncated['__all__']:\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Two Agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-16 13:13:09,443\tWARNING deprecation.py:50 -- DeprecationWarning: `WorkerSet(num_workers=... OR local_worker=...)` has been deprecated. Use `EnvRunnerGroup(num_env_runners=... AND local_env_runner=...)` instead. This will raise an error in the future!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of different environment steps: 102400\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-16 13:13:18,534\tWARNING util.py:61 -- Install gputil for GPU system monitoring.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration [1] => episode_reward_mean: -27.611940298507463, episode_len_mean: 30.402985074626866, agent_steps_trained: 2693, env_steps_trained: 2048, entropy: 1.608395733833313, learning_rate: 0.009999999999999998\n",
      "Checkpoint saved in directory /tmp/tmprff5mkxb\n",
      "iteration [2] => episode_reward_mean: -26.11, episode_len_mean: 30.57, agent_steps_trained: 5162, env_steps_trained: 4096, entropy: 1.598888381322225, learning_rate: 0.0010000000000000002\n",
      "iteration [3] => episode_reward_mean: -25.92, episode_len_mean: 30.53, agent_steps_trained: 7793, env_steps_trained: 6144, entropy: 1.5958628177642822, learning_rate: 0.0010000000000000002\n",
      "iteration [4] => episode_reward_mean: -21.45, episode_len_mean: 27.31, agent_steps_trained: 10391, env_steps_trained: 8192, entropy: 1.5846111023426055, learning_rate: 0.0010000000000000002\n",
      "iteration [5] => episode_reward_mean: -18.53, episode_len_mean: 25.63, agent_steps_trained: 12899, env_steps_trained: 10240, entropy: 1.5749792496363322, learning_rate: 0.0010000000000000002\n",
      "iteration [6] => episode_reward_mean: -15.05, episode_len_mean: 23.33, agent_steps_trained: 15382, env_steps_trained: 12288, entropy: 1.5598747332890828, learning_rate: 0.0010000000000000002\n",
      "Checkpoint saved in directory /tmp/tmpbq85kot1\n",
      "iteration [7] => episode_reward_mean: -13.16, episode_len_mean: 20.91, agent_steps_trained: 18023, env_steps_trained: 14336, entropy: 1.539201339483261, learning_rate: 0.0010000000000000002\n",
      "iteration [8] => episode_reward_mean: -11.570093457943925, episode_len_mean: 19.33644859813084, agent_steps_trained: 20659, env_steps_trained: 16384, entropy: 1.5134615528583526, learning_rate: 0.0010000000000000002\n",
      "iteration [9] => episode_reward_mean: -4.843537414965986, episode_len_mean: 14.040816326530612, agent_steps_trained: 23414, env_steps_trained: 18432, entropy: 1.4603713047504425, learning_rate: 0.0010000000000000002\n",
      "iteration [10] => episode_reward_mean: -3.9753086419753085, episode_len_mean: 12.574074074074074, agent_steps_trained: 26139, env_steps_trained: 20480, entropy: 1.4160573124885558, learning_rate: 0.0010000000000000002\n",
      "iteration [11] => episode_reward_mean: -3.5194805194805197, episode_len_mean: 13.324675324675324, agent_steps_trained: 28774, env_steps_trained: 22528, entropy: 1.385268623828888, learning_rate: 0.0010000000000000002\n",
      "Checkpoint saved in directory /tmp/tmpufr49nya\n",
      "iteration [12] => episode_reward_mean: 0.4392523364485981, episode_len_mean: 9.588785046728972, agent_steps_trained: 31583, env_steps_trained: 24576, entropy: 1.3403324055671693, learning_rate: 0.0010000000000000002\n",
      "iteration [13] => episode_reward_mean: 1.588235294117647, episode_len_mean: 8.027450980392157, agent_steps_trained: 34472, env_steps_trained: 26624, entropy: 1.2929457924582741, learning_rate: 0.0010000000000000005\n",
      "iteration [14] => episode_reward_mean: 0.004149377593360996, episode_len_mean: 8.46058091286307, agent_steps_trained: 37287, env_steps_trained: 28672, entropy: 1.2552172827720642, learning_rate: 0.0010000000000000002\n",
      "iteration [15] => episode_reward_mean: 3.34640522875817, episode_len_mean: 6.738562091503268, agent_steps_trained: 40198, env_steps_trained: 30720, entropy: 1.1931509917432612, learning_rate: 0.0010000000000000005\n",
      "iteration [16] => episode_reward_mean: 2.1596091205211727, episode_len_mean: 6.657980456026059, agent_steps_trained: 43285, env_steps_trained: 32768, entropy: 1.1749482383330663, learning_rate: 0.0010000000000000005\n",
      "Checkpoint saved in directory /tmp/tmpssjq1c_e\n",
      "iteration [17] => episode_reward_mean: 3.6303724928366763, episode_len_mean: 5.873925501432665, agent_steps_trained: 46408, env_steps_trained: 34816, entropy: 1.1532056758801141, learning_rate: 0.0010000000000000005\n",
      "iteration [18] => episode_reward_mean: 2.996923076923077, episode_len_mean: 6.3076923076923075, agent_steps_trained: 49424, env_steps_trained: 36864, entropy: 1.1264807766134088, learning_rate: 0.0010000000000000005\n",
      "iteration [19] => episode_reward_mean: 3.6763005780346822, episode_len_mean: 5.916184971098266, agent_steps_trained: 52536, env_steps_trained: 38912, entropy: 1.0858377705017725, learning_rate: 0.0010000000000000005\n",
      "iteration [20] => episode_reward_mean: 3.53125, episode_len_mean: 6.371875, agent_steps_trained: 55597, env_steps_trained: 40960, entropy: 1.0946354324167424, learning_rate: 0.0010000000000000005\n",
      "iteration [21] => episode_reward_mean: 3.7106017191977076, episode_len_mean: 5.862464183381089, agent_steps_trained: 58745, env_steps_trained: 43008, entropy: 1.0739898711442948, learning_rate: 0.0010000000000000005\n",
      "Checkpoint saved in directory /tmp/tmpvi1zyt80\n",
      "iteration [22] => episode_reward_mean: 3.138364779874214, episode_len_mean: 6.455974842767295, agent_steps_trained: 61749, env_steps_trained: 45056, entropy: 1.0680667969313535, learning_rate: 0.0010000000000000005\n",
      "iteration [23] => episode_reward_mean: 3.9944444444444445, episode_len_mean: 5.7, agent_steps_trained: 64880, env_steps_trained: 47104, entropy: 1.0404950812458993, learning_rate: 0.0010000000000000005\n",
      "iteration [24] => episode_reward_mean: 4.111716621253406, episode_len_mean: 5.580381471389646, agent_steps_trained: 68028, env_steps_trained: 49152, entropy: 1.018359263241291, learning_rate: 0.0010000000000000005\n",
      "iteration [25] => episode_reward_mean: 4.232620320855615, episode_len_mean: 5.478609625668449, agent_steps_trained: 71182, env_steps_trained: 51200, entropy: 0.9988243709007899, learning_rate: 0.0010000000000000005\n",
      "iteration [26] => episode_reward_mean: 3.8287292817679557, episode_len_mean: 5.654696132596685, agent_steps_trained: 74327, env_steps_trained: 53248, entropy: 0.992235970000426, learning_rate: 0.0010000000000000005\n",
      "Checkpoint saved in directory /tmp/tmpx56mjuzx\n",
      "iteration [27] => episode_reward_mean: 3.47008547008547, episode_len_mean: 5.843304843304844, agent_steps_trained: 77477, env_steps_trained: 55296, entropy: 0.9975042353073756, learning_rate: 0.0010000000000000005\n",
      "iteration [28] => episode_reward_mean: 4.289124668435013, episode_len_mean: 5.43236074270557, agent_steps_trained: 80657, env_steps_trained: 57344, entropy: 0.9462162852287292, learning_rate: 0.0010000000000000005\n",
      "iteration [29] => episode_reward_mean: 3.064516129032258, episode_len_mean: 6.005865102639296, agent_steps_trained: 83867, env_steps_trained: 59392, entropy: 0.9719623391826947, learning_rate: 0.0010000000000000005\n",
      "iteration [30] => episode_reward_mean: 3.5234159779614327, episode_len_mean: 5.6143250688705235, agent_steps_trained: 87043, env_steps_trained: 61440, entropy: 0.9397072444359461, learning_rate: 0.0010000000000000005\n",
      "iteration [31] => episode_reward_mean: 4.181818181818182, episode_len_mean: 5.661157024793388, agent_steps_trained: 90230, env_steps_trained: 63488, entropy: 0.9361631453037262, learning_rate: 0.0010000000000000005\n",
      "Checkpoint saved in directory /tmp/tmp12oqlyyn\n",
      "iteration [32] => episode_reward_mean: 3.4231805929919137, episode_len_mean: 5.528301886792453, agent_steps_trained: 93460, env_steps_trained: 65536, entropy: 0.9432402680317561, learning_rate: 0.0010000000000000005\n",
      "iteration [33] => episode_reward_mean: 3.9766081871345027, episode_len_mean: 5.988304093567251, agent_steps_trained: 96678, env_steps_trained: 67584, entropy: 0.9244529267152151, learning_rate: 0.0010000000000000005\n",
      "iteration [34] => episode_reward_mean: 3.9514285714285715, episode_len_mean: 5.848571428571429, agent_steps_trained: 99926, env_steps_trained: 69632, entropy: 0.9153794368108114, learning_rate: 0.0010000000000000005\n",
      "iteration [35] => episode_reward_mean: 4.265895953757226, episode_len_mean: 5.904624277456647, agent_steps_trained: 103179, env_steps_trained: 71680, entropy: 0.9050627211729686, learning_rate: 0.0010000000000000005\n",
      "iteration [36] => episode_reward_mean: 3.3622754491017965, episode_len_mean: 6.1467065868263475, agent_steps_trained: 106433, env_steps_trained: 73728, entropy: 0.8959699461857478, learning_rate: 0.0010000000000000005\n",
      "Checkpoint saved in directory /tmp/tmpdft7h1mz\n",
      "iteration [37] => episode_reward_mean: 4.143646408839779, episode_len_mean: 5.657458563535911, agent_steps_trained: 109747, env_steps_trained: 75776, entropy: 0.9069234291712444, learning_rate: 0.0010000000000000005\n",
      "iteration [38] => episode_reward_mean: 3.6231003039513676, episode_len_mean: 6.188449848024316, agent_steps_trained: 113013, env_steps_trained: 77824, entropy: 0.9447207257151604, learning_rate: 0.0010000000000000005\n",
      "iteration [39] => episode_reward_mean: 3.008670520231214, episode_len_mean: 5.9508670520231215, agent_steps_trained: 116311, env_steps_trained: 79872, entropy: 0.9216614743073781, learning_rate: 0.0010000000000000005\n",
      "iteration [40] => episode_reward_mean: 4.152354570637119, episode_len_mean: 5.678670360110804, agent_steps_trained: 119591, env_steps_trained: 81920, entropy: 0.8787869438529015, learning_rate: 0.0010000000000000005\n",
      "iteration [41] => episode_reward_mean: 3.7579250720461097, episode_len_mean: 5.890489913544669, agent_steps_trained: 122879, env_steps_trained: 83968, entropy: 0.8864236608147621, learning_rate: 0.0010000000000000005\n",
      "Checkpoint saved in directory /tmp/tmpt6hkvur_\n",
      "iteration [42] => episode_reward_mean: 3.492354740061162, episode_len_mean: 6.27217125382263, agent_steps_trained: 126130, env_steps_trained: 86016, entropy: 0.8857078517476717, learning_rate: 0.0010000000000000005\n",
      "iteration [43] => episode_reward_mean: 4.53972602739726, episode_len_mean: 5.610958904109589, agent_steps_trained: 129399, env_steps_trained: 88064, entropy: 0.8624318102995555, learning_rate: 0.0010000000000000005\n",
      "iteration [44] => episode_reward_mean: 4.291891891891892, episode_len_mean: 5.518918918918919, agent_steps_trained: 132667, env_steps_trained: 90112, entropy: 0.8355913410584132, learning_rate: 0.0010000000000000005\n",
      "iteration [45] => episode_reward_mean: 4.208695652173913, episode_len_mean: 5.956521739130435, agent_steps_trained: 135945, env_steps_trained: 92160, entropy: 0.853059995174408, learning_rate: 0.0010000000000000005\n",
      "iteration [46] => episode_reward_mean: 3.6300578034682083, episode_len_mean: 5.916184971098266, agent_steps_trained: 139210, env_steps_trained: 94208, entropy: 0.8608405972520511, learning_rate: 0.0010000000000000005\n",
      "Checkpoint saved in directory /tmp/tmppxb478nf\n",
      "iteration [47] => episode_reward_mean: 4.471389645776567, episode_len_mean: 5.583106267029973, agent_steps_trained: 142473, env_steps_trained: 96256, entropy: 0.8118783608078957, learning_rate: 0.0010000000000000005\n",
      "iteration [48] => episode_reward_mean: 4.1104651162790695, episode_len_mean: 5.915697674418604, agent_steps_trained: 145747, env_steps_trained: 98304, entropy: 0.8437534843881925, learning_rate: 0.0010000000000000005\n",
      "iteration [49] => episode_reward_mean: 4.48169014084507, episode_len_mean: 5.763380281690141, agent_steps_trained: 149041, env_steps_trained: 100352, entropy: 0.8081084613998731, learning_rate: 0.0010000000000000005\n",
      "iteration [50] => episode_reward_mean: 4.253405994550409, episode_len_mean: 5.618528610354224, agent_steps_trained: 152352, env_steps_trained: 102400, entropy: 0.7835429757833481, learning_rate: 0.0010000000000000005\n"
     ]
    }
   ],
   "source": [
    "from ray.rllib.algorithms.ppo import PPOConfig\n",
    "from ray.tune.logger import pretty_print\n",
    "from ray.tune.registry import register_env\n",
    "from gymnasium.wrappers.time_limit import TimeLimit\n",
    "\n",
    "\n",
    "register_env(\"my_env\", lambda _: PointCoverageEnv({\"height\": 4, \"width\": 4, \"n_agents\": 2, \"n_targets\": 2, \"max_steps\": 50}))\n",
    "\n",
    "train_batch_size = 2048\n",
    "sgd_minibatch_size = 256\n",
    "num_sgd_iter = 10\n",
    "trainings = 50\n",
    "\n",
    "total_env_steps = trainings*train_batch_size\n",
    "\n",
    "print(f\"number of different environment steps: {total_env_steps}\")\n",
    "\n",
    "\n",
    "algo2 = (\n",
    "    PPOConfig()\n",
    "    .training(gamma=0.99, \n",
    "              #lr=0.001,\n",
    "              lr_schedule=[\n",
    "                [0, 0.01],  \n",
    "                [1000, 0.001],  \n",
    "                [10000, 0.001],  \n",
    "              ],\n",
    "              kl_coeff=0.5, \n",
    "              train_batch_size=train_batch_size, \n",
    "              sgd_minibatch_size=sgd_minibatch_size, \n",
    "              num_sgd_iter=num_sgd_iter, \n",
    "              entropy_coeff_schedule = [\n",
    "                [0, 1],  # Start with relatively high entropy coefficient\n",
    "                [20480, 0],  # Gradually decrease entropy coefficient over 10,000 iterations\n",
    "              ])\n",
    "    .env_runners(num_env_runners=1)\n",
    "    .resources(num_gpus=0)\n",
    "    .environment(env=\"my_env\")\n",
    "    .build()\n",
    ")\n",
    "\n",
    "for i in range(trainings):\n",
    "    result = algo2.train()\n",
    "    customResultPrint(result)\n",
    "    if i % 5 == 0:\n",
    "        checkpoint_dir = algo2.save().checkpoint.path\n",
    "        print(f\"Checkpoint saved in directory {checkpoint_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[65]\n",
      "____________\n",
      "|   *     o|\n",
      "|          |\n",
      "|         x|\n",
      "|          |\n",
      "|          |\n",
      "|          |\n",
      "|          |\n",
      "|          |\n",
      "|          |\n",
      "|          |\n",
      "‾‾‾‾‾‾‾‾‾‾‾‾\n",
      "{'agent-0': array([3, 0, 9, 0, 9, 2, 3, 0], dtype=int32)}\n",
      "{'agent-0': -1}\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 17\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28mprint\u001b[39m(obs)\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28mprint\u001b[39m(reward)\n\u001b[0;32m---> 17\u001b[0m time\u001b[38;5;241m.\u001b[39msleep(\u001b[38;5;241m0.5\u001b[39m)\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m terminated[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__all__\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;129;01mor\u001b[39;00m truncated[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__all__\u001b[39m\u001b[38;5;124m'\u001b[39m]:\n\u001b[1;32m     20\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from IPython.display import clear_output\n",
    "import time\n",
    "\n",
    "env = PointCoverageEnv({\"height\": 10, \"width\": 10, \"n_agents\": 2, \"n_targets\": 2})\n",
    "obs, _ = env.reset()\n",
    "env.render()\n",
    "\n",
    "for i in range(100):\n",
    "    actions = algo2.compute_actions(obs)\n",
    "    print(actions, \"\\n\")\n",
    "    obs, reward, terminated, truncated, info = env.step(actions)\n",
    "    clear_output()\n",
    "    print(f\"[{i}]\")\n",
    "    env.render()\n",
    "    print(obs)\n",
    "    print(reward)\n",
    "    time.sleep(0.5)\n",
    "\n",
    "    if terminated['__all__'] or truncated['__all__']:\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "The observation returned by `reset()` method must be a numpy array",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[26], line 7\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mstable_baselines3\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcommon\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01menv_checker\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m check_env\n\u001b[1;32m      6\u001b[0m env \u001b[38;5;241m=\u001b[39m PointCoverageEnv({\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mheight\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m10\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwidth\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m50\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mn_agents\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m2\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mn_targets\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m2\u001b[39m})\n\u001b[0;32m----> 7\u001b[0m \u001b[43mcheck_env\u001b[49m\u001b[43m(\u001b[49m\u001b[43menv\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      9\u001b[0m model \u001b[38;5;241m=\u001b[39m PPO(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMlpPolicy\u001b[39m\u001b[38;5;124m\"\u001b[39m, env, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     10\u001b[0m model\u001b[38;5;241m.\u001b[39mlearn(total_timesteps\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m25000\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/envs/rayEnv/lib/python3.11/site-packages/stable_baselines3/common/env_checker.py:473\u001b[0m, in \u001b[0;36mcheck_env\u001b[0;34m(env, warn, skip_render_check)\u001b[0m\n\u001b[1;32m    470\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m    472\u001b[0m \u001b[38;5;66;03m# ============ Check the returned values ===============\u001b[39;00m\n\u001b[0;32m--> 473\u001b[0m \u001b[43m_check_returned_values\u001b[49m\u001b[43m(\u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mobservation_space\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maction_space\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    475\u001b[0m \u001b[38;5;66;03m# ==== Check the render method and the declared render modes ====\u001b[39;00m\n\u001b[1;32m    476\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m skip_render_check:\n",
      "File \u001b[0;32m~/anaconda3/envs/rayEnv/lib/python3.11/site-packages/stable_baselines3/common/env_checker.py:300\u001b[0m, in \u001b[0;36m_check_returned_values\u001b[0;34m(env, observation_space, action_space)\u001b[0m\n\u001b[1;32m    298\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAssertionError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError while checking key=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(e)) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[1;32m    299\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 300\u001b[0m     \u001b[43m_check_obs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mobservation_space\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mreset\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    302\u001b[0m \u001b[38;5;66;03m# Sample a random action\u001b[39;00m\n\u001b[1;32m    303\u001b[0m action \u001b[38;5;241m=\u001b[39m action_space\u001b[38;5;241m.\u001b[39msample()\n",
      "File \u001b[0;32m~/anaconda3/envs/rayEnv/lib/python3.11/site-packages/stable_baselines3/common/env_checker.py:209\u001b[0m, in \u001b[0;36m_check_obs\u001b[0;34m(obs, observation_space, method_name)\u001b[0m\n\u001b[1;32m    207\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m np\u001b[38;5;241m.\u001b[39missubdtype(\u001b[38;5;28mtype\u001b[39m(obs), np\u001b[38;5;241m.\u001b[39minteger), \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe observation returned by `\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmethod_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m()` method must be an int\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    208\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m _is_numpy_array_space(observation_space):\n\u001b[0;32m--> 209\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(obs, np\u001b[38;5;241m.\u001b[39mndarray), \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe observation returned by `\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmethod_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m()` method must be a numpy array\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    211\u001b[0m \u001b[38;5;66;03m# Additional checks for numpy arrays, so the error message is clearer (see GH#1399)\u001b[39;00m\n\u001b[1;32m    212\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(obs, np\u001b[38;5;241m.\u001b[39mndarray):\n\u001b[1;32m    213\u001b[0m     \u001b[38;5;66;03m# check obs dimensions, dtype and bounds\u001b[39;00m\n",
      "\u001b[0;31mAssertionError\u001b[0m: The observation returned by `reset()` method must be a numpy array"
     ]
    }
   ],
   "source": [
    "import gymnasium as gym\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "from stable_baselines3.common.env_checker import check_env\n",
    "\n",
    "env = PointCoverageEnv({\"height\": 10, \"width\": 50, \"n_agents\": 2, \"n_targets\": 2})\n",
    "check_env(env)\n",
    "\n",
    "model = PPO(\"MlpPolicy\", env, verbose=1)\n",
    "model.learn(total_timesteps=25000)\n",
    "\n",
    "obs, _ = env.reset()\n",
    "while True:\n",
    "    action, _states = model.predict(obs)\n",
    "    obs, rewards, dones, info = env.step(action)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Three Agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-16 13:22:07,017\tWARNING deprecation.py:50 -- DeprecationWarning: `WorkerSet(num_workers=... OR local_worker=...)` has been deprecated. Use `EnvRunnerGroup(num_env_runners=... AND local_env_runner=...)` instead. This will raise an error in the future!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of different environment steps: 102400\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-16 13:22:13,386\tWARNING util.py:61 -- Install gputil for GPU system monitoring.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration [1] => episode_reward_mean: -20.545454545454547, episode_len_mean: 36.70909090909091, agent_steps_trained: 3158, env_steps_trained: 2048, entropy: 1.608757315079371, learning_rate: 0.01\n",
      "Checkpoint saved in directory /tmp/tmpge2rfk8i\n",
      "iteration [2] => episode_reward_mean: -11.7, episode_len_mean: 32.92, agent_steps_trained: 6138, env_steps_trained: 4096, entropy: 1.599185993454673, learning_rate: 0.0010000000000000005\n",
      "iteration [3] => episode_reward_mean: -6.57, episode_len_mean: 29.54, agent_steps_trained: 9279, env_steps_trained: 6144, entropy: 1.5917007118463515, learning_rate: 0.0010000000000000005\n",
      "iteration [4] => episode_reward_mean: -8.28, episode_len_mean: 31.83, agent_steps_trained: 12213, env_steps_trained: 8192, entropy: 1.5871088786558671, learning_rate: 0.0010000000000000005\n",
      "iteration [5] => episode_reward_mean: -5.63, episode_len_mean: 29.88, agent_steps_trained: 15238, env_steps_trained: 10240, entropy: 1.571779600056735, learning_rate: 0.0010000000000000005\n",
      "iteration [6] => episode_reward_mean: -1.23, episode_len_mean: 26.29, agent_steps_trained: 18204, env_steps_trained: 12288, entropy: 1.5637190916321495, learning_rate: 0.0010000000000000005\n",
      "Checkpoint saved in directory /tmp/tmpbnryhhgc\n",
      "iteration [7] => episode_reward_mean: -2.95, episode_len_mean: 27.99, agent_steps_trained: 21143, env_steps_trained: 14336, entropy: 1.5457476431673223, learning_rate: 0.0010000000000000005\n",
      "iteration [8] => episode_reward_mean: -2.58, episode_len_mean: 26.29, agent_steps_trained: 24061, env_steps_trained: 16384, entropy: 1.5180352438579907, learning_rate: 0.0010000000000000005\n",
      "iteration [9] => episode_reward_mean: 5.56, episode_len_mean: 22.27, agent_steps_trained: 27277, env_steps_trained: 18432, entropy: 1.5019399106502533, learning_rate: 0.0010000000000000005\n",
      "iteration [10] => episode_reward_mean: 7.0, episode_len_mean: 21.53, agent_steps_trained: 30224, env_steps_trained: 20480, entropy: 1.4823434688828208, learning_rate: 0.0010000000000000005\n",
      "iteration [11] => episode_reward_mean: 8.547169811320755, episode_len_mean: 19.39622641509434, agent_steps_trained: 33381, env_steps_trained: 22528, entropy: 1.481738305091858, learning_rate: 0.0010000000000000005\n",
      "Checkpoint saved in directory /tmp/tmptfe2b66m\n",
      "iteration [12] => episode_reward_mean: 10.179487179487179, episode_len_mean: 17.53846153846154, agent_steps_trained: 36594, env_steps_trained: 24576, entropy: 1.4497334112723668, learning_rate: 0.0010000000000000005\n",
      "iteration [13] => episode_reward_mean: 4.04, episode_len_mean: 22.93, agent_steps_trained: 39560, env_steps_trained: 26624, entropy: 1.450662071054632, learning_rate: 0.0010000000000000005\n",
      "iteration [14] => episode_reward_mean: 13.464566929133857, episode_len_mean: 15.89763779527559, agent_steps_trained: 42741, env_steps_trained: 28672, entropy: 1.4148278792699178, learning_rate: 0.0010000000000000005\n",
      "iteration [15] => episode_reward_mean: 9.486956521739131, episode_len_mean: 18.05217391304348, agent_steps_trained: 45867, env_steps_trained: 30720, entropy: 1.4183725972970327, learning_rate: 0.0010000000000000005\n",
      "iteration [16] => episode_reward_mean: 12.565891472868216, episode_len_mean: 15.89922480620155, agent_steps_trained: 49022, env_steps_trained: 32768, entropy: 1.3995133111874263, learning_rate: 0.0010000000000000005\n",
      "Checkpoint saved in directory /tmp/tmp4n8lakjm\n",
      "iteration [17] => episode_reward_mean: 16.12676056338028, episode_len_mean: 14.429577464788732, agent_steps_trained: 52236, env_steps_trained: 34816, entropy: 1.3884950240453084, learning_rate: 0.0010000000000000005\n",
      "iteration [18] => episode_reward_mean: 9.469565217391304, episode_len_mean: 17.791304347826088, agent_steps_trained: 55399, env_steps_trained: 36864, entropy: 1.3866612563530605, learning_rate: 0.0010000000000000005\n",
      "iteration [19] => episode_reward_mean: 11.559055118110237, episode_len_mean: 16.133858267716537, agent_steps_trained: 58442, env_steps_trained: 38912, entropy: 1.3574978611686013, learning_rate: 0.0010000000000000005\n",
      "iteration [20] => episode_reward_mean: 12.723880597014926, episode_len_mean: 15.022388059701493, agent_steps_trained: 61548, env_steps_trained: 40960, entropy: 1.3208619127670924, learning_rate: 0.0010000000000000005\n",
      "iteration [21] => episode_reward_mean: 12.407692307692308, episode_len_mean: 16.023076923076925, agent_steps_trained: 64562, env_steps_trained: 43008, entropy: 1.3167971990325233, learning_rate: 0.0010000000000000005\n",
      "Checkpoint saved in directory /tmp/tmpj1i1dgdb\n",
      "iteration [22] => episode_reward_mean: 13.664285714285715, episode_len_mean: 14.4, agent_steps_trained: 67832, env_steps_trained: 45056, entropy: 1.3166669189929963, learning_rate: 0.0010000000000000005\n",
      "iteration [23] => episode_reward_mean: 11.3515625, episode_len_mean: 16.1796875, agent_steps_trained: 71113, env_steps_trained: 47104, entropy: 1.298007357120514, learning_rate: 0.0010000000000000005\n",
      "iteration [24] => episode_reward_mean: 14.964285714285714, episode_len_mean: 14.721428571428572, agent_steps_trained: 74307, env_steps_trained: 49152, entropy: 1.2305699149767557, learning_rate: 0.0010000000000000005\n",
      "iteration [25] => episode_reward_mean: 13.179856115107913, episode_len_mean: 14.719424460431656, agent_steps_trained: 77534, env_steps_trained: 51200, entropy: 1.270032599568367, learning_rate: 0.0010000000000000005\n",
      "iteration [26] => episode_reward_mean: 16.354609929078013, episode_len_mean: 14.340425531914894, agent_steps_trained: 80649, env_steps_trained: 53248, entropy: 1.247059672077497, learning_rate: 0.0010000000000000005\n",
      "Checkpoint saved in directory /tmp/tmpzhoq1y7o\n",
      "iteration [27] => episode_reward_mean: 11.645669291338583, episode_len_mean: 16.25984251968504, agent_steps_trained: 83648, env_steps_trained: 55296, entropy: 1.237025604464791, learning_rate: 0.0010000000000000005\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 52\u001b[0m\n\u001b[1;32m     43\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124miteration [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtraining_iteration\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m] => \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m\n\u001b[1;32m     44\u001b[0m           \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mepisode_reward_mean: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msampler_results\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mepisode_reward_mean\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m\n\u001b[1;32m     45\u001b[0m           \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mepisode_len_mean: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msampler_results\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mepisode_len_mean\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     48\u001b[0m           \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mentropy: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minfo\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlearner\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdefault_policy\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlearner_stats\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mentropy\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m\n\u001b[1;32m     49\u001b[0m           \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlearning_rate: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minfo\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlearner\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdefault_policy\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlearner_stats\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcur_lr\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     51\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(trainings):\n\u001b[0;32m---> 52\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43malgo2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     53\u001b[0m     customResultPrint(result)\n\u001b[1;32m     54\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m5\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m~/anaconda3/envs/rayEnv/lib/python3.11/site-packages/ray/tune/trainable/trainable.py:328\u001b[0m, in \u001b[0;36mTrainable.train\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    326\u001b[0m start \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m    327\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 328\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    329\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    330\u001b[0m     skipped \u001b[38;5;241m=\u001b[39m skip_exceptions(e)\n",
      "File \u001b[0;32m~/anaconda3/envs/rayEnv/lib/python3.11/site-packages/ray/rllib/algorithms/algorithm.py:873\u001b[0m, in \u001b[0;36mAlgorithm.step\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    863\u001b[0m     (\n\u001b[1;32m    864\u001b[0m         train_results,\n\u001b[1;32m    865\u001b[0m         eval_results,\n\u001b[1;32m    866\u001b[0m         train_iter_ctx,\n\u001b[1;32m    867\u001b[0m     ) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_run_one_training_iteration_and_evaluation_in_parallel()\n\u001b[1;32m    869\u001b[0m \u001b[38;5;66;03m# - No evaluation necessary, just run the next training iteration.\u001b[39;00m\n\u001b[1;32m    870\u001b[0m \u001b[38;5;66;03m# - We have to evaluate in this training iteration, but no parallelism ->\u001b[39;00m\n\u001b[1;32m    871\u001b[0m \u001b[38;5;66;03m#   evaluate after the training iteration is entirely done.\u001b[39;00m\n\u001b[1;32m    872\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 873\u001b[0m     train_results, train_iter_ctx \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_one_training_iteration\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    875\u001b[0m \u001b[38;5;66;03m# Sequential: Train (already done above), then evaluate.\u001b[39;00m\n\u001b[1;32m    876\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m evaluate_this_iter \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mevaluation_parallel_to_training:\n",
      "File \u001b[0;32m~/anaconda3/envs/rayEnv/lib/python3.11/site-packages/ray/rllib/algorithms/algorithm.py:3156\u001b[0m, in \u001b[0;36mAlgorithm._run_one_training_iteration\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   3154\u001b[0m             \u001b[38;5;66;03m# Try to train one step.\u001b[39;00m\n\u001b[1;32m   3155\u001b[0m             \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_timers[TRAINING_STEP_TIMER]:\n\u001b[0;32m-> 3156\u001b[0m                 results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3158\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m results, train_iter_ctx\n",
      "File \u001b[0;32m~/anaconda3/envs/rayEnv/lib/python3.11/site-packages/ray/rllib/algorithms/ppo/ppo.py:428\u001b[0m, in \u001b[0;36mPPO.training_step\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    424\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_training_step_new_api_stack()\n\u001b[1;32m    425\u001b[0m \u001b[38;5;66;03m# Old and hybrid API stacks (Policy, RolloutWorker, Connector, maybe RLModule,\u001b[39;00m\n\u001b[1;32m    426\u001b[0m \u001b[38;5;66;03m# maybe Learner).\u001b[39;00m\n\u001b[1;32m    427\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 428\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_training_step_old_and_hybrid_api_stacks\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/rayEnv/lib/python3.11/site-packages/ray/rllib/algorithms/ppo/ppo.py:562\u001b[0m, in \u001b[0;36mPPO._training_step_old_and_hybrid_api_stacks\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    557\u001b[0m     train_batch \u001b[38;5;241m=\u001b[39m synchronous_parallel_sample(\n\u001b[1;32m    558\u001b[0m         worker_set\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mworkers,\n\u001b[1;32m    559\u001b[0m         max_agent_steps\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mtotal_train_batch_size,\n\u001b[1;32m    560\u001b[0m     )\n\u001b[1;32m    561\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 562\u001b[0m     train_batch \u001b[38;5;241m=\u001b[39m \u001b[43msynchronous_parallel_sample\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    563\u001b[0m \u001b[43m        \u001b[49m\u001b[43mworker_set\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mworkers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    564\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_env_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtotal_train_batch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    565\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    566\u001b[0m train_batch \u001b[38;5;241m=\u001b[39m train_batch\u001b[38;5;241m.\u001b[39mas_multi_agent()\n\u001b[1;32m    567\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_counters[NUM_AGENT_STEPS_SAMPLED] \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m train_batch\u001b[38;5;241m.\u001b[39magent_steps()\n",
      "File \u001b[0;32m~/anaconda3/envs/rayEnv/lib/python3.11/site-packages/ray/rllib/execution/rollout_ops.py:97\u001b[0m, in \u001b[0;36msynchronous_parallel_sample\u001b[0;34m(worker_set, max_agent_steps, max_env_steps, concat, sample_timeout_s, _uses_new_env_runners, _return_metrics)\u001b[0m\n\u001b[1;32m     94\u001b[0m         stats_dicts \u001b[38;5;241m=\u001b[39m [worker_set\u001b[38;5;241m.\u001b[39mlocal_worker()\u001b[38;5;241m.\u001b[39mget_metrics()]\n\u001b[1;32m     95\u001b[0m \u001b[38;5;66;03m# Loop over remote workers' `sample()` method in parallel.\u001b[39;00m\n\u001b[1;32m     96\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 97\u001b[0m     sampled_data \u001b[38;5;241m=\u001b[39m \u001b[43mworker_set\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforeach_worker\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     98\u001b[0m \u001b[43m        \u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     99\u001b[0m \u001b[43m            \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mw\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mw\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msample\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    100\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m_return_metrics\u001b[49m\n\u001b[1;32m    101\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mw\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mw\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msample\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mw\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_metrics\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    102\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    103\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlocal_worker\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    104\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout_seconds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample_timeout_s\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    105\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    106\u001b[0m     \u001b[38;5;66;03m# Nothing was returned (maybe all workers are stalling) or no healthy\u001b[39;00m\n\u001b[1;32m    107\u001b[0m     \u001b[38;5;66;03m# remote workers left: Break.\u001b[39;00m\n\u001b[1;32m    108\u001b[0m     \u001b[38;5;66;03m# There is no point staying in this loop, since we will not be able to\u001b[39;00m\n\u001b[1;32m    109\u001b[0m     \u001b[38;5;66;03m# get any new samples if we don't have any healthy remote workers left.\u001b[39;00m\n\u001b[1;32m    110\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m sampled_data \u001b[38;5;129;01mor\u001b[39;00m worker_set\u001b[38;5;241m.\u001b[39mnum_healthy_remote_workers() \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m~/anaconda3/envs/rayEnv/lib/python3.11/site-packages/ray/rllib/env/env_runner_group.py:840\u001b[0m, in \u001b[0;36mEnvRunnerGroup.foreach_worker\u001b[0;34m(self, func, local_worker, healthy_only, remote_worker_ids, timeout_seconds, return_obj_refs, mark_healthy)\u001b[0m\n\u001b[1;32m    837\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_worker_manager\u001b[38;5;241m.\u001b[39mactor_ids():\n\u001b[1;32m    838\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m local_result\n\u001b[0;32m--> 840\u001b[0m remote_results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_worker_manager\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforeach_actor\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    841\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    842\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhealthy_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhealthy_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    843\u001b[0m \u001b[43m    \u001b[49m\u001b[43mremote_actor_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mremote_worker_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    844\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout_seconds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout_seconds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    845\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_obj_refs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_obj_refs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    846\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmark_healthy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmark_healthy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    847\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    849\u001b[0m _handle_remote_call_result_errors(\n\u001b[1;32m    850\u001b[0m     remote_results, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ignore_env_runner_failures\n\u001b[1;32m    851\u001b[0m )\n\u001b[1;32m    853\u001b[0m \u001b[38;5;66;03m# With application errors handled, return good results.\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/rayEnv/lib/python3.11/site-packages/ray/rllib/utils/actor_manager.py:622\u001b[0m, in \u001b[0;36mFaultTolerantActorManager.foreach_actor\u001b[0;34m(self, func, healthy_only, remote_actor_ids, timeout_seconds, return_obj_refs, mark_healthy)\u001b[0m\n\u001b[1;32m    616\u001b[0m remote_calls \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_actors(\n\u001b[1;32m    617\u001b[0m     func\u001b[38;5;241m=\u001b[39mfunc,\n\u001b[1;32m    618\u001b[0m     remote_actor_ids\u001b[38;5;241m=\u001b[39mremote_actor_ids,\n\u001b[1;32m    619\u001b[0m )\n\u001b[1;32m    621\u001b[0m \u001b[38;5;66;03m# Collect remote request results (if available given timeout and/or errors).\u001b[39;00m\n\u001b[0;32m--> 622\u001b[0m _, remote_results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fetch_result\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    623\u001b[0m \u001b[43m    \u001b[49m\u001b[43mremote_actor_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mremote_actor_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    624\u001b[0m \u001b[43m    \u001b[49m\u001b[43mremote_calls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mremote_calls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    625\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtags\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mremote_calls\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    626\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout_seconds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout_seconds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    627\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_obj_refs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_obj_refs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    628\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmark_healthy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmark_healthy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    629\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    631\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m remote_results\n",
      "File \u001b[0;32m~/anaconda3/envs/rayEnv/lib/python3.11/site-packages/ray/rllib/utils/actor_manager.py:476\u001b[0m, in \u001b[0;36mFaultTolerantActorManager._fetch_result\u001b[0;34m(self, remote_actor_ids, remote_calls, tags, timeout_seconds, return_obj_refs, mark_healthy)\u001b[0m\n\u001b[1;32m    473\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m remote_calls:\n\u001b[1;32m    474\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [], RemoteCallResults()\n\u001b[0;32m--> 476\u001b[0m ready, _ \u001b[38;5;241m=\u001b[39m \u001b[43mray\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    477\u001b[0m \u001b[43m    \u001b[49m\u001b[43mremote_calls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    478\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_returns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mremote_calls\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    479\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    480\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# Make sure remote results are fetched locally in parallel.\u001b[39;49;00m\n\u001b[1;32m    481\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfetch_local\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mreturn_obj_refs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    482\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    484\u001b[0m \u001b[38;5;66;03m# Remote data should already be fetched to local object store at this point.\u001b[39;00m\n\u001b[1;32m    485\u001b[0m remote_results \u001b[38;5;241m=\u001b[39m RemoteCallResults()\n",
      "File \u001b[0;32m~/anaconda3/envs/rayEnv/lib/python3.11/site-packages/ray/_private/auto_init_hook.py:21\u001b[0m, in \u001b[0;36mwrap_auto_init.<locals>.auto_init_wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(fn)\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mauto_init_wrapper\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     20\u001b[0m     auto_init_ray()\n\u001b[0;32m---> 21\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/rayEnv/lib/python3.11/site-packages/ray/_private/client_mode_hook.py:103\u001b[0m, in \u001b[0;36mclient_mode_hook.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    101\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m func\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minit\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m is_client_mode_enabled_by_default:\n\u001b[1;32m    102\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(ray, func\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m)(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m--> 103\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/rayEnv/lib/python3.11/site-packages/ray/_private/worker.py:2854\u001b[0m, in \u001b[0;36mwait\u001b[0;34m(ray_waitables, num_returns, timeout, fetch_local)\u001b[0m\n\u001b[1;32m   2852\u001b[0m timeout \u001b[38;5;241m=\u001b[39m timeout \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;241m10\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m6\u001b[39m\n\u001b[1;32m   2853\u001b[0m timeout_milliseconds \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(timeout \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m1000\u001b[39m)\n\u001b[0;32m-> 2854\u001b[0m ready_ids, remaining_ids \u001b[38;5;241m=\u001b[39m \u001b[43mworker\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcore_worker\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2855\u001b[0m \u001b[43m    \u001b[49m\u001b[43mray_waitables\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2856\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_returns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2857\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout_milliseconds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2858\u001b[0m \u001b[43m    \u001b[49m\u001b[43mworker\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcurrent_task_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2859\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfetch_local\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2860\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2861\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m ready_ids, remaining_ids\n",
      "File \u001b[0;32mpython/ray/_raylet.pyx:3812\u001b[0m, in \u001b[0;36mray._raylet.CoreWorker.wait\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpython/ray/_raylet.pyx:571\u001b[0m, in \u001b[0;36mray._raylet.check_status\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from ray.rllib.algorithms.ppo import PPOConfig\n",
    "from ray.tune.logger import pretty_print\n",
    "from ray.tune.registry import register_env\n",
    "from gymnasium.wrappers.time_limit import TimeLimit\n",
    "\n",
    "\n",
    "register_env(\"my_env\", lambda _: PointCoverageEnv({\"height\": 4, \"width\": 4, \"n_agents\": 3, \"n_targets\": 3, \"max_steps\": 50}))\n",
    "\n",
    "train_batch_size = 2048\n",
    "sgd_minibatch_size = 256\n",
    "num_sgd_iter = 10\n",
    "trainings = 50\n",
    "\n",
    "total_env_steps = trainings*train_batch_size\n",
    "\n",
    "print(f\"number of different environment steps: {total_env_steps}\")\n",
    "\n",
    "\n",
    "algo3 = (\n",
    "    PPOConfig()\n",
    "    .training(gamma=0.99, \n",
    "              #lr=0.001,\n",
    "              lr_schedule=[\n",
    "                [0, 0.01],  \n",
    "                [1000, 0.001],  \n",
    "                [10000, 0.001],  \n",
    "              ],\n",
    "              kl_coeff=0.5, \n",
    "              train_batch_size=train_batch_size, \n",
    "              sgd_minibatch_size=sgd_minibatch_size, \n",
    "              num_sgd_iter=num_sgd_iter, \n",
    "              entropy_coeff_schedule = [\n",
    "                [0, 1],  # Start with relatively high entropy coefficient\n",
    "                [20480, 0],  # Gradually decrease entropy coefficient over 10,000 iterations\n",
    "              ])\n",
    "    .env_runners(num_env_runners=1)\n",
    "    .resources(num_gpus=0)\n",
    "    .environment(env=\"my_env\")\n",
    "    .build()\n",
    ")\n",
    "\n",
    "def customResultPrint(result):\n",
    "    print(f\"iteration [{result['training_iteration']}] => \" +\n",
    "          f\"episode_reward_mean: {result['sampler_results']['episode_reward_mean']}, \" +\n",
    "          f\"episode_len_mean: {result['sampler_results']['episode_len_mean']}, \" +\n",
    "          f\"agent_steps_trained: {result['info']['num_agent_steps_trained']}, \" +\n",
    "          f\"env_steps_trained: {result['info']['num_env_steps_trained']}, \" + \n",
    "          f\"entropy: {result['info']['learner']['default_policy']['learner_stats']['entropy']}, \" +\n",
    "          f\"learning_rate: {result['info']['learner']['default_policy']['learner_stats']['cur_lr']}\")\n",
    "\n",
    "for i in range(trainings):\n",
    "    result = algo3.train()\n",
    "    customResultPrint(result)\n",
    "    if i % 5 == 0:\n",
    "        checkpoint_dir = algo3.save().checkpoint.path\n",
    "        print(f\"Checkpoint saved in directory {checkpoint_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________\n",
      "|          |\n",
      "|          |\n",
      "|          |\n",
      "|       o  |\n",
      "|          |\n",
      "|          |\n",
      "|  o   o   |\n",
      "|     x    |\n",
      "|     x    |\n",
      "|        x |\n",
      "‾‾‾‾‾‾‾‾‾‾‾‾\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'algo3' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[21], line 9\u001b[0m\n\u001b[1;32m      6\u001b[0m env\u001b[38;5;241m.\u001b[39mrender()\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m100\u001b[39m):\n\u001b[0;32m----> 9\u001b[0m     actions \u001b[38;5;241m=\u001b[39m \u001b[43malgo3\u001b[49m\u001b[38;5;241m.\u001b[39mcompute_actions(obs)\n\u001b[1;32m     10\u001b[0m     \u001b[38;5;28mprint\u001b[39m(actions, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     11\u001b[0m     obs, reward, terminated, truncated, info \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mstep(actions)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'algo3' is not defined"
     ]
    }
   ],
   "source": [
    "from IPython.display import clear_output\n",
    "import time\n",
    "\n",
    "env = PointCoverageEnv({\"height\": 10, \"width\": 10, \"n_agents\": 3, \"n_targets\": 3})\n",
    "obs, _ = env.reset()\n",
    "env.render()\n",
    "\n",
    "for i in range(100):\n",
    "    actions = algo3.compute_actions(obs)\n",
    "    print(actions, \"\\n\")\n",
    "    obs, reward, terminated, truncated, info = env.step(actions)\n",
    "    clear_output()\n",
    "    print(f\"[{i}]\")\n",
    "    env.render()\n",
    "    print(obs)\n",
    "    print(reward)\n",
    "    time.sleep(0.5)\n",
    "\n",
    "    if terminated['__all__'] or truncated['__all__']:\n",
    "        break\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rayEnv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
