{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Set\n",
    "from ray.rllib.env.multi_agent_env import MultiAgentEnv\n",
    "import random as rnd\n",
    "import math\n",
    "from gymnasium.spaces import Discrete, Box, Sequence, Dict\n",
    "from gymnasium.spaces.utils import flatten, flatten_space\n",
    "import numpy as np\n",
    "from IPython.display import clear_output\n",
    "\n",
    "from ray.rllib.utils.typing import AgentID\n",
    "\n",
    "class PointCoverageEnv(MultiAgentEnv):\n",
    "\n",
    "    actions_dict = [(0,-1),(0,1),(1,0),(-1,0),(0,0)]\n",
    "\n",
    "    def __init__(self, config):\n",
    "        self.observations_memory = config[\"observations_memory\"] if \"observations_memory\" in config.keys() else 1\n",
    "        self.width = config[\"width\"]\n",
    "        self.height = config[\"height\"]\n",
    "        self.n_agents = config[\"n_agents\"]\n",
    "        self.n_targets = config[\"n_targets\"]\n",
    "        self.max_steps = config[\"max_steps\"] if \"max_steps\" in config.keys() else None\n",
    "        self.use_nested_observation = config[\"use_nested_observation\"] if \"use_nested_observation\" in config.keys() else False\n",
    "        self.agents = ['agent-' + str(i) for i in range(self.n_agents)]\n",
    "        self.observation_space = self.observation_space('agent-0')\n",
    "        self.action_space = Discrete(5)\n",
    "\n",
    "    def unflatten_observation_space(self, agent):\n",
    "        coordinates_space = Box(low=np.array([0.0, 0.0], dtype=np.float32), high=np.array([1.0, 1.0], dtype=np.float32), dtype=np.float32)\n",
    "        obs_space = {\"position\": coordinates_space,\n",
    "                     \"targets\": Dict({f\"target-{i}\": coordinates_space for i in range(self.n_targets)})}\n",
    "        if self.n_agents > 1:\n",
    "            obs_space[\"other_agents\"] = Dict({f\"other_agent-{i}\": coordinates_space for i in range(self.n_agents-1)}) \n",
    "        \n",
    "        obs_space = Dict(obs_space)\n",
    "\n",
    "        if self.observations_memory > 1:\n",
    "            return Dict({f\"t(-{i})\": obs_space for i in range(self.observations_memory)})\n",
    "        return obs_space\n",
    "\n",
    "    def observation_space(self, agent):\n",
    "       if self.use_nested_observation:\n",
    "           return self.unflatten_observation_space(agent)\n",
    "       return flatten_space(self.unflatten_observation_space(agent))\n",
    "\n",
    "    def action_space(self, agent):\n",
    "        return Discrete(5)\n",
    "    \n",
    "    def __get_other_agents(self, agent):\n",
    "        return [other for other in self.agents if other != agent]\n",
    "\n",
    "    def __get_random_point(self):\n",
    "        return (rnd.randint(0, self.width-1), rnd.randint(0, self.height-1))\n",
    "    \n",
    "    def __get_normalized_position(self, position):\n",
    "        return (position[0]/self.width, position[1]/self.height)\n",
    "\n",
    "    def __get_unflatten_time_t_observation(self, agent):\n",
    "        time_t_obs = {\"position\": self.__get_normalized_position(self.agent_pos[agent]),\n",
    "               \"targets\": {f\"target-{i}\": self.__get_normalized_position(pos) for i, pos in enumerate(self.targets)}}\n",
    "        if self.n_agents > 1:\n",
    "            time_t_obs[\"other_agents\"] = {f\"other_agent-{i}\": self.__get_normalized_position(self.agent_pos[other]) for i, other in enumerate(self.__get_other_agents(agent))},\n",
    "        return time_t_obs\n",
    "\n",
    "    def __get_observation(self, agent):\n",
    "        time_t_obs = self.__get_unflatten_time_t_observation(agent)\n",
    "\n",
    "        obs = {}\n",
    "        if self.observations_memory > 1:\n",
    "            self.agents_memory[agent].pop(0)\n",
    "            self.agents_memory[agent].append(time_t_obs)\n",
    "            obs = {f\"t(-{i})\": self.agents_memory[agent][self.observations_memory-1-i] for i in range(self.observations_memory)}\n",
    "        else:\n",
    "            obs = time_t_obs\n",
    "\n",
    "        if self.use_nested_observation:\n",
    "            return obs\n",
    "        return flatten(self.unflatten_observation_space(agent), obs)\n",
    "\n",
    "    def __get_not_covered_targets(self):\n",
    "        return set(self.targets) - set(self.agent_pos.values())\n",
    "\n",
    "    def __is_target_contended(self, target):\n",
    "        return len([t for t in self.agent_pos.values() if target == t]) > 1\n",
    "\n",
    "    def __get_reward(self, agent):\n",
    "        return -1 + self.__get_global_reward()\n",
    "        if self.agent_pos[agent] in self.targets:\n",
    "            if self.agent_pos[agent] in [pos[1] for pos in self.old_agent_pos if pos[0] != agent]:\n",
    "                return -1 # someone was already covering the target -> no +10 reward\n",
    "            if self.__is_target_contended(self.agent_pos[agent]):\n",
    "                return -2 # someone arrived at the target at the same time of me -> someone has to leave\n",
    "            return 10\n",
    "        else:\n",
    "            return -1\n",
    "    \n",
    "    def __get_global_reward(self):\n",
    "        return (len(self.not_covered_target) - len(set(self.not_covered_target) - set(self.agent_pos.values())))*10\n",
    "    \n",
    "    def __update_agent_position(self, agent, x, y):\n",
    "        self.agent_pos[agent] = (max(min(self.agent_pos[agent][0] + x, self.width-1), 0),\n",
    "                                 max(min(self.agent_pos[agent][1] + y, self.height-1), 0))\n",
    "\n",
    "    def reset(self, seed=None, options=None):\n",
    "        self.agent_pos = {agent: self.__get_random_point() for agent in self.agents}\n",
    "        self.targets = [self.__get_random_point() for _ in range(self.n_targets)]\n",
    "        self.not_covered_target = self.targets.copy()\n",
    "        self.steps = 0;\n",
    "        self.agents_memory = {agent: [self.__get_unflatten_time_t_observation(agent)]*self.observations_memory for agent in self.agents}\n",
    "        return {agent: self.__get_observation(agent) for agent in self.agents}, {}\n",
    "     \n",
    "    def step(self, actions):\n",
    "        self.steps += 1\n",
    "        observations, rewards, terminated, truncated, infos = {}, {}, {}, {}, {}\n",
    "\n",
    "        self.old_agent_pos = self.agent_pos.copy()\n",
    "        for agent, action in actions.items():\n",
    "            self.__update_agent_position(agent, self.actions_dict[action][0], self.actions_dict[action][1])\n",
    "\n",
    "        for agent in actions.keys():\n",
    "            if not (self.agent_pos[agent] in self.targets and not self.__is_target_contended(self.agent_pos[agent])):\n",
    "                observations[agent] = self.__get_observation(agent)\n",
    "                rewards[agent] = self.__get_reward(agent)\n",
    "                terminated[agent] = False\n",
    "                truncated[agent] = False\n",
    "                infos[agent] = {}\n",
    "        \n",
    "        if self.max_steps != None and self.steps > self.max_steps:\n",
    "            truncated['__all__'] = True\n",
    "        else:\n",
    "            truncated['__all__'] = False\n",
    "\n",
    "        self.not_covered_target = list(set(self.not_covered_target) - set(self.agent_pos.values())) \n",
    "\n",
    "        terminated['__all__'] = len(self.__get_not_covered_targets()) == 0\n",
    "        return observations, rewards, terminated, truncated, infos\n",
    "     \n",
    "    def render(self, mode='text'):\n",
    "        str = '_' * (self.width+2) + '\\n'\n",
    "        for i in range(self.height):\n",
    "            str = str + \"|\"\n",
    "            for j in range(self.width):\n",
    "                if (j,i) in self.agent_pos.values() and (j,i) in self.targets:\n",
    "                    str = str + '*'\n",
    "                elif (j,i) in self.agent_pos.values():\n",
    "                    str = str + 'o'\n",
    "                elif (j,i) in self.targets:\n",
    "                    str = str + 'x'\n",
    "                else:\n",
    "                    str = str + ' '\n",
    "            str = str + '|\\n'\n",
    "        str = str + 'â€¾' * (self.width+2)\n",
    "        print(str)\n",
    "\n",
    "    def get_agent_ids(self):\n",
    "       return self.agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "tuple indices must be integers or slices, not str",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[44], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mjson\u001b[39;00m\n\u001b[1;32m      3\u001b[0m env \u001b[38;5;241m=\u001b[39m PointCoverageEnv({\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mheight\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m10\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwidth\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m10\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mn_agents\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m2\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mn_targets\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m2\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muse_nested_observation\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mobservations_memory\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m2\u001b[39m})\n\u001b[0;32m----> 4\u001b[0m obs, _ \u001b[38;5;241m=\u001b[39m \u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreset\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \n\u001b[1;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(json\u001b[38;5;241m.\u001b[39mdumps(obs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124magent-0\u001b[39m\u001b[38;5;124m'\u001b[39m], indent\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m))\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m#env.render()\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[21], line 110\u001b[0m, in \u001b[0;36mPointCoverageEnv.reset\u001b[0;34m(self, seed, options)\u001b[0m\n\u001b[1;32m    108\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msteps \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m;\n\u001b[1;32m    109\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39magents_memory \u001b[38;5;241m=\u001b[39m {agent: [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__get_unflatten_time_t_observation(agent)]\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobservations_memory \u001b[38;5;28;01mfor\u001b[39;00m agent \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39magents}\n\u001b[0;32m--> 110\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m{\u001b[49m\u001b[43magent\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__get_observation\u001b[49m\u001b[43m(\u001b[49m\u001b[43magent\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43magent\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43magents\u001b[49m\u001b[43m}\u001b[49m, {}\n",
      "Cell \u001b[0;32mIn[21], line 110\u001b[0m, in \u001b[0;36m<dictcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    108\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msteps \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m;\n\u001b[1;32m    109\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39magents_memory \u001b[38;5;241m=\u001b[39m {agent: [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__get_unflatten_time_t_observation(agent)]\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobservations_memory \u001b[38;5;28;01mfor\u001b[39;00m agent \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39magents}\n\u001b[0;32m--> 110\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m {agent: \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__get_observation\u001b[49m\u001b[43m(\u001b[49m\u001b[43magent\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m agent \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39magents}, {}\n",
      "Cell \u001b[0;32mIn[21], line 78\u001b[0m, in \u001b[0;36mPointCoverageEnv.__get_observation\u001b[0;34m(self, agent)\u001b[0m\n\u001b[1;32m     76\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muse_nested_observation:\n\u001b[1;32m     77\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m obs\n\u001b[0;32m---> 78\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mflatten\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munflatten_observation_space\u001b[49m\u001b[43m(\u001b[49m\u001b[43magent\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mobs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/tianEnv/lib/python3.11/functools.py:909\u001b[0m, in \u001b[0;36msingledispatch.<locals>.wrapper\u001b[0;34m(*args, **kw)\u001b[0m\n\u001b[1;32m    905\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m args:\n\u001b[1;32m    906\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfuncname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m requires at least \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    907\u001b[0m                     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m1 positional argument\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m--> 909\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mdispatch\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;18;43m__class__\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/tianEnv/lib/python3.11/site-packages/gymnasium/spaces/utils.py:196\u001b[0m, in \u001b[0;36m_flatten_dict\u001b[0;34m(space, x)\u001b[0m\n\u001b[1;32m    192\u001b[0m \u001b[38;5;129m@flatten\u001b[39m\u001b[38;5;241m.\u001b[39mregister(Dict)\n\u001b[1;32m    193\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_flatten_dict\u001b[39m(space: Dict, x: \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Any]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Any] \u001b[38;5;241m|\u001b[39m NDArray[Any]:\n\u001b[1;32m    194\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m space\u001b[38;5;241m.\u001b[39mis_np_flattenable:\n\u001b[1;32m    195\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39mconcatenate(\n\u001b[0;32m--> 196\u001b[0m             \u001b[43m[\u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marray\u001b[49m\u001b[43m(\u001b[49m\u001b[43mflatten\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m[\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43ms\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mspace\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mspaces\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitems\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m    197\u001b[0m         )\n\u001b[1;32m    198\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m OrderedDict((key, flatten(s, x[key])) \u001b[38;5;28;01mfor\u001b[39;00m key, s \u001b[38;5;129;01min\u001b[39;00m space\u001b[38;5;241m.\u001b[39mspaces\u001b[38;5;241m.\u001b[39mitems())\n",
      "File \u001b[0;32m~/anaconda3/envs/tianEnv/lib/python3.11/site-packages/gymnasium/spaces/utils.py:196\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    192\u001b[0m \u001b[38;5;129m@flatten\u001b[39m\u001b[38;5;241m.\u001b[39mregister(Dict)\n\u001b[1;32m    193\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_flatten_dict\u001b[39m(space: Dict, x: \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Any]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Any] \u001b[38;5;241m|\u001b[39m NDArray[Any]:\n\u001b[1;32m    194\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m space\u001b[38;5;241m.\u001b[39mis_np_flattenable:\n\u001b[1;32m    195\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39mconcatenate(\n\u001b[0;32m--> 196\u001b[0m             [np\u001b[38;5;241m.\u001b[39marray(\u001b[43mflatten\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m[\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m) \u001b[38;5;28;01mfor\u001b[39;00m key, s \u001b[38;5;129;01min\u001b[39;00m space\u001b[38;5;241m.\u001b[39mspaces\u001b[38;5;241m.\u001b[39mitems()]\n\u001b[1;32m    197\u001b[0m         )\n\u001b[1;32m    198\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m OrderedDict((key, flatten(s, x[key])) \u001b[38;5;28;01mfor\u001b[39;00m key, s \u001b[38;5;129;01min\u001b[39;00m space\u001b[38;5;241m.\u001b[39mspaces\u001b[38;5;241m.\u001b[39mitems())\n",
      "File \u001b[0;32m~/anaconda3/envs/tianEnv/lib/python3.11/functools.py:909\u001b[0m, in \u001b[0;36msingledispatch.<locals>.wrapper\u001b[0;34m(*args, **kw)\u001b[0m\n\u001b[1;32m    905\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m args:\n\u001b[1;32m    906\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfuncname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m requires at least \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    907\u001b[0m                     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m1 positional argument\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m--> 909\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mdispatch\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;18;43m__class__\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/tianEnv/lib/python3.11/site-packages/gymnasium/spaces/utils.py:196\u001b[0m, in \u001b[0;36m_flatten_dict\u001b[0;34m(space, x)\u001b[0m\n\u001b[1;32m    192\u001b[0m \u001b[38;5;129m@flatten\u001b[39m\u001b[38;5;241m.\u001b[39mregister(Dict)\n\u001b[1;32m    193\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_flatten_dict\u001b[39m(space: Dict, x: \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Any]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Any] \u001b[38;5;241m|\u001b[39m NDArray[Any]:\n\u001b[1;32m    194\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m space\u001b[38;5;241m.\u001b[39mis_np_flattenable:\n\u001b[1;32m    195\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39mconcatenate(\n\u001b[0;32m--> 196\u001b[0m             \u001b[43m[\u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marray\u001b[49m\u001b[43m(\u001b[49m\u001b[43mflatten\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m[\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43ms\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mspace\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mspaces\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitems\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m    197\u001b[0m         )\n\u001b[1;32m    198\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m OrderedDict((key, flatten(s, x[key])) \u001b[38;5;28;01mfor\u001b[39;00m key, s \u001b[38;5;129;01min\u001b[39;00m space\u001b[38;5;241m.\u001b[39mspaces\u001b[38;5;241m.\u001b[39mitems())\n",
      "File \u001b[0;32m~/anaconda3/envs/tianEnv/lib/python3.11/site-packages/gymnasium/spaces/utils.py:196\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    192\u001b[0m \u001b[38;5;129m@flatten\u001b[39m\u001b[38;5;241m.\u001b[39mregister(Dict)\n\u001b[1;32m    193\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_flatten_dict\u001b[39m(space: Dict, x: \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Any]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Any] \u001b[38;5;241m|\u001b[39m NDArray[Any]:\n\u001b[1;32m    194\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m space\u001b[38;5;241m.\u001b[39mis_np_flattenable:\n\u001b[1;32m    195\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39mconcatenate(\n\u001b[0;32m--> 196\u001b[0m             [np\u001b[38;5;241m.\u001b[39marray(\u001b[43mflatten\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m[\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m) \u001b[38;5;28;01mfor\u001b[39;00m key, s \u001b[38;5;129;01min\u001b[39;00m space\u001b[38;5;241m.\u001b[39mspaces\u001b[38;5;241m.\u001b[39mitems()]\n\u001b[1;32m    197\u001b[0m         )\n\u001b[1;32m    198\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m OrderedDict((key, flatten(s, x[key])) \u001b[38;5;28;01mfor\u001b[39;00m key, s \u001b[38;5;129;01min\u001b[39;00m space\u001b[38;5;241m.\u001b[39mspaces\u001b[38;5;241m.\u001b[39mitems())\n",
      "File \u001b[0;32m~/anaconda3/envs/tianEnv/lib/python3.11/functools.py:909\u001b[0m, in \u001b[0;36msingledispatch.<locals>.wrapper\u001b[0;34m(*args, **kw)\u001b[0m\n\u001b[1;32m    905\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m args:\n\u001b[1;32m    906\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfuncname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m requires at least \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    907\u001b[0m                     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m1 positional argument\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m--> 909\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mdispatch\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;18;43m__class__\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/tianEnv/lib/python3.11/site-packages/gymnasium/spaces/utils.py:196\u001b[0m, in \u001b[0;36m_flatten_dict\u001b[0;34m(space, x)\u001b[0m\n\u001b[1;32m    192\u001b[0m \u001b[38;5;129m@flatten\u001b[39m\u001b[38;5;241m.\u001b[39mregister(Dict)\n\u001b[1;32m    193\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_flatten_dict\u001b[39m(space: Dict, x: \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Any]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Any] \u001b[38;5;241m|\u001b[39m NDArray[Any]:\n\u001b[1;32m    194\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m space\u001b[38;5;241m.\u001b[39mis_np_flattenable:\n\u001b[1;32m    195\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39mconcatenate(\n\u001b[0;32m--> 196\u001b[0m             \u001b[43m[\u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marray\u001b[49m\u001b[43m(\u001b[49m\u001b[43mflatten\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m[\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43ms\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mspace\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mspaces\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitems\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m    197\u001b[0m         )\n\u001b[1;32m    198\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m OrderedDict((key, flatten(s, x[key])) \u001b[38;5;28;01mfor\u001b[39;00m key, s \u001b[38;5;129;01min\u001b[39;00m space\u001b[38;5;241m.\u001b[39mspaces\u001b[38;5;241m.\u001b[39mitems())\n",
      "File \u001b[0;32m~/anaconda3/envs/tianEnv/lib/python3.11/site-packages/gymnasium/spaces/utils.py:196\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    192\u001b[0m \u001b[38;5;129m@flatten\u001b[39m\u001b[38;5;241m.\u001b[39mregister(Dict)\n\u001b[1;32m    193\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_flatten_dict\u001b[39m(space: Dict, x: \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Any]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Any] \u001b[38;5;241m|\u001b[39m NDArray[Any]:\n\u001b[1;32m    194\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m space\u001b[38;5;241m.\u001b[39mis_np_flattenable:\n\u001b[1;32m    195\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39mconcatenate(\n\u001b[0;32m--> 196\u001b[0m             [np\u001b[38;5;241m.\u001b[39marray(flatten(s, \u001b[43mx\u001b[49m\u001b[43m[\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m]\u001b[49m)) \u001b[38;5;28;01mfor\u001b[39;00m key, s \u001b[38;5;129;01min\u001b[39;00m space\u001b[38;5;241m.\u001b[39mspaces\u001b[38;5;241m.\u001b[39mitems()]\n\u001b[1;32m    197\u001b[0m         )\n\u001b[1;32m    198\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m OrderedDict((key, flatten(s, x[key])) \u001b[38;5;28;01mfor\u001b[39;00m key, s \u001b[38;5;129;01min\u001b[39;00m space\u001b[38;5;241m.\u001b[39mspaces\u001b[38;5;241m.\u001b[39mitems())\n",
      "\u001b[0;31mTypeError\u001b[0m: tuple indices must be integers or slices, not str"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "env = PointCoverageEnv({\"height\": 10, \"width\": 10, \"n_agents\": 2, \"n_targets\": 2, \"use_nested_observation\": False, \"observations_memory\": 2})\n",
    "obs, _ = env.reset() \n",
    "print(json.dumps(obs['agent-0'], indent=2))\n",
    "#env.render()\n",
    "\n",
    "obs, _, _, _, _ = env.step({'agent-0': 1, 'agent-1': 2})\n",
    "print(json.dumps(obs['agent-0'], indent=2))\n",
    "\n",
    "obs, _, _, _, _ = env.step({'agent-0': 1, 'agent-1': 2})\n",
    "print(json.dumps(obs['agent-0'], indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import ray\n",
    "\n",
    "def customResultPrint(result):\n",
    "    print(f\"iteration [{result['training_iteration']}] => \" +\n",
    "          f\"episode_reward_mean: {result['sampler_results']['episode_reward_mean']}, \" +\n",
    "          f\"episode_len_mean: {result['sampler_results']['episode_len_mean']}, \" +\n",
    "          f\"agent_steps_trained: {result['info']['num_agent_steps_trained']}, \" +\n",
    "          f\"env_steps_trained: {result['info']['num_env_steps_trained']}, \" + \n",
    "          f\"entropy: {result['info']['learner']['default_policy']['learner_stats']['entropy']}, \" +\n",
    "          f\"learning_rate: {result['info']['learner']['default_policy']['learner_stats']['cur_lr']}\")\n",
    "\n",
    "#ray.shutdown()\n",
    "#ray.init()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Single agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### no memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-17 08:11:46,981\tWARNING deprecation.py:50 -- DeprecationWarning: `WorkerSet(num_workers=... OR local_worker=...)` has been deprecated. Use `EnvRunnerGroup(num_env_runners=... AND local_env_runner=...)` instead. This will raise an error in the future!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of different environment steps: 61440\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-17 08:11:53,541\tWARNING util.py:61 -- Install gputil for GPU system monitoring.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration [1] => episode_reward_mean: -90.81818181818181, episode_len_mean: 91.04545454545455, agent_steps_trained: 2048, env_steps_trained: 2048, entropy: 1.60349660217762, learning_rate: 0.0010000000000000002\n",
      "Checkpoint saved in directory /tmp/tmpjr_c5tqy\n",
      "iteration [2] => episode_reward_mean: -84.68085106382979, episode_len_mean: 85.04255319148936, agent_steps_trained: 4096, env_steps_trained: 4096, entropy: 1.6017870739102364, learning_rate: 0.0010000000000000002\n",
      "iteration [3] => episode_reward_mean: -85.05633802816901, episode_len_mean: 85.3943661971831, agent_steps_trained: 6144, env_steps_trained: 6144, entropy: 1.5836288914084435, learning_rate: 0.0010000000000000002\n",
      "iteration [4] => episode_reward_mean: -81.5959595959596, episode_len_mean: 81.97979797979798, agent_steps_trained: 8192, env_steps_trained: 8192, entropy: 1.5643436506390571, learning_rate: 0.0010000000000000002\n",
      "iteration [5] => episode_reward_mean: -72.38, episode_len_mean: 72.85, agent_steps_trained: 10240, env_steps_trained: 10240, entropy: 1.5459915816783905, learning_rate: 0.0010000000000000002\n",
      "iteration [6] => episode_reward_mean: -56.1, episode_len_mean: 56.81, agent_steps_trained: 12288, env_steps_trained: 12288, entropy: 1.5135473862290383, learning_rate: 0.0010000000000000002\n",
      "Checkpoint saved in directory /tmp/tmpm194im4k\n",
      "iteration [7] => episode_reward_mean: -32.83, episode_len_mean: 33.81, agent_steps_trained: 14336, env_steps_trained: 14336, entropy: 1.4627402365207671, learning_rate: 0.0010000000000000002\n",
      "iteration [8] => episode_reward_mean: -21.33, episode_len_mean: 22.33, agent_steps_trained: 16384, env_steps_trained: 16384, entropy: 1.3706035643815995, learning_rate: 0.0010000000000000002\n",
      "iteration [9] => episode_reward_mean: -18.0, episode_len_mean: 19.0, agent_steps_trained: 18432, env_steps_trained: 18432, entropy: 1.3125217989087106, learning_rate: 0.0010000000000000002\n",
      "iteration [10] => episode_reward_mean: -13.709219858156029, episode_len_mean: 14.709219858156029, agent_steps_trained: 20480, env_steps_trained: 20480, entropy: 1.221819657087326, learning_rate: 0.0010000000000000002\n",
      "iteration [11] => episode_reward_mean: -10.433333333333334, episode_len_mean: 11.433333333333334, agent_steps_trained: 22528, env_steps_trained: 22528, entropy: 1.0828382983803748, learning_rate: 0.0010000000000000002\n",
      "Checkpoint saved in directory /tmp/tmpx6selbfj\n",
      "iteration [12] => episode_reward_mean: -9.596858638743456, episode_len_mean: 10.596858638743456, agent_steps_trained: 24576, env_steps_trained: 24576, entropy: 0.995860131084919, learning_rate: 0.0010000000000000002\n",
      "iteration [13] => episode_reward_mean: -8.174107142857142, episode_len_mean: 9.174107142857142, agent_steps_trained: 26624, env_steps_trained: 26624, entropy: 0.8723068416118622, learning_rate: 0.0010000000000000002\n",
      "iteration [14] => episode_reward_mean: -7.733050847457627, episode_len_mean: 8.733050847457626, agent_steps_trained: 28672, env_steps_trained: 28672, entropy: 0.7680503740906716, learning_rate: 0.0010000000000000002\n",
      "iteration [15] => episode_reward_mean: -6.628252788104089, episode_len_mean: 7.628252788104089, agent_steps_trained: 30720, env_steps_trained: 30720, entropy: 0.6629793271422386, learning_rate: 0.0010000000000000002\n",
      "iteration [16] => episode_reward_mean: -6.072664359861592, episode_len_mean: 7.072664359861592, agent_steps_trained: 32768, env_steps_trained: 32768, entropy: 0.5652224730700255, learning_rate: 0.0010000000000000002\n",
      "Checkpoint saved in directory /tmp/tmpbal924en\n",
      "iteration [17] => episode_reward_mean: -6.030927835051546, episode_len_mean: 7.030927835051546, agent_steps_trained: 34816, env_steps_trained: 34816, entropy: 0.5024684444069862, learning_rate: 0.0010000000000000002\n",
      "iteration [18] => episode_reward_mean: -5.836666666666667, episode_len_mean: 6.836666666666667, agent_steps_trained: 36864, env_steps_trained: 36864, entropy: 0.4797315191477537, learning_rate: 0.0010000000000000002\n",
      "iteration [19] => episode_reward_mean: -5.965986394557823, episode_len_mean: 6.965986394557823, agent_steps_trained: 38912, env_steps_trained: 38912, entropy: 0.436360764503479, learning_rate: 0.0010000000000000002\n",
      "iteration [20] => episode_reward_mean: -5.803986710963455, episode_len_mean: 6.803986710963455, agent_steps_trained: 40960, env_steps_trained: 40960, entropy: 0.4278663232922554, learning_rate: 0.0010000000000000002\n",
      "iteration [21] => episode_reward_mean: -5.515923566878981, episode_len_mean: 6.515923566878981, agent_steps_trained: 43008, env_steps_trained: 43008, entropy: 0.37097432650625706, learning_rate: 0.0010000000000000002\n",
      "Checkpoint saved in directory /tmp/tmprgnj8h_i\n",
      "iteration [22] => episode_reward_mean: -5.938983050847457, episode_len_mean: 6.938983050847457, agent_steps_trained: 45056, env_steps_trained: 45056, entropy: 0.3917484857141972, learning_rate: 0.0010000000000000002\n",
      "iteration [23] => episode_reward_mean: -5.79734219269103, episode_len_mean: 6.79734219269103, agent_steps_trained: 47104, env_steps_trained: 47104, entropy: 0.37333511784672735, learning_rate: 0.0010000000000000002\n",
      "iteration [24] => episode_reward_mean: -5.925675675675675, episode_len_mean: 6.925675675675675, agent_steps_trained: 49152, env_steps_trained: 49152, entropy: 0.3468496672809124, learning_rate: 0.0010000000000000002\n",
      "iteration [25] => episode_reward_mean: -5.466876971608833, episode_len_mean: 6.466876971608833, agent_steps_trained: 51200, env_steps_trained: 51200, entropy: 0.3546841099858284, learning_rate: 0.0010000000000000002\n",
      "iteration [26] => episode_reward_mean: -5.833333333333333, episode_len_mean: 6.833333333333333, agent_steps_trained: 53248, env_steps_trained: 53248, entropy: 0.36969437524676324, learning_rate: 0.0010000000000000002\n",
      "Checkpoint saved in directory /tmp/tmpqeg6pug1\n",
      "iteration [27] => episode_reward_mean: -5.522292993630574, episode_len_mean: 6.522292993630574, agent_steps_trained: 55296, env_steps_trained: 55296, entropy: 0.34244338497519494, learning_rate: 0.0010000000000000002\n",
      "iteration [28] => episode_reward_mean: -5.713815789473684, episode_len_mean: 6.713815789473684, agent_steps_trained: 57344, env_steps_trained: 57344, entropy: 0.3466445583850145, learning_rate: 0.0010000000000000002\n",
      "iteration [29] => episode_reward_mean: -5.697068403908795, episode_len_mean: 6.697068403908795, agent_steps_trained: 59392, env_steps_trained: 59392, entropy: 0.34955844059586527, learning_rate: 0.0010000000000000002\n",
      "iteration [30] => episode_reward_mean: -5.689542483660131, episode_len_mean: 6.689542483660131, agent_steps_trained: 61440, env_steps_trained: 61440, entropy: 0.3420299269258976, learning_rate: 0.0010000000000000002\n"
     ]
    }
   ],
   "source": [
    "from ray.rllib.algorithms.ppo import PPOConfig\n",
    "from ray.tune.logger import pretty_print\n",
    "from ray.tune.registry import register_env\n",
    "from gymnasium.wrappers.time_limit import TimeLimit\n",
    "\n",
    "observations_memory = 1\n",
    "register_env(\"my_env\", lambda _: PointCoverageEnv({\"height\": 10, \"width\": 10, \"n_agents\": 1, \"n_targets\": 1, \"max_steps\": 100,  \"observations_memory\": observations_memory}))\n",
    "\n",
    "train_batch_size = 2048\n",
    "sgd_minibatch_size = 256\n",
    "num_sgd_iter = 10\n",
    "trainings = 30\n",
    "\n",
    "total_env_steps = trainings*train_batch_size\n",
    "\n",
    "print(f\"number of different environment steps: {total_env_steps}\")\n",
    "\n",
    "\n",
    "algo = (\n",
    "    PPOConfig()\n",
    "    .training(gamma=0.99, \n",
    "              lr=0.001,\n",
    "              kl_coeff=0.5, \n",
    "              train_batch_size=train_batch_size, \n",
    "              sgd_minibatch_size=sgd_minibatch_size, \n",
    "              num_sgd_iter=num_sgd_iter)\n",
    "    .env_runners(num_env_runners=1)\n",
    "    .resources(num_gpus=0)\n",
    "    .environment(env=\"my_env\")\n",
    "    .build()\n",
    ")\n",
    "\n",
    "for i in range(trainings):\n",
    "    result = algo.train()\n",
    "    customResultPrint(result)\n",
    "    if i % 5 == 0:\n",
    "        checkpoint_dir = algo.save().checkpoint.path\n",
    "        print(f\"Checkpoint saved in directory {checkpoint_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### memory = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-17 08:13:28,081\tWARNING deprecation.py:50 -- DeprecationWarning: `WorkerSet(num_workers=... OR local_worker=...)` has been deprecated. Use `EnvRunnerGroup(num_env_runners=... AND local_env_runner=...)` instead. This will raise an error in the future!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of different environment steps: 61440\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-17 08:13:31,885\tWARNING util.py:61 -- Install gputil for GPU system monitoring.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration [1] => episode_reward_mean: -96.38095238095238, episode_len_mean: 96.42857142857143, agent_steps_trained: 2048, env_steps_trained: 2048, entropy: 1.5989278897643089, learning_rate: 0.0010000000000000002\n",
      "Checkpoint saved in directory /tmp/tmpvz7f9dso\n",
      "iteration [2] => episode_reward_mean: -84.91666666666667, episode_len_mean: 85.16666666666667, agent_steps_trained: 4096, env_steps_trained: 4096, entropy: 1.5958691835403442, learning_rate: 0.0010000000000000002\n",
      "iteration [3] => episode_reward_mean: -79.41558441558442, episode_len_mean: 79.77922077922078, agent_steps_trained: 6144, env_steps_trained: 6144, entropy: 1.5710871025919915, learning_rate: 0.0010000000000000002\n",
      "iteration [4] => episode_reward_mean: -78.2, episode_len_mean: 78.61, agent_steps_trained: 8192, env_steps_trained: 8192, entropy: 1.55942050665617, learning_rate: 0.0010000000000000002\n",
      "iteration [5] => episode_reward_mean: -61.6, episode_len_mean: 62.22, agent_steps_trained: 10240, env_steps_trained: 10240, entropy: 1.524157890677452, learning_rate: 0.0010000000000000002\n",
      "iteration [6] => episode_reward_mean: -45.11, episode_len_mean: 45.91, agent_steps_trained: 12288, env_steps_trained: 12288, entropy: 1.485199399292469, learning_rate: 0.0010000000000000002\n",
      "Checkpoint saved in directory /tmp/tmp7jz6l64r\n",
      "iteration [7] => episode_reward_mean: -35.56, episode_len_mean: 36.49, agent_steps_trained: 14336, env_steps_trained: 14336, entropy: 1.4704326003789903, learning_rate: 0.0010000000000000002\n",
      "iteration [8] => episode_reward_mean: -27.88, episode_len_mean: 28.86, agent_steps_trained: 16384, env_steps_trained: 16384, entropy: 1.41787341684103, learning_rate: 0.0010000000000000002\n",
      "iteration [9] => episode_reward_mean: -19.435643564356436, episode_len_mean: 20.435643564356436, agent_steps_trained: 18432, env_steps_trained: 18432, entropy: 1.3372400358319283, learning_rate: 0.0010000000000000002\n",
      "iteration [10] => episode_reward_mean: -15.251968503937007, episode_len_mean: 16.251968503937007, agent_steps_trained: 20480, env_steps_trained: 20480, entropy: 1.2452780351042747, learning_rate: 0.0010000000000000002\n",
      "iteration [11] => episode_reward_mean: -11.748427672955975, episode_len_mean: 12.748427672955975, agent_steps_trained: 22528, env_steps_trained: 22528, entropy: 1.1169996768236161, learning_rate: 0.0010000000000000002\n",
      "Checkpoint saved in directory /tmp/tmpmscyhwn9\n",
      "iteration [12] => episode_reward_mean: -9.561224489795919, episode_len_mean: 10.561224489795919, agent_steps_trained: 24576, env_steps_trained: 24576, entropy: 0.9989799000322819, learning_rate: 0.0010000000000000002\n",
      "iteration [13] => episode_reward_mean: -8.574766355140186, episode_len_mean: 9.574766355140186, agent_steps_trained: 26624, env_steps_trained: 26624, entropy: 0.8458545915782452, learning_rate: 0.0010000000000000002\n",
      "iteration [14] => episode_reward_mean: -7.368852459016393, episode_len_mean: 8.368852459016393, agent_steps_trained: 28672, env_steps_trained: 28672, entropy: 0.7525907345116138, learning_rate: 0.0010000000000000002\n",
      "iteration [15] => episode_reward_mean: -7.187250996015936, episode_len_mean: 8.187250996015937, agent_steps_trained: 30720, env_steps_trained: 30720, entropy: 0.656770808249712, learning_rate: 0.0010000000000000002\n",
      "iteration [16] => episode_reward_mean: -6.0899653979238755, episode_len_mean: 7.0899653979238755, agent_steps_trained: 32768, env_steps_trained: 32768, entropy: 0.5609090276062488, learning_rate: 0.0010000000000000002\n",
      "Checkpoint saved in directory /tmp/tmphgfmqjwz\n",
      "iteration [17] => episode_reward_mean: -5.972602739726027, episode_len_mean: 6.972602739726027, agent_steps_trained: 34816, env_steps_trained: 34816, entropy: 0.5158733755350113, learning_rate: 0.0010000000000000002\n",
      "iteration [18] => episode_reward_mean: -6.023890784982935, episode_len_mean: 7.023890784982935, agent_steps_trained: 36864, env_steps_trained: 36864, entropy: 0.4805953077971935, learning_rate: 0.0010000000000000002\n",
      "iteration [19] => episode_reward_mean: -6.522058823529412, episode_len_mean: 7.522058823529412, agent_steps_trained: 38912, env_steps_trained: 38912, entropy: 0.4501120515167713, learning_rate: 0.0010000000000000002\n",
      "iteration [20] => episode_reward_mean: -5.75, episode_len_mean: 6.75, agent_steps_trained: 40960, env_steps_trained: 40960, entropy: 0.4062701418995857, learning_rate: 0.0010000000000000002\n",
      "iteration [21] => episode_reward_mean: -5.735973597359736, episode_len_mean: 6.735973597359736, agent_steps_trained: 43008, env_steps_trained: 43008, entropy: 0.387417646497488, learning_rate: 0.0010000000000000002\n",
      "Checkpoint saved in directory /tmp/tmpwa7o1ig2\n",
      "iteration [22] => episode_reward_mean: -5.898989898989899, episode_len_mean: 6.898989898989899, agent_steps_trained: 45056, env_steps_trained: 45056, entropy: 0.38118677623569963, learning_rate: 0.0010000000000000002\n",
      "iteration [23] => episode_reward_mean: -5.662337662337662, episode_len_mean: 6.662337662337662, agent_steps_trained: 47104, env_steps_trained: 47104, entropy: 0.35799172781407834, learning_rate: 0.0010000000000000002\n",
      "iteration [24] => episode_reward_mean: -6.0446735395189, episode_len_mean: 7.0446735395189, agent_steps_trained: 49152, env_steps_trained: 49152, entropy: 0.3575042337179184, learning_rate: 0.0010000000000000002\n",
      "iteration [25] => episode_reward_mean: -5.624595469255663, episode_len_mean: 6.624595469255663, agent_steps_trained: 51200, env_steps_trained: 51200, entropy: 0.3371390331536531, learning_rate: 0.0010000000000000002\n",
      "iteration [26] => episode_reward_mean: -6.307142857142857, episode_len_mean: 7.307142857142857, agent_steps_trained: 53248, env_steps_trained: 53248, entropy: 0.35694034211337566, learning_rate: 0.0010000000000000002\n",
      "Checkpoint saved in directory /tmp/tmpu5zsa7o2\n",
      "iteration [27] => episode_reward_mean: -6.072413793103448, episode_len_mean: 7.072413793103448, agent_steps_trained: 55296, env_steps_trained: 55296, entropy: 0.3680661506950855, learning_rate: 0.0010000000000000002\n",
      "iteration [28] => episode_reward_mean: -5.559485530546624, episode_len_mean: 6.559485530546624, agent_steps_trained: 57344, env_steps_trained: 57344, entropy: 0.33589093163609507, learning_rate: 0.0010000000000000002\n",
      "iteration [29] => episode_reward_mean: -5.616129032258065, episode_len_mean: 6.616129032258065, agent_steps_trained: 59392, env_steps_trained: 59392, entropy: 0.33774342834949495, learning_rate: 0.0010000000000000002\n",
      "iteration [30] => episode_reward_mean: -5.407523510971787, episode_len_mean: 6.407523510971787, agent_steps_trained: 61440, env_steps_trained: 61440, entropy: 0.34115011282265184, learning_rate: 0.0010000000000000002\n"
     ]
    }
   ],
   "source": [
    "from ray.rllib.algorithms.ppo import PPOConfig\n",
    "from ray.tune.logger import pretty_print\n",
    "from ray.tune.registry import register_env\n",
    "from gymnasium.wrappers.time_limit import TimeLimit\n",
    "\n",
    "observations_memory = 2\n",
    "register_env(\"my_env\", lambda _: PointCoverageEnv({\"height\": 10, \"width\": 10, \"n_agents\": 1, \"n_targets\": 1, \"max_steps\": 100, \"observations_memory\": observations_memory}))\n",
    "\n",
    "train_batch_size = 2048\n",
    "sgd_minibatch_size = 256\n",
    "num_sgd_iter = 10\n",
    "trainings = 30\n",
    "\n",
    "total_env_steps = trainings*train_batch_size\n",
    "\n",
    "print(f\"number of different environment steps: {total_env_steps}\")\n",
    "\n",
    "\n",
    "algo = (\n",
    "    PPOConfig()\n",
    "    .training(gamma=0.99, \n",
    "              lr=0.001,\n",
    "              kl_coeff=0.5, \n",
    "              train_batch_size=train_batch_size, \n",
    "              sgd_minibatch_size=sgd_minibatch_size, \n",
    "              num_sgd_iter=num_sgd_iter)\n",
    "    .env_runners(num_env_runners=1)\n",
    "    .resources(num_gpus=0)\n",
    "    .environment(env=\"my_env\")\n",
    "    .build()\n",
    ")\n",
    "\n",
    "for i in range(trainings):\n",
    "    result = algo.train()\n",
    "    customResultPrint(result)\n",
    "    if i % 5 == 0:\n",
    "        checkpoint_dir = algo.save().checkpoint.path\n",
    "        print(f\"Checkpoint saved in directory {checkpoint_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### memory = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-17 08:18:20,170\tWARNING deprecation.py:50 -- DeprecationWarning: `WorkerSet(num_workers=... OR local_worker=...)` has been deprecated. Use `EnvRunnerGroup(num_env_runners=... AND local_env_runner=...)` instead. This will raise an error in the future!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of different environment steps: 61440\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-17 08:18:25,422\tWARNING util.py:61 -- Install gputil for GPU system monitoring.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration [1] => episode_reward_mean: -83.95833333333333, episode_len_mean: 84.29166666666667, agent_steps_trained: 2048, env_steps_trained: 2048, entropy: 1.6013797476887703, learning_rate: 0.0010000000000000002\n",
      "Checkpoint saved in directory /tmp/tmpz5bng69m\n",
      "iteration [2] => episode_reward_mean: -83.85416666666667, episode_len_mean: 84.1875, agent_steps_trained: 4096, env_steps_trained: 4096, entropy: 1.5922476097941398, learning_rate: 0.0010000000000000002\n",
      "iteration [3] => episode_reward_mean: -74.95061728395062, episode_len_mean: 75.39506172839506, agent_steps_trained: 6144, env_steps_trained: 6144, entropy: 1.5758594870567322, learning_rate: 0.0010000000000000002\n",
      "iteration [4] => episode_reward_mean: -67.45, episode_len_mean: 68.0, agent_steps_trained: 8192, env_steps_trained: 8192, entropy: 1.5425284832715989, learning_rate: 0.0010000000000000002\n",
      "iteration [5] => episode_reward_mean: -56.35, episode_len_mean: 57.09, agent_steps_trained: 10240, env_steps_trained: 10240, entropy: 1.525013267993927, learning_rate: 0.0010000000000000002\n",
      "iteration [6] => episode_reward_mean: -43.8, episode_len_mean: 44.68, agent_steps_trained: 12288, env_steps_trained: 12288, entropy: 1.461374543607235, learning_rate: 0.0010000000000000002\n",
      "Checkpoint saved in directory /tmp/tmpkulj_s0a\n",
      "iteration [7] => episode_reward_mean: -31.01, episode_len_mean: 31.99, agent_steps_trained: 14336, env_steps_trained: 14336, entropy: 1.4072833761572838, learning_rate: 0.0010000000000000002\n",
      "iteration [8] => episode_reward_mean: -23.12, episode_len_mean: 24.12, agent_steps_trained: 16384, env_steps_trained: 16384, entropy: 1.3569801941514015, learning_rate: 0.0010000000000000002\n",
      "iteration [9] => episode_reward_mean: -13.450704225352112, episode_len_mean: 14.450704225352112, agent_steps_trained: 18432, env_steps_trained: 18432, entropy: 1.2446532428264618, learning_rate: 0.0010000000000000002\n",
      "iteration [10] => episode_reward_mean: -11.968354430379748, episode_len_mean: 12.968354430379748, agent_steps_trained: 20480, env_steps_trained: 20480, entropy: 1.1151885107159614, learning_rate: 0.0010000000000000002\n",
      "iteration [11] => episode_reward_mean: -9.551546391752577, episode_len_mean: 10.551546391752577, agent_steps_trained: 22528, env_steps_trained: 22528, entropy: 0.998683550208807, learning_rate: 0.0010000000000000002\n",
      "Checkpoint saved in directory /tmp/tmp3ecq6edc\n",
      "iteration [12] => episode_reward_mean: -8.525581395348837, episode_len_mean: 9.525581395348837, agent_steps_trained: 24576, env_steps_trained: 24576, entropy: 0.879890113323927, learning_rate: 0.0010000000000000002\n",
      "iteration [13] => episode_reward_mean: -7.621848739495798, episode_len_mean: 8.621848739495798, agent_steps_trained: 26624, env_steps_trained: 26624, entropy: 0.7719681441783905, learning_rate: 0.0010000000000000002\n",
      "iteration [14] => episode_reward_mean: -7.098814229249012, episode_len_mean: 8.098814229249012, agent_steps_trained: 28672, env_steps_trained: 28672, entropy: 0.6901816509664058, learning_rate: 0.0010000000000000002\n",
      "iteration [15] => episode_reward_mean: -6.640449438202247, episode_len_mean: 7.640449438202247, agent_steps_trained: 30720, env_steps_trained: 30720, entropy: 0.633736765384674, learning_rate: 0.0010000000000000002\n",
      "iteration [16] => episode_reward_mean: -6.235915492957746, episode_len_mean: 7.235915492957746, agent_steps_trained: 32768, env_steps_trained: 32768, entropy: 0.5420838166028261, learning_rate: 0.0010000000000000002\n",
      "Checkpoint saved in directory /tmp/tmpiykkrs47\n",
      "iteration [17] => episode_reward_mean: -6.164335664335664, episode_len_mean: 7.164335664335664, agent_steps_trained: 34816, env_steps_trained: 34816, entropy: 0.49410367868840693, learning_rate: 0.0010000000000000002\n",
      "iteration [18] => episode_reward_mean: -5.91554054054054, episode_len_mean: 6.91554054054054, agent_steps_trained: 36864, env_steps_trained: 36864, entropy: 0.4457156345248222, learning_rate: 0.0010000000000000002\n",
      "iteration [19] => episode_reward_mean: -5.823333333333333, episode_len_mean: 6.823333333333333, agent_steps_trained: 38912, env_steps_trained: 38912, entropy: 0.4140625078231096, learning_rate: 0.0010000000000000002\n",
      "iteration [20] => episode_reward_mean: -6.055363321799308, episode_len_mean: 7.055363321799308, agent_steps_trained: 40960, env_steps_trained: 40960, entropy: 0.4308756645768881, learning_rate: 0.0010000000000000002\n",
      "iteration [21] => episode_reward_mean: -5.589743589743589, episode_len_mean: 6.589743589743589, agent_steps_trained: 43008, env_steps_trained: 43008, entropy: 0.37640302553772925, learning_rate: 0.0010000000000000002\n",
      "Checkpoint saved in directory /tmp/tmp5kfschfe\n",
      "iteration [22] => episode_reward_mean: -5.538216560509555, episode_len_mean: 6.538216560509555, agent_steps_trained: 45056, env_steps_trained: 45056, entropy: 0.3808427777141333, learning_rate: 0.0010000000000000002\n",
      "iteration [23] => episode_reward_mean: -5.976027397260274, episode_len_mean: 6.976027397260274, agent_steps_trained: 47104, env_steps_trained: 47104, entropy: 0.38141451105475427, learning_rate: 0.0010000000000000002\n",
      "iteration [24] => episode_reward_mean: -5.578274760383387, episode_len_mean: 6.578274760383387, agent_steps_trained: 49152, env_steps_trained: 49152, entropy: 0.36326026879251005, learning_rate: 0.0010000000000000002\n",
      "iteration [25] => episode_reward_mean: -5.7368421052631575, episode_len_mean: 6.7368421052631575, agent_steps_trained: 51200, env_steps_trained: 51200, entropy: 0.36689983010292054, learning_rate: 0.0010000000000000002\n",
      "iteration [26] => episode_reward_mean: -5.7368421052631575, episode_len_mean: 6.7368421052631575, agent_steps_trained: 53248, env_steps_trained: 53248, entropy: 0.39709242470562456, learning_rate: 0.0010000000000000002\n",
      "Checkpoint saved in directory /tmp/tmplj0ewn2s\n",
      "iteration [27] => episode_reward_mean: -5.515923566878981, episode_len_mean: 6.515923566878981, agent_steps_trained: 55296, env_steps_trained: 55296, entropy: 0.3632656019181013, learning_rate: 0.0010000000000000002\n",
      "iteration [28] => episode_reward_mean: -5.560897435897436, episode_len_mean: 6.560897435897436, agent_steps_trained: 57344, env_steps_trained: 57344, entropy: 0.35889632888138295, learning_rate: 0.0010000000000000002\n",
      "iteration [29] => episode_reward_mean: -5.922297297297297, episode_len_mean: 6.922297297297297, agent_steps_trained: 59392, env_steps_trained: 59392, entropy: 0.3657961390912533, learning_rate: 0.0010000000000000002\n",
      "iteration [30] => episode_reward_mean: -5.585209003215434, episode_len_mean: 6.585209003215434, agent_steps_trained: 61440, env_steps_trained: 61440, entropy: 0.34659082628786564, learning_rate: 0.0010000000000000002\n"
     ]
    }
   ],
   "source": [
    "from ray.rllib.algorithms.ppo import PPOConfig\n",
    "from ray.tune.logger import pretty_print\n",
    "from ray.tune.registry import register_env\n",
    "from gymnasium.wrappers.time_limit import TimeLimit\n",
    "\n",
    "observations_memory = 3\n",
    "register_env(\"my_env\", lambda _: PointCoverageEnv({\"height\": 10, \"width\": 10, \"n_agents\": 1, \"n_targets\": 1, \"max_steps\": 100, \"observations_memory\": observations_memory}))\n",
    "\n",
    "train_batch_size = 2048\n",
    "sgd_minibatch_size = 256\n",
    "num_sgd_iter = 10\n",
    "trainings = 30\n",
    "\n",
    "total_env_steps = trainings*train_batch_size\n",
    "\n",
    "print(f\"number of different environment steps: {total_env_steps}\")\n",
    "\n",
    "\n",
    "algo = (\n",
    "    PPOConfig()\n",
    "    .training(gamma=0.99, \n",
    "              lr=0.001,\n",
    "              kl_coeff=0.5, \n",
    "              train_batch_size=train_batch_size, \n",
    "              sgd_minibatch_size=sgd_minibatch_size, \n",
    "              num_sgd_iter=num_sgd_iter)\n",
    "    .env_runners(num_env_runners=1)\n",
    "    .resources(num_gpus=0)\n",
    "    .environment(env=\"my_env\")\n",
    "    .build()\n",
    ")\n",
    "\n",
    "for i in range(trainings):\n",
    "    result = algo.train()\n",
    "    customResultPrint(result)\n",
    "    if i % 5 == 0:\n",
    "        checkpoint_dir = algo.save().checkpoint.path\n",
    "        print(f\"Checkpoint saved in directory {checkpoint_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2]\n",
      "______________________________________________________________________________________________________\n",
      "|                     x                                                                              |\n",
      "|                                                                                                    |\n",
      "|                                                                                                    |\n",
      "|                                                                                                    |\n",
      "|                                                                                                    |\n",
      "|                                                                                                    |\n",
      "|                                                                                                    |\n",
      "|                                                                                                    |\n",
      "|                                                 o                                                  |\n",
      "|                                                                                                    |\n",
      "|                                                                                                    |\n",
      "|                                                                                                    |\n",
      "|                                                                                                    |\n",
      "|                                                                                                    |\n",
      "|                                                                                                    |\n",
      "|                                                                                                    |\n",
      "|                                                                                                    |\n",
      "|                                                                                                    |\n",
      "|                                                                                                    |\n",
      "|                                                                                                    |\n",
      "â€¾â€¾â€¾â€¾â€¾â€¾â€¾â€¾â€¾â€¾â€¾â€¾â€¾â€¾â€¾â€¾â€¾â€¾â€¾â€¾â€¾â€¾â€¾â€¾â€¾â€¾â€¾â€¾â€¾â€¾â€¾â€¾â€¾â€¾â€¾â€¾â€¾â€¾â€¾â€¾â€¾â€¾â€¾â€¾â€¾â€¾â€¾â€¾â€¾â€¾â€¾â€¾â€¾â€¾â€¾â€¾â€¾â€¾â€¾â€¾â€¾â€¾â€¾â€¾â€¾â€¾â€¾â€¾â€¾â€¾â€¾â€¾â€¾â€¾â€¾â€¾â€¾â€¾â€¾â€¾â€¾â€¾â€¾â€¾â€¾â€¾â€¾â€¾â€¾â€¾â€¾â€¾â€¾â€¾â€¾â€¾â€¾â€¾â€¾â€¾â€¾â€¾\n",
      "{'agent-0': array([0.49, 0.4 , 0.21, 0.  , 0.49, 0.45, 0.21, 0.  , 0.49, 0.5 , 0.21,\n",
      "       0.  ], dtype=float32)}\n",
      "{'agent-0': -1}\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[42], line 21\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28mprint\u001b[39m(obs)\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28mprint\u001b[39m(reward)\n\u001b[0;32m---> 21\u001b[0m \u001b[43mtime\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0.5\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m terminated[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__all__\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;129;01mor\u001b[39;00m truncated[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__all__\u001b[39m\u001b[38;5;124m'\u001b[39m]:\n\u001b[1;32m     24\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from IPython.display import clear_output\n",
    "import time\n",
    "import torch\n",
    "from gymnasium.spaces.utils import flatten\n",
    "env = PointCoverageEnv({\"height\": 20, \"width\": 100, \"n_agents\": 1, \"n_targets\": 1, \"observations_memory\": observations_memory})\n",
    "obs_space = env.observation_space\n",
    "obs, _ = env.reset()\n",
    "print(obs)\n",
    "env.render()\n",
    "\n",
    "for i in range(100):\n",
    "    actions = algo.compute_actions({agent: o for agent, o in obs.items()})\n",
    "    print(actions, \"\\n\")\n",
    "    \n",
    "    obs, reward, terminated, truncated, info = env.step(actions)\n",
    "    clear_output()\n",
    "    print(f\"[{i}]\")\n",
    "    env.render()\n",
    "    print(obs)\n",
    "    print(reward)\n",
    "    time.sleep(0.5)\n",
    "\n",
    "    if terminated['__all__'] or truncated['__all__']:\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Two Agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nicolo/anaconda3/envs/rayEnv/lib/python3.11/site-packages/ray/rllib/algorithms/algorithm.py:521: RayDeprecationWarning: This API is deprecated and may be removed in future Ray releases. You could suppress this warning by setting env variable PYTHONWARNINGS=\"ignore::DeprecationWarning\"\n",
      "`UnifiedLogger` will be removed in Ray 2.7.\n",
      "  return UnifiedLogger(config, logdir, loggers=None)\n",
      "/home/nicolo/anaconda3/envs/rayEnv/lib/python3.11/site-packages/ray/tune/logger/unified.py:53: RayDeprecationWarning: This API is deprecated and may be removed in future Ray releases. You could suppress this warning by setting env variable PYTHONWARNINGS=\"ignore::DeprecationWarning\"\n",
      "The `JsonLogger interface is deprecated in favor of the `ray.tune.json.JsonLoggerCallback` interface and will be removed in Ray 2.7.\n",
      "  self._loggers.append(cls(self.config, self.logdir, self.trial))\n",
      "/home/nicolo/anaconda3/envs/rayEnv/lib/python3.11/site-packages/ray/tune/logger/unified.py:53: RayDeprecationWarning: This API is deprecated and may be removed in future Ray releases. You could suppress this warning by setting env variable PYTHONWARNINGS=\"ignore::DeprecationWarning\"\n",
      "The `CSVLogger interface is deprecated in favor of the `ray.tune.csv.CSVLoggerCallback` interface and will be removed in Ray 2.7.\n",
      "  self._loggers.append(cls(self.config, self.logdir, self.trial))\n",
      "/home/nicolo/anaconda3/envs/rayEnv/lib/python3.11/site-packages/ray/tune/logger/unified.py:53: RayDeprecationWarning: This API is deprecated and may be removed in future Ray releases. You could suppress this warning by setting env variable PYTHONWARNINGS=\"ignore::DeprecationWarning\"\n",
      "The `TBXLogger interface is deprecated in favor of the `ray.tune.tensorboardx.TBXLoggerCallback` interface and will be removed in Ray 2.7.\n",
      "  self._loggers.append(cls(self.config, self.logdir, self.trial))\n",
      "2024-05-16 15:25:29,272\tWARNING deprecation.py:50 -- DeprecationWarning: `WorkerSet(num_workers=... OR local_worker=...)` has been deprecated. Use `EnvRunnerGroup(num_env_runners=... AND local_env_runner=...)` instead. This will raise an error in the future!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of different environment steps: 102400\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-16 15:25:36,209\tWARNING util.py:61 -- Install gputil for GPU system monitoring.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration [1] => episode_reward_mean: -29.33783783783784, episode_len_mean: 27.635135135135137, agent_steps_trained: 2939, env_steps_trained: 2048, entropy: 1.6092265833507884, learning_rate: 0.004999999999999999\n",
      "Checkpoint saved in directory /tmp/tmpaoix4pvi\n",
      "iteration [2] => episode_reward_mean: -28.75, episode_len_mean: 26.81, agent_steps_trained: 5918, env_steps_trained: 4096, entropy: 1.6015533913265576, learning_rate: 0.0010000000000000005\n",
      "iteration [3] => episode_reward_mean: -26.17, episode_len_mean: 26.12, agent_steps_trained: 8782, env_steps_trained: 6144, entropy: 1.5955791971900246, learning_rate: 0.0010000000000000005\n",
      "iteration [4] => episode_reward_mean: -22.33, episode_len_mean: 24.54, agent_steps_trained: 11547, env_steps_trained: 8192, entropy: 1.5917757105827333, learning_rate: 0.0010000000000000002\n",
      "iteration [5] => episode_reward_mean: -23.24, episode_len_mean: 24.13, agent_steps_trained: 14487, env_steps_trained: 10240, entropy: 1.5833317550745878, learning_rate: 0.0010000000000000005\n",
      "iteration [6] => episode_reward_mean: -21.88, episode_len_mean: 24.59, agent_steps_trained: 17267, env_steps_trained: 12288, entropy: 1.5780158996582032, learning_rate: 0.0010000000000000002\n",
      "Checkpoint saved in directory /tmp/tmpbcw96ekn\n",
      "iteration [7] => episode_reward_mean: -17.42, episode_len_mean: 21.29, agent_steps_trained: 20072, env_steps_trained: 14336, entropy: 1.5740169847011567, learning_rate: 0.0010000000000000002\n",
      "iteration [8] => episode_reward_mean: -16.52, episode_len_mean: 21.57, agent_steps_trained: 22856, env_steps_trained: 16384, entropy: 1.5621886849403381, learning_rate: 0.0010000000000000002\n",
      "iteration [9] => episode_reward_mean: -20.09, episode_len_mean: 22.76, agent_steps_trained: 25714, env_steps_trained: 18432, entropy: 1.5666580698706887, learning_rate: 0.0010000000000000005\n",
      "iteration [10] => episode_reward_mean: -16.99, episode_len_mean: 21.64, agent_steps_trained: 28418, env_steps_trained: 20480, entropy: 1.556549118757248, learning_rate: 0.0010000000000000002\n",
      "iteration [11] => episode_reward_mean: -11.88785046728972, episode_len_mean: 19.252336448598133, agent_steps_trained: 31076, env_steps_trained: 22528, entropy: 1.536821072101593, learning_rate: 0.0010000000000000002\n",
      "Checkpoint saved in directory /tmp/tmp9mmxyvo3\n",
      "iteration [12] => episode_reward_mean: -13.74766355140187, episode_len_mean: 19.037383177570092, agent_steps_trained: 33886, env_steps_trained: 24576, entropy: 1.5268812656402588, learning_rate: 0.0010000000000000002\n",
      "iteration [13] => episode_reward_mean: -11.365217391304348, episode_len_mean: 17.782608695652176, agent_steps_trained: 36586, env_steps_trained: 26624, entropy: 1.5100736975669862, learning_rate: 0.0010000000000000002\n",
      "iteration [14] => episode_reward_mean: -10.991935483870968, episode_len_mean: 16.588709677419356, agent_steps_trained: 39417, env_steps_trained: 28672, entropy: 1.4867044297131624, learning_rate: 0.0010000000000000005\n",
      "iteration [15] => episode_reward_mean: -8.295454545454545, episode_len_mean: 15.424242424242424, agent_steps_trained: 42217, env_steps_trained: 30720, entropy: 1.4522847604751588, learning_rate: 0.0010000000000000002\n",
      "iteration [16] => episode_reward_mean: -5.104575163398692, episode_len_mean: 13.359477124183007, agent_steps_trained: 44987, env_steps_trained: 32768, entropy: 1.4053384792804717, learning_rate: 0.0010000000000000002\n",
      "Checkpoint saved in directory /tmp/tmp8x3mdzl_\n",
      "iteration [17] => episode_reward_mean: -4.735849056603773, episode_len_mean: 13.0125786163522, agent_steps_trained: 47807, env_steps_trained: 34816, entropy: 1.396926719492132, learning_rate: 0.0010000000000000005\n",
      "iteration [18] => episode_reward_mean: -3.5865921787709496, episode_len_mean: 11.452513966480447, agent_steps_trained: 50620, env_steps_trained: 36864, entropy: 1.3626119101047516, learning_rate: 0.0010000000000000002\n",
      "iteration [19] => episode_reward_mean: -3.448087431693989, episode_len_mean: 11.207650273224044, agent_steps_trained: 53550, env_steps_trained: 38912, entropy: 1.3159431891007858, learning_rate: 0.0010000000000000005\n",
      "iteration [20] => episode_reward_mean: -0.28125, episode_len_mean: 9.129464285714286, agent_steps_trained: 56504, env_steps_trained: 40960, entropy: 1.261268459666859, learning_rate: 0.0010000000000000005\n",
      "iteration [21] => episode_reward_mean: -0.7857142857142857, episode_len_mean: 9.116071428571429, agent_steps_trained: 59490, env_steps_trained: 43008, entropy: 1.2140394405885175, learning_rate: 0.0010000000000000005\n",
      "Checkpoint saved in directory /tmp/tmpvzbxzyh9\n",
      "iteration [22] => episode_reward_mean: 0.29454545454545455, episode_len_mean: 7.487272727272727, agent_steps_trained: 62617, env_steps_trained: 45056, entropy: 1.1374920189380646, learning_rate: 0.0010000000000000005\n",
      "iteration [23] => episode_reward_mean: 0.68, episode_len_mean: 7.4363636363636365, agent_steps_trained: 65807, env_steps_trained: 47104, entropy: 1.1342063973347345, learning_rate: 0.0010000000000000005\n",
      "iteration [24] => episode_reward_mean: 1.727891156462585, episode_len_mean: 6.9727891156462585, agent_steps_trained: 68958, env_steps_trained: 49152, entropy: 1.085968118906021, learning_rate: 0.0010000000000000005\n",
      "iteration [25] => episode_reward_mean: 1.7611464968152866, episode_len_mean: 6.519108280254777, agent_steps_trained: 72163, env_steps_trained: 51200, entropy: 1.0596569071213404, learning_rate: 0.0010000000000000005\n",
      "iteration [26] => episode_reward_mean: 1.6166666666666667, episode_len_mean: 6.816666666666666, agent_steps_trained: 75261, env_steps_trained: 53248, entropy: 1.0309517830610275, learning_rate: 0.0010000000000000005\n",
      "Checkpoint saved in directory /tmp/tmp4po01zuv\n",
      "iteration [27] => episode_reward_mean: 2.111455108359133, episode_len_mean: 6.3467492260061915, agent_steps_trained: 78493, env_steps_trained: 55296, entropy: 1.0250632847348848, learning_rate: 0.0010000000000000005\n",
      "iteration [28] => episode_reward_mean: 2.521604938271605, episode_len_mean: 6.330246913580247, agent_steps_trained: 81688, env_steps_trained: 57344, entropy: 0.9901097938418388, learning_rate: 0.0010000000000000005\n",
      "iteration [29] => episode_reward_mean: 1.7763975155279503, episode_len_mean: 6.357142857142857, agent_steps_trained: 84881, env_steps_trained: 59392, entropy: 0.9818476423621177, learning_rate: 0.0010000000000000005\n",
      "iteration [30] => episode_reward_mean: 2.7904191616766467, episode_len_mean: 6.11377245508982, agent_steps_trained: 88106, env_steps_trained: 61440, entropy: 0.9846598287423451, learning_rate: 0.0010000000000000005\n",
      "iteration [31] => episode_reward_mean: 2.434250764525994, episode_len_mean: 6.2629969418960245, agent_steps_trained: 91347, env_steps_trained: 63488, entropy: 0.9436396335562071, learning_rate: 0.0010000000000000005\n",
      "Checkpoint saved in directory /tmp/tmpta9kxh36\n",
      "iteration [32] => episode_reward_mean: 1.6910828025477707, episode_len_mean: 6.531847133757962, agent_steps_trained: 94544, env_steps_trained: 65536, entropy: 0.9298240800698598, learning_rate: 0.0010000000000000005\n",
      "iteration [33] => episode_reward_mean: 2.0555555555555554, episode_len_mean: 6.699346405228758, agent_steps_trained: 97757, env_steps_trained: 67584, entropy: 0.93309941192468, learning_rate: 0.0010000000000000005\n",
      "iteration [34] => episode_reward_mean: 2.4846625766871164, episode_len_mean: 6.279141104294479, agent_steps_trained: 100948, env_steps_trained: 69632, entropy: 0.8986850668986638, learning_rate: 0.0010000000000000005\n",
      "iteration [35] => episode_reward_mean: 2.364705882352941, episode_len_mean: 6.0323529411764705, agent_steps_trained: 104231, env_steps_trained: 71680, entropy: 0.8960331285993258, learning_rate: 0.0010000000000000005\n",
      "iteration [36] => episode_reward_mean: 1.674772036474164, episode_len_mean: 6.224924012158055, agent_steps_trained: 107501, env_steps_trained: 73728, entropy: 0.8659887825449307, learning_rate: 0.0010000000000000005\n",
      "Checkpoint saved in directory /tmp/tmpn5acmi27\n",
      "iteration [37] => episode_reward_mean: 1.8328358208955224, episode_len_mean: 6.08955223880597, agent_steps_trained: 110871, env_steps_trained: 75776, entropy: 0.8756794530611772, learning_rate: 0.001\n",
      "iteration [38] => episode_reward_mean: 2.082317073170732, episode_len_mean: 6.2682926829268295, agent_steps_trained: 114188, env_steps_trained: 77824, entropy: 0.8773503666122754, learning_rate: 0.0010000000000000005\n",
      "iteration [39] => episode_reward_mean: 2.5611940298507463, episode_len_mean: 6.107462686567164, agent_steps_trained: 117379, env_steps_trained: 79872, entropy: 0.8195353319247564, learning_rate: 0.0010000000000000005\n",
      "iteration [40] => episode_reward_mean: 1.9908256880733946, episode_len_mean: 6.259938837920489, agent_steps_trained: 120681, env_steps_trained: 81920, entropy: 0.849241844813029, learning_rate: 0.0010000000000000005\n",
      "iteration [41] => episode_reward_mean: 1.3289473684210527, episode_len_mean: 6.746710526315789, agent_steps_trained: 124002, env_steps_trained: 83968, entropy: 0.8259192859133084, learning_rate: 0.0010000000000000005\n",
      "Checkpoint saved in directory /tmp/tmph3rpo1u3\n",
      "iteration [42] => episode_reward_mean: 2.2236024844720497, episode_len_mean: 6.350931677018633, agent_steps_trained: 127337, env_steps_trained: 86016, entropy: 0.8244692311837123, learning_rate: 0.001\n",
      "iteration [43] => episode_reward_mean: 2.4643962848297214, episode_len_mean: 6.340557275541796, agent_steps_trained: 130675, env_steps_trained: 88064, entropy: 0.8157124299269456, learning_rate: 0.001\n",
      "iteration [44] => episode_reward_mean: 1.856269113149847, episode_len_mean: 6.26605504587156, agent_steps_trained: 134036, env_steps_trained: 90112, entropy: 0.7937247482629922, learning_rate: 0.001\n",
      "iteration [45] => episode_reward_mean: 2.366863905325444, episode_len_mean: 6.059171597633136, agent_steps_trained: 137353, env_steps_trained: 92160, entropy: 0.8025060286124547, learning_rate: 0.0010000000000000005\n",
      "iteration [46] => episode_reward_mean: 2.933333333333333, episode_len_mean: 6.196969696969697, agent_steps_trained: 140720, env_steps_trained: 94208, entropy: 0.8113101853774144, learning_rate: 0.001\n",
      "Checkpoint saved in directory /tmp/tmpuok_3bzb\n",
      "iteration [47] => episode_reward_mean: 2.9855907780979827, episode_len_mean: 5.916426512968299, agent_steps_trained: 143986, env_steps_trained: 96256, entropy: 0.7922235856453578, learning_rate: 0.0010000000000000005\n",
      "iteration [48] => episode_reward_mean: 2.121875, episode_len_mean: 6.4, agent_steps_trained: 147260, env_steps_trained: 98304, entropy: 0.7812144870559374, learning_rate: 0.0010000000000000005\n",
      "iteration [49] => episode_reward_mean: 1.4801223241590213, episode_len_mean: 6.250764525993884, agent_steps_trained: 150593, env_steps_trained: 100352, entropy: 0.7865998497376075, learning_rate: 0.001\n",
      "iteration [50] => episode_reward_mean: 2.2698412698412698, episode_len_mean: 6.514285714285714, agent_steps_trained: 153881, env_steps_trained: 102400, entropy: 0.7847823843359947, learning_rate: 0.0010000000000000005\n"
     ]
    }
   ],
   "source": [
    "from ray.rllib.algorithms.ppo import PPOConfig\n",
    "from ray.tune.logger import pretty_print\n",
    "from ray.tune.registry import register_env\n",
    "from gymnasium.wrappers.time_limit import TimeLimit\n",
    "\n",
    "\n",
    "register_env(\"my_env\", lambda _: PointCoverageEnv({\"height\": 5, \"width\": 5, \"n_agents\": 2, \"n_targets\": 2, \"max_steps\": 30}))\n",
    "\n",
    "train_batch_size = 2048\n",
    "sgd_minibatch_size = 256\n",
    "num_sgd_iter = 10\n",
    "trainings = 50\n",
    "\n",
    "total_env_steps = trainings*train_batch_size\n",
    "\n",
    "print(f\"number of different environment steps: {total_env_steps}\")\n",
    "\n",
    "\n",
    "algo2 = (\n",
    "    PPOConfig()\n",
    "    .training(gamma=0.99, \n",
    "              #lr=0.001,\n",
    "              lr_schedule=[\n",
    "                [0, 0.005],  \n",
    "                [1000, 0.001],  \n",
    "                [10000, 0.001],  \n",
    "              ],\n",
    "              kl_coeff=0.5, \n",
    "              train_batch_size=train_batch_size, \n",
    "              sgd_minibatch_size=sgd_minibatch_size, \n",
    "              num_sgd_iter=num_sgd_iter, \n",
    "              entropy_coeff_schedule = [\n",
    "                [0, 0.8],  # Start with relatively high entropy coefficient\n",
    "                [40480, 0],  # Gradually decrease entropy coefficient over 10,000 iterations\n",
    "              ])\n",
    "    .env_runners(num_env_runners=1)\n",
    "    .resources(num_gpus=0)\n",
    "    .environment(env=\"my_env\")\n",
    "    .build()\n",
    ")\n",
    "\n",
    "for i in range(trainings):\n",
    "    result = algo2.train()\n",
    "    customResultPrint(result)\n",
    "    if i % 5 == 0:\n",
    "        checkpoint_dir = algo2.save().checkpoint.path\n",
    "        print(f\"Checkpoint saved in directory {checkpoint_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[54]\n",
      "________________________________\n",
      "|                              |\n",
      "|                              |\n",
      "|                              |\n",
      "|                     *        |\n",
      "|                              |\n",
      "|                              |\n",
      "|                              |\n",
      "|                              |\n",
      "|  *                           |\n",
      "|                              |\n",
      "â€¾â€¾â€¾â€¾â€¾â€¾â€¾â€¾â€¾â€¾â€¾â€¾â€¾â€¾â€¾â€¾â€¾â€¾â€¾â€¾â€¾â€¾â€¾â€¾â€¾â€¾â€¾â€¾â€¾â€¾â€¾â€¾\n",
      "{}\n",
      "{}\n"
     ]
    }
   ],
   "source": [
    "from IPython.display import clear_output\n",
    "import time\n",
    "\n",
    "env = PointCoverageEnv({\"height\": 10, \"width\": 30, \"n_agents\": 2, \"n_targets\": 2})\n",
    "obs, _ = env.reset()\n",
    "env.render()\n",
    "\n",
    "for i in range(100):\n",
    "    actions = algo2.compute_actions(obs)\n",
    "    print(actions, \"\\n\")\n",
    "    obs, reward, terminated, truncated, info = env.step(actions)\n",
    "    clear_output()\n",
    "    print(f\"[{i}]\")\n",
    "    env.render()\n",
    "    print(obs)\n",
    "    print(reward)\n",
    "    time.sleep(0.5)\n",
    "\n",
    "    if terminated['__all__'] or truncated['__all__']:\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Three Agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-16 15:37:09,231\tWARNING deprecation.py:50 -- DeprecationWarning: `WorkerSet(num_workers=... OR local_worker=...)` has been deprecated. Use `EnvRunnerGroup(num_env_runners=... AND local_env_runner=...)` instead. This will raise an error in the future!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of different environment steps: 102400\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-16 15:37:16,447\tWARNING util.py:61 -- Install gputil for GPU system monitoring.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration [1] => episode_reward_mean: -18.105263157894736, episode_len_mean: 26.92105263157895, agent_steps_trained: 3826, env_steps_trained: 2048, entropy: 1.609091877085822, learning_rate: 0.005\n",
      "Checkpoint saved in directory /tmp/tmpxadcgia4\n",
      "iteration [2] => episode_reward_mean: -18.36, episode_len_mean: 27.42, agent_steps_trained: 7508, env_steps_trained: 4096, entropy: 1.603918524299349, learning_rate: 0.001\n",
      "iteration [3] => episode_reward_mean: -13.75, episode_len_mean: 27.31, agent_steps_trained: 10915, env_steps_trained: 6144, entropy: 1.595703953046065, learning_rate: 0.001\n",
      "iteration [4] => episode_reward_mean: -11.53, episode_len_mean: 26.55, agent_steps_trained: 14476, env_steps_trained: 8192, entropy: 1.5948491188196035, learning_rate: 0.001\n",
      "iteration [5] => episode_reward_mean: -11.56, episode_len_mean: 26.83, agent_steps_trained: 18006, env_steps_trained: 10240, entropy: 1.5882749713384188, learning_rate: 0.001\n",
      "iteration [6] => episode_reward_mean: -12.65, episode_len_mean: 26.72, agent_steps_trained: 21569, env_steps_trained: 12288, entropy: 1.5847441049722524, learning_rate: 0.001\n",
      "Checkpoint saved in directory /tmp/tmp5n6y4f0g\n",
      "iteration [7] => episode_reward_mean: -7.28, episode_len_mean: 25.18, agent_steps_trained: 25052, env_steps_trained: 14336, entropy: 1.5751991326992327, learning_rate: 0.001\n",
      "iteration [8] => episode_reward_mean: -4.96, episode_len_mean: 23.55, agent_steps_trained: 28497, env_steps_trained: 16384, entropy: 1.5711096974519583, learning_rate: 0.001\n",
      "iteration [9] => episode_reward_mean: -7.05, episode_len_mean: 24.23, agent_steps_trained: 32077, env_steps_trained: 18432, entropy: 1.5599557207180903, learning_rate: 0.001\n",
      "iteration [10] => episode_reward_mean: -6.78, episode_len_mean: 23.66, agent_steps_trained: 35518, env_steps_trained: 20480, entropy: 1.5410287261009217, learning_rate: 0.001\n",
      "iteration [11] => episode_reward_mean: -5.97, episode_len_mean: 23.4, agent_steps_trained: 38975, env_steps_trained: 22528, entropy: 1.5350211079304035, learning_rate: 0.001\n",
      "Checkpoint saved in directory /tmp/tmpg6k86ha6\n",
      "iteration [12] => episode_reward_mean: -2.05, episode_len_mean: 23.08, agent_steps_trained: 42201, env_steps_trained: 24576, entropy: 1.516617618004481, learning_rate: 0.0010000000000000005\n",
      "iteration [13] => episode_reward_mean: -1.69, episode_len_mean: 22.05, agent_steps_trained: 45604, env_steps_trained: 26624, entropy: 1.5038254921252912, learning_rate: 0.001\n",
      "iteration [14] => episode_reward_mean: -1.54, episode_len_mean: 22.39, agent_steps_trained: 49023, env_steps_trained: 28672, entropy: 1.4954009927236116, learning_rate: 0.001\n",
      "iteration [15] => episode_reward_mean: 1.5096153846153846, episode_len_mean: 19.64423076923077, agent_steps_trained: 52426, env_steps_trained: 30720, entropy: 1.4482355365386377, learning_rate: 0.001\n",
      "iteration [16] => episode_reward_mean: 1.5, episode_len_mean: 20.41, agent_steps_trained: 55893, env_steps_trained: 32768, entropy: 1.4323068407865671, learning_rate: 0.001\n",
      "Checkpoint saved in directory /tmp/tmpfzvr9up8\n",
      "iteration [17] => episode_reward_mean: -1.62, episode_len_mean: 21.26, agent_steps_trained: 59292, env_steps_trained: 34816, entropy: 1.4145525299585782, learning_rate: 0.001\n",
      "iteration [18] => episode_reward_mean: 2.018018018018018, episode_len_mean: 18.63963963963964, agent_steps_trained: 62892, env_steps_trained: 36864, entropy: 1.3837242713996343, learning_rate: 0.001\n",
      "iteration [19] => episode_reward_mean: 5.930434782608696, episode_len_mean: 17.817391304347826, agent_steps_trained: 66256, env_steps_trained: 38912, entropy: 1.3427495525433466, learning_rate: 0.001\n",
      "iteration [20] => episode_reward_mean: 2.088235294117647, episode_len_mean: 20.098039215686274, agent_steps_trained: 69699, env_steps_trained: 40960, entropy: 1.3669304214991056, learning_rate: 0.001\n",
      "iteration [21] => episode_reward_mean: 1.3177570093457944, episode_len_mean: 19.074766355140188, agent_steps_trained: 73173, env_steps_trained: 43008, entropy: 1.353693867646731, learning_rate: 0.001\n",
      "Checkpoint saved in directory /tmp/tmp368k_4qq\n",
      "iteration [22] => episode_reward_mean: 3.4864864864864864, episode_len_mean: 18.405405405405407, agent_steps_trained: 76681, env_steps_trained: 45056, entropy: 1.3415880450835596, learning_rate: 0.001\n",
      "iteration [23] => episode_reward_mean: 4.672, episode_len_mean: 16.392, agent_steps_trained: 80408, env_steps_trained: 47104, entropy: 1.3384717438902174, learning_rate: 0.001\n",
      "iteration [24] => episode_reward_mean: 3.5130434782608697, episode_len_mean: 17.756521739130434, agent_steps_trained: 83908, env_steps_trained: 49152, entropy: 1.298675979100741, learning_rate: 0.001\n",
      "iteration [25] => episode_reward_mean: 9.030075187969924, episode_len_mean: 15.481203007518797, agent_steps_trained: 87374, env_steps_trained: 51200, entropy: 1.2948517698508042, learning_rate: 0.001\n",
      "iteration [26] => episode_reward_mean: 3.5213675213675213, episode_len_mean: 17.53846153846154, agent_steps_trained: 90856, env_steps_trained: 53248, entropy: 1.297991864497845, learning_rate: 0.001\n",
      "Checkpoint saved in directory /tmp/tmpmyjnpw9a\n",
      "iteration [27] => episode_reward_mean: 5.551724137931035, episode_len_mean: 17.620689655172413, agent_steps_trained: 94397, env_steps_trained: 55296, entropy: 1.3107950018002437, learning_rate: 0.001\n",
      "iteration [28] => episode_reward_mean: 7.7109375, episode_len_mean: 15.9453125, agent_steps_trained: 97881, env_steps_trained: 57344, entropy: 1.288069210602687, learning_rate: 0.001\n",
      "iteration [29] => episode_reward_mean: 6.4609375, episode_len_mean: 16.046875, agent_steps_trained: 101436, env_steps_trained: 59392, entropy: 1.3011834364670973, learning_rate: 0.001\n",
      "iteration [30] => episode_reward_mean: 3.8482142857142856, episode_len_mean: 18.142857142857142, agent_steps_trained: 104867, env_steps_trained: 61440, entropy: 1.2857761905743526, learning_rate: 0.001\n",
      "iteration [31] => episode_reward_mean: 8.088888888888889, episode_len_mean: 15.348148148148148, agent_steps_trained: 108468, env_steps_trained: 63488, entropy: 1.2534972088677543, learning_rate: 0.001\n",
      "Checkpoint saved in directory /tmp/tmp1xjiqlns\n",
      "iteration [32] => episode_reward_mean: 7.511278195488722, episode_len_mean: 15.38345864661654, agent_steps_trained: 111946, env_steps_trained: 65536, entropy: 1.2504174342522254, learning_rate: 0.001\n",
      "iteration [33] => episode_reward_mean: 7.503759398496241, episode_len_mean: 15.263157894736842, agent_steps_trained: 115511, env_steps_trained: 67584, entropy: 1.2323498221544118, learning_rate: 0.001\n",
      "iteration [34] => episode_reward_mean: 6.428571428571429, episode_len_mean: 16.357142857142858, agent_steps_trained: 118921, env_steps_trained: 69632, entropy: 1.2288158783545862, learning_rate: 0.001\n",
      "iteration [35] => episode_reward_mean: 8.338235294117647, episode_len_mean: 14.904411764705882, agent_steps_trained: 122655, env_steps_trained: 71680, entropy: 1.2182922857148306, learning_rate: 0.001\n",
      "iteration [36] => episode_reward_mean: 9.0, episode_len_mean: 15.22962962962963, agent_steps_trained: 126212, env_steps_trained: 73728, entropy: 1.2283447751632104, learning_rate: 0.001\n",
      "Checkpoint saved in directory /tmp/tmpaqk52zfm\n",
      "iteration [37] => episode_reward_mean: 11.875862068965517, episode_len_mean: 14.2, agent_steps_trained: 129745, env_steps_trained: 75776, entropy: 1.1843390639011677, learning_rate: 0.001\n",
      "iteration [38] => episode_reward_mean: 8.462686567164178, episode_len_mean: 15.313432835820896, agent_steps_trained: 133256, env_steps_trained: 77824, entropy: 1.19593833226424, learning_rate: 0.001\n",
      "iteration [39] => episode_reward_mean: 11.324503311258278, episode_len_mean: 13.596026490066226, agent_steps_trained: 136836, env_steps_trained: 79872, entropy: 1.1926280901982234, learning_rate: 0.001\n",
      "iteration [40] => episode_reward_mean: 12.175, episode_len_mean: 12.61875, agent_steps_trained: 140536, env_steps_trained: 81920, entropy: 1.1804586972509112, learning_rate: 0.001\n",
      "iteration [41] => episode_reward_mean: 12.35483870967742, episode_len_mean: 13.361290322580645, agent_steps_trained: 144121, env_steps_trained: 83968, entropy: 1.1589094749518802, learning_rate: 0.001\n",
      "Checkpoint saved in directory /tmp/tmpym3vutnq\n",
      "iteration [42] => episode_reward_mean: 9.977941176470589, episode_len_mean: 14.963235294117647, agent_steps_trained: 147543, env_steps_trained: 86016, entropy: 1.1730037707548875, learning_rate: 0.001\n",
      "iteration [43] => episode_reward_mean: 14.107142857142858, episode_len_mean: 12.244047619047619, agent_steps_trained: 151048, env_steps_trained: 88064, entropy: 1.1505446195602418, learning_rate: 0.001\n",
      "iteration [44] => episode_reward_mean: 13.564935064935066, episode_len_mean: 13.292207792207792, agent_steps_trained: 154622, env_steps_trained: 90112, entropy: 1.1569711804389953, learning_rate: 0.001\n",
      "iteration [45] => episode_reward_mean: 11.69811320754717, episode_len_mean: 12.924528301886792, agent_steps_trained: 158272, env_steps_trained: 92160, entropy: 1.1446772038936615, learning_rate: 0.001\n",
      "iteration [46] => episode_reward_mean: 13.75925925925926, episode_len_mean: 12.62962962962963, agent_steps_trained: 161831, env_steps_trained: 94208, entropy: 1.119362914103728, learning_rate: 0.001\n",
      "Checkpoint saved in directory /tmp/tmpn0y3edjc\n",
      "iteration [47] => episode_reward_mean: 12.974025974025974, episode_len_mean: 13.285714285714286, agent_steps_trained: 165434, env_steps_trained: 96256, entropy: 1.1366191055093493, learning_rate: 0.001\n",
      "iteration [48] => episode_reward_mean: 11.993506493506494, episode_len_mean: 13.168831168831169, agent_steps_trained: 168877, env_steps_trained: 98304, entropy: 1.152009488069094, learning_rate: 0.001\n",
      "iteration [49] => episode_reward_mean: 13.38888888888889, episode_len_mean: 12.75925925925926, agent_steps_trained: 172426, env_steps_trained: 100352, entropy: 1.1355909989430355, learning_rate: 0.001\n",
      "iteration [50] => episode_reward_mean: 13.844155844155845, episode_len_mean: 13.311688311688311, agent_steps_trained: 175958, env_steps_trained: 102400, entropy: 1.1074482697706955, learning_rate: 0.001\n"
     ]
    }
   ],
   "source": [
    "from ray.rllib.algorithms.ppo import PPOConfig\n",
    "from ray.tune.logger import pretty_print\n",
    "from ray.tune.registry import register_env\n",
    "from gymnasium.wrappers.time_limit import TimeLimit\n",
    "\n",
    "\n",
    "register_env(\"my_env\", lambda _: PointCoverageEnv({\"height\": 5, \"width\": 5, \"n_agents\": 3, \"n_targets\": 3, \"max_steps\": 30}))\n",
    "\n",
    "train_batch_size = 2048\n",
    "sgd_minibatch_size = 256\n",
    "num_sgd_iter = 10\n",
    "trainings = 50\n",
    "\n",
    "total_env_steps = trainings*train_batch_size\n",
    "\n",
    "print(f\"number of different environment steps: {total_env_steps}\")\n",
    "\n",
    "\n",
    "algo3 = (\n",
    "    PPOConfig()\n",
    "    .training(gamma=0.99, \n",
    "              #lr=0.001,\n",
    "              lr_schedule=[\n",
    "                [0, 0.005],  \n",
    "                [1000, 0.001],  \n",
    "                [10000, 0.001],  \n",
    "              ],\n",
    "              kl_coeff=0.5, \n",
    "              train_batch_size=train_batch_size, \n",
    "              sgd_minibatch_size=sgd_minibatch_size, \n",
    "              num_sgd_iter=num_sgd_iter, \n",
    "              entropy_coeff_schedule = [\n",
    "                [0, 0.8],  # Start with relatively high entropy coefficient\n",
    "                [40480, 0],  # Gradually decrease entropy coefficient over 10,000 iterations\n",
    "              ])\n",
    "    .env_runners(num_env_runners=1)\n",
    "    .resources(num_gpus=0)\n",
    "    .environment(env=\"my_env\")\n",
    "    .build()\n",
    ")\n",
    "\n",
    "for i in range(trainings):\n",
    "    result = algo3.train()\n",
    "    customResultPrint(result)\n",
    "    if i % 5 == 0:\n",
    "        checkpoint_dir = algo3.save().checkpoint.path\n",
    "        print(f\"Checkpoint saved in directory {checkpoint_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[99]\n",
      "____________\n",
      "|   o      |\n",
      "|          |\n",
      "|      x   |\n",
      "| x        |\n",
      "|          |\n",
      "|          |\n",
      "|        o |\n",
      "|          |\n",
      "|      o   |\n",
      "|         x|\n",
      "â€¾â€¾â€¾â€¾â€¾â€¾â€¾â€¾â€¾â€¾â€¾â€¾\n",
      "{'agent-0': array([0.8, 0.6, 0.3, 0. , 0.6, 0.8, 0.1, 0.3, 0.6, 0.2, 0.9, 0.9],\n",
      "      dtype=float32), 'agent-1': array([0.6, 0.8, 0.3, 0. , 0.8, 0.6, 0.1, 0.3, 0.6, 0.2, 0.9, 0.9],\n",
      "      dtype=float32), 'agent-2': array([0.6, 0.8, 0.8, 0.6, 0.3, 0. , 0.1, 0.3, 0.6, 0.2, 0.9, 0.9],\n",
      "      dtype=float32)}\n",
      "{'agent-0': -1, 'agent-1': -1, 'agent-2': -1}\n"
     ]
    }
   ],
   "source": [
    "from IPython.display import clear_output\n",
    "import time\n",
    "\n",
    "env = PointCoverageEnv({\"height\": 10, \"width\": 10, \"n_agents\": 3, \"n_targets\": 3})\n",
    "obs, _ = env.reset()\n",
    "env.render()\n",
    "\n",
    "for i in range(100):\n",
    "    actions = algo3.compute_actions(obs)\n",
    "    print(actions, \"\\n\")\n",
    "    obs, reward, terminated, truncated, info = env.step(actions)\n",
    "    clear_output()\n",
    "    print(f\"[{i}]\")\n",
    "    env.render()\n",
    "    print(obs)\n",
    "    print(reward)\n",
    "    time.sleep(0.5)\n",
    "\n",
    "    if terminated['__all__'] or truncated['__all__']:\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tianshou"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0.0\n"
     ]
    }
   ],
   "source": [
    "import tianshou\n",
    "import torch\n",
    "from tianshou.highlevel.config import SamplingConfig\n",
    "from tianshou.highlevel.env import (\n",
    "    EnvFactoryRegistered,\n",
    "    VectorEnvType,\n",
    ")\n",
    "from tianshou.highlevel.experiment import DQNExperimentBuilder, ExperimentConfig,  PPOExperimentBuilder\n",
    "from tianshou.highlevel.params.policy_params import DQNParams\n",
    "from tianshou.highlevel.trainer import (\n",
    "    EpochTestCallbackDQNSetEps,\n",
    "    EpochTrainCallbackDQNSetEps,\n",
    "    EpochStopCallbackRewardThreshold\n",
    ")\n",
    "from tianshou.env import DummyVectorEnv\n",
    "from tianshou.utils.net.discrete import (Actor, Net, Critic)\n",
    "from tianshou.utils.net.common import ActorCritic\n",
    "from tianshou.policy.base import BasePolicy\n",
    "from tianshou.policy.modelfree.ppo import PPOPolicy\n",
    "from tianshou.data.collector import Collector\n",
    "from tianshou.data.buffer.vecbuf import VectorReplayBuffer\n",
    "from tianshou.trainer.base import OnpolicyTrainer\n",
    "from tianshou.data.batch import Batch \n",
    "from tianshou.policy.multiagent.mapolicy import MultiAgentPolicyManager\n",
    "print(tianshou.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://tianshou.org/en/stable/02_notebooks/L7_Experiment.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'agent-0': array([0.7, 0.8, 0.8, 0. , 0.9, 0.5, 0.1, 0.3], dtype=float32), 'agent-1': array([0.8, 0. , 0.7, 0.8, 0.9, 0.5, 0.1, 0.3], dtype=float32)}\n",
      "Batch(\n",
      "    agent-0: array([0.7, 0.8, 0.8, 0. , 0.9, 0.5, 0.1, 0.3], dtype=float32),\n",
      "    agent-1: array([0.8, 0. , 0.7, 0.8, 0.9, 0.5, 0.1, 0.3], dtype=float32),\n",
      ")\n",
      "(8,)\n"
     ]
    }
   ],
   "source": [
    "env = PointCoverageEnv({\"height\": 10, \"width\": 10, \"n_agents\": 2, \"n_targets\": 2, \"max_steps\": 100, \"use_nested_observation\": False})\n",
    "obs, _ = env.reset()\n",
    "print(obs)\n",
    "print(Batch(obs)) \n",
    "print(env.observation_space.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'dict' object has no attribute 'agent_id'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 52\u001b[0m\n\u001b[1;32m     34\u001b[0m train_collector \u001b[38;5;241m=\u001b[39m Collector(\n\u001b[1;32m     35\u001b[0m     policy\u001b[38;5;241m=\u001b[39mmapolicy_manager,\n\u001b[1;32m     36\u001b[0m     env\u001b[38;5;241m=\u001b[39mtrain_envs,\n\u001b[1;32m     37\u001b[0m     buffer\u001b[38;5;241m=\u001b[39mVectorReplayBuffer(\u001b[38;5;241m20000\u001b[39m, \u001b[38;5;28mlen\u001b[39m(train_envs)),\n\u001b[1;32m     38\u001b[0m )\n\u001b[1;32m     39\u001b[0m test_collector \u001b[38;5;241m=\u001b[39m Collector(policy\u001b[38;5;241m=\u001b[39mmapolicy_manager, env\u001b[38;5;241m=\u001b[39mtest_envs)\n\u001b[1;32m     41\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mOnpolicyTrainer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     42\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpolicy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmapolicy_manager\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     43\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_collector\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_collector\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     44\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtest_collector\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtest_collector\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     45\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_epoch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     46\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstep_per_epoch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m50000\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     47\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrepeat_per_collect\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     48\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepisode_per_test\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     49\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m256\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     50\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstep_per_collect\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2000\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     51\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstop_fn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mmean_reward\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmean_reward\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m>\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m195\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m---> 52\u001b[0m \u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     54\u001b[0m result\u001b[38;5;241m.\u001b[39mpprint_asdict()\n",
      "File \u001b[0;32m~/anaconda3/envs/tianEnv/lib/python3.11/site-packages/tianshou/trainer/base.py:557\u001b[0m, in \u001b[0;36mBaseTrainer.run\u001b[0;34m(self, reset_prior_to_run)\u001b[0m\n\u001b[1;32m    551\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Consume iterator.\u001b[39;00m\n\u001b[1;32m    552\u001b[0m \n\u001b[1;32m    553\u001b[0m \u001b[38;5;124;03mSee itertools - recipes. Use functions that consume iterators at C speed\u001b[39;00m\n\u001b[1;32m    554\u001b[0m \u001b[38;5;124;03m(feed the entire iterator into a zero-length deque).\u001b[39;00m\n\u001b[1;32m    555\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    556\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m reset_prior_to_run:\n\u001b[0;32m--> 557\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreset\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    558\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    559\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_run \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/tianEnv/lib/python3.11/site-packages/tianshou/trainer/base.py:261\u001b[0m, in \u001b[0;36mBaseTrainer.reset\u001b[0;34m(self, reset_collectors, reset_buffer)\u001b[0m\n\u001b[1;32m    259\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mepisode_per_test \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    260\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtest_collector, AsyncCollector)  \u001b[38;5;66;03m# Issue 700\u001b[39;00m\n\u001b[0;32m--> 261\u001b[0m test_result \u001b[38;5;241m=\u001b[39m \u001b[43mtest_episode\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    262\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtest_collector\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    263\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtest_fn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    264\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstart_epoch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    265\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mepisode_per_test\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    266\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlogger\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    267\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv_step\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    268\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreward_metric\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    269\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    270\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m test_result\u001b[38;5;241m.\u001b[39mreturns_stat \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m  \u001b[38;5;66;03m# for mypy\u001b[39;00m\n\u001b[1;32m    271\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbest_epoch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstart_epoch\n",
      "File \u001b[0;32m~/anaconda3/envs/tianEnv/lib/python3.11/site-packages/tianshou/trainer/utils.py:30\u001b[0m, in \u001b[0;36mtest_episode\u001b[0;34m(collector, test_fn, epoch, n_episode, logger, global_step, reward_metric)\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m test_fn:\n\u001b[1;32m     29\u001b[0m     test_fn(epoch, global_step)\n\u001b[0;32m---> 30\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mcollector\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollect\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn_episode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_episode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m reward_metric:  \u001b[38;5;66;03m# TODO: move into collector\u001b[39;00m\n\u001b[1;32m     32\u001b[0m     rew \u001b[38;5;241m=\u001b[39m reward_metric(result\u001b[38;5;241m.\u001b[39mreturns)\n",
      "File \u001b[0;32m~/anaconda3/envs/tianEnv/lib/python3.11/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/tianEnv/lib/python3.11/site-packages/tianshou/data/collector.py:304\u001b[0m, in \u001b[0;36mBaseCollector.collect\u001b[0;34m(self, n_step, n_episode, random, render, reset_before_collect, gym_reset_kwargs)\u001b[0m\n\u001b[1;32m    301\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreset(reset_buffer\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, gym_reset_kwargs\u001b[38;5;241m=\u001b[39mgym_reset_kwargs)\n\u001b[1;32m    303\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch_train_mode(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpolicy, \u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[0;32m--> 304\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_collect\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    305\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_step\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_step\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    306\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_episode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_episode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    307\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrandom\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrandom\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    308\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrender\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrender\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    309\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgym_reset_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgym_reset_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    310\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/tianEnv/lib/python3.11/site-packages/tianshou/data/collector.py:507\u001b[0m, in \u001b[0;36mCollector._collect\u001b[0;34m(self, n_step, n_episode, random, render, gym_reset_kwargs)\u001b[0m\n\u001b[1;32m    487\u001b[0m last_hidden_state_RH \u001b[38;5;241m=\u001b[39m _nullable_slice(\n\u001b[1;32m    488\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pre_collect_hidden_state_RH,\n\u001b[1;32m    489\u001b[0m     ready_env_ids_R,\n\u001b[1;32m    490\u001b[0m )\n\u001b[1;32m    492\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m    493\u001b[0m     \u001b[38;5;66;03m# todo check if we need this when using cur_rollout_batch\u001b[39;00m\n\u001b[1;32m    494\u001b[0m     \u001b[38;5;66;03m# if len(cur_rollout_batch) != len(ready_env_ids):\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    500\u001b[0m \n\u001b[1;32m    501\u001b[0m     \u001b[38;5;66;03m# get the next action\u001b[39;00m\n\u001b[1;32m    502\u001b[0m     (\n\u001b[1;32m    503\u001b[0m         act_RA,\n\u001b[1;32m    504\u001b[0m         act_normalized_RA,\n\u001b[1;32m    505\u001b[0m         policy_R,\n\u001b[1;32m    506\u001b[0m         hidden_state_RH,\n\u001b[0;32m--> 507\u001b[0m     ) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_compute_action_policy_hidden\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    508\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrandom\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrandom\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    509\u001b[0m \u001b[43m        \u001b[49m\u001b[43mready_env_ids_R\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mready_env_ids_R\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    510\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlast_obs_RO\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlast_obs_RO\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    511\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlast_info_R\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlast_info_R\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    512\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlast_hidden_state_RH\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlast_hidden_state_RH\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    513\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    515\u001b[0m     obs_next_RO, rew_R, terminated_R, truncated_R, info_R \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv\u001b[38;5;241m.\u001b[39mstep(\n\u001b[1;32m    516\u001b[0m         act_normalized_RA,\n\u001b[1;32m    517\u001b[0m         ready_env_ids_R,\n\u001b[1;32m    518\u001b[0m     )\n\u001b[1;32m    519\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(info_R, \u001b[38;5;28mdict\u001b[39m):  \u001b[38;5;66;03m# type: ignore[unreachable]\u001b[39;00m\n\u001b[1;32m    520\u001b[0m         \u001b[38;5;66;03m# This can happen if the env is an envpool env. Then the info returned by step is a dict\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/tianEnv/lib/python3.11/site-packages/tianshou/data/collector.py:419\u001b[0m, in \u001b[0;36mCollector._compute_action_policy_hidden\u001b[0;34m(self, random, ready_env_ids_R, last_obs_RO, last_info_R, last_hidden_state_RH)\u001b[0m\n\u001b[1;32m    416\u001b[0m info_batch \u001b[38;5;241m=\u001b[39m _HACKY_create_info_batch(last_info_R)\n\u001b[1;32m    417\u001b[0m obs_batch_R \u001b[38;5;241m=\u001b[39m cast(ObsBatchProtocol, Batch(obs\u001b[38;5;241m=\u001b[39mlast_obs_RO, info\u001b[38;5;241m=\u001b[39minfo_batch))\n\u001b[0;32m--> 419\u001b[0m act_batch_RA \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpolicy\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    420\u001b[0m \u001b[43m    \u001b[49m\u001b[43mobs_batch_R\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    421\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlast_hidden_state_RH\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    422\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    424\u001b[0m act_RA \u001b[38;5;241m=\u001b[39m to_numpy(act_batch_RA\u001b[38;5;241m.\u001b[39mact)\n\u001b[1;32m    425\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexploration_noise:\n",
      "File \u001b[0;32m~/anaconda3/envs/tianEnv/lib/python3.11/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/tianEnv/lib/python3.11/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/tianEnv/lib/python3.11/site-packages/tianshou/policy/multiagent/mapolicy.py:222\u001b[0m, in \u001b[0;36mMultiAgentPolicyManager.forward\u001b[0;34m(self, batch, state, **kwargs)\u001b[0m\n\u001b[1;32m    213\u001b[0m results: \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mtuple\u001b[39m[\u001b[38;5;28mbool\u001b[39m, np\u001b[38;5;241m.\u001b[39mndarray, Batch, np\u001b[38;5;241m.\u001b[39mndarray \u001b[38;5;241m|\u001b[39m Batch, Batch]] \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    214\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m agent_id, policy \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpolicies\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m    215\u001b[0m     \u001b[38;5;66;03m# This part of code is difficult to understand.\u001b[39;00m\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;66;03m# Let's follow an example with two agents\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    220\u001b[0m     \u001b[38;5;66;03m# agent_index for agent 2 is [1, 3, 5]\u001b[39;00m\n\u001b[1;32m    221\u001b[0m     \u001b[38;5;66;03m# we separate the transition of each agent according to agent_id\u001b[39;00m\n\u001b[0;32m--> 222\u001b[0m     agent_index \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mnonzero(\u001b[43mbatch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mobs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43magent_id\u001b[49m \u001b[38;5;241m==\u001b[39m agent_id)[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    223\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(agent_index) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    224\u001b[0m         \u001b[38;5;66;03m# (has_data, agent_index, out, act, state)\u001b[39;00m\n\u001b[1;32m    225\u001b[0m         results\u001b[38;5;241m.\u001b[39mappend((\u001b[38;5;28;01mFalse\u001b[39;00m, np\u001b[38;5;241m.\u001b[39marray([\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]), Batch(), Batch(), Batch()))\n",
      "File \u001b[0;32m~/anaconda3/envs/tianEnv/lib/python3.11/site-packages/tianshou/data/batch.py:474\u001b[0m, in \u001b[0;36mBatch.__getattr__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    472\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getattr__\u001b[39m(\u001b[38;5;28mself\u001b[39m, key: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m    473\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Return self.key. The \"Any\" return type is needed for mypy.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 474\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__dict__\u001b[39m, key)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'dict' object has no attribute 'agent_id'"
     ]
    }
   ],
   "source": [
    "train_size, test_size = (20, 10)\n",
    "device = \"cpu\"\n",
    "\n",
    "env = PointCoverageEnv({\"height\": 10, \"width\": 10, \"n_agents\": 2, \"n_targets\": 2, \"max_steps\": 100, \"use_nested_observation\": False})\n",
    "train_envs = DummyVectorEnv([lambda: PointCoverageEnv({\"height\": 10, \"width\": 10, \"n_agents\": 2, \"n_targets\": 2, \"max_steps\": 100, \"use_nested_observation\": False}) for _ in range(train_size)])\n",
    "test_envs = DummyVectorEnv([lambda: PointCoverageEnv({\"height\": 10, \"width\": 10, \"n_agents\": 2, \"n_targets\": 2, \"max_steps\": 100, \"use_nested_observation\": False}) for _ in range(test_size)])\n",
    "\n",
    "assert env.observation_space.shape is not None\n",
    "assert isinstance(env.action_space, Discrete) \n",
    "\n",
    "net = Net(state_shape=env.observation_space.shape, hidden_sizes=[64, 64], device=device)\n",
    "actor = Actor(preprocess_net=net, action_shape=env.action_space.n, device=device).to(device)\n",
    "critic = Critic(preprocess_net=net, device=device).to(device)\n",
    "actor_critic = ActorCritic(actor=actor, critic=critic)\n",
    "\n",
    "# optimizer of the actor and the critic\n",
    "optim = torch.optim.Adam(actor_critic.parameters(), lr=0.0003)\n",
    "\n",
    "dist = torch.distributions.Categorical\n",
    "policy: BasePolicy\n",
    "policy = PPOPolicy(\n",
    "    actor=actor,\n",
    "    critic=critic,\n",
    "    optim=optim,\n",
    "    dist_fn=dist,\n",
    "    action_space=env.action_space,\n",
    "    deterministic_eval=True,\n",
    "    action_scaling=False,\n",
    ")\n",
    "\n",
    "mapolicy_manager = MultiAgentPolicyManager(policies=[policy, policy], env=env)\n",
    "\n",
    "\n",
    "train_collector = Collector(\n",
    "    policy=mapolicy_manager,\n",
    "    env=train_envs,\n",
    "    buffer=VectorReplayBuffer(20000, len(train_envs)),\n",
    ")\n",
    "test_collector = Collector(policy=mapolicy_manager, env=test_envs)\n",
    "\n",
    "result = OnpolicyTrainer(\n",
    "    policy=mapolicy_manager,\n",
    "    train_collector=train_collector,\n",
    "    test_collector=test_collector,\n",
    "    max_epoch=10,\n",
    "    step_per_epoch=50000,\n",
    "    repeat_per_collect=10,\n",
    "    episode_per_test=10,\n",
    "    batch_size=256,\n",
    "    step_per_collect=2000,\n",
    "    stop_fn=lambda mean_reward: mean_reward >= 195,\n",
    ").run()\n",
    "\n",
    "result.pprint_asdict()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rayEnv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
